Starting job 1013356
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = False,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = True,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 361077.718750
Epoch [1/1], Batch [11], Loss: 101113.015625
Epoch [1/1], Batch [21], Loss: 107537.687500
Epoch [1/1], Batch [31], Loss: 101143.914062
Epoch [1/1], Batch [41], Loss: 102581.640625
Epoch [1/1], Batch [51], Loss: 107341.578125
Epoch [1/1], Batch [61], Loss: 105374.937500
Epoch [1/1], Batch [71], Loss: 101957.140625
Epoch [1/1], Batch [81], Loss: 99776.375000
Epoch [1/1], Batch [91], Loss: 98263.078125
Epoch [1/1], Batch [101], Loss: 105217.250000
Epoch [1/1], Batch [111], Loss: 107674.703125
Epoch [1/1], Batch [121], Loss: 100492.585938
Epoch [1/1], Batch [131], Loss: 106235.265625
Epoch [1/1], Batch [141], Loss: 102744.062500
Epoch [1/1], Batch [151], Loss: 102142.695312
Epoch [1/1], Batch [161], Loss: 104674.484375
Epoch [1/1], Batch [171], Loss: 103043.296875
Epoch [1/1], Batch [181], Loss: 103967.585938
Epoch [1/1], Batch [191], Loss: 100839.718750
Epoch [1/1], Batch [201], Loss: 106565.195312
Epoch [1/1], Batch [211], Loss: 101432.734375
Epoch [1/1], Batch [221], Loss: 104736.890625
Epoch [1/1], Batch [231], Loss: 96308.937500
Epoch [1/1], Batch [241], Loss: 91160.531250
Epoch [1/1], Batch [251], Loss: 81076.578125
Epoch [1/1], Batch [261], Loss: 70644.351562
Epoch [1/1], Batch [271], Loss: 66507.429688
Epoch [1/1], Batch [281], Loss: 64140.171875
Epoch [1/1], Batch [291], Loss: 54512.765625
Epoch [1/1], Batch [301], Loss: 55623.847656
Epoch [1/1], Batch [311], Loss: 56911.582031
Epoch [1/1], Batch [321], Loss: 57296.160156
Epoch [1/1], Batch [331], Loss: 54827.480469
Epoch [1/1], Batch [341], Loss: 53406.539062
Epoch [1/1], Batch [351], Loss: 55803.484375
Epoch [1/1], Batch [361], Loss: 54897.554688
Epoch [1/1], Batch [371], Loss: 53484.796875
Epoch [1/1], Batch [381], Loss: 53645.203125
Epoch [1/1], Batch [391], Loss: 55263.710938
Epoch [1/1], Batch [401], Loss: 53419.484375
Epoch [1/1], Batch [411], Loss: 51396.578125
Epoch [1/1], Batch [421], Loss: 53713.339844
Epoch [1/1], Batch [431], Loss: 52772.082031
Epoch [1/1], Batch [441], Loss: 51077.117188
Epoch [1/1], Batch [451], Loss: 50039.765625
Epoch [1/1], Batch [461], Loss: 50164.718750
Epoch [1/1], Batch [471], Loss: 51812.062500
Epoch [1/1], Batch [481], Loss: 50704.410156
Epoch [1/1], Batch [491], Loss: 51393.101562
Epoch [1/1], Batch [501], Loss: 49745.976562
Epoch [1/1], Batch [511], Loss: 50196.628906
Epoch [1/1], Batch [521], Loss: 49325.226562
Epoch [1/1], Batch [531], Loss: 50600.441406
Epoch [1/1], Batch [541], Loss: 52430.066406
Epoch [1/1], Batch [551], Loss: 50282.515625
Epoch [1/1], Batch [561], Loss: 48858.632812
Epoch [1/1], Batch [571], Loss: 47752.382812
Epoch [1/1], Batch [581], Loss: 49643.468750
Epoch [1/1], Batch [591], Loss: 49763.093750
Epoch [1/1], Batch [601], Loss: 48791.937500
Epoch [1/1], Batch [611], Loss: 51657.226562
Epoch [1/1], Batch [621], Loss: 49358.339844
Epoch [1/1], Batch [631], Loss: 50346.289062
Epoch [1/1], Batch [641], Loss: 49354.800781
Epoch [1/1], Batch [651], Loss: 50064.039062
Epoch [1/1], Batch [661], Loss: 48796.289062
Epoch [1/1], Batch [671], Loss: 48844.765625
Epoch [1/1], Batch [681], Loss: 49121.015625
Epoch [1/1], Batch [691], Loss: 47828.457031
Epoch [1/1], Batch [701], Loss: 49557.703125
Epoch [1/1], Batch [711], Loss: 48626.054688
Epoch [1/1], Batch [721], Loss: 48862.175781
Epoch [1/1], Batch [731], Loss: 47455.699219
Epoch [1/1], Batch [741], Loss: 49829.617188
Epoch [1/1], Batch [751], Loss: 46515.960938
Epoch [1/1], Batch [761], Loss: 47155.574219
Epoch [1/1], Batch [771], Loss: 47494.476562
Epoch [1/1], Batch [781], Loss: 49538.585938
Epoch [1/1], Batch [791], Loss: 50019.984375
Epoch [1/1], Batch [801], Loss: 47822.335938
Epoch [1/1], Batch [811], Loss: 48838.570312
Epoch [1/1], Batch [821], Loss: 48390.757812
Epoch [1/1], Batch [831], Loss: 49280.960938
Epoch [1/1], Batch [841], Loss: 48477.726562
Epoch [1/1], Batch [851], Loss: 46559.765625
Epoch [1/1], Batch [861], Loss: 47550.289062
Epoch [1/1], Batch [871], Loss: 47989.261719
Epoch [1/1], Batch [881], Loss: 51794.765625
Epoch [1/1], Batch [891], Loss: 48619.789062
Epoch [1/1], Batch [901], Loss: 46803.968750
Epoch [1/1], Batch [911], Loss: 52287.414062
Epoch [1/1], Batch [921], Loss: 48068.066406
Epoch [1/1], Batch [931], Loss: 46269.828125
Epoch [1/1], Batch [941], Loss: 46678.292969
Epoch [1/1], Batch [951], Loss: 45674.066406
Epoch [1/1], Batch [961], Loss: 48821.160156
Epoch [1/1], Batch [971], Loss: 48850.031250
Epoch [1/1], Batch [981], Loss: 49350.796875
Epoch [1/1], Batch [991], Loss: 49134.031250
Epoch [1/1], Batch [1001], Loss: 45948.105469
Epoch [1/1], Batch [1011], Loss: 46111.781250
Epoch [1/1], Batch [1021], Loss: 48165.285156
Epoch [1/1], Batch [1031], Loss: 47881.156250
Epoch [1/1], Batch [1041], Loss: 44261.140625
Epoch [1/1], Batch [1051], Loss: 47680.886719
Epoch [1/1], Batch [1061], Loss: 46155.226562
Epoch [1/1], Batch [1071], Loss: 46333.804688
Epoch [1/1], Batch [1081], Loss: 46015.089844
Epoch [1/1], Batch [1091], Loss: 43930.539062
Epoch [1/1], Batch [1101], Loss: 46598.878906
Epoch [1/1], Batch [1111], Loss: 48133.390625
Epoch [1/1], Batch [1121], Loss: 44137.390625
Epoch [1/1], Batch [1131], Loss: 46651.308594
Epoch [1/1], Batch [1141], Loss: 45829.734375
Epoch [1/1], Batch [1151], Loss: 46580.351562
Epoch [1/1], Batch [1161], Loss: 46900.406250
Epoch [1/1], Batch [1171], Loss: 48087.207031
Epoch [1/1], Batch [1181], Loss: 44451.757812
Epoch [1/1], Batch [1191], Loss: 48374.507812
Epoch [1/1], Batch [1201], Loss: 48915.687500
Epoch [1/1], Batch [1211], Loss: 48144.519531
Epoch [1/1], Batch [1221], Loss: 46112.398438
Epoch [1/1], Batch [1231], Loss: 48327.960938
Epoch [1/1], Batch [1241], Loss: 46514.015625
Epoch [1/1], Batch [1251], Loss: 45146.765625
Epoch [1/1], Batch [1261], Loss: 47360.898438
Epoch [1/1], Batch [1271], Loss: 48653.542969
Epoch [1/1], Batch [1281], Loss: 47492.742188
Epoch [1/1], Batch [1291], Loss: 48824.742188
Epoch [1/1], Batch [1301], Loss: 44764.859375
Epoch [1/1], Batch [1311], Loss: 46423.753906
Epoch [1/1], Batch [1321], Loss: 46347.375000
Epoch [1/1], Batch [1331], Loss: 47452.359375
Epoch [1/1], Batch [1341], Loss: 44063.632812
Epoch [1/1], Batch [1351], Loss: 45890.687500
Epoch [1/1], Batch [1361], Loss: 47390.902344
Epoch [1/1], Batch [1371], Loss: 44917.101562
Epoch [1/1], Batch [1381], Loss: 47757.226562
Epoch [1/1], Batch [1391], Loss: 47148.546875
Epoch [1/1], Batch [1401], Loss: 48349.132812
Epoch [1/1], Batch [1411], Loss: 45133.648438
Epoch [1/1], Batch [1421], Loss: 44157.800781
Epoch [1/1], Batch [1431], Loss: 46520.070312
Epoch [1/1], Batch [1441], Loss: 46964.054688
Epoch [1/1], Batch [1451], Loss: 47377.136719
Epoch [1/1], Batch [1461], Loss: 46641.675781
Epoch [1/1], Batch [1471], Loss: 45632.460938
Epoch [1/1], Batch [1481], Loss: 47932.457031
Epoch [1/1], Batch [1491], Loss: 47982.203125
Epoch [1/1], Batch [1501], Loss: 43003.964844
Epoch [1/1], Batch [1511], Loss: 47636.554688
Epoch [1/1], Batch [1521], Loss: 45799.507812
Epoch [1/1], Batch [1531], Loss: 43683.375000
Epoch [1/1], Batch [1541], Loss: 45060.789062
Epoch [1/1], Batch [1551], Loss: 45937.015625
Epoch [1/1], Batch [1561], Loss: 45684.371094
Epoch [1/1], Batch [1571], Loss: 46318.710938
Epoch [1/1], Batch [1581], Loss: 45937.804688
Epoch [1/1], Batch [1591], Loss: 45737.390625
Epoch [1/1], Batch [1601], Loss: 43463.054688
Epoch [1/1], Batch [1611], Loss: 46153.917969
Epoch [1/1], Batch [1621], Loss: 44355.359375
Epoch [1/1], Batch [1631], Loss: 47572.781250
Epoch [1/1], Batch [1641], Loss: 45542.421875
Epoch [1/1], Batch [1651], Loss: 45074.718750
Epoch [1/1], Batch [1661], Loss: 44232.203125
Epoch [1/1], Batch [1671], Loss: 46705.148438
Epoch [1/1], Batch [1681], Loss: 45755.781250
Epoch [1/1], Batch [1691], Loss: 45054.917969
Epoch [1/1], Batch [1701], Loss: 47407.234375
Epoch [1/1], Batch [1711], Loss: 47552.187500
Epoch [1/1], Batch [1721], Loss: 47050.660156
Epoch [1/1], Batch [1731], Loss: 44782.171875
Epoch [1/1], Batch [1741], Loss: 46606.406250
Epoch [1/1], Batch [1751], Loss: 45434.968750
Epoch [1/1], Batch [1761], Loss: 46624.171875
Epoch [1/1], Batch [1771], Loss: 44376.601562
Epoch [1/1], Batch [1781], Loss: 44627.000000
Epoch [1/1], Batch [1791], Loss: 43240.027344
Epoch [1/1], Batch [1801], Loss: 45434.281250
Epoch [1/1], Batch [1811], Loss: 45368.929688
Epoch [1/1], Batch [1821], Loss: 45036.187500
Epoch [1/1], Batch [1831], Loss: 44735.976562
Epoch [1/1], Batch [1841], Loss: 45336.195312
Epoch [1/1], Batch [1851], Loss: 45222.625000
Epoch [1/1], Batch [1861], Loss: 46307.363281
Epoch [1/1], Batch [1871], Loss: 43578.968750
Epoch [1/1], Batch [1881], Loss: 45679.359375
Epoch [1/1], Batch [1891], Loss: 47318.484375
Epoch [1/1], Batch [1901], Loss: 45638.152344
Epoch [1/1], Batch [1911], Loss: 43244.578125
Epoch [1/1], Batch [1921], Loss: 48239.300781
Epoch [1/1], Batch [1931], Loss: 45864.046875
Epoch [1/1], Batch [1941], Loss: 44372.976562
Epoch [1/1], Batch [1951], Loss: 47092.269531
Epoch [1/1], Batch [1961], Loss: 45570.269531
Epoch [1/1], Batch [1971], Loss: 46056.011719
Epoch [1/1], Batch [1981], Loss: 45239.406250
Epoch [1/1], Batch [1991], Loss: 45097.320312
Epoch [1/1], Batch [2001], Loss: 44846.601562
Epoch [1/1], Batch [2011], Loss: 43729.062500
Epoch [1/1], Batch [2021], Loss: 43792.523438
Epoch [1/1], Batch [2031], Loss: 45457.320312
Epoch [1/1], Batch [2041], Loss: 44472.070312
Epoch [1/1], Batch [2051], Loss: 43390.406250
Epoch [1/1], Batch [2061], Loss: 45654.375000
Epoch [1/1], Batch [2071], Loss: 45001.765625
Epoch [1/1], Batch [2081], Loss: 46060.578125
Epoch [1/1], Batch [2091], Loss: 43754.437500
Epoch [1/1], Batch [2101], Loss: 45688.746094
Epoch [1/1], Batch [2111], Loss: 44029.265625
Epoch [1/1], Batch [2121], Loss: 45810.500000
Epoch [1/1], Batch [2131], Loss: 45244.238281
Epoch [1/1], Batch [2141], Loss: 42994.570312
Epoch [1/1], Batch [2151], Loss: 44283.679688
Epoch [1/1], Batch [2161], Loss: 44302.464844
Epoch [1/1], Batch [2171], Loss: 45391.898438
Epoch [1/1], Batch [2181], Loss: 45195.421875
Epoch [1/1], Batch [2191], Loss: 45789.210938
Epoch [1/1], Batch [2201], Loss: 46809.687500
Epoch [1/1], Batch [2211], Loss: 44915.867188
Epoch [1/1], Batch [2221], Loss: 45744.265625
Epoch [1/1], Batch [2231], Loss: 43214.695312
Epoch [1/1], Batch [2241], Loss: 45581.695312
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 54527.5409
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 44767.6466
Elapsed time: 410.36 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 44669.8753
Elapsed time: 427.72 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 194657.031250
Epoch [1/1], Batch [11], Loss: 146718.343750
Epoch [1/1], Batch [21], Loss: 125508.234375
Epoch [1/1], Batch [31], Loss: 121768.500000
Epoch [1/1], Batch [41], Loss: 116734.226562
Epoch [1/1], Batch [51], Loss: 105911.562500
Epoch [1/1], Batch [61], Loss: 101792.218750
Epoch [1/1], Batch [71], Loss: 98437.328125
Epoch [1/1], Batch [81], Loss: 97819.445312
Epoch [1/1], Batch [91], Loss: 93926.757812
Epoch [1/1], Batch [101], Loss: 94929.578125
Epoch [1/1], Batch [111], Loss: 88787.203125
Epoch [1/1], Batch [121], Loss: 93977.265625
Epoch [1/1], Batch [131], Loss: 90756.000000
Epoch [1/1], Batch [141], Loss: 85253.289062
Epoch [1/1], Batch [151], Loss: 87303.578125
Epoch [1/1], Batch [161], Loss: 86533.195312
Epoch [1/1], Batch [171], Loss: 84568.843750
Epoch [1/1], Batch [181], Loss: 88805.781250
Epoch [1/1], Batch [191], Loss: 85401.265625
Epoch [1/1], Batch [201], Loss: 83984.273438
Epoch [1/1], Batch [211], Loss: 84638.484375
Epoch [1/1], Batch [221], Loss: 84040.632812
Epoch [1/1], Batch [231], Loss: 84455.945312
Epoch [1/1], Batch [241], Loss: 82600.125000
Epoch [1/1], Batch [251], Loss: 82360.515625
Epoch [1/1], Batch [261], Loss: 83069.562500
Epoch [1/1], Batch [271], Loss: 79325.359375
Epoch [1/1], Batch [281], Loss: 81989.039062
Epoch [1/1], Batch [291], Loss: 81187.054688
Epoch [1/1], Batch [301], Loss: 81778.640625
Epoch [1/1], Batch [311], Loss: 83114.156250
Epoch [1/1], Batch [321], Loss: 79469.421875
Epoch [1/1], Batch [331], Loss: 79369.078125
Epoch [1/1], Batch [341], Loss: 77037.984375
Epoch [1/1], Batch [351], Loss: 83292.804688
Epoch [1/1], Batch [361], Loss: 83641.484375
Epoch [1/1], Batch [371], Loss: 79678.687500
Epoch [1/1], Batch [381], Loss: 80057.468750
Epoch [1/1], Batch [391], Loss: 76673.109375
Epoch [1/1], Batch [401], Loss: 75847.984375
Epoch [1/1], Batch [411], Loss: 78297.609375
Epoch [1/1], Batch [421], Loss: 78078.039062
Epoch [1/1], Batch [431], Loss: 80305.156250
Epoch [1/1], Batch [441], Loss: 78919.515625
Epoch [1/1], Batch [451], Loss: 78073.421875
Epoch [1/1], Batch [461], Loss: 81136.976562
Epoch [1/1], Batch [471], Loss: 78771.140625
Epoch [1/1], Batch [481], Loss: 77647.046875
Epoch [1/1], Batch [491], Loss: 78153.226562
Epoch [1/1], Batch [501], Loss: 81295.984375
Epoch [1/1], Batch [511], Loss: 77585.453125
Epoch [1/1], Batch [521], Loss: 76208.476562
Epoch [1/1], Batch [531], Loss: 79193.835938
Epoch [1/1], Batch [541], Loss: 76792.507812
Epoch [1/1], Batch [551], Loss: 74010.687500
Epoch [1/1], Batch [561], Loss: 76124.937500
Epoch [1/1], Batch [571], Loss: 76827.921875
Epoch [1/1], Batch [581], Loss: 80236.187500
Epoch [1/1], Batch [591], Loss: 78648.390625
Epoch [1/1], Batch [601], Loss: 78448.593750
Epoch [1/1], Batch [611], Loss: 78058.953125
Epoch [1/1], Batch [621], Loss: 79160.023438
Epoch [1/1], Batch [631], Loss: 74875.250000
Epoch [1/1], Batch [641], Loss: 78422.960938
Epoch [1/1], Batch [651], Loss: 75336.109375
Epoch [1/1], Batch [661], Loss: 76404.812500
Epoch [1/1], Batch [671], Loss: 76613.320312
Epoch [1/1], Batch [681], Loss: 77583.078125
Epoch [1/1], Batch [691], Loss: 82815.421875
Epoch [1/1], Batch [701], Loss: 77033.710938
Epoch [1/1], Batch [711], Loss: 76517.343750
Epoch [1/1], Batch [721], Loss: 73851.492188
Epoch [1/1], Batch [731], Loss: 76449.234375
Epoch [1/1], Batch [741], Loss: 74711.812500
Epoch [1/1], Batch [751], Loss: 80052.828125
Epoch [1/1], Batch [761], Loss: 77163.328125
Epoch [1/1], Batch [771], Loss: 76237.210938
Epoch [1/1], Batch [781], Loss: 76319.640625
Epoch [1/1], Batch [791], Loss: 77223.070312
Epoch [1/1], Batch [801], Loss: 75527.132812
Epoch [1/1], Batch [811], Loss: 71773.406250
Epoch [1/1], Batch [821], Loss: 77592.242188
Epoch [1/1], Batch [831], Loss: 75925.039062
Epoch [1/1], Batch [841], Loss: 75463.906250
Epoch [1/1], Batch [851], Loss: 72353.148438
Epoch [1/1], Batch [861], Loss: 75194.781250
Epoch [1/1], Batch [871], Loss: 75308.851562
Epoch [1/1], Batch [881], Loss: 74711.593750
Epoch [1/1], Batch [891], Loss: 74383.062500
Epoch [1/1], Batch [901], Loss: 76308.757812
Epoch [1/1], Batch [911], Loss: 76883.484375
Epoch [1/1], Batch [921], Loss: 74298.156250
Epoch [1/1], Batch [931], Loss: 77978.031250
Epoch [1/1], Batch [941], Loss: 75932.484375
Epoch [1/1], Batch [951], Loss: 77716.406250
Epoch [1/1], Batch [961], Loss: 71238.703125
Epoch [1/1], Batch [971], Loss: 75257.867188
Epoch [1/1], Batch [981], Loss: 75153.171875
Epoch [1/1], Batch [991], Loss: 72648.546875
Epoch [1/1], Batch [1001], Loss: 71418.070312
Epoch [1/1], Batch [1011], Loss: 74452.632812
Epoch [1/1], Batch [1021], Loss: 74426.382812
Epoch [1/1], Batch [1031], Loss: 70846.640625
Epoch [1/1], Batch [1041], Loss: 77365.203125
Epoch [1/1], Batch [1051], Loss: 72454.390625
Epoch [1/1], Batch [1061], Loss: 76312.171875
Epoch [1/1], Batch [1071], Loss: 74377.843750
Epoch [1/1], Batch [1081], Loss: 73927.984375
Epoch [1/1], Batch [1091], Loss: 77349.601562
Epoch [1/1], Batch [1101], Loss: 75386.976562
Epoch [1/1], Batch [1111], Loss: 73916.710938
Epoch [1/1], Batch [1121], Loss: 69949.984375
Epoch [1/1], Batch [1131], Loss: 77035.007812
Epoch [1/1], Batch [1141], Loss: 76656.328125
Epoch [1/1], Batch [1151], Loss: 74516.078125
Epoch [1/1], Batch [1161], Loss: 72631.992188
Epoch [1/1], Batch [1171], Loss: 70840.875000
Epoch [1/1], Batch [1181], Loss: 71683.078125
Epoch [1/1], Batch [1191], Loss: 73168.937500
Epoch [1/1], Batch [1201], Loss: 71746.070312
Epoch [1/1], Batch [1211], Loss: 74248.468750
Epoch [1/1], Batch [1221], Loss: 72946.015625
Epoch [1/1], Batch [1231], Loss: 74165.156250
Epoch [1/1], Batch [1241], Loss: 75499.328125
Epoch [1/1], Batch [1251], Loss: 73408.976562
Epoch [1/1], Batch [1261], Loss: 74302.546875
Epoch [1/1], Batch [1271], Loss: 74526.375000
Epoch [1/1], Batch [1281], Loss: 77929.093750
Epoch [1/1], Batch [1291], Loss: 74450.562500
Epoch [1/1], Batch [1301], Loss: 70007.554688
Epoch [1/1], Batch [1311], Loss: 76090.125000
Epoch [1/1], Batch [1321], Loss: 71785.398438
Epoch [1/1], Batch [1331], Loss: 76086.460938
Epoch [1/1], Batch [1341], Loss: 74279.671875
Epoch [1/1], Batch [1351], Loss: 72034.578125
Epoch [1/1], Batch [1361], Loss: 72116.320312
Epoch [1/1], Batch [1371], Loss: 76779.828125
Epoch [1/1], Batch [1381], Loss: 71233.507812
Epoch [1/1], Batch [1391], Loss: 72711.453125
Epoch [1/1], Batch [1401], Loss: 71383.015625
Epoch [1/1], Batch [1411], Loss: 72006.804688
Epoch [1/1], Batch [1421], Loss: 72611.289062
Epoch [1/1], Batch [1431], Loss: 73745.070312
Epoch [1/1], Batch [1441], Loss: 73042.843750
Epoch [1/1], Batch [1451], Loss: 69432.570312
Epoch [1/1], Batch [1461], Loss: 70310.156250
Epoch [1/1], Batch [1471], Loss: 70419.062500
Epoch [1/1], Batch [1481], Loss: 73308.406250
Epoch [1/1], Batch [1491], Loss: 75454.664062
Epoch [1/1], Batch [1501], Loss: 72890.250000
Epoch [1/1], Batch [1511], Loss: 69843.984375
Epoch [1/1], Batch [1521], Loss: 74499.921875
Epoch [1/1], Batch [1531], Loss: 72803.117188
Epoch [1/1], Batch [1541], Loss: 72948.906250
Epoch [1/1], Batch [1551], Loss: 72140.703125
Epoch [1/1], Batch [1561], Loss: 71350.109375
Epoch [1/1], Batch [1571], Loss: 73116.265625
Epoch [1/1], Batch [1581], Loss: 74269.171875
Epoch [1/1], Batch [1591], Loss: 73290.390625
Epoch [1/1], Batch [1601], Loss: 73026.585938
Epoch [1/1], Batch [1611], Loss: 72980.421875
Epoch [1/1], Batch [1621], Loss: 71605.140625
Epoch [1/1], Batch [1631], Loss: 71652.812500
Epoch [1/1], Batch [1641], Loss: 70842.421875
Epoch [1/1], Batch [1651], Loss: 72487.656250
Epoch [1/1], Batch [1661], Loss: 69276.765625
Epoch [1/1], Batch [1671], Loss: 74865.617188
Epoch [1/1], Batch [1681], Loss: 77310.140625
Epoch [1/1], Batch [1691], Loss: 74221.203125
Epoch [1/1], Batch [1701], Loss: 73626.312500
Epoch [1/1], Batch [1711], Loss: 69531.117188
Epoch [1/1], Batch [1721], Loss: 71403.437500
Epoch [1/1], Batch [1731], Loss: 76133.296875
Epoch [1/1], Batch [1741], Loss: 74577.000000
Epoch [1/1], Batch [1751], Loss: 70741.398438
Epoch [1/1], Batch [1761], Loss: 75231.968750
Epoch [1/1], Batch [1771], Loss: 71130.609375
Epoch [1/1], Batch [1781], Loss: 68309.421875
Epoch [1/1], Batch [1791], Loss: 72983.328125
Epoch [1/1], Batch [1801], Loss: 70499.335938
Epoch [1/1], Batch [1811], Loss: 71684.359375
Epoch [1/1], Batch [1821], Loss: 69972.664062
Epoch [1/1], Batch [1831], Loss: 73199.843750
Epoch [1/1], Batch [1841], Loss: 74293.609375
Epoch [1/1], Batch [1851], Loss: 71539.515625
Epoch [1/1], Batch [1861], Loss: 74566.625000
Epoch [1/1], Batch [1871], Loss: 70067.437500
Epoch [1/1], Batch [1881], Loss: 75400.187500
Epoch [1/1], Batch [1891], Loss: 72301.406250
Epoch [1/1], Batch [1901], Loss: 72942.750000
Epoch [1/1], Batch [1911], Loss: 73301.875000
Epoch [1/1], Batch [1921], Loss: 73777.632812
Epoch [1/1], Batch [1931], Loss: 70539.773438
Epoch [1/1], Batch [1941], Loss: 71909.531250
Epoch [1/1], Batch [1951], Loss: 71120.578125
Epoch [1/1], Batch [1961], Loss: 72814.703125
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 78183.8271
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 71385.2276
Elapsed time: 939.40 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 71872.3268
Elapsed time: 959.80 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 132479.812500
Epoch [1/1], Batch [11], Loss: 145742.109375
Epoch [1/1], Batch [21], Loss: 143111.500000
Epoch [1/1], Batch [31], Loss: 137271.531250
Epoch [1/1], Batch [41], Loss: 119995.992188
Epoch [1/1], Batch [51], Loss: 119002.015625
Epoch [1/1], Batch [61], Loss: 119361.984375
Epoch [1/1], Batch [71], Loss: 121258.562500
Epoch [1/1], Batch [81], Loss: 109793.531250
Epoch [1/1], Batch [91], Loss: 114174.375000
Epoch [1/1], Batch [101], Loss: 111645.171875
Epoch [1/1], Batch [111], Loss: 112678.375000
Epoch [1/1], Batch [121], Loss: 111159.421875
Epoch [1/1], Batch [131], Loss: 123138.640625
Epoch [1/1], Batch [141], Loss: 120132.062500
Epoch [1/1], Batch [151], Loss: 109656.984375
Epoch [1/1], Batch [161], Loss: 113652.640625
Epoch [1/1], Batch [171], Loss: 113975.828125
Epoch [1/1], Batch [181], Loss: 110494.171875
Epoch [1/1], Batch [191], Loss: 112111.562500
Epoch [1/1], Batch [201], Loss: 116119.531250
Epoch [1/1], Batch [211], Loss: 113000.171875
Epoch [1/1], Batch [221], Loss: 114631.453125
Epoch [1/1], Batch [231], Loss: 110485.171875
Epoch [1/1], Batch [241], Loss: 111346.492188
Epoch [1/1], Batch [251], Loss: 106815.835938
Epoch [1/1], Batch [261], Loss: 112456.359375
Epoch [1/1], Batch [271], Loss: 105356.953125
Epoch [1/1], Batch [281], Loss: 108533.609375
Epoch [1/1], Batch [291], Loss: 108767.476562
Epoch [1/1], Batch [301], Loss: 110651.156250
Epoch [1/1], Batch [311], Loss: 108057.171875
Epoch [1/1], Batch [321], Loss: 108274.070312
Epoch [1/1], Batch [331], Loss: 109903.726562
Epoch [1/1], Batch [341], Loss: 108760.328125
Epoch [1/1], Batch [351], Loss: 105243.187500
Epoch [1/1], Batch [361], Loss: 109897.140625
Epoch [1/1], Batch [371], Loss: 102608.171875
Epoch [1/1], Batch [381], Loss: 108050.265625
Epoch [1/1], Batch [391], Loss: 107210.640625
Epoch [1/1], Batch [401], Loss: 109940.140625
Epoch [1/1], Batch [411], Loss: 107645.312500
Epoch [1/1], Batch [421], Loss: 104896.562500
Epoch [1/1], Batch [431], Loss: 109270.414062
Epoch [1/1], Batch [441], Loss: 100954.968750
Epoch [1/1], Batch [451], Loss: 106578.234375
Epoch [1/1], Batch [461], Loss: 105857.179688
Epoch [1/1], Batch [471], Loss: 111397.656250
Epoch [1/1], Batch [481], Loss: 105558.101562
Epoch [1/1], Batch [491], Loss: 108133.882812
Epoch [1/1], Batch [501], Loss: 104883.031250
Epoch [1/1], Batch [511], Loss: 110064.460938
Epoch [1/1], Batch [521], Loss: 108036.750000
Epoch [1/1], Batch [531], Loss: 106991.109375
Epoch [1/1], Batch [541], Loss: 104422.515625
Epoch [1/1], Batch [551], Loss: 105352.718750
Epoch [1/1], Batch [561], Loss: 103218.546875
Epoch [1/1], Batch [571], Loss: 103176.351562
Epoch [1/1], Batch [581], Loss: 104973.328125
Epoch [1/1], Batch [591], Loss: 111371.312500
Epoch [1/1], Batch [601], Loss: 107791.875000
Epoch [1/1], Batch [611], Loss: 105776.171875
Epoch [1/1], Batch [621], Loss: 105607.656250
Epoch [1/1], Batch [631], Loss: 105173.750000
Epoch [1/1], Batch [641], Loss: 106134.117188
Epoch [1/1], Batch [651], Loss: 108719.656250
Epoch [1/1], Batch [661], Loss: 110230.593750
Epoch [1/1], Batch [671], Loss: 108251.492188
Epoch [1/1], Batch [681], Loss: 105028.695312
Epoch [1/1], Batch [691], Loss: 104190.734375
Epoch [1/1], Batch [701], Loss: 107058.523438
Epoch [1/1], Batch [711], Loss: 105505.890625
Epoch [1/1], Batch [721], Loss: 104222.781250
Epoch [1/1], Batch [731], Loss: 106082.984375
Epoch [1/1], Batch [741], Loss: 98387.906250
Epoch [1/1], Batch [751], Loss: 105438.187500
Epoch [1/1], Batch [761], Loss: 103829.890625
Epoch [1/1], Batch [771], Loss: 106326.507812
Epoch [1/1], Batch [781], Loss: 102827.164062
Epoch [1/1], Batch [791], Loss: 104758.007812
Epoch [1/1], Batch [801], Loss: 101400.953125
Epoch [1/1], Batch [811], Loss: 103050.390625
Epoch [1/1], Batch [821], Loss: 104768.984375
Epoch [1/1], Batch [831], Loss: 106345.523438
Epoch [1/1], Batch [841], Loss: 108517.109375
Epoch [1/1], Batch [851], Loss: 98427.296875
Epoch [1/1], Batch [861], Loss: 103036.718750
Epoch [1/1], Batch [871], Loss: 106998.117188
Epoch [1/1], Batch [881], Loss: 103544.812500
Epoch [1/1], Batch [891], Loss: 105619.757812
Epoch [1/1], Batch [901], Loss: 101215.875000
Epoch [1/1], Batch [911], Loss: 103948.859375
Epoch [1/1], Batch [921], Loss: 102006.406250
Epoch [1/1], Batch [931], Loss: 105875.687500
Epoch [1/1], Batch [941], Loss: 103424.367188
Epoch [1/1], Batch [951], Loss: 100432.453125
Epoch [1/1], Batch [961], Loss: 104936.609375
Epoch [1/1], Batch [971], Loss: 107709.976562
Epoch [1/1], Batch [981], Loss: 107177.015625
Epoch [1/1], Batch [991], Loss: 109599.718750
Epoch [1/1], Batch [1001], Loss: 97901.703125
Epoch [1/1], Batch [1011], Loss: 102711.140625
Epoch [1/1], Batch [1021], Loss: 109984.390625
Epoch [1/1], Batch [1031], Loss: 101257.468750
Epoch [1/1], Batch [1041], Loss: 104260.304688
Epoch [1/1], Batch [1051], Loss: 109470.562500
Epoch [1/1], Batch [1061], Loss: 103090.085938
Epoch [1/1], Batch [1071], Loss: 103149.968750
Epoch [1/1], Batch [1081], Loss: 108903.593750
Epoch [1/1], Batch [1091], Loss: 107387.273438
Epoch [1/1], Batch [1101], Loss: 101233.562500
Epoch [1/1], Batch [1111], Loss: 110038.109375
Epoch [1/1], Batch [1121], Loss: 104226.726562
Epoch [1/1], Batch [1131], Loss: 104205.906250
Epoch [1/1], Batch [1141], Loss: 106400.890625
Epoch [1/1], Batch [1151], Loss: 101643.078125
Epoch [1/1], Batch [1161], Loss: 100773.875000
Epoch [1/1], Batch [1171], Loss: 101231.890625
Epoch [1/1], Batch [1181], Loss: 107643.781250
Epoch [1/1], Batch [1191], Loss: 100353.671875
Epoch [1/1], Batch [1201], Loss: 101255.640625
Epoch [1/1], Batch [1211], Loss: 103533.765625
Epoch [1/1], Batch [1221], Loss: 97945.531250
Epoch [1/1], Batch [1231], Loss: 103310.437500
Epoch [1/1], Batch [1241], Loss: 101977.984375
Epoch [1/1], Batch [1251], Loss: 107892.289062
Epoch [1/1], Batch [1261], Loss: 103790.773438
Epoch [1/1], Batch [1271], Loss: 99443.843750
Epoch [1/1], Batch [1281], Loss: 106146.960938
Epoch [1/1], Batch [1291], Loss: 101069.945312
Epoch [1/1], Batch [1301], Loss: 100856.320312
Epoch [1/1], Batch [1311], Loss: 107135.062500
Epoch [1/1], Batch [1321], Loss: 106096.445312
Epoch [1/1], Batch [1331], Loss: 97671.218750
Epoch [1/1], Batch [1341], Loss: 104595.992188
Epoch [1/1], Batch [1351], Loss: 101403.093750
Epoch [1/1], Batch [1361], Loss: 103741.140625
Epoch [1/1], Batch [1371], Loss: 98923.757812
Epoch [1/1], Batch [1381], Loss: 97759.796875
Epoch [1/1], Batch [1391], Loss: 95613.703125
Epoch [1/1], Batch [1401], Loss: 100677.992188
Epoch [1/1], Batch [1411], Loss: 102131.093750
Epoch [1/1], Batch [1421], Loss: 102037.953125
Epoch [1/1], Batch [1431], Loss: 97860.453125
Epoch [1/1], Batch [1441], Loss: 101615.875000
Epoch [1/1], Batch [1451], Loss: 99235.234375
Epoch [1/1], Batch [1461], Loss: 102851.453125
Epoch [1/1], Batch [1471], Loss: 103994.187500
Epoch [1/1], Batch [1481], Loss: 105801.492188
Epoch [1/1], Batch [1491], Loss: 102824.148438
Epoch [1/1], Batch [1501], Loss: 109492.984375
Epoch [1/1], Batch [1511], Loss: 103224.671875
Epoch [1/1], Batch [1521], Loss: 105454.414062
Epoch [1/1], Batch [1531], Loss: 101871.835938
Epoch [1/1], Batch [1541], Loss: 105567.164062
Epoch [1/1], Batch [1551], Loss: 97732.898438
Epoch [1/1], Batch [1561], Loss: 98748.507812
Epoch [1/1], Batch [1571], Loss: 101894.484375
Epoch [1/1], Batch [1581], Loss: 101277.617188
Epoch [1/1], Batch [1591], Loss: 107913.046875
Epoch [1/1], Batch [1601], Loss: 105180.453125
Epoch [1/1], Batch [1611], Loss: 96341.843750
Epoch [1/1], Batch [1621], Loss: 101926.921875
Epoch [1/1], Batch [1631], Loss: 100586.398438
Epoch [1/1], Batch [1641], Loss: 103656.523438
Epoch [1/1], Batch [1651], Loss: 97012.468750
Epoch [1/1], Batch [1661], Loss: 101317.367188
Epoch [1/1], Batch [1671], Loss: 105675.750000
Epoch [1/1], Batch [1681], Loss: 98600.109375
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 106517.4968
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 100220.6096
Elapsed time: 1528.00 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 100767.5994
Elapsed time: 1550.09 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 146963.015625
Epoch [1/1], Batch [11], Loss: 176983.937500
Epoch [1/1], Batch [21], Loss: 168759.156250
Epoch [1/1], Batch [31], Loss: 159908.468750
Epoch [1/1], Batch [41], Loss: 159606.046875
Epoch [1/1], Batch [51], Loss: 150978.718750
Epoch [1/1], Batch [61], Loss: 155716.015625
Epoch [1/1], Batch [71], Loss: 146139.890625
Epoch [1/1], Batch [81], Loss: 151281.281250
Epoch [1/1], Batch [91], Loss: 149811.765625
Epoch [1/1], Batch [101], Loss: 148165.265625
Epoch [1/1], Batch [111], Loss: 141395.562500
Epoch [1/1], Batch [121], Loss: 148297.781250
Epoch [1/1], Batch [131], Loss: 143389.171875
Epoch [1/1], Batch [141], Loss: 144921.218750
Epoch [1/1], Batch [151], Loss: 142578.593750
Epoch [1/1], Batch [161], Loss: 146270.281250
Epoch [1/1], Batch [171], Loss: 142455.609375
Epoch [1/1], Batch [181], Loss: 137518.578125
Epoch [1/1], Batch [191], Loss: 144478.593750
Epoch [1/1], Batch [201], Loss: 143539.437500
Epoch [1/1], Batch [211], Loss: 140669.593750
Epoch [1/1], Batch [221], Loss: 139686.921875
Epoch [1/1], Batch [231], Loss: 138045.187500
Epoch [1/1], Batch [241], Loss: 140735.562500
Epoch [1/1], Batch [251], Loss: 144932.265625
Epoch [1/1], Batch [261], Loss: 138023.750000
Epoch [1/1], Batch [271], Loss: 141092.125000
Epoch [1/1], Batch [281], Loss: 143736.437500
Epoch [1/1], Batch [291], Loss: 140839.828125
Epoch [1/1], Batch [301], Loss: 137164.453125
Epoch [1/1], Batch [311], Loss: 140780.343750
Epoch [1/1], Batch [321], Loss: 136494.125000
Epoch [1/1], Batch [331], Loss: 141635.375000
Epoch [1/1], Batch [341], Loss: 141122.375000
Epoch [1/1], Batch [351], Loss: 139752.453125
Epoch [1/1], Batch [361], Loss: 142066.984375
Epoch [1/1], Batch [371], Loss: 133820.531250
Epoch [1/1], Batch [381], Loss: 137107.921875
Epoch [1/1], Batch [391], Loss: 133434.031250
Epoch [1/1], Batch [401], Loss: 139636.890625
Epoch [1/1], Batch [411], Loss: 137064.218750
Epoch [1/1], Batch [421], Loss: 138433.906250
Epoch [1/1], Batch [431], Loss: 150529.531250
Epoch [1/1], Batch [441], Loss: 137896.156250
Epoch [1/1], Batch [451], Loss: 128655.742188
Epoch [1/1], Batch [461], Loss: 149108.500000
Epoch [1/1], Batch [471], Loss: 137738.328125
Epoch [1/1], Batch [481], Loss: 138642.203125
Epoch [1/1], Batch [491], Loss: 137944.625000
Epoch [1/1], Batch [501], Loss: 138552.312500
Epoch [1/1], Batch [511], Loss: 133283.234375
Epoch [1/1], Batch [521], Loss: 135801.765625
Epoch [1/1], Batch [531], Loss: 141111.250000
Epoch [1/1], Batch [541], Loss: 140020.343750
Epoch [1/1], Batch [551], Loss: 145660.968750
Epoch [1/1], Batch [561], Loss: 131606.609375
Epoch [1/1], Batch [571], Loss: 130580.156250
Epoch [1/1], Batch [581], Loss: 139789.500000
Epoch [1/1], Batch [591], Loss: 137010.937500
Epoch [1/1], Batch [601], Loss: 142804.031250
Epoch [1/1], Batch [611], Loss: 136181.234375
Epoch [1/1], Batch [621], Loss: 137993.687500
Epoch [1/1], Batch [631], Loss: 132476.359375
Epoch [1/1], Batch [641], Loss: 134926.906250
Epoch [1/1], Batch [651], Loss: 136667.406250
Epoch [1/1], Batch [661], Loss: 133285.078125
Epoch [1/1], Batch [671], Loss: 135598.609375
Epoch [1/1], Batch [681], Loss: 134431.046875
Epoch [1/1], Batch [691], Loss: 135556.062500
Epoch [1/1], Batch [701], Loss: 142140.640625
Epoch [1/1], Batch [711], Loss: 136097.828125
Epoch [1/1], Batch [721], Loss: 133980.859375
Epoch [1/1], Batch [731], Loss: 135195.937500
Epoch [1/1], Batch [741], Loss: 133044.843750
Epoch [1/1], Batch [751], Loss: 132866.343750
Epoch [1/1], Batch [761], Loss: 132260.437500
Epoch [1/1], Batch [771], Loss: 134285.156250
Epoch [1/1], Batch [781], Loss: 149864.156250
Epoch [1/1], Batch [791], Loss: 134598.437500
Epoch [1/1], Batch [801], Loss: 136687.937500
Epoch [1/1], Batch [811], Loss: 138333.875000
Epoch [1/1], Batch [821], Loss: 138968.484375
Epoch [1/1], Batch [831], Loss: 134914.000000
Epoch [1/1], Batch [841], Loss: 132031.656250
Epoch [1/1], Batch [851], Loss: 139060.718750
Epoch [1/1], Batch [861], Loss: 139172.609375
Epoch [1/1], Batch [871], Loss: 134758.640625
Epoch [1/1], Batch [881], Loss: 131768.218750
Epoch [1/1], Batch [891], Loss: 137118.593750
Epoch [1/1], Batch [901], Loss: 138620.343750
Epoch [1/1], Batch [911], Loss: 132234.625000
Epoch [1/1], Batch [921], Loss: 130835.750000
Epoch [1/1], Batch [931], Loss: 134157.687500
Epoch [1/1], Batch [941], Loss: 139200.375000
Epoch [1/1], Batch [951], Loss: 133819.562500
Epoch [1/1], Batch [961], Loss: 135254.531250
Epoch [1/1], Batch [971], Loss: 137284.343750
Epoch [1/1], Batch [981], Loss: 134744.531250
Epoch [1/1], Batch [991], Loss: 130514.046875
Epoch [1/1], Batch [1001], Loss: 130408.328125
Epoch [1/1], Batch [1011], Loss: 129081.953125
Epoch [1/1], Batch [1021], Loss: 133009.375000
Epoch [1/1], Batch [1031], Loss: 137382.593750
Epoch [1/1], Batch [1041], Loss: 133056.562500
Epoch [1/1], Batch [1051], Loss: 129695.476562
Epoch [1/1], Batch [1061], Loss: 136608.687500
Epoch [1/1], Batch [1071], Loss: 133248.125000
Epoch [1/1], Batch [1081], Loss: 137539.781250
Epoch [1/1], Batch [1091], Loss: 134412.031250
Epoch [1/1], Batch [1101], Loss: 131545.421875
Epoch [1/1], Batch [1111], Loss: 140103.937500
Epoch [1/1], Batch [1121], Loss: 138837.562500
Epoch [1/1], Batch [1131], Loss: 135817.000000
Epoch [1/1], Batch [1141], Loss: 130796.679688
Epoch [1/1], Batch [1151], Loss: 131418.281250
Epoch [1/1], Batch [1161], Loss: 138398.609375
Epoch [1/1], Batch [1171], Loss: 134552.750000
Epoch [1/1], Batch [1181], Loss: 132748.750000
Epoch [1/1], Batch [1191], Loss: 131308.593750
Epoch [1/1], Batch [1201], Loss: 133277.875000
Epoch [1/1], Batch [1211], Loss: 129253.523438
Epoch [1/1], Batch [1221], Loss: 128568.109375
Epoch [1/1], Batch [1231], Loss: 141651.593750
Epoch [1/1], Batch [1241], Loss: 133138.937500
Epoch [1/1], Batch [1251], Loss: 126711.640625
Epoch [1/1], Batch [1261], Loss: 137329.406250
Epoch [1/1], Batch [1271], Loss: 139368.718750
Epoch [1/1], Batch [1281], Loss: 130993.617188
Epoch [1/1], Batch [1291], Loss: 136369.968750
Epoch [1/1], Batch [1301], Loss: 133358.843750
Epoch [1/1], Batch [1311], Loss: 137010.734375
Epoch [1/1], Batch [1321], Loss: 137178.843750
Epoch [1/1], Batch [1331], Loss: 137123.921875
Epoch [1/1], Batch [1341], Loss: 130274.960938
Epoch [1/1], Batch [1351], Loss: 136906.187500
Epoch [1/1], Batch [1361], Loss: 129554.484375
Epoch [1/1], Batch [1371], Loss: 133966.031250
Epoch [1/1], Batch [1381], Loss: 139231.906250
Epoch [1/1], Batch [1391], Loss: 130920.109375
Epoch [1/1], Batch [1401], Loss: 133007.531250
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 138550.8143
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 132542.4035
Elapsed time: 2133.33 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 133585.2133
Elapsed time: 2155.39 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 170497.343750
Epoch [1/1], Batch [11], Loss: 202891.062500
Epoch [1/1], Batch [21], Loss: 179908.625000
Epoch [1/1], Batch [31], Loss: 176922.828125
Epoch [1/1], Batch [41], Loss: 178366.546875
Epoch [1/1], Batch [51], Loss: 186157.093750
Epoch [1/1], Batch [61], Loss: 179557.109375
Epoch [1/1], Batch [71], Loss: 172714.781250
Epoch [1/1], Batch [81], Loss: 178964.500000
Epoch [1/1], Batch [91], Loss: 179182.484375
Epoch [1/1], Batch [101], Loss: 179568.968750
Epoch [1/1], Batch [111], Loss: 179513.859375
Epoch [1/1], Batch [121], Loss: 183862.750000
Epoch [1/1], Batch [131], Loss: 184439.687500
Epoch [1/1], Batch [141], Loss: 173439.765625
Epoch [1/1], Batch [151], Loss: 179734.468750
Epoch [1/1], Batch [161], Loss: 170476.687500
Epoch [1/1], Batch [171], Loss: 174618.578125
Epoch [1/1], Batch [181], Loss: 181441.093750
Epoch [1/1], Batch [191], Loss: 170692.890625
Epoch [1/1], Batch [201], Loss: 173712.531250
Epoch [1/1], Batch [211], Loss: 180139.812500
Epoch [1/1], Batch [221], Loss: 162290.000000
Epoch [1/1], Batch [231], Loss: 169347.281250
Epoch [1/1], Batch [241], Loss: 179987.593750
Epoch [1/1], Batch [251], Loss: 170245.781250
Epoch [1/1], Batch [261], Loss: 173497.312500
Epoch [1/1], Batch [271], Loss: 172518.875000
Epoch [1/1], Batch [281], Loss: 176257.671875
Epoch [1/1], Batch [291], Loss: 178636.984375
Epoch [1/1], Batch [301], Loss: 173763.265625
Epoch [1/1], Batch [311], Loss: 173183.828125
Epoch [1/1], Batch [321], Loss: 177594.375000
Epoch [1/1], Batch [331], Loss: 176430.406250
Epoch [1/1], Batch [341], Loss: 169375.375000
Epoch [1/1], Batch [351], Loss: 165420.734375
Epoch [1/1], Batch [361], Loss: 177583.437500
Epoch [1/1], Batch [371], Loss: 175345.640625
Epoch [1/1], Batch [381], Loss: 169535.406250
Epoch [1/1], Batch [391], Loss: 173925.625000
Epoch [1/1], Batch [401], Loss: 169588.718750
Epoch [1/1], Batch [411], Loss: 165248.093750
Epoch [1/1], Batch [421], Loss: 177007.312500
Epoch [1/1], Batch [431], Loss: 172441.734375
Epoch [1/1], Batch [441], Loss: 169152.718750
Epoch [1/1], Batch [451], Loss: 169477.562500
Epoch [1/1], Batch [461], Loss: 162268.437500
Epoch [1/1], Batch [471], Loss: 181873.203125
Epoch [1/1], Batch [481], Loss: 176593.687500
Epoch [1/1], Batch [491], Loss: 167678.625000
Epoch [1/1], Batch [501], Loss: 165894.671875
Epoch [1/1], Batch [511], Loss: 194247.171875
Epoch [1/1], Batch [521], Loss: 180424.234375
Epoch [1/1], Batch [531], Loss: 173542.031250
Epoch [1/1], Batch [541], Loss: 178664.031250
Epoch [1/1], Batch [551], Loss: 178349.359375
Epoch [1/1], Batch [561], Loss: 179108.875000
Epoch [1/1], Batch [571], Loss: 179249.531250
Epoch [1/1], Batch [581], Loss: 175864.000000
Epoch [1/1], Batch [591], Loss: 171783.093750
Epoch [1/1], Batch [601], Loss: 181253.406250
Epoch [1/1], Batch [611], Loss: 175701.453125
Epoch [1/1], Batch [621], Loss: 174551.937500
Epoch [1/1], Batch [631], Loss: 176351.437500
Epoch [1/1], Batch [641], Loss: 166785.468750
Epoch [1/1], Batch [651], Loss: 171564.437500
Epoch [1/1], Batch [661], Loss: 169050.062500
Epoch [1/1], Batch [671], Loss: 169635.468750
Epoch [1/1], Batch [681], Loss: 175393.546875
Epoch [1/1], Batch [691], Loss: 173383.187500
Epoch [1/1], Batch [701], Loss: 170361.437500
Epoch [1/1], Batch [711], Loss: 172524.562500
Epoch [1/1], Batch [721], Loss: 167843.906250
Epoch [1/1], Batch [731], Loss: 168530.593750
Epoch [1/1], Batch [741], Loss: 168993.968750
Epoch [1/1], Batch [751], Loss: 169953.843750
Epoch [1/1], Batch [761], Loss: 175367.281250
Epoch [1/1], Batch [771], Loss: 175788.609375
Epoch [1/1], Batch [781], Loss: 167246.171875
Epoch [1/1], Batch [791], Loss: 171785.656250
Epoch [1/1], Batch [801], Loss: 166560.281250
Epoch [1/1], Batch [811], Loss: 173351.656250
Epoch [1/1], Batch [821], Loss: 165694.687500
Epoch [1/1], Batch [831], Loss: 178415.031250
Epoch [1/1], Batch [841], Loss: 164597.218750
Epoch [1/1], Batch [851], Loss: 167292.953125
Epoch [1/1], Batch [861], Loss: 181071.125000
Epoch [1/1], Batch [871], Loss: 168981.781250
Epoch [1/1], Batch [881], Loss: 164908.656250
Epoch [1/1], Batch [891], Loss: 167524.468750
Epoch [1/1], Batch [901], Loss: 175350.500000
Epoch [1/1], Batch [911], Loss: 169249.875000
Epoch [1/1], Batch [921], Loss: 168565.437500
Epoch [1/1], Batch [931], Loss: 165215.703125
Epoch [1/1], Batch [941], Loss: 164624.343750
Epoch [1/1], Batch [951], Loss: 167114.937500
Epoch [1/1], Batch [961], Loss: 163526.718750
Epoch [1/1], Batch [971], Loss: 170537.625000
Epoch [1/1], Batch [981], Loss: 174874.156250
Epoch [1/1], Batch [991], Loss: 167879.406250
Epoch [1/1], Batch [1001], Loss: 167016.125000
Epoch [1/1], Batch [1011], Loss: 180633.343750
Epoch [1/1], Batch [1021], Loss: 166287.062500
Epoch [1/1], Batch [1031], Loss: 173040.609375
Epoch [1/1], Batch [1041], Loss: 164373.750000
Epoch [1/1], Batch [1051], Loss: 171337.093750
Epoch [1/1], Batch [1061], Loss: 162924.109375
Epoch [1/1], Batch [1071], Loss: 173633.390625
Epoch [1/1], Batch [1081], Loss: 166982.031250
Epoch [1/1], Batch [1091], Loss: 166360.890625
Epoch [1/1], Batch [1101], Loss: 170346.937500
Epoch [1/1], Batch [1111], Loss: 170983.906250
Epoch [1/1], Batch [1121], Loss: 168986.234375
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 173050.8003
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 168487.6219
Elapsed time: 2712.99 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 168900.1375
Elapsed time: 2733.66 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 222783.968750
Epoch [1/1], Batch [11], Loss: 233721.750000
Epoch [1/1], Batch [21], Loss: 214521.953125
Epoch [1/1], Batch [31], Loss: 212296.343750
Epoch [1/1], Batch [41], Loss: 220255.906250
Epoch [1/1], Batch [51], Loss: 208446.250000
Epoch [1/1], Batch [61], Loss: 216806.437500
Epoch [1/1], Batch [71], Loss: 205738.062500
Epoch [1/1], Batch [81], Loss: 197452.000000
Epoch [1/1], Batch [91], Loss: 217100.593750
Epoch [1/1], Batch [101], Loss: 216110.718750
Epoch [1/1], Batch [111], Loss: 207033.000000
Epoch [1/1], Batch [121], Loss: 217909.656250
Epoch [1/1], Batch [131], Loss: 212639.109375
Epoch [1/1], Batch [141], Loss: 210376.968750
Epoch [1/1], Batch [151], Loss: 208698.984375
Epoch [1/1], Batch [161], Loss: 198244.531250
Epoch [1/1], Batch [171], Loss: 212735.734375
Epoch [1/1], Batch [181], Loss: 203530.718750
Epoch [1/1], Batch [191], Loss: 200758.343750
Epoch [1/1], Batch [201], Loss: 199271.468750
Epoch [1/1], Batch [211], Loss: 204262.437500
Epoch [1/1], Batch [221], Loss: 204556.859375
Epoch [1/1], Batch [231], Loss: 202550.828125
Epoch [1/1], Batch [241], Loss: 203196.406250
Epoch [1/1], Batch [251], Loss: 203209.781250
Epoch [1/1], Batch [261], Loss: 203204.703125
Epoch [1/1], Batch [271], Loss: 202687.968750
Epoch [1/1], Batch [281], Loss: 212105.656250
Epoch [1/1], Batch [291], Loss: 210953.156250
Epoch [1/1], Batch [301], Loss: 211553.437500
Epoch [1/1], Batch [311], Loss: 201227.906250
Epoch [1/1], Batch [321], Loss: 205783.296875
Epoch [1/1], Batch [331], Loss: 207721.000000
Epoch [1/1], Batch [341], Loss: 226906.562500
Epoch [1/1], Batch [351], Loss: 204720.125000
Epoch [1/1], Batch [361], Loss: 201308.656250
Epoch [1/1], Batch [371], Loss: 210299.812500
Epoch [1/1], Batch [381], Loss: 211295.046875
Epoch [1/1], Batch [391], Loss: 203511.343750
Epoch [1/1], Batch [401], Loss: 229587.656250
Epoch [1/1], Batch [411], Loss: 212610.343750
Epoch [1/1], Batch [421], Loss: 201948.218750
Epoch [1/1], Batch [431], Loss: 210417.062500
Epoch [1/1], Batch [441], Loss: 207200.718750
Epoch [1/1], Batch [451], Loss: 197435.437500
Epoch [1/1], Batch [461], Loss: 209920.421875
Epoch [1/1], Batch [471], Loss: 209347.375000
Epoch [1/1], Batch [481], Loss: 207326.156250
Epoch [1/1], Batch [491], Loss: 213672.218750
Epoch [1/1], Batch [501], Loss: 214983.687500
Epoch [1/1], Batch [511], Loss: 214472.718750
Epoch [1/1], Batch [521], Loss: 212843.125000
Epoch [1/1], Batch [531], Loss: 207584.406250
Epoch [1/1], Batch [541], Loss: 213179.875000
Epoch [1/1], Batch [551], Loss: 208790.531250
Epoch [1/1], Batch [561], Loss: 203835.343750
Epoch [1/1], Batch [571], Loss: 212521.171875
Epoch [1/1], Batch [581], Loss: 211943.515625
Epoch [1/1], Batch [591], Loss: 201790.828125
Epoch [1/1], Batch [601], Loss: 200604.312500
Epoch [1/1], Batch [611], Loss: 199357.484375
Epoch [1/1], Batch [621], Loss: 212752.859375
Epoch [1/1], Batch [631], Loss: 212629.843750
Epoch [1/1], Batch [641], Loss: 207498.968750
Epoch [1/1], Batch [651], Loss: 213327.937500
Epoch [1/1], Batch [661], Loss: 221413.671875
Epoch [1/1], Batch [671], Loss: 204750.031250
Epoch [1/1], Batch [681], Loss: 208518.140625
Epoch [1/1], Batch [691], Loss: 208972.578125
Epoch [1/1], Batch [701], Loss: 205348.125000
Epoch [1/1], Batch [711], Loss: 206034.531250
Epoch [1/1], Batch [721], Loss: 209498.015625
Epoch [1/1], Batch [731], Loss: 212050.515625
Epoch [1/1], Batch [741], Loss: 208651.343750
Epoch [1/1], Batch [751], Loss: 199950.890625
Epoch [1/1], Batch [761], Loss: 206416.984375
Epoch [1/1], Batch [771], Loss: 194389.578125
Epoch [1/1], Batch [781], Loss: 192054.187500
Epoch [1/1], Batch [791], Loss: 200260.968750
Epoch [1/1], Batch [801], Loss: 204350.859375
Epoch [1/1], Batch [811], Loss: 201225.375000
Epoch [1/1], Batch [821], Loss: 200222.687500
Epoch [1/1], Batch [831], Loss: 201173.906250
Epoch [1/1], Batch [841], Loss: 206198.187500
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 207505.4547
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 199861.8130
Elapsed time: 3212.04 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 200379.0326
Elapsed time: 3229.76 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 237156.625000
Epoch [1/1], Batch [11], Loss: 241620.093750
Epoch [1/1], Batch [21], Loss: 258953.406250
Epoch [1/1], Batch [31], Loss: 236854.031250
Epoch [1/1], Batch [41], Loss: 275743.937500
Epoch [1/1], Batch [51], Loss: 260406.984375
Epoch [1/1], Batch [61], Loss: 262966.718750
Epoch [1/1], Batch [71], Loss: 260268.312500
Epoch [1/1], Batch [81], Loss: 241620.406250
Epoch [1/1], Batch [91], Loss: 260766.218750
Epoch [1/1], Batch [101], Loss: 255755.921875
Epoch [1/1], Batch [111], Loss: 245785.109375
Epoch [1/1], Batch [121], Loss: 260355.390625
Epoch [1/1], Batch [131], Loss: 255146.687500
Epoch [1/1], Batch [141], Loss: 241724.203125
Epoch [1/1], Batch [151], Loss: 289195.000000
Epoch [1/1], Batch [161], Loss: 274878.187500
Epoch [1/1], Batch [171], Loss: 276985.937500
Epoch [1/1], Batch [181], Loss: 278225.656250
Epoch [1/1], Batch [191], Loss: 255761.718750
Epoch [1/1], Batch [201], Loss: 265230.281250
Epoch [1/1], Batch [211], Loss: 256226.718750
Epoch [1/1], Batch [221], Loss: 262318.812500
Epoch [1/1], Batch [231], Loss: 248486.750000
Epoch [1/1], Batch [241], Loss: 260580.281250
Epoch [1/1], Batch [251], Loss: 266249.437500
Epoch [1/1], Batch [261], Loss: 243925.453125
Epoch [1/1], Batch [271], Loss: 247384.734375
Epoch [1/1], Batch [281], Loss: 255681.078125
Epoch [1/1], Batch [291], Loss: 249927.687500
Epoch [1/1], Batch [301], Loss: 247307.015625
Epoch [1/1], Batch [311], Loss: 251247.343750
Epoch [1/1], Batch [321], Loss: 259411.031250
Epoch [1/1], Batch [331], Loss: 243911.921875
Epoch [1/1], Batch [341], Loss: 253677.406250
Epoch [1/1], Batch [351], Loss: 249711.375000
Epoch [1/1], Batch [361], Loss: 254807.937500
Epoch [1/1], Batch [371], Loss: 248897.484375
Epoch [1/1], Batch [381], Loss: 245868.750000
Epoch [1/1], Batch [391], Loss: 247975.531250
Epoch [1/1], Batch [401], Loss: 240530.812500
Epoch [1/1], Batch [411], Loss: 243115.015625
Epoch [1/1], Batch [421], Loss: 241378.968750
Epoch [1/1], Batch [431], Loss: 242535.546875
Epoch [1/1], Batch [441], Loss: 246497.640625
Epoch [1/1], Batch [451], Loss: 261485.828125
Epoch [1/1], Batch [461], Loss: 249871.421875
Epoch [1/1], Batch [471], Loss: 243019.250000
Epoch [1/1], Batch [481], Loss: 257580.593750
Epoch [1/1], Batch [491], Loss: 240621.656250
Epoch [1/1], Batch [501], Loss: 246558.031250
Epoch [1/1], Batch [511], Loss: 241988.328125
Epoch [1/1], Batch [521], Loss: 241777.500000
Epoch [1/1], Batch [531], Loss: 244749.875000
Epoch [1/1], Batch [541], Loss: 242242.625000
Epoch [1/1], Batch [551], Loss: 246878.796875
Epoch [1/1], Batch [561], Loss: 255547.125000
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 252006.4032
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 240475.9439
Elapsed time: 3591.13 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 242439.2450
Elapsed time: 3604.47 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 281949.312500
Epoch [1/1], Batch [11], Loss: 294861.687500
Epoch [1/1], Batch [21], Loss: 288349.312500
Epoch [1/1], Batch [31], Loss: 276801.750000
Epoch [1/1], Batch [41], Loss: 285552.687500
Epoch [1/1], Batch [51], Loss: 304950.375000
Epoch [1/1], Batch [61], Loss: 282711.250000
Epoch [1/1], Batch [71], Loss: 276531.531250
Epoch [1/1], Batch [81], Loss: 278175.593750
Epoch [1/1], Batch [91], Loss: 295552.312500
Epoch [1/1], Batch [101], Loss: 298642.312500
Epoch [1/1], Batch [111], Loss: 298892.312500
Epoch [1/1], Batch [121], Loss: 290354.875000
Epoch [1/1], Batch [131], Loss: 277350.187500
Epoch [1/1], Batch [141], Loss: 290739.468750
Epoch [1/1], Batch [151], Loss: 291078.968750
Epoch [1/1], Batch [161], Loss: 309325.718750
Epoch [1/1], Batch [171], Loss: 290978.500000
Epoch [1/1], Batch [181], Loss: 304476.531250
Epoch [1/1], Batch [191], Loss: 288880.468750
Epoch [1/1], Batch [201], Loss: 275065.062500
Epoch [1/1], Batch [211], Loss: 279975.468750
Epoch [1/1], Batch [221], Loss: 301556.750000
Epoch [1/1], Batch [231], Loss: 281445.875000
Epoch [1/1], Batch [241], Loss: 280372.500000
Epoch [1/1], Batch [251], Loss: 276000.937500
Epoch [1/1], Batch [261], Loss: 289015.250000
Epoch [1/1], Batch [271], Loss: 286700.312500
Epoch [1/1], Batch [281], Loss: 285860.718750
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 286963.6909
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 319484.1956
Elapsed time: 3808.81 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 320743.8882
Elapsed time: 3816.22 seconds

Training complete!
Totoal elapsed time: 3816.22 seconds
CUDA is available!
