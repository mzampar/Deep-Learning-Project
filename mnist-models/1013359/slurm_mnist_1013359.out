Starting job 1013359
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = True,
    transpose = True,
    use_lstm_output = False,
    scheduler = True,
    initial_lr = 0.1,
    gamma = 0.95.

CUDA is available!
Using learning rate scheduler with initial_lr = 0.1 and gamma = 0.95.
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 0.679213
Epoch [1/1], Batch [11], Loss: 0.181022
Epoch [1/1], Batch [21], Loss: 0.162046
Epoch [1/1], Batch [31], Loss: 0.169160
Epoch [1/1], Batch [41], Loss: 0.164050
Epoch [1/1], Batch [51], Loss: 0.161671
Epoch [1/1], Batch [61], Loss: 0.165864
Epoch [1/1], Batch [71], Loss: 0.162124
Epoch [1/1], Batch [81], Loss: 0.161162
Epoch [1/1], Batch [91], Loss: 0.161778
Epoch [1/1], Batch [101], Loss: 0.160819
Epoch [1/1], Batch [111], Loss: 0.165345
Epoch [1/1], Batch [121], Loss: 0.165112
Epoch [1/1], Batch [131], Loss: 0.168307
Epoch [1/1], Batch [141], Loss: 0.161487
Epoch [1/1], Batch [151], Loss: 0.143916
Epoch [1/1], Batch [161], Loss: 0.118499
Epoch [1/1], Batch [171], Loss: 0.111935
Epoch [1/1], Batch [181], Loss: 0.108531
Epoch [1/1], Batch [191], Loss: 0.104229
Epoch [1/1], Batch [201], Loss: 0.104477
Epoch [1/1], Batch [211], Loss: 0.103292
Epoch [1/1], Batch [221], Loss: 0.098138
Epoch [1/1], Batch [231], Loss: 0.097837
Epoch [1/1], Batch [241], Loss: 0.100378
Epoch [1/1], Batch [251], Loss: 0.097337
Epoch [1/1], Batch [261], Loss: 0.095538
Epoch [1/1], Batch [271], Loss: 0.095203
Epoch [1/1], Batch [281], Loss: 0.098184
Epoch [1/1], Batch [291], Loss: 0.095914
Epoch [1/1], Batch [301], Loss: 0.099171
Epoch [1/1], Batch [311], Loss: 0.094121
Epoch [1/1], Batch [321], Loss: 0.092189
Epoch [1/1], Batch [331], Loss: 0.094387
Epoch [1/1], Batch [341], Loss: 0.091824
Epoch [1/1], Batch [351], Loss: 0.094356
Epoch [1/1], Batch [361], Loss: 0.095960
Epoch [1/1], Batch [371], Loss: 0.092768
Epoch [1/1], Batch [381], Loss: 0.094551
Epoch [1/1], Batch [391], Loss: 0.090634
Epoch [1/1], Batch [401], Loss: 0.089130
Epoch [1/1], Batch [411], Loss: 0.091425
Epoch [1/1], Batch [421], Loss: 0.092016
Epoch [1/1], Batch [431], Loss: 0.096508
Epoch [1/1], Batch [441], Loss: 0.093584
Epoch [1/1], Batch [451], Loss: 0.085773
Epoch [1/1], Batch [461], Loss: 0.088556
Epoch [1/1], Batch [471], Loss: 0.093593
Epoch [1/1], Batch [481], Loss: 0.095365
Epoch [1/1], Batch [491], Loss: 0.090993
Epoch [1/1], Batch [501], Loss: 0.091802
Epoch [1/1], Batch [511], Loss: 0.097376
Epoch [1/1], Batch [521], Loss: 0.091109
Epoch [1/1], Batch [531], Loss: 0.089566
Epoch [1/1], Batch [541], Loss: 0.088735
Epoch [1/1], Batch [551], Loss: 0.089517
Epoch [1/1], Batch [561], Loss: 0.089999
Epoch [1/1], Batch [571], Loss: 0.092538
Epoch [1/1], Batch [581], Loss: 0.087938
Epoch [1/1], Batch [591], Loss: 0.091422
Epoch [1/1], Batch [601], Loss: 0.094126
Epoch [1/1], Batch [611], Loss: 0.088104
Epoch [1/1], Batch [621], Loss: 0.091235
Epoch [1/1], Batch [631], Loss: 0.089938
Epoch [1/1], Batch [641], Loss: 0.091384
Epoch [1/1], Batch [651], Loss: 0.097567
Epoch [1/1], Batch [661], Loss: 0.089951
Epoch [1/1], Batch [671], Loss: 0.089926
Epoch [1/1], Batch [681], Loss: 0.092175
Epoch [1/1], Batch [691], Loss: 0.094260
Epoch [1/1], Batch [701], Loss: 0.087736
Epoch [1/1], Batch [711], Loss: 0.089543
Epoch [1/1], Batch [721], Loss: 0.091708
Epoch [1/1], Batch [731], Loss: 0.088869
Epoch [1/1], Batch [741], Loss: 0.091870
Epoch [1/1], Batch [751], Loss: 0.090702
Epoch [1/1], Batch [761], Loss: 0.087245
Epoch [1/1], Batch [771], Loss: 0.088675
Epoch [1/1], Batch [781], Loss: 0.089551
Epoch [1/1], Batch [791], Loss: 0.091004
Epoch [1/1], Batch [801], Loss: 0.090928
Epoch [1/1], Batch [811], Loss: 0.086799
Epoch [1/1], Batch [821], Loss: 0.089178
Epoch [1/1], Batch [831], Loss: 0.089559
Epoch [1/1], Batch [841], Loss: 0.092328
Epoch [1/1], Batch [851], Loss: 0.090712
Epoch [1/1], Batch [861], Loss: 0.089137
Epoch [1/1], Batch [871], Loss: 0.088672
Epoch [1/1], Batch [881], Loss: 0.090971
Epoch [1/1], Batch [891], Loss: 0.091359
Epoch [1/1], Batch [901], Loss: 0.090284
Epoch [1/1], Batch [911], Loss: 0.087887
Epoch [1/1], Batch [921], Loss: 0.083775
Epoch [1/1], Batch [931], Loss: 0.087109
Epoch [1/1], Batch [941], Loss: 0.088677
Epoch [1/1], Batch [951], Loss: 0.087723
Epoch [1/1], Batch [961], Loss: 0.092625
Epoch [1/1], Batch [971], Loss: 0.091777
Epoch [1/1], Batch [981], Loss: 0.085697
Epoch [1/1], Batch [991], Loss: 0.089994
Epoch [1/1], Batch [1001], Loss: 0.091128
Epoch [1/1], Batch [1011], Loss: 0.088747
Epoch [1/1], Batch [1021], Loss: 0.087390
Epoch [1/1], Batch [1031], Loss: 0.091323
Epoch [1/1], Batch [1041], Loss: 0.086290
Epoch [1/1], Batch [1051], Loss: 0.087712
Epoch [1/1], Batch [1061], Loss: 0.088800
Epoch [1/1], Batch [1071], Loss: 0.089113
Epoch [1/1], Batch [1081], Loss: 0.093023
Epoch [1/1], Batch [1091], Loss: 0.089945
Epoch [1/1], Batch [1101], Loss: 0.091632
Epoch [1/1], Batch [1111], Loss: 0.089710
Epoch [1/1], Batch [1121], Loss: 0.088952
Epoch [1/1], Batch [1131], Loss: 0.089211
Epoch [1/1], Batch [1141], Loss: 0.086894
Epoch [1/1], Batch [1151], Loss: 0.087625
Epoch [1/1], Batch [1161], Loss: 0.086888
Epoch [1/1], Batch [1171], Loss: 0.089899
Epoch [1/1], Batch [1181], Loss: 0.087282
Epoch [1/1], Batch [1191], Loss: 0.092233
Epoch [1/1], Batch [1201], Loss: 0.088204
Epoch [1/1], Batch [1211], Loss: 0.091679
Epoch [1/1], Batch [1221], Loss: 0.089512
Epoch [1/1], Batch [1231], Loss: 0.088558
Epoch [1/1], Batch [1241], Loss: 0.085613
Epoch [1/1], Batch [1251], Loss: 0.090320
Epoch [1/1], Batch [1261], Loss: 0.090344
Epoch [1/1], Batch [1271], Loss: 0.086816
Epoch [1/1], Batch [1281], Loss: 0.093985
Epoch [1/1], Batch [1291], Loss: 0.085795
Epoch [1/1], Batch [1301], Loss: 0.088776
Epoch [1/1], Batch [1311], Loss: 0.086809
Epoch [1/1], Batch [1321], Loss: 0.087715
Epoch [1/1], Batch [1331], Loss: 0.085240
Epoch [1/1], Batch [1341], Loss: 0.088349
Epoch [1/1], Batch [1351], Loss: 0.093036
Epoch [1/1], Batch [1361], Loss: 0.087900
Epoch [1/1], Batch [1371], Loss: 0.088990
Epoch [1/1], Batch [1381], Loss: 0.086782
Epoch [1/1], Batch [1391], Loss: 0.085960
Epoch [1/1], Batch [1401], Loss: 0.089268
Epoch [1/1], Batch [1411], Loss: 0.089408
Epoch [1/1], Batch [1421], Loss: 0.088013
Epoch [1/1], Batch [1431], Loss: 0.086806
Epoch [1/1], Batch [1441], Loss: 0.086353
Epoch [1/1], Batch [1451], Loss: 0.090662
Epoch [1/1], Batch [1461], Loss: 0.091215
Epoch [1/1], Batch [1471], Loss: 0.089408
Epoch [1/1], Batch [1481], Loss: 0.091538
Epoch [1/1], Batch [1491], Loss: 0.086884
Epoch [1/1], Batch [1501], Loss: 0.086501
Epoch [1/1], Batch [1511], Loss: 0.090507
Epoch [1/1], Batch [1521], Loss: 0.087369
Epoch [1/1], Batch [1531], Loss: 0.087483
Epoch [1/1], Batch [1541], Loss: 0.086141
Epoch [1/1], Batch [1551], Loss: 0.088702
Epoch [1/1], Batch [1561], Loss: 0.087881
Epoch [1/1], Batch [1571], Loss: 0.090338
Epoch [1/1], Batch [1581], Loss: 0.087970
Epoch [1/1], Batch [1591], Loss: 0.088319
Epoch [1/1], Batch [1601], Loss: 0.087963
Epoch [1/1], Batch [1611], Loss: 0.088623
Epoch [1/1], Batch [1621], Loss: 0.089028
Epoch [1/1], Batch [1631], Loss: 0.089570
Epoch [1/1], Batch [1641], Loss: 0.089272
Epoch [1/1], Batch [1651], Loss: 0.088647
Epoch [1/1], Batch [1661], Loss: 0.088581
Epoch [1/1], Batch [1671], Loss: 0.091595
Epoch [1/1], Batch [1681], Loss: 0.086537
Epoch [1/1], Batch [1691], Loss: 0.086082
Epoch [1/1], Batch [1701], Loss: 0.087136
Epoch [1/1], Batch [1711], Loss: 0.088643
Epoch [1/1], Batch [1721], Loss: 0.085706
Epoch [1/1], Batch [1731], Loss: 0.088777
Epoch [1/1], Batch [1741], Loss: 0.090625
Epoch [1/1], Batch [1751], Loss: 0.086262
Epoch [1/1], Batch [1761], Loss: 0.090470
Epoch [1/1], Batch [1771], Loss: 0.088943
Epoch [1/1], Batch [1781], Loss: 0.087698
Epoch [1/1], Batch [1791], Loss: 0.084648
Epoch [1/1], Batch [1801], Loss: 0.085578
Epoch [1/1], Batch [1811], Loss: 0.089461
Epoch [1/1], Batch [1821], Loss: 0.089198
Epoch [1/1], Batch [1831], Loss: 0.087913
Epoch [1/1], Batch [1841], Loss: 0.087215
Epoch [1/1], Batch [1851], Loss: 0.086507
Epoch [1/1], Batch [1861], Loss: 0.082867
Epoch [1/1], Batch [1871], Loss: 0.090981
Epoch [1/1], Batch [1881], Loss: 0.089218
Epoch [1/1], Batch [1891], Loss: 0.087877
Epoch [1/1], Batch [1901], Loss: 0.084806
Epoch [1/1], Batch [1911], Loss: 0.087039
Epoch [1/1], Batch [1921], Loss: 0.085300
Epoch [1/1], Batch [1931], Loss: 0.084304
Epoch [1/1], Batch [1941], Loss: 0.088163
Epoch [1/1], Batch [1951], Loss: 0.092401
Epoch [1/1], Batch [1961], Loss: 0.089120
Epoch [1/1], Batch [1971], Loss: 0.085289
Epoch [1/1], Batch [1981], Loss: 0.087749
Epoch [1/1], Batch [1991], Loss: 0.081129
Epoch [1/1], Batch [2001], Loss: 0.085941
Epoch [1/1], Batch [2011], Loss: 0.088714
Epoch [1/1], Batch [2021], Loss: 0.090002
Epoch [1/1], Batch [2031], Loss: 0.086685
Epoch [1/1], Batch [2041], Loss: 0.088447
Epoch [1/1], Batch [2051], Loss: 0.088582
Epoch [1/1], Batch [2061], Loss: 0.086310
Epoch [1/1], Batch [2071], Loss: 0.087968
Epoch [1/1], Batch [2081], Loss: 0.090190
Epoch [1/1], Batch [2091], Loss: 0.087857
Epoch [1/1], Batch [2101], Loss: 0.088640
Epoch [1/1], Batch [2111], Loss: 0.087103
Epoch [1/1], Batch [2121], Loss: 0.087828
Epoch [1/1], Batch [2131], Loss: 0.083935
Epoch [1/1], Batch [2141], Loss: 0.086646
Epoch [1/1], Batch [2151], Loss: 0.086819
Epoch [1/1], Batch [2161], Loss: 0.082842
Epoch [1/1], Batch [2171], Loss: 0.086571
Epoch [1/1], Batch [2181], Loss: 0.087124
Epoch [1/1], Batch [2191], Loss: 0.089121
Epoch [1/1], Batch [2201], Loss: 0.085274
Epoch [1/1], Batch [2211], Loss: 0.091465
Epoch [1/1], Batch [2221], Loss: 0.088576
Epoch [1/1], Batch [2231], Loss: 0.085291
Epoch [1/1], Batch [2241], Loss: 0.089586
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 0.0956
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 0.0875
Elapsed time: 504.67 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 0.0870
Elapsed time: 525.35 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 0.104880
Epoch [1/1], Batch [11], Loss: 0.112122
Epoch [1/1], Batch [21], Loss: 0.101924
Epoch [1/1], Batch [31], Loss: 0.101217
Epoch [1/1], Batch [41], Loss: 0.096193
Epoch [1/1], Batch [51], Loss: 0.098730
Epoch [1/1], Batch [61], Loss: 0.095143
Epoch [1/1], Batch [71], Loss: 0.094907
Epoch [1/1], Batch [81], Loss: 0.099238
Epoch [1/1], Batch [91], Loss: 0.094943
Epoch [1/1], Batch [101], Loss: 0.095183
Epoch [1/1], Batch [111], Loss: 0.099434
Epoch [1/1], Batch [121], Loss: 0.094426
Epoch [1/1], Batch [131], Loss: 0.095922
Epoch [1/1], Batch [141], Loss: 0.096126
Epoch [1/1], Batch [151], Loss: 0.099853
Epoch [1/1], Batch [161], Loss: 0.093318
Epoch [1/1], Batch [171], Loss: 0.098705
Epoch [1/1], Batch [181], Loss: 0.096398
Epoch [1/1], Batch [191], Loss: 0.096639
Epoch [1/1], Batch [201], Loss: 0.094309
Epoch [1/1], Batch [211], Loss: 0.091853
Epoch [1/1], Batch [221], Loss: 0.094273
Epoch [1/1], Batch [231], Loss: 0.098731
Epoch [1/1], Batch [241], Loss: 0.095796
Epoch [1/1], Batch [251], Loss: 0.100015
Epoch [1/1], Batch [261], Loss: 0.092177
Epoch [1/1], Batch [271], Loss: 0.099597
Epoch [1/1], Batch [281], Loss: 0.096062
Epoch [1/1], Batch [291], Loss: 0.092954
Epoch [1/1], Batch [301], Loss: 0.097719
Epoch [1/1], Batch [311], Loss: 0.091146
Epoch [1/1], Batch [321], Loss: 0.091390
Epoch [1/1], Batch [331], Loss: 0.098655
Epoch [1/1], Batch [341], Loss: 0.097476
Epoch [1/1], Batch [351], Loss: 0.098960
Epoch [1/1], Batch [361], Loss: 0.093961
Epoch [1/1], Batch [371], Loss: 0.096917
Epoch [1/1], Batch [381], Loss: 0.096236
Epoch [1/1], Batch [391], Loss: 0.094455
Epoch [1/1], Batch [401], Loss: 0.094454
Epoch [1/1], Batch [411], Loss: 0.094308
Epoch [1/1], Batch [421], Loss: 0.097284
Epoch [1/1], Batch [431], Loss: 0.098052
Epoch [1/1], Batch [441], Loss: 0.094304
Epoch [1/1], Batch [451], Loss: 0.098109
Epoch [1/1], Batch [461], Loss: 0.095755
Epoch [1/1], Batch [471], Loss: 0.094133
Epoch [1/1], Batch [481], Loss: 0.094723
Epoch [1/1], Batch [491], Loss: 0.092860
Epoch [1/1], Batch [501], Loss: 0.094587
Epoch [1/1], Batch [511], Loss: 0.095440
Epoch [1/1], Batch [521], Loss: 0.095305
Epoch [1/1], Batch [531], Loss: 0.093033
Epoch [1/1], Batch [541], Loss: 0.096767
Epoch [1/1], Batch [551], Loss: 0.096381
Epoch [1/1], Batch [561], Loss: 0.095889
Epoch [1/1], Batch [571], Loss: 0.094532
Epoch [1/1], Batch [581], Loss: 0.097271
Epoch [1/1], Batch [591], Loss: 0.098652
Epoch [1/1], Batch [601], Loss: 0.098941
Epoch [1/1], Batch [611], Loss: 0.094980
Epoch [1/1], Batch [621], Loss: 0.093497
Epoch [1/1], Batch [631], Loss: 0.101101
Epoch [1/1], Batch [641], Loss: 0.095749
Epoch [1/1], Batch [651], Loss: 0.096459
Epoch [1/1], Batch [661], Loss: 0.092789
Epoch [1/1], Batch [671], Loss: 0.094415
Epoch [1/1], Batch [681], Loss: 0.092840
Epoch [1/1], Batch [691], Loss: 0.096781
Epoch [1/1], Batch [701], Loss: 0.096486
Epoch [1/1], Batch [711], Loss: 0.097027
Epoch [1/1], Batch [721], Loss: 0.094374
Epoch [1/1], Batch [731], Loss: 0.089636
Epoch [1/1], Batch [741], Loss: 0.097685
Epoch [1/1], Batch [751], Loss: 0.094425
Epoch [1/1], Batch [761], Loss: 0.091830
Epoch [1/1], Batch [771], Loss: 0.089584
Epoch [1/1], Batch [781], Loss: 0.094997
Epoch [1/1], Batch [791], Loss: 0.092413
Epoch [1/1], Batch [801], Loss: 0.094200
Epoch [1/1], Batch [811], Loss: 0.093859
Epoch [1/1], Batch [821], Loss: 0.094703
Epoch [1/1], Batch [831], Loss: 0.093767
Epoch [1/1], Batch [841], Loss: 0.090966
Epoch [1/1], Batch [851], Loss: 0.095323
Epoch [1/1], Batch [861], Loss: 0.089673
Epoch [1/1], Batch [871], Loss: 0.093694
Epoch [1/1], Batch [881], Loss: 0.094390
Epoch [1/1], Batch [891], Loss: 0.093048
Epoch [1/1], Batch [901], Loss: 0.092508
Epoch [1/1], Batch [911], Loss: 0.094894
Epoch [1/1], Batch [921], Loss: 0.097936
Epoch [1/1], Batch [931], Loss: 0.095981
Epoch [1/1], Batch [941], Loss: 0.091533
Epoch [1/1], Batch [951], Loss: 0.092727
Epoch [1/1], Batch [961], Loss: 0.094914
Epoch [1/1], Batch [971], Loss: 0.091450
Epoch [1/1], Batch [981], Loss: 0.098786
Epoch [1/1], Batch [991], Loss: 0.095677
Epoch [1/1], Batch [1001], Loss: 0.095040
Epoch [1/1], Batch [1011], Loss: 0.094974
Epoch [1/1], Batch [1021], Loss: 0.094826
Epoch [1/1], Batch [1031], Loss: 0.098442
Epoch [1/1], Batch [1041], Loss: 0.097526
Epoch [1/1], Batch [1051], Loss: 0.095480
Epoch [1/1], Batch [1061], Loss: 0.098844
Epoch [1/1], Batch [1071], Loss: 0.101778
Epoch [1/1], Batch [1081], Loss: 0.093345
Epoch [1/1], Batch [1091], Loss: 0.094853
Epoch [1/1], Batch [1101], Loss: 0.091269
Epoch [1/1], Batch [1111], Loss: 0.095353
Epoch [1/1], Batch [1121], Loss: 0.095414
Epoch [1/1], Batch [1131], Loss: 0.097571
Epoch [1/1], Batch [1141], Loss: 0.093887
Epoch [1/1], Batch [1151], Loss: 0.091226
Epoch [1/1], Batch [1161], Loss: 0.096551
Epoch [1/1], Batch [1171], Loss: 0.095522
Epoch [1/1], Batch [1181], Loss: 0.098445
Epoch [1/1], Batch [1191], Loss: 0.096019
Epoch [1/1], Batch [1201], Loss: 0.097383
Epoch [1/1], Batch [1211], Loss: 0.098633
Epoch [1/1], Batch [1221], Loss: 0.096340
Epoch [1/1], Batch [1231], Loss: 0.096268
Epoch [1/1], Batch [1241], Loss: 0.092755
Epoch [1/1], Batch [1251], Loss: 0.097899
Epoch [1/1], Batch [1261], Loss: 0.093078
Epoch [1/1], Batch [1271], Loss: 0.097338
Epoch [1/1], Batch [1281], Loss: 0.093355
Epoch [1/1], Batch [1291], Loss: 0.089988
Epoch [1/1], Batch [1301], Loss: 0.096439
Epoch [1/1], Batch [1311], Loss: 0.095290
Epoch [1/1], Batch [1321], Loss: 0.095472
Epoch [1/1], Batch [1331], Loss: 0.092816
Epoch [1/1], Batch [1341], Loss: 0.097181
Epoch [1/1], Batch [1351], Loss: 0.091612
Epoch [1/1], Batch [1361], Loss: 0.091611
Epoch [1/1], Batch [1371], Loss: 0.092211
Epoch [1/1], Batch [1381], Loss: 0.093677
Epoch [1/1], Batch [1391], Loss: 0.090967
Epoch [1/1], Batch [1401], Loss: 0.093189
Epoch [1/1], Batch [1411], Loss: 0.094689
Epoch [1/1], Batch [1421], Loss: 0.093487
Epoch [1/1], Batch [1431], Loss: 0.096735
Epoch [1/1], Batch [1441], Loss: 0.091861
Epoch [1/1], Batch [1451], Loss: 0.091019
Epoch [1/1], Batch [1461], Loss: 0.088951
Epoch [1/1], Batch [1471], Loss: 0.090700
Epoch [1/1], Batch [1481], Loss: 0.090405
Epoch [1/1], Batch [1491], Loss: 0.099803
Epoch [1/1], Batch [1501], Loss: 0.095258
Epoch [1/1], Batch [1511], Loss: 0.097211
Epoch [1/1], Batch [1521], Loss: 0.093701
Epoch [1/1], Batch [1531], Loss: 0.094649
Epoch [1/1], Batch [1541], Loss: 0.089998
Epoch [1/1], Batch [1551], Loss: 0.096761
Epoch [1/1], Batch [1561], Loss: 0.097169
Epoch [1/1], Batch [1571], Loss: 0.093066
Epoch [1/1], Batch [1581], Loss: 0.093309
Epoch [1/1], Batch [1591], Loss: 0.093365
Epoch [1/1], Batch [1601], Loss: 0.090948
Epoch [1/1], Batch [1611], Loss: 0.088929
Epoch [1/1], Batch [1621], Loss: 0.095014
Epoch [1/1], Batch [1631], Loss: 0.090776
Epoch [1/1], Batch [1641], Loss: 0.093667
Epoch [1/1], Batch [1651], Loss: 0.092938
Epoch [1/1], Batch [1661], Loss: 0.097454
Epoch [1/1], Batch [1671], Loss: 0.096726
Epoch [1/1], Batch [1681], Loss: 0.095450
Epoch [1/1], Batch [1691], Loss: 0.093904
Epoch [1/1], Batch [1701], Loss: 0.091482
Epoch [1/1], Batch [1711], Loss: 0.097665
Epoch [1/1], Batch [1721], Loss: 0.097830
Epoch [1/1], Batch [1731], Loss: 0.093510
Epoch [1/1], Batch [1741], Loss: 0.093794
Epoch [1/1], Batch [1751], Loss: 0.092156
Epoch [1/1], Batch [1761], Loss: 0.090821
Epoch [1/1], Batch [1771], Loss: 0.093465
Epoch [1/1], Batch [1781], Loss: 0.089954
Epoch [1/1], Batch [1791], Loss: 0.094887
Epoch [1/1], Batch [1801], Loss: 0.092868
Epoch [1/1], Batch [1811], Loss: 0.092389
Epoch [1/1], Batch [1821], Loss: 0.092642
Epoch [1/1], Batch [1831], Loss: 0.089917
Epoch [1/1], Batch [1841], Loss: 0.095105
Epoch [1/1], Batch [1851], Loss: 0.094798
Epoch [1/1], Batch [1861], Loss: 0.093446
Epoch [1/1], Batch [1871], Loss: 0.098237
Epoch [1/1], Batch [1881], Loss: 0.088434
Epoch [1/1], Batch [1891], Loss: 0.094457
Epoch [1/1], Batch [1901], Loss: 0.094510
Epoch [1/1], Batch [1911], Loss: 0.094590
Epoch [1/1], Batch [1921], Loss: 0.093310
Epoch [1/1], Batch [1931], Loss: 0.097089
Epoch [1/1], Batch [1941], Loss: 0.096234
Epoch [1/1], Batch [1951], Loss: 0.099419
Epoch [1/1], Batch [1961], Loss: 0.096476
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 0.0951
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 0.0939
Elapsed time: 1212.61 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 0.0943
Elapsed time: 1237.57 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 0.103963
Epoch [1/1], Batch [11], Loss: 0.101014
Epoch [1/1], Batch [21], Loss: 0.101119
Epoch [1/1], Batch [31], Loss: 0.101193
Epoch [1/1], Batch [41], Loss: 0.103283
Epoch [1/1], Batch [51], Loss: 0.104140
Epoch [1/1], Batch [61], Loss: 0.101159
Epoch [1/1], Batch [71], Loss: 0.100200
Epoch [1/1], Batch [81], Loss: 0.104636
Epoch [1/1], Batch [91], Loss: 0.101691
Epoch [1/1], Batch [101], Loss: 0.104746
Epoch [1/1], Batch [111], Loss: 0.101414
Epoch [1/1], Batch [121], Loss: 0.099962
Epoch [1/1], Batch [131], Loss: 0.104433
Epoch [1/1], Batch [141], Loss: 0.100637
Epoch [1/1], Batch [151], Loss: 0.101847
Epoch [1/1], Batch [161], Loss: 0.098683
Epoch [1/1], Batch [171], Loss: 0.101614
Epoch [1/1], Batch [181], Loss: 0.100293
Epoch [1/1], Batch [191], Loss: 0.104704
Epoch [1/1], Batch [201], Loss: 0.105233
Epoch [1/1], Batch [211], Loss: 0.103211
Epoch [1/1], Batch [221], Loss: 0.104897
Epoch [1/1], Batch [231], Loss: 0.105328
Epoch [1/1], Batch [241], Loss: 0.096472
Epoch [1/1], Batch [251], Loss: 0.103182
Epoch [1/1], Batch [261], Loss: 0.101004
Epoch [1/1], Batch [271], Loss: 0.102076
Epoch [1/1], Batch [281], Loss: 0.098574
Epoch [1/1], Batch [291], Loss: 0.101358
Epoch [1/1], Batch [301], Loss: 0.104266
Epoch [1/1], Batch [311], Loss: 0.100145
Epoch [1/1], Batch [321], Loss: 0.101754
Epoch [1/1], Batch [331], Loss: 0.099296
Epoch [1/1], Batch [341], Loss: 0.102375
Epoch [1/1], Batch [351], Loss: 0.101636
Epoch [1/1], Batch [361], Loss: 0.101227
Epoch [1/1], Batch [371], Loss: 0.107676
Epoch [1/1], Batch [381], Loss: 0.101089
Epoch [1/1], Batch [391], Loss: 0.101076
Epoch [1/1], Batch [401], Loss: 0.101978
Epoch [1/1], Batch [411], Loss: 0.100243
Epoch [1/1], Batch [421], Loss: 0.099418
Epoch [1/1], Batch [431], Loss: 0.099824
Epoch [1/1], Batch [441], Loss: 0.104364
Epoch [1/1], Batch [451], Loss: 0.100423
Epoch [1/1], Batch [461], Loss: 0.099028
Epoch [1/1], Batch [471], Loss: 0.103668
Epoch [1/1], Batch [481], Loss: 0.098099
Epoch [1/1], Batch [491], Loss: 0.098385
Epoch [1/1], Batch [501], Loss: 0.100315
Epoch [1/1], Batch [511], Loss: 0.105263
Epoch [1/1], Batch [521], Loss: 0.097631
Epoch [1/1], Batch [531], Loss: 0.100409
Epoch [1/1], Batch [541], Loss: 0.100283
Epoch [1/1], Batch [551], Loss: 0.105854
Epoch [1/1], Batch [561], Loss: 0.099576
Epoch [1/1], Batch [571], Loss: 0.103797
Epoch [1/1], Batch [581], Loss: 0.097602
Epoch [1/1], Batch [591], Loss: 0.098454
Epoch [1/1], Batch [601], Loss: 0.098033
Epoch [1/1], Batch [611], Loss: 0.096776
Epoch [1/1], Batch [621], Loss: 0.104949
Epoch [1/1], Batch [631], Loss: 0.103543
Epoch [1/1], Batch [641], Loss: 0.101872
Epoch [1/1], Batch [651], Loss: 0.104631
Epoch [1/1], Batch [661], Loss: 0.102222
Epoch [1/1], Batch [671], Loss: 0.098671
Epoch [1/1], Batch [681], Loss: 0.103902
Epoch [1/1], Batch [691], Loss: 0.100525
Epoch [1/1], Batch [701], Loss: 0.100451
Epoch [1/1], Batch [711], Loss: 0.099722
Epoch [1/1], Batch [721], Loss: 0.097953
Epoch [1/1], Batch [731], Loss: 0.104789
Epoch [1/1], Batch [741], Loss: 0.101309
Epoch [1/1], Batch [751], Loss: 0.104823
Epoch [1/1], Batch [761], Loss: 0.098075
Epoch [1/1], Batch [771], Loss: 0.103899
Epoch [1/1], Batch [781], Loss: 0.102826
Epoch [1/1], Batch [791], Loss: 0.104189
Epoch [1/1], Batch [801], Loss: 0.101468
Epoch [1/1], Batch [811], Loss: 0.102052
Epoch [1/1], Batch [821], Loss: 0.105015
Epoch [1/1], Batch [831], Loss: 0.100344
Epoch [1/1], Batch [841], Loss: 0.099060
Epoch [1/1], Batch [851], Loss: 0.109049
Epoch [1/1], Batch [861], Loss: 0.099823
Epoch [1/1], Batch [871], Loss: 0.104748
Epoch [1/1], Batch [881], Loss: 0.099063
Epoch [1/1], Batch [891], Loss: 0.102102
Epoch [1/1], Batch [901], Loss: 0.096188
Epoch [1/1], Batch [911], Loss: 0.103977
Epoch [1/1], Batch [921], Loss: 0.098269
Epoch [1/1], Batch [931], Loss: 0.104908
Epoch [1/1], Batch [941], Loss: 0.099766
Epoch [1/1], Batch [951], Loss: 0.102419
Epoch [1/1], Batch [961], Loss: 0.102879
Epoch [1/1], Batch [971], Loss: 0.101605
Epoch [1/1], Batch [981], Loss: 0.101962
Epoch [1/1], Batch [991], Loss: 0.102482
Epoch [1/1], Batch [1001], Loss: 0.104004
Epoch [1/1], Batch [1011], Loss: 0.104092
Epoch [1/1], Batch [1021], Loss: 0.100786
Epoch [1/1], Batch [1031], Loss: 0.102701
Epoch [1/1], Batch [1041], Loss: 0.102754
Epoch [1/1], Batch [1051], Loss: 0.098364
Epoch [1/1], Batch [1061], Loss: 0.106637
Epoch [1/1], Batch [1071], Loss: 0.101621
Epoch [1/1], Batch [1081], Loss: 0.100589
Epoch [1/1], Batch [1091], Loss: 0.102735
Epoch [1/1], Batch [1101], Loss: 0.097836
Epoch [1/1], Batch [1111], Loss: 0.102051
Epoch [1/1], Batch [1121], Loss: 0.100748
Epoch [1/1], Batch [1131], Loss: 0.101410
Epoch [1/1], Batch [1141], Loss: 0.102351
Epoch [1/1], Batch [1151], Loss: 0.098290
Epoch [1/1], Batch [1161], Loss: 0.104202
Epoch [1/1], Batch [1171], Loss: 0.100959
Epoch [1/1], Batch [1181], Loss: 0.104061
Epoch [1/1], Batch [1191], Loss: 0.102896
Epoch [1/1], Batch [1201], Loss: 0.102839
Epoch [1/1], Batch [1211], Loss: 0.097812
Epoch [1/1], Batch [1221], Loss: 0.108544
Epoch [1/1], Batch [1231], Loss: 0.100507
Epoch [1/1], Batch [1241], Loss: 0.100175
Epoch [1/1], Batch [1251], Loss: 0.098415
Epoch [1/1], Batch [1261], Loss: 0.101704
Epoch [1/1], Batch [1271], Loss: 0.100771
Epoch [1/1], Batch [1281], Loss: 0.105376
Epoch [1/1], Batch [1291], Loss: 0.099127
Epoch [1/1], Batch [1301], Loss: 0.099453
Epoch [1/1], Batch [1311], Loss: 0.098985
Epoch [1/1], Batch [1321], Loss: 0.098077
Epoch [1/1], Batch [1331], Loss: 0.103454
Epoch [1/1], Batch [1341], Loss: 0.103157
Epoch [1/1], Batch [1351], Loss: 0.104229
Epoch [1/1], Batch [1361], Loss: 0.099748
Epoch [1/1], Batch [1371], Loss: 0.100275
Epoch [1/1], Batch [1381], Loss: 0.100915
Epoch [1/1], Batch [1391], Loss: 0.106156
Epoch [1/1], Batch [1401], Loss: 0.102188
Epoch [1/1], Batch [1411], Loss: 0.103548
Epoch [1/1], Batch [1421], Loss: 0.103805
Epoch [1/1], Batch [1431], Loss: 0.101183
Epoch [1/1], Batch [1441], Loss: 0.101599
Epoch [1/1], Batch [1451], Loss: 0.102175
Epoch [1/1], Batch [1461], Loss: 0.100739
Epoch [1/1], Batch [1471], Loss: 0.101916
Epoch [1/1], Batch [1481], Loss: 0.102145
Epoch [1/1], Batch [1491], Loss: 0.098666
Epoch [1/1], Batch [1501], Loss: 0.099828
Epoch [1/1], Batch [1511], Loss: 0.104148
Epoch [1/1], Batch [1521], Loss: 0.105413
Epoch [1/1], Batch [1531], Loss: 0.104849
Epoch [1/1], Batch [1541], Loss: 0.100742
Epoch [1/1], Batch [1551], Loss: 0.105287
Epoch [1/1], Batch [1561], Loss: 0.102458
Epoch [1/1], Batch [1571], Loss: 0.102473
Epoch [1/1], Batch [1581], Loss: 0.099178
Epoch [1/1], Batch [1591], Loss: 0.099385
Epoch [1/1], Batch [1601], Loss: 0.103362
Epoch [1/1], Batch [1611], Loss: 0.101204
Epoch [1/1], Batch [1621], Loss: 0.104529
Epoch [1/1], Batch [1631], Loss: 0.102158
Epoch [1/1], Batch [1641], Loss: 0.103880
Epoch [1/1], Batch [1651], Loss: 0.104467
Epoch [1/1], Batch [1661], Loss: 0.100721
Epoch [1/1], Batch [1671], Loss: 0.110341
Epoch [1/1], Batch [1681], Loss: 0.099591
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 0.1018
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 0.1025
Elapsed time: 1940.22 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 0.1031
Elapsed time: 1967.39 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 0.116089
Epoch [1/1], Batch [11], Loss: 0.113116
Epoch [1/1], Batch [21], Loss: 0.110294
Epoch [1/1], Batch [31], Loss: 0.112269
Epoch [1/1], Batch [41], Loss: 0.111464
Epoch [1/1], Batch [51], Loss: 0.107614
Epoch [1/1], Batch [61], Loss: 0.111715
Epoch [1/1], Batch [71], Loss: 0.107234
Epoch [1/1], Batch [81], Loss: 0.105542
Epoch [1/1], Batch [91], Loss: 0.112943
Epoch [1/1], Batch [101], Loss: 0.105437
Epoch [1/1], Batch [111], Loss: 0.110555
Epoch [1/1], Batch [121], Loss: 0.106623
Epoch [1/1], Batch [131], Loss: 0.108588
Epoch [1/1], Batch [141], Loss: 0.104630
Epoch [1/1], Batch [151], Loss: 0.108352
Epoch [1/1], Batch [161], Loss: 0.108458
Epoch [1/1], Batch [171], Loss: 0.111902
Epoch [1/1], Batch [181], Loss: 0.106240
Epoch [1/1], Batch [191], Loss: 0.107290
Epoch [1/1], Batch [201], Loss: 0.107531
Epoch [1/1], Batch [211], Loss: 0.109218
Epoch [1/1], Batch [221], Loss: 0.111015
Epoch [1/1], Batch [231], Loss: 0.114315
Epoch [1/1], Batch [241], Loss: 0.106678
Epoch [1/1], Batch [251], Loss: 0.113086
Epoch [1/1], Batch [261], Loss: 0.109832
Epoch [1/1], Batch [271], Loss: 0.107476
Epoch [1/1], Batch [281], Loss: 0.109849
Epoch [1/1], Batch [291], Loss: 0.111569
Epoch [1/1], Batch [301], Loss: 0.110306
Epoch [1/1], Batch [311], Loss: 0.111253
Epoch [1/1], Batch [321], Loss: 0.114292
Epoch [1/1], Batch [331], Loss: 0.114656
Epoch [1/1], Batch [341], Loss: 0.109468
Epoch [1/1], Batch [351], Loss: 0.109137
Epoch [1/1], Batch [361], Loss: 0.109664
Epoch [1/1], Batch [371], Loss: 0.111486
Epoch [1/1], Batch [381], Loss: 0.113592
Epoch [1/1], Batch [391], Loss: 0.110774
Epoch [1/1], Batch [401], Loss: 0.108633
Epoch [1/1], Batch [411], Loss: 0.109590
Epoch [1/1], Batch [421], Loss: 0.111770
Epoch [1/1], Batch [431], Loss: 0.109039
Epoch [1/1], Batch [441], Loss: 0.107558
Epoch [1/1], Batch [451], Loss: 0.108363
Epoch [1/1], Batch [461], Loss: 0.112009
Epoch [1/1], Batch [471], Loss: 0.105530
Epoch [1/1], Batch [481], Loss: 0.110057
Epoch [1/1], Batch [491], Loss: 0.105615
Epoch [1/1], Batch [501], Loss: 0.106548
Epoch [1/1], Batch [511], Loss: 0.110559
Epoch [1/1], Batch [521], Loss: 0.109243
Epoch [1/1], Batch [531], Loss: 0.109394
Epoch [1/1], Batch [541], Loss: 0.112219
Epoch [1/1], Batch [551], Loss: 0.109059
Epoch [1/1], Batch [561], Loss: 0.113412
Epoch [1/1], Batch [571], Loss: 0.107021
Epoch [1/1], Batch [581], Loss: 0.108478
Epoch [1/1], Batch [591], Loss: 0.109521
Epoch [1/1], Batch [601], Loss: 0.107943
Epoch [1/1], Batch [611], Loss: 0.114077
Epoch [1/1], Batch [621], Loss: 0.108616
Epoch [1/1], Batch [631], Loss: 0.108664
Epoch [1/1], Batch [641], Loss: 0.110623
Epoch [1/1], Batch [651], Loss: 0.105253
Epoch [1/1], Batch [661], Loss: 0.105986
Epoch [1/1], Batch [671], Loss: 0.112576
Epoch [1/1], Batch [681], Loss: 0.113216
Epoch [1/1], Batch [691], Loss: 0.104858
Epoch [1/1], Batch [701], Loss: 0.115413
Epoch [1/1], Batch [711], Loss: 0.110469
Epoch [1/1], Batch [721], Loss: 0.111969
Epoch [1/1], Batch [731], Loss: 0.109645
Epoch [1/1], Batch [741], Loss: 0.109775
Epoch [1/1], Batch [751], Loss: 0.108974
Epoch [1/1], Batch [761], Loss: 0.108658
Epoch [1/1], Batch [771], Loss: 0.112280
Epoch [1/1], Batch [781], Loss: 0.114867
Epoch [1/1], Batch [791], Loss: 0.110040
Epoch [1/1], Batch [801], Loss: 0.111318
Epoch [1/1], Batch [811], Loss: 0.109958
Epoch [1/1], Batch [821], Loss: 0.111866
Epoch [1/1], Batch [831], Loss: 0.109472
Epoch [1/1], Batch [841], Loss: 0.115105
Epoch [1/1], Batch [851], Loss: 0.108972
Epoch [1/1], Batch [861], Loss: 0.110610
Epoch [1/1], Batch [871], Loss: 0.112246
Epoch [1/1], Batch [881], Loss: 0.112045
Epoch [1/1], Batch [891], Loss: 0.111332
Epoch [1/1], Batch [901], Loss: 0.113184
Epoch [1/1], Batch [911], Loss: 0.113258
Epoch [1/1], Batch [921], Loss: 0.116981
Epoch [1/1], Batch [931], Loss: 0.114741
Epoch [1/1], Batch [941], Loss: 0.113531
Epoch [1/1], Batch [951], Loss: 0.111072
Epoch [1/1], Batch [961], Loss: 0.109305
Epoch [1/1], Batch [971], Loss: 0.105594
Epoch [1/1], Batch [981], Loss: 0.111477
Epoch [1/1], Batch [991], Loss: 0.115056
Epoch [1/1], Batch [1001], Loss: 0.111866
Epoch [1/1], Batch [1011], Loss: 0.109224
Epoch [1/1], Batch [1021], Loss: 0.112967
Epoch [1/1], Batch [1031], Loss: 0.106149
Epoch [1/1], Batch [1041], Loss: 0.110943
Epoch [1/1], Batch [1051], Loss: 0.110079
Epoch [1/1], Batch [1061], Loss: 0.110187
Epoch [1/1], Batch [1071], Loss: 0.114980
Epoch [1/1], Batch [1081], Loss: 0.107957
Epoch [1/1], Batch [1091], Loss: 0.108806
Epoch [1/1], Batch [1101], Loss: 0.112555
Epoch [1/1], Batch [1111], Loss: 0.112758
Epoch [1/1], Batch [1121], Loss: 0.113081
Epoch [1/1], Batch [1131], Loss: 0.112903
Epoch [1/1], Batch [1141], Loss: 0.108599
Epoch [1/1], Batch [1151], Loss: 0.111597
Epoch [1/1], Batch [1161], Loss: 0.111180
Epoch [1/1], Batch [1171], Loss: 0.111850
Epoch [1/1], Batch [1181], Loss: 0.111934
Epoch [1/1], Batch [1191], Loss: 0.109703
Epoch [1/1], Batch [1201], Loss: 0.111684
Epoch [1/1], Batch [1211], Loss: 0.115323
Epoch [1/1], Batch [1221], Loss: 0.113945
Epoch [1/1], Batch [1231], Loss: 0.113251
Epoch [1/1], Batch [1241], Loss: 0.106943
Epoch [1/1], Batch [1251], Loss: 0.113432
Epoch [1/1], Batch [1261], Loss: 0.109792
Epoch [1/1], Batch [1271], Loss: 0.111436
Epoch [1/1], Batch [1281], Loss: 0.111204
Epoch [1/1], Batch [1291], Loss: 0.115317
Epoch [1/1], Batch [1301], Loss: 0.111874
Epoch [1/1], Batch [1311], Loss: 0.115360
Epoch [1/1], Batch [1321], Loss: 0.112391
Epoch [1/1], Batch [1331], Loss: 0.113392
Epoch [1/1], Batch [1341], Loss: 0.115234
Epoch [1/1], Batch [1351], Loss: 0.115826
Epoch [1/1], Batch [1361], Loss: 0.117825
Epoch [1/1], Batch [1371], Loss: 0.109907
Epoch [1/1], Batch [1381], Loss: 0.113969
Epoch [1/1], Batch [1391], Loss: 0.114454
Epoch [1/1], Batch [1401], Loss: 0.112070
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 0.1108
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 0.1125
Elapsed time: 2692.21 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 0.1119
Elapsed time: 2719.73 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 0.121883
Epoch [1/1], Batch [11], Loss: 0.116520
Epoch [1/1], Batch [21], Loss: 0.119357
Epoch [1/1], Batch [31], Loss: 0.119136
Epoch [1/1], Batch [41], Loss: 0.115913
Epoch [1/1], Batch [51], Loss: 0.117985
Epoch [1/1], Batch [61], Loss: 0.113318
Epoch [1/1], Batch [71], Loss: 0.120085
Epoch [1/1], Batch [81], Loss: 0.113867
Epoch [1/1], Batch [91], Loss: 0.116714
Epoch [1/1], Batch [101], Loss: 0.117276
Epoch [1/1], Batch [111], Loss: 0.119025
Epoch [1/1], Batch [121], Loss: 0.118860
Epoch [1/1], Batch [131], Loss: 0.117696
Epoch [1/1], Batch [141], Loss: 0.124733
Epoch [1/1], Batch [151], Loss: 0.117911
Epoch [1/1], Batch [161], Loss: 0.117254
Epoch [1/1], Batch [171], Loss: 0.113651
Epoch [1/1], Batch [181], Loss: 0.117315
Epoch [1/1], Batch [191], Loss: 0.115891
Epoch [1/1], Batch [201], Loss: 0.119640
Epoch [1/1], Batch [211], Loss: 0.121197
Epoch [1/1], Batch [221], Loss: 0.115197
Epoch [1/1], Batch [231], Loss: 0.116504
Epoch [1/1], Batch [241], Loss: 0.116411
Epoch [1/1], Batch [251], Loss: 0.117219
Epoch [1/1], Batch [261], Loss: 0.119790
Epoch [1/1], Batch [271], Loss: 0.118149
Epoch [1/1], Batch [281], Loss: 0.121885
Epoch [1/1], Batch [291], Loss: 0.115761
Epoch [1/1], Batch [301], Loss: 0.117184
Epoch [1/1], Batch [311], Loss: 0.118394
Epoch [1/1], Batch [321], Loss: 0.116954
Epoch [1/1], Batch [331], Loss: 0.118430
Epoch [1/1], Batch [341], Loss: 0.121481
Epoch [1/1], Batch [351], Loss: 0.116900
Epoch [1/1], Batch [361], Loss: 0.119584
Epoch [1/1], Batch [371], Loss: 0.118647
Epoch [1/1], Batch [381], Loss: 0.119225
Epoch [1/1], Batch [391], Loss: 0.117785
Epoch [1/1], Batch [401], Loss: 0.117932
Epoch [1/1], Batch [411], Loss: 0.118537
Epoch [1/1], Batch [421], Loss: 0.116869
Epoch [1/1], Batch [431], Loss: 0.115764
Epoch [1/1], Batch [441], Loss: 0.117080
Epoch [1/1], Batch [451], Loss: 0.119634
Epoch [1/1], Batch [461], Loss: 0.122137
Epoch [1/1], Batch [471], Loss: 0.119345
Epoch [1/1], Batch [481], Loss: 0.119956
Epoch [1/1], Batch [491], Loss: 0.121639
Epoch [1/1], Batch [501], Loss: 0.118705
Epoch [1/1], Batch [511], Loss: 0.123228
Epoch [1/1], Batch [521], Loss: 0.118595
Epoch [1/1], Batch [531], Loss: 0.124127
Epoch [1/1], Batch [541], Loss: 0.120778
Epoch [1/1], Batch [551], Loss: 0.121600
Epoch [1/1], Batch [561], Loss: 0.116286
Epoch [1/1], Batch [571], Loss: 0.120385
Epoch [1/1], Batch [581], Loss: 0.124161
Epoch [1/1], Batch [591], Loss: 0.121055
Epoch [1/1], Batch [601], Loss: 0.124935
Epoch [1/1], Batch [611], Loss: 0.120710
Epoch [1/1], Batch [621], Loss: 0.119606
Epoch [1/1], Batch [631], Loss: 0.118948
Epoch [1/1], Batch [641], Loss: 0.124022
Epoch [1/1], Batch [651], Loss: 0.119996
Epoch [1/1], Batch [661], Loss: 0.123130
Epoch [1/1], Batch [671], Loss: 0.125300
Epoch [1/1], Batch [681], Loss: 0.119801
Epoch [1/1], Batch [691], Loss: 0.115387
Epoch [1/1], Batch [701], Loss: 0.119483
Epoch [1/1], Batch [711], Loss: 0.125325
Epoch [1/1], Batch [721], Loss: 0.121727
Epoch [1/1], Batch [731], Loss: 0.125815
Epoch [1/1], Batch [741], Loss: 0.121373
Epoch [1/1], Batch [751], Loss: 0.124042
Epoch [1/1], Batch [761], Loss: 0.120059
Epoch [1/1], Batch [771], Loss: 0.119399
Epoch [1/1], Batch [781], Loss: 0.121858
Epoch [1/1], Batch [791], Loss: 0.116119
Epoch [1/1], Batch [801], Loss: 0.120333
Epoch [1/1], Batch [811], Loss: 0.121029
Epoch [1/1], Batch [821], Loss: 0.118458
Epoch [1/1], Batch [831], Loss: 0.126808
Epoch [1/1], Batch [841], Loss: 0.122210
Epoch [1/1], Batch [851], Loss: 0.122754
Epoch [1/1], Batch [861], Loss: 0.123375
Epoch [1/1], Batch [871], Loss: 0.125222
Epoch [1/1], Batch [881], Loss: 0.123474
Epoch [1/1], Batch [891], Loss: 0.119689
Epoch [1/1], Batch [901], Loss: 0.119350
Epoch [1/1], Batch [911], Loss: 0.122659
Epoch [1/1], Batch [921], Loss: 0.118458
Epoch [1/1], Batch [931], Loss: 0.124416
Epoch [1/1], Batch [941], Loss: 0.118476
Epoch [1/1], Batch [951], Loss: 0.118049
Epoch [1/1], Batch [961], Loss: 0.120726
Epoch [1/1], Batch [971], Loss: 0.127773
Epoch [1/1], Batch [981], Loss: 0.125015
Epoch [1/1], Batch [991], Loss: 0.119773
Epoch [1/1], Batch [1001], Loss: 0.119720
Epoch [1/1], Batch [1011], Loss: 0.122621
Epoch [1/1], Batch [1021], Loss: 0.121284
Epoch [1/1], Batch [1031], Loss: 0.123584
Epoch [1/1], Batch [1041], Loss: 0.122397
Epoch [1/1], Batch [1051], Loss: 0.125097
Epoch [1/1], Batch [1061], Loss: 0.115641
Epoch [1/1], Batch [1071], Loss: 0.124774
Epoch [1/1], Batch [1081], Loss: 0.124046
Epoch [1/1], Batch [1091], Loss: 0.122143
Epoch [1/1], Batch [1101], Loss: 0.122868
Epoch [1/1], Batch [1111], Loss: 0.118500
Epoch [1/1], Batch [1121], Loss: 0.121941
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 0.1199
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 0.1228
Elapsed time: 3407.97 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 0.1230
Elapsed time: 3433.80 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 0.128333
Epoch [1/1], Batch [11], Loss: 0.127016
Epoch [1/1], Batch [21], Loss: 0.130725
Epoch [1/1], Batch [31], Loss: 0.129218
Epoch [1/1], Batch [41], Loss: 0.129611
Epoch [1/1], Batch [51], Loss: 0.128921
Epoch [1/1], Batch [61], Loss: 0.124687
Epoch [1/1], Batch [71], Loss: 0.130032
Epoch [1/1], Batch [81], Loss: 0.126083
Epoch [1/1], Batch [91], Loss: 0.133620
Epoch [1/1], Batch [101], Loss: 0.130832
Epoch [1/1], Batch [111], Loss: 0.125776
Epoch [1/1], Batch [121], Loss: 0.133461
Epoch [1/1], Batch [131], Loss: 0.130790
Epoch [1/1], Batch [141], Loss: 0.124385
Epoch [1/1], Batch [151], Loss: 0.130156
Epoch [1/1], Batch [161], Loss: 0.130851
Epoch [1/1], Batch [171], Loss: 0.133259
Epoch [1/1], Batch [181], Loss: 0.130656
Epoch [1/1], Batch [191], Loss: 0.122492
Epoch [1/1], Batch [201], Loss: 0.132519
Epoch [1/1], Batch [211], Loss: 0.132498
Epoch [1/1], Batch [221], Loss: 0.127576
Epoch [1/1], Batch [231], Loss: 0.127006
Epoch [1/1], Batch [241], Loss: 0.130207
Epoch [1/1], Batch [251], Loss: 0.129998
Epoch [1/1], Batch [261], Loss: 0.133874
Epoch [1/1], Batch [271], Loss: 0.125598
Epoch [1/1], Batch [281], Loss: 0.122277
Epoch [1/1], Batch [291], Loss: 0.131399
Epoch [1/1], Batch [301], Loss: 0.133597
Epoch [1/1], Batch [311], Loss: 0.128435
Epoch [1/1], Batch [321], Loss: 0.133177
Epoch [1/1], Batch [331], Loss: 0.127984
Epoch [1/1], Batch [341], Loss: 0.132399
Epoch [1/1], Batch [351], Loss: 0.131022
Epoch [1/1], Batch [361], Loss: 0.131891
Epoch [1/1], Batch [371], Loss: 0.133393
Epoch [1/1], Batch [381], Loss: 0.129192
Epoch [1/1], Batch [391], Loss: 0.133312
Epoch [1/1], Batch [401], Loss: 0.131757
Epoch [1/1], Batch [411], Loss: 0.131932
Epoch [1/1], Batch [421], Loss: 0.125965
Epoch [1/1], Batch [431], Loss: 0.131225
Epoch [1/1], Batch [441], Loss: 0.131696
Epoch [1/1], Batch [451], Loss: 0.135858
Epoch [1/1], Batch [461], Loss: 0.135144
Epoch [1/1], Batch [471], Loss: 0.133602
Epoch [1/1], Batch [481], Loss: 0.134784
Epoch [1/1], Batch [491], Loss: 0.133247
Epoch [1/1], Batch [501], Loss: 0.129825
Epoch [1/1], Batch [511], Loss: 0.133698
Epoch [1/1], Batch [521], Loss: 0.133137
Epoch [1/1], Batch [531], Loss: 0.131041
Epoch [1/1], Batch [541], Loss: 0.132638
Epoch [1/1], Batch [551], Loss: 0.137092
Epoch [1/1], Batch [561], Loss: 0.139542
Epoch [1/1], Batch [571], Loss: 0.134954
Epoch [1/1], Batch [581], Loss: 0.136137
Epoch [1/1], Batch [591], Loss: 0.131592
Epoch [1/1], Batch [601], Loss: 0.139316
Epoch [1/1], Batch [611], Loss: 0.139744
Epoch [1/1], Batch [621], Loss: 0.132951
Epoch [1/1], Batch [631], Loss: 0.126162
Epoch [1/1], Batch [641], Loss: 0.130207
Epoch [1/1], Batch [651], Loss: 0.132525
Epoch [1/1], Batch [661], Loss: 0.135937
Epoch [1/1], Batch [671], Loss: 0.141241
Epoch [1/1], Batch [681], Loss: 0.139847
Epoch [1/1], Batch [691], Loss: 0.135705
Epoch [1/1], Batch [701], Loss: 0.139039
Epoch [1/1], Batch [711], Loss: 0.137857
Epoch [1/1], Batch [721], Loss: 0.132684
Epoch [1/1], Batch [731], Loss: 0.136995
Epoch [1/1], Batch [741], Loss: 0.137888
Epoch [1/1], Batch [751], Loss: 0.141282
Epoch [1/1], Batch [761], Loss: 0.136399
Epoch [1/1], Batch [771], Loss: 0.138073
Epoch [1/1], Batch [781], Loss: 0.131346
Epoch [1/1], Batch [791], Loss: 0.132778
Epoch [1/1], Batch [801], Loss: 0.138682
Epoch [1/1], Batch [811], Loss: 0.125407
Epoch [1/1], Batch [821], Loss: 0.136694
Epoch [1/1], Batch [831], Loss: 0.140174
Epoch [1/1], Batch [841], Loss: 0.136247
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 0.1325
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 0.1385
Elapsed time: 4031.95 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 0.1386
Elapsed time: 4054.24 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 0.138722
Epoch [1/1], Batch [11], Loss: 0.144696
Epoch [1/1], Batch [21], Loss: 0.138801
Epoch [1/1], Batch [31], Loss: 0.135881
Epoch [1/1], Batch [41], Loss: 0.146826
Epoch [1/1], Batch [51], Loss: 0.136288
Epoch [1/1], Batch [61], Loss: 0.140461
Epoch [1/1], Batch [71], Loss: 0.147185
Epoch [1/1], Batch [81], Loss: 0.141083
Epoch [1/1], Batch [91], Loss: 0.143907
Epoch [1/1], Batch [101], Loss: 0.142295
Epoch [1/1], Batch [111], Loss: 0.139419
Epoch [1/1], Batch [121], Loss: 0.146728
Epoch [1/1], Batch [131], Loss: 0.135595
Epoch [1/1], Batch [141], Loss: 0.142971
Epoch [1/1], Batch [151], Loss: 0.146891
Epoch [1/1], Batch [161], Loss: 0.141899
Epoch [1/1], Batch [171], Loss: 0.144743
Epoch [1/1], Batch [181], Loss: 0.139742
Epoch [1/1], Batch [191], Loss: 0.138546
Epoch [1/1], Batch [201], Loss: 0.143261
Epoch [1/1], Batch [211], Loss: 0.144532
Epoch [1/1], Batch [221], Loss: 0.142750
Epoch [1/1], Batch [231], Loss: 0.145105
Epoch [1/1], Batch [241], Loss: 0.141596
Epoch [1/1], Batch [251], Loss: 0.142579
Epoch [1/1], Batch [261], Loss: 0.143769
Epoch [1/1], Batch [271], Loss: 0.148879
Epoch [1/1], Batch [281], Loss: 0.148204
Epoch [1/1], Batch [291], Loss: 0.145381
Epoch [1/1], Batch [301], Loss: 0.148721
Epoch [1/1], Batch [311], Loss: 0.147737
Epoch [1/1], Batch [321], Loss: 0.148038
Epoch [1/1], Batch [331], Loss: 0.145970
Epoch [1/1], Batch [341], Loss: 0.150517
Epoch [1/1], Batch [351], Loss: 0.145299
Epoch [1/1], Batch [361], Loss: 0.146047
Epoch [1/1], Batch [371], Loss: 0.150283
Epoch [1/1], Batch [381], Loss: 0.151259
Epoch [1/1], Batch [391], Loss: 0.149580
Epoch [1/1], Batch [401], Loss: 0.151933
Epoch [1/1], Batch [411], Loss: 0.156817
Epoch [1/1], Batch [421], Loss: 0.147862
Epoch [1/1], Batch [431], Loss: 0.145052
Epoch [1/1], Batch [441], Loss: 0.145101
Epoch [1/1], Batch [451], Loss: 0.140595
Epoch [1/1], Batch [461], Loss: 0.149470
Epoch [1/1], Batch [471], Loss: 0.150388
Epoch [1/1], Batch [481], Loss: 0.146412
Epoch [1/1], Batch [491], Loss: 0.154395
Epoch [1/1], Batch [501], Loss: 0.151437
Epoch [1/1], Batch [511], Loss: 0.147091
Epoch [1/1], Batch [521], Loss: 0.146385
Epoch [1/1], Batch [531], Loss: 0.147422
Epoch [1/1], Batch [541], Loss: 0.151470
Epoch [1/1], Batch [551], Loss: 0.147155
Epoch [1/1], Batch [561], Loss: 0.147829
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 0.1451
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 0.1474
Elapsed time: 4541.67 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 0.1477
Elapsed time: 4558.50 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 0.154551
Epoch [1/1], Batch [11], Loss: 0.151848
Epoch [1/1], Batch [21], Loss: 0.154364
Epoch [1/1], Batch [31], Loss: 0.156138
Epoch [1/1], Batch [41], Loss: 0.149014
Epoch [1/1], Batch [51], Loss: 0.148998
Epoch [1/1], Batch [61], Loss: 0.143155
Epoch [1/1], Batch [71], Loss: 0.148751
Epoch [1/1], Batch [81], Loss: 0.150953
Epoch [1/1], Batch [91], Loss: 0.155677
Epoch [1/1], Batch [101], Loss: 0.142177
Epoch [1/1], Batch [111], Loss: 0.155471
Epoch [1/1], Batch [121], Loss: 0.149772
Epoch [1/1], Batch [131], Loss: 0.152238
Epoch [1/1], Batch [141], Loss: 0.153535
Epoch [1/1], Batch [151], Loss: 0.153378
Epoch [1/1], Batch [161], Loss: 0.155879
Epoch [1/1], Batch [171], Loss: 0.152332
Epoch [1/1], Batch [181], Loss: 0.166065
Epoch [1/1], Batch [191], Loss: 0.160355
Epoch [1/1], Batch [201], Loss: 0.166081
Epoch [1/1], Batch [211], Loss: 0.158288
Epoch [1/1], Batch [221], Loss: 0.157366
Epoch [1/1], Batch [231], Loss: 0.158130
Epoch [1/1], Batch [241], Loss: 0.158474
Epoch [1/1], Batch [251], Loss: 0.157487
Epoch [1/1], Batch [261], Loss: 0.151563
Epoch [1/1], Batch [271], Loss: 0.149520
Epoch [1/1], Batch [281], Loss: 0.151186
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 0.1546
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 0.1513
Elapsed time: 4858.27 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 0.1521
Elapsed time: 4867.66 seconds

Training complete!
Totoal elapsed time: 4867.66 seconds
CUDA is available!
