Starting job 1013352
Training with:
    architecture = [32, 32],
    stride = 2,
    filter_size = [3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = True,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 304569.593750
Epoch [1/1], Batch [11], Loss: 95977.312500
Epoch [1/1], Batch [21], Loss: 79372.812500
Epoch [1/1], Batch [31], Loss: 73444.351562
Epoch [1/1], Batch [41], Loss: 65872.914062
Epoch [1/1], Batch [51], Loss: 64081.320312
Epoch [1/1], Batch [61], Loss: 58342.328125
Epoch [1/1], Batch [71], Loss: 59975.718750
Epoch [1/1], Batch [81], Loss: 58638.468750
Epoch [1/1], Batch [91], Loss: 57826.199219
Epoch [1/1], Batch [101], Loss: 59732.531250
Epoch [1/1], Batch [111], Loss: 55115.171875
Epoch [1/1], Batch [121], Loss: 56412.554688
Epoch [1/1], Batch [131], Loss: 55809.898438
Epoch [1/1], Batch [141], Loss: 56178.984375
Epoch [1/1], Batch [151], Loss: 57138.898438
Epoch [1/1], Batch [161], Loss: 52304.296875
Epoch [1/1], Batch [171], Loss: 56698.554688
Epoch [1/1], Batch [181], Loss: 53420.062500
Epoch [1/1], Batch [191], Loss: 52170.179688
Epoch [1/1], Batch [201], Loss: 55066.265625
Epoch [1/1], Batch [211], Loss: 56386.898438
Epoch [1/1], Batch [221], Loss: 52641.511719
Epoch [1/1], Batch [231], Loss: 52475.925781
Epoch [1/1], Batch [241], Loss: 52236.476562
Epoch [1/1], Batch [251], Loss: 53666.367188
Epoch [1/1], Batch [261], Loss: 53458.542969
Epoch [1/1], Batch [271], Loss: 55898.835938
Epoch [1/1], Batch [281], Loss: 53504.882812
Epoch [1/1], Batch [291], Loss: 54899.570312
Epoch [1/1], Batch [301], Loss: 51091.496094
Epoch [1/1], Batch [311], Loss: 51603.460938
Epoch [1/1], Batch [321], Loss: 53949.101562
Epoch [1/1], Batch [331], Loss: 55616.890625
Epoch [1/1], Batch [341], Loss: 52683.171875
Epoch [1/1], Batch [351], Loss: 54407.023438
Epoch [1/1], Batch [361], Loss: 51758.914062
Epoch [1/1], Batch [371], Loss: 53302.441406
Epoch [1/1], Batch [381], Loss: 52543.605469
Epoch [1/1], Batch [391], Loss: 54940.875000
Epoch [1/1], Batch [401], Loss: 50926.851562
Epoch [1/1], Batch [411], Loss: 51125.406250
Epoch [1/1], Batch [421], Loss: 54246.031250
Epoch [1/1], Batch [431], Loss: 55676.476562
Epoch [1/1], Batch [441], Loss: 53433.488281
Epoch [1/1], Batch [451], Loss: 54875.320312
Epoch [1/1], Batch [461], Loss: 52740.875000
Epoch [1/1], Batch [471], Loss: 52827.550781
Epoch [1/1], Batch [481], Loss: 48456.414062
Epoch [1/1], Batch [491], Loss: 50648.269531
Epoch [1/1], Batch [501], Loss: 52627.632812
Epoch [1/1], Batch [511], Loss: 52324.324219
Epoch [1/1], Batch [521], Loss: 51370.789062
Epoch [1/1], Batch [531], Loss: 51638.730469
Epoch [1/1], Batch [541], Loss: 51136.324219
Epoch [1/1], Batch [551], Loss: 52517.265625
Epoch [1/1], Batch [561], Loss: 52455.007812
Epoch [1/1], Batch [571], Loss: 52604.652344
Epoch [1/1], Batch [581], Loss: 51282.843750
Epoch [1/1], Batch [591], Loss: 53442.402344
Epoch [1/1], Batch [601], Loss: 54203.507812
Epoch [1/1], Batch [611], Loss: 49941.332031
Epoch [1/1], Batch [621], Loss: 49838.339844
Epoch [1/1], Batch [631], Loss: 51667.453125
Epoch [1/1], Batch [641], Loss: 54728.476562
Epoch [1/1], Batch [651], Loss: 53045.656250
Epoch [1/1], Batch [661], Loss: 50166.289062
Epoch [1/1], Batch [671], Loss: 53370.117188
Epoch [1/1], Batch [681], Loss: 53113.859375
Epoch [1/1], Batch [691], Loss: 52381.414062
Epoch [1/1], Batch [701], Loss: 51893.964844
Epoch [1/1], Batch [711], Loss: 50682.078125
Epoch [1/1], Batch [721], Loss: 51254.328125
Epoch [1/1], Batch [731], Loss: 52036.468750
Epoch [1/1], Batch [741], Loss: 54447.382812
Epoch [1/1], Batch [751], Loss: 53513.375000
Epoch [1/1], Batch [761], Loss: 54787.269531
Epoch [1/1], Batch [771], Loss: 52366.601562
Epoch [1/1], Batch [781], Loss: 49934.468750
Epoch [1/1], Batch [791], Loss: 52554.476562
Epoch [1/1], Batch [801], Loss: 53154.031250
Epoch [1/1], Batch [811], Loss: 51572.718750
Epoch [1/1], Batch [821], Loss: 52552.902344
Epoch [1/1], Batch [831], Loss: 50837.851562
Epoch [1/1], Batch [841], Loss: 52174.546875
Epoch [1/1], Batch [851], Loss: 50806.875000
Epoch [1/1], Batch [861], Loss: 52206.214844
Epoch [1/1], Batch [871], Loss: 52359.832031
Epoch [1/1], Batch [881], Loss: 51864.500000
Epoch [1/1], Batch [891], Loss: 50309.851562
Epoch [1/1], Batch [901], Loss: 50186.593750
Epoch [1/1], Batch [911], Loss: 51031.570312
Epoch [1/1], Batch [921], Loss: 48954.218750
Epoch [1/1], Batch [931], Loss: 51596.023438
Epoch [1/1], Batch [941], Loss: 50765.351562
Epoch [1/1], Batch [951], Loss: 51843.968750
Epoch [1/1], Batch [961], Loss: 49309.734375
Epoch [1/1], Batch [971], Loss: 50815.898438
Epoch [1/1], Batch [981], Loss: 50395.015625
Epoch [1/1], Batch [991], Loss: 48466.507812
Epoch [1/1], Batch [1001], Loss: 50724.453125
Epoch [1/1], Batch [1011], Loss: 51496.089844
Epoch [1/1], Batch [1021], Loss: 48661.097656
Epoch [1/1], Batch [1031], Loss: 50356.433594
Epoch [1/1], Batch [1041], Loss: 49049.593750
Epoch [1/1], Batch [1051], Loss: 48533.523438
Epoch [1/1], Batch [1061], Loss: 51190.070312
Epoch [1/1], Batch [1071], Loss: 51152.480469
Epoch [1/1], Batch [1081], Loss: 51135.000000
Epoch [1/1], Batch [1091], Loss: 49861.660156
Epoch [1/1], Batch [1101], Loss: 52059.101562
Epoch [1/1], Batch [1111], Loss: 50705.289062
Epoch [1/1], Batch [1121], Loss: 52699.980469
Epoch [1/1], Batch [1131], Loss: 51244.414062
Epoch [1/1], Batch [1141], Loss: 52531.222656
Epoch [1/1], Batch [1151], Loss: 53295.367188
Epoch [1/1], Batch [1161], Loss: 49465.820312
Epoch [1/1], Batch [1171], Loss: 50473.906250
Epoch [1/1], Batch [1181], Loss: 53472.203125
Epoch [1/1], Batch [1191], Loss: 50432.105469
Epoch [1/1], Batch [1201], Loss: 50594.742188
Epoch [1/1], Batch [1211], Loss: 50806.085938
Epoch [1/1], Batch [1221], Loss: 48876.433594
Epoch [1/1], Batch [1231], Loss: 51824.566406
Epoch [1/1], Batch [1241], Loss: 51286.960938
Epoch [1/1], Batch [1251], Loss: 49054.601562
Epoch [1/1], Batch [1261], Loss: 48895.273438
Epoch [1/1], Batch [1271], Loss: 50811.039062
Epoch [1/1], Batch [1281], Loss: 48370.109375
Epoch [1/1], Batch [1291], Loss: 50383.875000
Epoch [1/1], Batch [1301], Loss: 49873.562500
Epoch [1/1], Batch [1311], Loss: 49640.777344
Epoch [1/1], Batch [1321], Loss: 50797.570312
Epoch [1/1], Batch [1331], Loss: 49557.187500
Epoch [1/1], Batch [1341], Loss: 49523.531250
Epoch [1/1], Batch [1351], Loss: 49855.585938
Epoch [1/1], Batch [1361], Loss: 49966.593750
Epoch [1/1], Batch [1371], Loss: 52630.308594
Epoch [1/1], Batch [1381], Loss: 51787.425781
Epoch [1/1], Batch [1391], Loss: 51625.433594
Epoch [1/1], Batch [1401], Loss: 49500.574219
Epoch [1/1], Batch [1411], Loss: 50514.613281
Epoch [1/1], Batch [1421], Loss: 49189.285156
Epoch [1/1], Batch [1431], Loss: 52436.781250
Epoch [1/1], Batch [1441], Loss: 51480.695312
Epoch [1/1], Batch [1451], Loss: 51045.156250
Epoch [1/1], Batch [1461], Loss: 51210.835938
Epoch [1/1], Batch [1471], Loss: 47169.902344
Epoch [1/1], Batch [1481], Loss: 50683.804688
Epoch [1/1], Batch [1491], Loss: 49947.125000
Epoch [1/1], Batch [1501], Loss: 50407.562500
Epoch [1/1], Batch [1511], Loss: 50709.835938
Epoch [1/1], Batch [1521], Loss: 50409.250000
Epoch [1/1], Batch [1531], Loss: 50012.488281
Epoch [1/1], Batch [1541], Loss: 50694.589844
Epoch [1/1], Batch [1551], Loss: 49323.613281
Epoch [1/1], Batch [1561], Loss: 51594.453125
Epoch [1/1], Batch [1571], Loss: 52480.757812
Epoch [1/1], Batch [1581], Loss: 52132.074219
Epoch [1/1], Batch [1591], Loss: 50184.929688
Epoch [1/1], Batch [1601], Loss: 49950.511719
Epoch [1/1], Batch [1611], Loss: 50378.546875
Epoch [1/1], Batch [1621], Loss: 50592.054688
Epoch [1/1], Batch [1631], Loss: 49455.945312
Epoch [1/1], Batch [1641], Loss: 51411.832031
Epoch [1/1], Batch [1651], Loss: 50203.285156
Epoch [1/1], Batch [1661], Loss: 50808.179688
Epoch [1/1], Batch [1671], Loss: 49031.476562
Epoch [1/1], Batch [1681], Loss: 50347.164062
Epoch [1/1], Batch [1691], Loss: 51718.187500
Epoch [1/1], Batch [1701], Loss: 51566.875000
Epoch [1/1], Batch [1711], Loss: 48506.089844
Epoch [1/1], Batch [1721], Loss: 48902.753906
Epoch [1/1], Batch [1731], Loss: 50266.242188
Epoch [1/1], Batch [1741], Loss: 49386.390625
Epoch [1/1], Batch [1751], Loss: 51096.570312
Epoch [1/1], Batch [1761], Loss: 51983.914062
Epoch [1/1], Batch [1771], Loss: 50912.484375
Epoch [1/1], Batch [1781], Loss: 52115.308594
Epoch [1/1], Batch [1791], Loss: 50971.101562
Epoch [1/1], Batch [1801], Loss: 48928.914062
Epoch [1/1], Batch [1811], Loss: 50653.242188
Epoch [1/1], Batch [1821], Loss: 48414.601562
Epoch [1/1], Batch [1831], Loss: 49481.117188
Epoch [1/1], Batch [1841], Loss: 49073.718750
Epoch [1/1], Batch [1851], Loss: 47740.289062
Epoch [1/1], Batch [1861], Loss: 50244.679688
Epoch [1/1], Batch [1871], Loss: 50358.023438
Epoch [1/1], Batch [1881], Loss: 49997.785156
Epoch [1/1], Batch [1891], Loss: 49456.742188
Epoch [1/1], Batch [1901], Loss: 49798.835938
Epoch [1/1], Batch [1911], Loss: 49333.929688
Epoch [1/1], Batch [1921], Loss: 49099.746094
Epoch [1/1], Batch [1931], Loss: 50495.359375
Epoch [1/1], Batch [1941], Loss: 50721.140625
Epoch [1/1], Batch [1951], Loss: 48079.609375
Epoch [1/1], Batch [1961], Loss: 50676.171875
Epoch [1/1], Batch [1971], Loss: 50407.238281
Epoch [1/1], Batch [1981], Loss: 49724.609375
Epoch [1/1], Batch [1991], Loss: 50947.074219
Epoch [1/1], Batch [2001], Loss: 50265.589844
Epoch [1/1], Batch [2011], Loss: 49637.742188
Epoch [1/1], Batch [2021], Loss: 48075.843750
Epoch [1/1], Batch [2031], Loss: 52591.546875
Epoch [1/1], Batch [2041], Loss: 51471.679688
Epoch [1/1], Batch [2051], Loss: 50399.914062
Epoch [1/1], Batch [2061], Loss: 49277.437500
Epoch [1/1], Batch [2071], Loss: 48324.000000
Epoch [1/1], Batch [2081], Loss: 52348.027344
Epoch [1/1], Batch [2091], Loss: 50505.843750
Epoch [1/1], Batch [2101], Loss: 48150.914062
Epoch [1/1], Batch [2111], Loss: 48142.882812
Epoch [1/1], Batch [2121], Loss: 49809.406250
Epoch [1/1], Batch [2131], Loss: 48235.476562
Epoch [1/1], Batch [2141], Loss: 50870.882812
Epoch [1/1], Batch [2151], Loss: 48174.910156
Epoch [1/1], Batch [2161], Loss: 48943.046875
Epoch [1/1], Batch [2171], Loss: 50039.531250
Epoch [1/1], Batch [2181], Loss: 50945.171875
Epoch [1/1], Batch [2191], Loss: 52132.929688
Epoch [1/1], Batch [2201], Loss: 49808.437500
Epoch [1/1], Batch [2211], Loss: 50954.550781
Epoch [1/1], Batch [2221], Loss: 48177.718750
Epoch [1/1], Batch [2231], Loss: 49401.820312
Epoch [1/1], Batch [2241], Loss: 47485.140625
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 52199.1329
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 49214.4924
Elapsed time: 424.25 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 49817.9208
Elapsed time: 442.88 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 98557.312500
Epoch [1/1], Batch [11], Loss: 94355.195312
Epoch [1/1], Batch [21], Loss: 92753.031250
Epoch [1/1], Batch [31], Loss: 89173.703125
Epoch [1/1], Batch [41], Loss: 86435.734375
Epoch [1/1], Batch [51], Loss: 86572.015625
Epoch [1/1], Batch [61], Loss: 83487.148438
Epoch [1/1], Batch [71], Loss: 85725.093750
Epoch [1/1], Batch [81], Loss: 89863.664062
Epoch [1/1], Batch [91], Loss: 82612.375000
Epoch [1/1], Batch [101], Loss: 85737.523438
Epoch [1/1], Batch [111], Loss: 87847.250000
Epoch [1/1], Batch [121], Loss: 82804.640625
Epoch [1/1], Batch [131], Loss: 86504.218750
Epoch [1/1], Batch [141], Loss: 86092.718750
Epoch [1/1], Batch [151], Loss: 84543.523438
Epoch [1/1], Batch [161], Loss: 84217.156250
Epoch [1/1], Batch [171], Loss: 82092.148438
Epoch [1/1], Batch [181], Loss: 91717.148438
Epoch [1/1], Batch [191], Loss: 84101.937500
Epoch [1/1], Batch [201], Loss: 87179.687500
Epoch [1/1], Batch [211], Loss: 84477.851562
Epoch [1/1], Batch [221], Loss: 84559.468750
Epoch [1/1], Batch [231], Loss: 86209.484375
Epoch [1/1], Batch [241], Loss: 84280.578125
Epoch [1/1], Batch [251], Loss: 84421.906250
Epoch [1/1], Batch [261], Loss: 81253.906250
Epoch [1/1], Batch [271], Loss: 84108.867188
Epoch [1/1], Batch [281], Loss: 85180.726562
Epoch [1/1], Batch [291], Loss: 85994.210938
Epoch [1/1], Batch [301], Loss: 83166.085938
Epoch [1/1], Batch [311], Loss: 85979.265625
Epoch [1/1], Batch [321], Loss: 83006.882812
Epoch [1/1], Batch [331], Loss: 82705.093750
Epoch [1/1], Batch [341], Loss: 83077.070312
Epoch [1/1], Batch [351], Loss: 86032.812500
Epoch [1/1], Batch [361], Loss: 85039.875000
Epoch [1/1], Batch [371], Loss: 84214.710938
Epoch [1/1], Batch [381], Loss: 80669.734375
Epoch [1/1], Batch [391], Loss: 82563.171875
Epoch [1/1], Batch [401], Loss: 82817.593750
Epoch [1/1], Batch [411], Loss: 84038.382812
Epoch [1/1], Batch [421], Loss: 81362.929688
Epoch [1/1], Batch [431], Loss: 80071.570312
Epoch [1/1], Batch [441], Loss: 84970.859375
Epoch [1/1], Batch [451], Loss: 84684.226562
Epoch [1/1], Batch [461], Loss: 79687.250000
Epoch [1/1], Batch [471], Loss: 79521.320312
Epoch [1/1], Batch [481], Loss: 82914.265625
Epoch [1/1], Batch [491], Loss: 82237.070312
Epoch [1/1], Batch [501], Loss: 86439.953125
Epoch [1/1], Batch [511], Loss: 79948.000000
Epoch [1/1], Batch [521], Loss: 83994.851562
Epoch [1/1], Batch [531], Loss: 79976.273438
Epoch [1/1], Batch [541], Loss: 79950.515625
Epoch [1/1], Batch [551], Loss: 84372.812500
Epoch [1/1], Batch [561], Loss: 82259.484375
Epoch [1/1], Batch [571], Loss: 85070.429688
Epoch [1/1], Batch [581], Loss: 83749.632812
Epoch [1/1], Batch [591], Loss: 79747.671875
Epoch [1/1], Batch [601], Loss: 88667.640625
Epoch [1/1], Batch [611], Loss: 81695.234375
Epoch [1/1], Batch [621], Loss: 81947.750000
Epoch [1/1], Batch [631], Loss: 78291.242188
Epoch [1/1], Batch [641], Loss: 81319.429688
Epoch [1/1], Batch [651], Loss: 83699.820312
Epoch [1/1], Batch [661], Loss: 82145.898438
Epoch [1/1], Batch [671], Loss: 82721.640625
Epoch [1/1], Batch [681], Loss: 77951.531250
Epoch [1/1], Batch [691], Loss: 82780.601562
Epoch [1/1], Batch [701], Loss: 81030.335938
Epoch [1/1], Batch [711], Loss: 84906.039062
Epoch [1/1], Batch [721], Loss: 84618.507812
Epoch [1/1], Batch [731], Loss: 84958.968750
Epoch [1/1], Batch [741], Loss: 81995.335938
Epoch [1/1], Batch [751], Loss: 81268.734375
Epoch [1/1], Batch [761], Loss: 80056.085938
Epoch [1/1], Batch [771], Loss: 80404.031250
Epoch [1/1], Batch [781], Loss: 80908.125000
Epoch [1/1], Batch [791], Loss: 82502.593750
Epoch [1/1], Batch [801], Loss: 81393.625000
Epoch [1/1], Batch [811], Loss: 81863.148438
Epoch [1/1], Batch [821], Loss: 80694.234375
Epoch [1/1], Batch [831], Loss: 81752.953125
Epoch [1/1], Batch [841], Loss: 80234.914062
Epoch [1/1], Batch [851], Loss: 81377.101562
Epoch [1/1], Batch [861], Loss: 79100.312500
Epoch [1/1], Batch [871], Loss: 84661.609375
Epoch [1/1], Batch [881], Loss: 82428.656250
Epoch [1/1], Batch [891], Loss: 81581.351562
Epoch [1/1], Batch [901], Loss: 76706.031250
Epoch [1/1], Batch [911], Loss: 83501.765625
Epoch [1/1], Batch [921], Loss: 80467.343750
Epoch [1/1], Batch [931], Loss: 77324.859375
Epoch [1/1], Batch [941], Loss: 80974.343750
Epoch [1/1], Batch [951], Loss: 80024.460938
Epoch [1/1], Batch [961], Loss: 77941.734375
Epoch [1/1], Batch [971], Loss: 79820.921875
Epoch [1/1], Batch [981], Loss: 75907.656250
Epoch [1/1], Batch [991], Loss: 81413.570312
Epoch [1/1], Batch [1001], Loss: 78743.203125
Epoch [1/1], Batch [1011], Loss: 84153.578125
Epoch [1/1], Batch [1021], Loss: 79650.414062
Epoch [1/1], Batch [1031], Loss: 81647.820312
Epoch [1/1], Batch [1041], Loss: 80133.453125
Epoch [1/1], Batch [1051], Loss: 82971.187500
Epoch [1/1], Batch [1061], Loss: 75627.882812
Epoch [1/1], Batch [1071], Loss: 78823.265625
Epoch [1/1], Batch [1081], Loss: 79331.023438
Epoch [1/1], Batch [1091], Loss: 82646.234375
Epoch [1/1], Batch [1101], Loss: 80240.695312
Epoch [1/1], Batch [1111], Loss: 76708.140625
Epoch [1/1], Batch [1121], Loss: 82134.703125
Epoch [1/1], Batch [1131], Loss: 82731.562500
Epoch [1/1], Batch [1141], Loss: 79523.640625
Epoch [1/1], Batch [1151], Loss: 78340.304688
Epoch [1/1], Batch [1161], Loss: 80553.429688
Epoch [1/1], Batch [1171], Loss: 79468.203125
Epoch [1/1], Batch [1181], Loss: 81722.656250
Epoch [1/1], Batch [1191], Loss: 81228.203125
Epoch [1/1], Batch [1201], Loss: 81423.210938
Epoch [1/1], Batch [1211], Loss: 81853.156250
Epoch [1/1], Batch [1221], Loss: 80670.015625
Epoch [1/1], Batch [1231], Loss: 79285.046875
Epoch [1/1], Batch [1241], Loss: 79559.992188
Epoch [1/1], Batch [1251], Loss: 78280.015625
Epoch [1/1], Batch [1261], Loss: 78522.437500
Epoch [1/1], Batch [1271], Loss: 75532.421875
Epoch [1/1], Batch [1281], Loss: 81545.500000
Epoch [1/1], Batch [1291], Loss: 78537.515625
Epoch [1/1], Batch [1301], Loss: 79675.281250
Epoch [1/1], Batch [1311], Loss: 79252.687500
Epoch [1/1], Batch [1321], Loss: 78762.523438
Epoch [1/1], Batch [1331], Loss: 77068.562500
Epoch [1/1], Batch [1341], Loss: 76278.437500
Epoch [1/1], Batch [1351], Loss: 78476.171875
Epoch [1/1], Batch [1361], Loss: 79164.859375
Epoch [1/1], Batch [1371], Loss: 78585.171875
Epoch [1/1], Batch [1381], Loss: 77719.273438
Epoch [1/1], Batch [1391], Loss: 74425.585938
Epoch [1/1], Batch [1401], Loss: 80650.210938
Epoch [1/1], Batch [1411], Loss: 81519.437500
Epoch [1/1], Batch [1421], Loss: 80943.000000
Epoch [1/1], Batch [1431], Loss: 81826.703125
Epoch [1/1], Batch [1441], Loss: 80675.203125
Epoch [1/1], Batch [1451], Loss: 79622.617188
Epoch [1/1], Batch [1461], Loss: 78902.734375
Epoch [1/1], Batch [1471], Loss: 77793.460938
Epoch [1/1], Batch [1481], Loss: 79264.968750
Epoch [1/1], Batch [1491], Loss: 81876.218750
Epoch [1/1], Batch [1501], Loss: 78070.078125
Epoch [1/1], Batch [1511], Loss: 79634.609375
Epoch [1/1], Batch [1521], Loss: 79185.062500
Epoch [1/1], Batch [1531], Loss: 78409.218750
Epoch [1/1], Batch [1541], Loss: 81099.898438
Epoch [1/1], Batch [1551], Loss: 79504.414062
Epoch [1/1], Batch [1561], Loss: 79320.320312
Epoch [1/1], Batch [1571], Loss: 80399.750000
Epoch [1/1], Batch [1581], Loss: 79519.078125
Epoch [1/1], Batch [1591], Loss: 78296.609375
Epoch [1/1], Batch [1601], Loss: 80621.640625
Epoch [1/1], Batch [1611], Loss: 79935.695312
Epoch [1/1], Batch [1621], Loss: 77385.734375
Epoch [1/1], Batch [1631], Loss: 77169.640625
Epoch [1/1], Batch [1641], Loss: 78892.867188
Epoch [1/1], Batch [1651], Loss: 78084.281250
Epoch [1/1], Batch [1661], Loss: 82184.265625
Epoch [1/1], Batch [1671], Loss: 76778.218750
Epoch [1/1], Batch [1681], Loss: 81099.484375
Epoch [1/1], Batch [1691], Loss: 80748.351562
Epoch [1/1], Batch [1701], Loss: 81738.015625
Epoch [1/1], Batch [1711], Loss: 83994.726562
Epoch [1/1], Batch [1721], Loss: 77070.734375
Epoch [1/1], Batch [1731], Loss: 79425.812500
Epoch [1/1], Batch [1741], Loss: 81271.156250
Epoch [1/1], Batch [1751], Loss: 81484.906250
Epoch [1/1], Batch [1761], Loss: 79280.125000
Epoch [1/1], Batch [1771], Loss: 78918.039062
Epoch [1/1], Batch [1781], Loss: 76514.156250
Epoch [1/1], Batch [1791], Loss: 77400.218750
Epoch [1/1], Batch [1801], Loss: 77604.562500
Epoch [1/1], Batch [1811], Loss: 78249.734375
Epoch [1/1], Batch [1821], Loss: 81550.390625
Epoch [1/1], Batch [1831], Loss: 76815.093750
Epoch [1/1], Batch [1841], Loss: 76970.593750
Epoch [1/1], Batch [1851], Loss: 77459.937500
Epoch [1/1], Batch [1861], Loss: 80370.734375
Epoch [1/1], Batch [1871], Loss: 76547.531250
Epoch [1/1], Batch [1881], Loss: 77090.460938
Epoch [1/1], Batch [1891], Loss: 80631.007812
Epoch [1/1], Batch [1901], Loss: 81903.046875
Epoch [1/1], Batch [1911], Loss: 81650.062500
Epoch [1/1], Batch [1921], Loss: 79902.132812
Epoch [1/1], Batch [1931], Loss: 80060.296875
Epoch [1/1], Batch [1941], Loss: 77844.234375
Epoch [1/1], Batch [1951], Loss: 77425.304688
Epoch [1/1], Batch [1961], Loss: 75479.750000
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 81300.3465
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 78335.3880
Elapsed time: 967.56 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 79034.7358
Elapsed time: 989.62 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 120748.976562
Epoch [1/1], Batch [11], Loss: 117869.625000
Epoch [1/1], Batch [21], Loss: 116356.031250
Epoch [1/1], Batch [31], Loss: 113271.445312
Epoch [1/1], Batch [41], Loss: 116303.945312
Epoch [1/1], Batch [51], Loss: 117563.890625
Epoch [1/1], Batch [61], Loss: 110010.554688
Epoch [1/1], Batch [71], Loss: 110308.828125
Epoch [1/1], Batch [81], Loss: 110680.789062
Epoch [1/1], Batch [91], Loss: 110419.890625
Epoch [1/1], Batch [101], Loss: 114964.507812
Epoch [1/1], Batch [111], Loss: 108810.664062
Epoch [1/1], Batch [121], Loss: 113419.703125
Epoch [1/1], Batch [131], Loss: 111820.507812
Epoch [1/1], Batch [141], Loss: 108501.945312
Epoch [1/1], Batch [151], Loss: 117193.796875
Epoch [1/1], Batch [161], Loss: 114875.062500
Epoch [1/1], Batch [171], Loss: 111552.835938
Epoch [1/1], Batch [181], Loss: 109708.570312
Epoch [1/1], Batch [191], Loss: 107495.031250
Epoch [1/1], Batch [201], Loss: 113983.992188
Epoch [1/1], Batch [211], Loss: 110258.640625
Epoch [1/1], Batch [221], Loss: 110037.671875
Epoch [1/1], Batch [231], Loss: 112765.203125
Epoch [1/1], Batch [241], Loss: 105692.515625
Epoch [1/1], Batch [251], Loss: 110352.359375
Epoch [1/1], Batch [261], Loss: 113693.343750
Epoch [1/1], Batch [271], Loss: 112048.476562
Epoch [1/1], Batch [281], Loss: 106553.242188
Epoch [1/1], Batch [291], Loss: 110850.296875
Epoch [1/1], Batch [301], Loss: 109281.781250
Epoch [1/1], Batch [311], Loss: 115316.062500
Epoch [1/1], Batch [321], Loss: 107381.375000
Epoch [1/1], Batch [331], Loss: 107050.367188
Epoch [1/1], Batch [341], Loss: 112390.101562
Epoch [1/1], Batch [351], Loss: 112872.312500
Epoch [1/1], Batch [361], Loss: 109545.500000
Epoch [1/1], Batch [371], Loss: 107283.867188
Epoch [1/1], Batch [381], Loss: 110288.312500
Epoch [1/1], Batch [391], Loss: 115123.218750
Epoch [1/1], Batch [401], Loss: 111461.906250
Epoch [1/1], Batch [411], Loss: 107247.671875
Epoch [1/1], Batch [421], Loss: 111626.500000
Epoch [1/1], Batch [431], Loss: 108449.734375
Epoch [1/1], Batch [441], Loss: 107854.812500
Epoch [1/1], Batch [451], Loss: 110513.390625
Epoch [1/1], Batch [461], Loss: 108967.484375
Epoch [1/1], Batch [471], Loss: 116465.093750
Epoch [1/1], Batch [481], Loss: 114425.156250
Epoch [1/1], Batch [491], Loss: 109671.257812
Epoch [1/1], Batch [501], Loss: 108976.757812
Epoch [1/1], Batch [511], Loss: 104963.195312
Epoch [1/1], Batch [521], Loss: 108691.609375
Epoch [1/1], Batch [531], Loss: 104885.593750
Epoch [1/1], Batch [541], Loss: 106647.984375
Epoch [1/1], Batch [551], Loss: 111451.039062
Epoch [1/1], Batch [561], Loss: 107329.093750
Epoch [1/1], Batch [571], Loss: 108999.234375
Epoch [1/1], Batch [581], Loss: 103572.328125
Epoch [1/1], Batch [591], Loss: 107075.875000
Epoch [1/1], Batch [601], Loss: 107706.875000
Epoch [1/1], Batch [611], Loss: 106978.328125
Epoch [1/1], Batch [621], Loss: 111604.281250
Epoch [1/1], Batch [631], Loss: 107886.164062
Epoch [1/1], Batch [641], Loss: 113301.664062
Epoch [1/1], Batch [651], Loss: 111159.437500
Epoch [1/1], Batch [661], Loss: 109957.546875
Epoch [1/1], Batch [671], Loss: 106488.968750
Epoch [1/1], Batch [681], Loss: 110828.828125
Epoch [1/1], Batch [691], Loss: 108754.031250
Epoch [1/1], Batch [701], Loss: 106978.304688
Epoch [1/1], Batch [711], Loss: 111456.250000
Epoch [1/1], Batch [721], Loss: 105633.375000
Epoch [1/1], Batch [731], Loss: 111879.578125
Epoch [1/1], Batch [741], Loss: 107859.015625
Epoch [1/1], Batch [751], Loss: 112823.593750
Epoch [1/1], Batch [761], Loss: 110307.671875
Epoch [1/1], Batch [771], Loss: 108570.664062
Epoch [1/1], Batch [781], Loss: 102670.210938
Epoch [1/1], Batch [791], Loss: 114439.117188
Epoch [1/1], Batch [801], Loss: 109357.484375
Epoch [1/1], Batch [811], Loss: 106513.078125
Epoch [1/1], Batch [821], Loss: 113552.406250
Epoch [1/1], Batch [831], Loss: 109109.984375
Epoch [1/1], Batch [841], Loss: 109387.289062
Epoch [1/1], Batch [851], Loss: 107657.546875
Epoch [1/1], Batch [861], Loss: 104927.765625
Epoch [1/1], Batch [871], Loss: 109080.703125
Epoch [1/1], Batch [881], Loss: 106309.546875
Epoch [1/1], Batch [891], Loss: 109754.890625
Epoch [1/1], Batch [901], Loss: 106934.796875
Epoch [1/1], Batch [911], Loss: 113094.312500
Epoch [1/1], Batch [921], Loss: 109536.000000
Epoch [1/1], Batch [931], Loss: 111415.210938
Epoch [1/1], Batch [941], Loss: 105756.835938
Epoch [1/1], Batch [951], Loss: 113003.273438
Epoch [1/1], Batch [961], Loss: 107035.859375
Epoch [1/1], Batch [971], Loss: 106609.125000
Epoch [1/1], Batch [981], Loss: 109461.914062
Epoch [1/1], Batch [991], Loss: 114890.851562
Epoch [1/1], Batch [1001], Loss: 105884.734375
Epoch [1/1], Batch [1011], Loss: 104536.765625
Epoch [1/1], Batch [1021], Loss: 105390.125000
Epoch [1/1], Batch [1031], Loss: 108598.000000
Epoch [1/1], Batch [1041], Loss: 108933.875000
Epoch [1/1], Batch [1051], Loss: 111946.460938
Epoch [1/1], Batch [1061], Loss: 108999.031250
Epoch [1/1], Batch [1071], Loss: 108481.781250
Epoch [1/1], Batch [1081], Loss: 107241.960938
Epoch [1/1], Batch [1091], Loss: 110835.632812
Epoch [1/1], Batch [1101], Loss: 103362.171875
Epoch [1/1], Batch [1111], Loss: 111080.531250
Epoch [1/1], Batch [1121], Loss: 111250.718750
Epoch [1/1], Batch [1131], Loss: 104848.796875
Epoch [1/1], Batch [1141], Loss: 105498.664062
Epoch [1/1], Batch [1151], Loss: 110362.421875
Epoch [1/1], Batch [1161], Loss: 102414.203125
Epoch [1/1], Batch [1171], Loss: 104478.125000
Epoch [1/1], Batch [1181], Loss: 106490.609375
Epoch [1/1], Batch [1191], Loss: 108885.937500
Epoch [1/1], Batch [1201], Loss: 110490.093750
Epoch [1/1], Batch [1211], Loss: 108743.117188
Epoch [1/1], Batch [1221], Loss: 111125.812500
Epoch [1/1], Batch [1231], Loss: 110092.843750
Epoch [1/1], Batch [1241], Loss: 106114.921875
Epoch [1/1], Batch [1251], Loss: 109243.468750
Epoch [1/1], Batch [1261], Loss: 107305.617188
Epoch [1/1], Batch [1271], Loss: 109766.726562
Epoch [1/1], Batch [1281], Loss: 108756.296875
Epoch [1/1], Batch [1291], Loss: 108513.125000
Epoch [1/1], Batch [1301], Loss: 109942.117188
Epoch [1/1], Batch [1311], Loss: 111643.734375
Epoch [1/1], Batch [1321], Loss: 109413.226562
Epoch [1/1], Batch [1331], Loss: 105728.210938
Epoch [1/1], Batch [1341], Loss: 107482.125000
Epoch [1/1], Batch [1351], Loss: 108931.906250
Epoch [1/1], Batch [1361], Loss: 105577.875000
Epoch [1/1], Batch [1371], Loss: 105388.125000
Epoch [1/1], Batch [1381], Loss: 113918.851562
Epoch [1/1], Batch [1391], Loss: 111328.695312
Epoch [1/1], Batch [1401], Loss: 108967.515625
Epoch [1/1], Batch [1411], Loss: 107611.078125
Epoch [1/1], Batch [1421], Loss: 107950.468750
Epoch [1/1], Batch [1431], Loss: 108530.437500
Epoch [1/1], Batch [1441], Loss: 112333.953125
Epoch [1/1], Batch [1451], Loss: 105201.328125
Epoch [1/1], Batch [1461], Loss: 108535.281250
Epoch [1/1], Batch [1471], Loss: 106397.281250
Epoch [1/1], Batch [1481], Loss: 103646.023438
Epoch [1/1], Batch [1491], Loss: 104582.570312
Epoch [1/1], Batch [1501], Loss: 106691.625000
Epoch [1/1], Batch [1511], Loss: 105699.140625
Epoch [1/1], Batch [1521], Loss: 106394.859375
Epoch [1/1], Batch [1531], Loss: 107719.242188
Epoch [1/1], Batch [1541], Loss: 109314.023438
Epoch [1/1], Batch [1551], Loss: 107518.968750
Epoch [1/1], Batch [1561], Loss: 112809.484375
Epoch [1/1], Batch [1571], Loss: 114674.171875
Epoch [1/1], Batch [1581], Loss: 104285.882812
Epoch [1/1], Batch [1591], Loss: 106095.328125
Epoch [1/1], Batch [1601], Loss: 110452.859375
Epoch [1/1], Batch [1611], Loss: 109097.179688
Epoch [1/1], Batch [1621], Loss: 105006.218750
Epoch [1/1], Batch [1631], Loss: 106290.750000
Epoch [1/1], Batch [1641], Loss: 104159.179688
Epoch [1/1], Batch [1651], Loss: 104215.828125
Epoch [1/1], Batch [1661], Loss: 111619.781250
Epoch [1/1], Batch [1671], Loss: 104817.695312
Epoch [1/1], Batch [1681], Loss: 111781.484375
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 109182.0423
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 107435.9417
Elapsed time: 1568.52 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 108877.4587
Elapsed time: 1592.35 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 150203.828125
Epoch [1/1], Batch [11], Loss: 148743.453125
Epoch [1/1], Batch [21], Loss: 144495.140625
Epoch [1/1], Batch [31], Loss: 142190.531250
Epoch [1/1], Batch [41], Loss: 146245.515625
Epoch [1/1], Batch [51], Loss: 146886.359375
Epoch [1/1], Batch [61], Loss: 139424.375000
Epoch [1/1], Batch [71], Loss: 141825.687500
Epoch [1/1], Batch [81], Loss: 137419.750000
Epoch [1/1], Batch [91], Loss: 141898.875000
Epoch [1/1], Batch [101], Loss: 139778.734375
Epoch [1/1], Batch [111], Loss: 139659.015625
Epoch [1/1], Batch [121], Loss: 132087.390625
Epoch [1/1], Batch [131], Loss: 138673.578125
Epoch [1/1], Batch [141], Loss: 132554.625000
Epoch [1/1], Batch [151], Loss: 142212.531250
Epoch [1/1], Batch [161], Loss: 134149.578125
Epoch [1/1], Batch [171], Loss: 136178.625000
Epoch [1/1], Batch [181], Loss: 138727.937500
Epoch [1/1], Batch [191], Loss: 134035.343750
Epoch [1/1], Batch [201], Loss: 145661.937500
Epoch [1/1], Batch [211], Loss: 134383.671875
Epoch [1/1], Batch [221], Loss: 141835.000000
Epoch [1/1], Batch [231], Loss: 137052.609375
Epoch [1/1], Batch [241], Loss: 134252.312500
Epoch [1/1], Batch [251], Loss: 134637.515625
Epoch [1/1], Batch [261], Loss: 141221.187500
Epoch [1/1], Batch [271], Loss: 140391.187500
Epoch [1/1], Batch [281], Loss: 135319.296875
Epoch [1/1], Batch [291], Loss: 146529.687500
Epoch [1/1], Batch [301], Loss: 131793.281250
Epoch [1/1], Batch [311], Loss: 137323.500000
Epoch [1/1], Batch [321], Loss: 138222.734375
Epoch [1/1], Batch [331], Loss: 138379.000000
Epoch [1/1], Batch [341], Loss: 134752.000000
Epoch [1/1], Batch [351], Loss: 132921.750000
Epoch [1/1], Batch [361], Loss: 142004.625000
Epoch [1/1], Batch [371], Loss: 135209.359375
Epoch [1/1], Batch [381], Loss: 140821.937500
Epoch [1/1], Batch [391], Loss: 134237.093750
Epoch [1/1], Batch [401], Loss: 138703.937500
Epoch [1/1], Batch [411], Loss: 135744.062500
Epoch [1/1], Batch [421], Loss: 136581.218750
Epoch [1/1], Batch [431], Loss: 133330.937500
Epoch [1/1], Batch [441], Loss: 132295.937500
Epoch [1/1], Batch [451], Loss: 141459.250000
Epoch [1/1], Batch [461], Loss: 134775.046875
Epoch [1/1], Batch [471], Loss: 134130.750000
Epoch [1/1], Batch [481], Loss: 145420.375000
Epoch [1/1], Batch [491], Loss: 137692.890625
Epoch [1/1], Batch [501], Loss: 136814.484375
Epoch [1/1], Batch [511], Loss: 135546.531250
Epoch [1/1], Batch [521], Loss: 139372.656250
Epoch [1/1], Batch [531], Loss: 130941.500000
Epoch [1/1], Batch [541], Loss: 139445.125000
Epoch [1/1], Batch [551], Loss: 139677.687500
Epoch [1/1], Batch [561], Loss: 137666.312500
Epoch [1/1], Batch [571], Loss: 133461.062500
Epoch [1/1], Batch [581], Loss: 135210.531250
Epoch [1/1], Batch [591], Loss: 135801.843750
Epoch [1/1], Batch [601], Loss: 136313.437500
Epoch [1/1], Batch [611], Loss: 133632.000000
Epoch [1/1], Batch [621], Loss: 140409.437500
Epoch [1/1], Batch [631], Loss: 138487.890625
Epoch [1/1], Batch [641], Loss: 135557.656250
Epoch [1/1], Batch [651], Loss: 140476.453125
Epoch [1/1], Batch [661], Loss: 136242.187500
Epoch [1/1], Batch [671], Loss: 141206.812500
Epoch [1/1], Batch [681], Loss: 139761.593750
Epoch [1/1], Batch [691], Loss: 135680.718750
Epoch [1/1], Batch [701], Loss: 133894.500000
Epoch [1/1], Batch [711], Loss: 139247.921875
Epoch [1/1], Batch [721], Loss: 137767.953125
Epoch [1/1], Batch [731], Loss: 139327.750000
Epoch [1/1], Batch [741], Loss: 134029.765625
Epoch [1/1], Batch [751], Loss: 132761.562500
Epoch [1/1], Batch [761], Loss: 133542.484375
Epoch [1/1], Batch [771], Loss: 132570.343750
Epoch [1/1], Batch [781], Loss: 132244.781250
Epoch [1/1], Batch [791], Loss: 133877.546875
Epoch [1/1], Batch [801], Loss: 137653.093750
Epoch [1/1], Batch [811], Loss: 134231.312500
Epoch [1/1], Batch [821], Loss: 137795.359375
Epoch [1/1], Batch [831], Loss: 140148.093750
Epoch [1/1], Batch [841], Loss: 136118.937500
Epoch [1/1], Batch [851], Loss: 134265.250000
Epoch [1/1], Batch [861], Loss: 133432.781250
Epoch [1/1], Batch [871], Loss: 139957.078125
Epoch [1/1], Batch [881], Loss: 132550.687500
Epoch [1/1], Batch [891], Loss: 139001.359375
Epoch [1/1], Batch [901], Loss: 133446.093750
Epoch [1/1], Batch [911], Loss: 133058.171875
Epoch [1/1], Batch [921], Loss: 139092.531250
Epoch [1/1], Batch [931], Loss: 138965.906250
Epoch [1/1], Batch [941], Loss: 138650.781250
Epoch [1/1], Batch [951], Loss: 136706.781250
Epoch [1/1], Batch [961], Loss: 137519.921875
Epoch [1/1], Batch [971], Loss: 136866.359375
Epoch [1/1], Batch [981], Loss: 137280.656250
Epoch [1/1], Batch [991], Loss: 136876.015625
Epoch [1/1], Batch [1001], Loss: 137035.125000
Epoch [1/1], Batch [1011], Loss: 133454.406250
Epoch [1/1], Batch [1021], Loss: 137448.281250
Epoch [1/1], Batch [1031], Loss: 129763.328125
Epoch [1/1], Batch [1041], Loss: 137495.765625
Epoch [1/1], Batch [1051], Loss: 135180.093750
Epoch [1/1], Batch [1061], Loss: 135436.234375
Epoch [1/1], Batch [1071], Loss: 133998.109375
Epoch [1/1], Batch [1081], Loss: 136372.656250
Epoch [1/1], Batch [1091], Loss: 136278.921875
Epoch [1/1], Batch [1101], Loss: 137910.703125
Epoch [1/1], Batch [1111], Loss: 131045.250000
Epoch [1/1], Batch [1121], Loss: 133886.921875
Epoch [1/1], Batch [1131], Loss: 134085.781250
Epoch [1/1], Batch [1141], Loss: 129610.187500
Epoch [1/1], Batch [1151], Loss: 129234.687500
Epoch [1/1], Batch [1161], Loss: 131947.562500
Epoch [1/1], Batch [1171], Loss: 140127.468750
Epoch [1/1], Batch [1181], Loss: 140787.765625
Epoch [1/1], Batch [1191], Loss: 134690.906250
Epoch [1/1], Batch [1201], Loss: 135516.296875
Epoch [1/1], Batch [1211], Loss: 134947.500000
Epoch [1/1], Batch [1221], Loss: 139275.656250
Epoch [1/1], Batch [1231], Loss: 136533.437500
Epoch [1/1], Batch [1241], Loss: 132671.109375
Epoch [1/1], Batch [1251], Loss: 136713.656250
Epoch [1/1], Batch [1261], Loss: 141172.078125
Epoch [1/1], Batch [1271], Loss: 134508.546875
Epoch [1/1], Batch [1281], Loss: 139292.546875
Epoch [1/1], Batch [1291], Loss: 127176.046875
Epoch [1/1], Batch [1301], Loss: 137440.875000
Epoch [1/1], Batch [1311], Loss: 132850.812500
Epoch [1/1], Batch [1321], Loss: 137213.625000
Epoch [1/1], Batch [1331], Loss: 134826.031250
Epoch [1/1], Batch [1341], Loss: 135934.125000
Epoch [1/1], Batch [1351], Loss: 130429.109375
Epoch [1/1], Batch [1361], Loss: 133375.390625
Epoch [1/1], Batch [1371], Loss: 136825.937500
Epoch [1/1], Batch [1381], Loss: 135194.046875
Epoch [1/1], Batch [1391], Loss: 134330.125000
Epoch [1/1], Batch [1401], Loss: 137203.406250
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 137045.7415
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 133993.5795
Elapsed time: 2187.19 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 135994.4271
Elapsed time: 2211.17 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 178599.906250
Epoch [1/1], Batch [11], Loss: 168353.156250
Epoch [1/1], Batch [21], Loss: 167254.203125
Epoch [1/1], Batch [31], Loss: 161089.125000
Epoch [1/1], Batch [41], Loss: 170767.921875
Epoch [1/1], Batch [51], Loss: 161819.734375
Epoch [1/1], Batch [61], Loss: 170956.875000
Epoch [1/1], Batch [71], Loss: 170374.906250
Epoch [1/1], Batch [81], Loss: 169751.453125
Epoch [1/1], Batch [91], Loss: 163451.312500
Epoch [1/1], Batch [101], Loss: 171012.250000
Epoch [1/1], Batch [111], Loss: 161843.156250
Epoch [1/1], Batch [121], Loss: 173384.500000
Epoch [1/1], Batch [131], Loss: 169845.437500
Epoch [1/1], Batch [141], Loss: 168031.000000
Epoch [1/1], Batch [151], Loss: 164041.156250
Epoch [1/1], Batch [161], Loss: 163787.125000
Epoch [1/1], Batch [171], Loss: 164204.015625
Epoch [1/1], Batch [181], Loss: 164692.093750
Epoch [1/1], Batch [191], Loss: 163222.437500
Epoch [1/1], Batch [201], Loss: 162246.281250
Epoch [1/1], Batch [211], Loss: 165549.531250
Epoch [1/1], Batch [221], Loss: 164310.968750
Epoch [1/1], Batch [231], Loss: 161441.187500
Epoch [1/1], Batch [241], Loss: 170066.906250
Epoch [1/1], Batch [251], Loss: 167763.140625
Epoch [1/1], Batch [261], Loss: 163151.375000
Epoch [1/1], Batch [271], Loss: 161574.328125
Epoch [1/1], Batch [281], Loss: 169762.812500
Epoch [1/1], Batch [291], Loss: 172891.343750
Epoch [1/1], Batch [301], Loss: 165705.468750
Epoch [1/1], Batch [311], Loss: 165209.937500
Epoch [1/1], Batch [321], Loss: 171065.562500
Epoch [1/1], Batch [331], Loss: 168437.078125
Epoch [1/1], Batch [341], Loss: 168912.078125
Epoch [1/1], Batch [351], Loss: 165004.421875
Epoch [1/1], Batch [361], Loss: 161406.359375
Epoch [1/1], Batch [371], Loss: 159775.875000
Epoch [1/1], Batch [381], Loss: 163091.296875
Epoch [1/1], Batch [391], Loss: 164792.593750
Epoch [1/1], Batch [401], Loss: 165835.406250
Epoch [1/1], Batch [411], Loss: 161101.656250
Epoch [1/1], Batch [421], Loss: 166287.937500
Epoch [1/1], Batch [431], Loss: 170275.656250
Epoch [1/1], Batch [441], Loss: 159279.359375
Epoch [1/1], Batch [451], Loss: 164947.828125
Epoch [1/1], Batch [461], Loss: 170654.265625
Epoch [1/1], Batch [471], Loss: 162630.625000
Epoch [1/1], Batch [481], Loss: 157720.828125
Epoch [1/1], Batch [491], Loss: 166293.156250
Epoch [1/1], Batch [501], Loss: 175049.812500
Epoch [1/1], Batch [511], Loss: 164874.140625
Epoch [1/1], Batch [521], Loss: 168768.687500
Epoch [1/1], Batch [531], Loss: 161131.781250
Epoch [1/1], Batch [541], Loss: 166527.250000
Epoch [1/1], Batch [551], Loss: 162510.125000
Epoch [1/1], Batch [561], Loss: 165049.546875
Epoch [1/1], Batch [571], Loss: 160213.781250
Epoch [1/1], Batch [581], Loss: 160300.343750
Epoch [1/1], Batch [591], Loss: 161194.343750
Epoch [1/1], Batch [601], Loss: 163780.421875
Epoch [1/1], Batch [611], Loss: 163417.812500
Epoch [1/1], Batch [621], Loss: 161019.343750
Epoch [1/1], Batch [631], Loss: 157840.640625
Epoch [1/1], Batch [641], Loss: 171939.375000
Epoch [1/1], Batch [651], Loss: 169733.031250
Epoch [1/1], Batch [661], Loss: 165478.812500
Epoch [1/1], Batch [671], Loss: 159606.000000
Epoch [1/1], Batch [681], Loss: 164637.953125
Epoch [1/1], Batch [691], Loss: 158838.500000
Epoch [1/1], Batch [701], Loss: 168134.187500
Epoch [1/1], Batch [711], Loss: 163873.718750
Epoch [1/1], Batch [721], Loss: 173595.015625
Epoch [1/1], Batch [731], Loss: 171914.968750
Epoch [1/1], Batch [741], Loss: 168801.156250
Epoch [1/1], Batch [751], Loss: 162159.187500
Epoch [1/1], Batch [761], Loss: 155878.312500
Epoch [1/1], Batch [771], Loss: 164796.562500
Epoch [1/1], Batch [781], Loss: 166505.468750
Epoch [1/1], Batch [791], Loss: 159163.812500
Epoch [1/1], Batch [801], Loss: 163943.140625
Epoch [1/1], Batch [811], Loss: 163838.468750
Epoch [1/1], Batch [821], Loss: 161887.000000
Epoch [1/1], Batch [831], Loss: 159323.921875
Epoch [1/1], Batch [841], Loss: 160064.234375
Epoch [1/1], Batch [851], Loss: 167080.734375
Epoch [1/1], Batch [861], Loss: 165439.812500
Epoch [1/1], Batch [871], Loss: 163477.343750
Epoch [1/1], Batch [881], Loss: 165542.718750
Epoch [1/1], Batch [891], Loss: 168089.562500
Epoch [1/1], Batch [901], Loss: 157825.687500
Epoch [1/1], Batch [911], Loss: 160388.656250
Epoch [1/1], Batch [921], Loss: 164513.093750
Epoch [1/1], Batch [931], Loss: 162147.093750
Epoch [1/1], Batch [941], Loss: 160840.593750
Epoch [1/1], Batch [951], Loss: 164725.531250
Epoch [1/1], Batch [961], Loss: 161087.296875
Epoch [1/1], Batch [971], Loss: 160135.375000
Epoch [1/1], Batch [981], Loss: 165347.156250
Epoch [1/1], Batch [991], Loss: 154521.937500
Epoch [1/1], Batch [1001], Loss: 166231.156250
Epoch [1/1], Batch [1011], Loss: 158559.031250
Epoch [1/1], Batch [1021], Loss: 166435.125000
Epoch [1/1], Batch [1031], Loss: 162256.078125
Epoch [1/1], Batch [1041], Loss: 162131.078125
Epoch [1/1], Batch [1051], Loss: 163671.781250
Epoch [1/1], Batch [1061], Loss: 167109.859375
Epoch [1/1], Batch [1071], Loss: 162639.375000
Epoch [1/1], Batch [1081], Loss: 165267.281250
Epoch [1/1], Batch [1091], Loss: 155125.484375
Epoch [1/1], Batch [1101], Loss: 157785.328125
Epoch [1/1], Batch [1111], Loss: 164917.015625
Epoch [1/1], Batch [1121], Loss: 165182.562500
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 164934.9256
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 161056.5205
Elapsed time: 2773.74 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 163455.8109
Elapsed time: 2796.27 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 197568.328125
Epoch [1/1], Batch [11], Loss: 184321.140625
Epoch [1/1], Batch [21], Loss: 191193.265625
Epoch [1/1], Batch [31], Loss: 197826.062500
Epoch [1/1], Batch [41], Loss: 198211.156250
Epoch [1/1], Batch [51], Loss: 196413.468750
Epoch [1/1], Batch [61], Loss: 195930.953125
Epoch [1/1], Batch [71], Loss: 201721.062500
Epoch [1/1], Batch [81], Loss: 185303.000000
Epoch [1/1], Batch [91], Loss: 187633.875000
Epoch [1/1], Batch [101], Loss: 198904.437500
Epoch [1/1], Batch [111], Loss: 198307.484375
Epoch [1/1], Batch [121], Loss: 191250.343750
Epoch [1/1], Batch [131], Loss: 194985.656250
Epoch [1/1], Batch [141], Loss: 208396.656250
Epoch [1/1], Batch [151], Loss: 191873.750000
Epoch [1/1], Batch [161], Loss: 189034.765625
Epoch [1/1], Batch [171], Loss: 192990.562500
Epoch [1/1], Batch [181], Loss: 193634.687500
Epoch [1/1], Batch [191], Loss: 193450.031250
Epoch [1/1], Batch [201], Loss: 191654.453125
Epoch [1/1], Batch [211], Loss: 189450.281250
Epoch [1/1], Batch [221], Loss: 202702.406250
Epoch [1/1], Batch [231], Loss: 193800.656250
Epoch [1/1], Batch [241], Loss: 193639.703125
Epoch [1/1], Batch [251], Loss: 195247.843750
Epoch [1/1], Batch [261], Loss: 191853.234375
Epoch [1/1], Batch [271], Loss: 184860.218750
Epoch [1/1], Batch [281], Loss: 190301.156250
Epoch [1/1], Batch [291], Loss: 190861.546875
Epoch [1/1], Batch [301], Loss: 196073.656250
Epoch [1/1], Batch [311], Loss: 197436.562500
Epoch [1/1], Batch [321], Loss: 190943.062500
Epoch [1/1], Batch [331], Loss: 196852.250000
Epoch [1/1], Batch [341], Loss: 188947.343750
Epoch [1/1], Batch [351], Loss: 197744.765625
Epoch [1/1], Batch [361], Loss: 194865.937500
Epoch [1/1], Batch [371], Loss: 191344.578125
Epoch [1/1], Batch [381], Loss: 191898.265625
Epoch [1/1], Batch [391], Loss: 192218.046875
Epoch [1/1], Batch [401], Loss: 195839.546875
Epoch [1/1], Batch [411], Loss: 189486.406250
Epoch [1/1], Batch [421], Loss: 195163.171875
Epoch [1/1], Batch [431], Loss: 199298.656250
Epoch [1/1], Batch [441], Loss: 198100.281250
Epoch [1/1], Batch [451], Loss: 188255.484375
Epoch [1/1], Batch [461], Loss: 199577.687500
Epoch [1/1], Batch [471], Loss: 190179.968750
Epoch [1/1], Batch [481], Loss: 185316.093750
Epoch [1/1], Batch [491], Loss: 194235.000000
Epoch [1/1], Batch [501], Loss: 194802.734375
Epoch [1/1], Batch [511], Loss: 192771.343750
Epoch [1/1], Batch [521], Loss: 191484.718750
Epoch [1/1], Batch [531], Loss: 199720.937500
Epoch [1/1], Batch [541], Loss: 186795.156250
Epoch [1/1], Batch [551], Loss: 201566.781250
Epoch [1/1], Batch [561], Loss: 191744.937500
Epoch [1/1], Batch [571], Loss: 191803.843750
Epoch [1/1], Batch [581], Loss: 190635.562500
Epoch [1/1], Batch [591], Loss: 195343.531250
Epoch [1/1], Batch [601], Loss: 189510.718750
Epoch [1/1], Batch [611], Loss: 188685.343750
Epoch [1/1], Batch [621], Loss: 183403.312500
Epoch [1/1], Batch [631], Loss: 191290.656250
Epoch [1/1], Batch [641], Loss: 186832.875000
Epoch [1/1], Batch [651], Loss: 194883.531250
Epoch [1/1], Batch [661], Loss: 194161.156250
Epoch [1/1], Batch [671], Loss: 194555.937500
Epoch [1/1], Batch [681], Loss: 189504.921875
Epoch [1/1], Batch [691], Loss: 194470.921875
Epoch [1/1], Batch [701], Loss: 189120.906250
Epoch [1/1], Batch [711], Loss: 191311.468750
Epoch [1/1], Batch [721], Loss: 191913.218750
Epoch [1/1], Batch [731], Loss: 199259.812500
Epoch [1/1], Batch [741], Loss: 190074.734375
Epoch [1/1], Batch [751], Loss: 194716.343750
Epoch [1/1], Batch [761], Loss: 193211.593750
Epoch [1/1], Batch [771], Loss: 195010.031250
Epoch [1/1], Batch [781], Loss: 187800.906250
Epoch [1/1], Batch [791], Loss: 187103.375000
Epoch [1/1], Batch [801], Loss: 186577.781250
Epoch [1/1], Batch [811], Loss: 193930.187500
Epoch [1/1], Batch [821], Loss: 183259.171875
Epoch [1/1], Batch [831], Loss: 192227.375000
Epoch [1/1], Batch [841], Loss: 187337.750000
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 192758.9296
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 188340.8905
Elapsed time: 3284.72 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 193013.1316
Elapsed time: 3304.06 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 227743.500000
Epoch [1/1], Batch [11], Loss: 228421.234375
Epoch [1/1], Batch [21], Loss: 212035.000000
Epoch [1/1], Batch [31], Loss: 220844.828125
Epoch [1/1], Batch [41], Loss: 220314.937500
Epoch [1/1], Batch [51], Loss: 220241.843750
Epoch [1/1], Batch [61], Loss: 219508.687500
Epoch [1/1], Batch [71], Loss: 224334.718750
Epoch [1/1], Batch [81], Loss: 225040.281250
Epoch [1/1], Batch [91], Loss: 222336.578125
Epoch [1/1], Batch [101], Loss: 227311.656250
Epoch [1/1], Batch [111], Loss: 218025.562500
Epoch [1/1], Batch [121], Loss: 218720.250000
Epoch [1/1], Batch [131], Loss: 234316.562500
Epoch [1/1], Batch [141], Loss: 230365.156250
Epoch [1/1], Batch [151], Loss: 208595.375000
Epoch [1/1], Batch [161], Loss: 225774.593750
Epoch [1/1], Batch [171], Loss: 218942.015625
Epoch [1/1], Batch [181], Loss: 222500.453125
Epoch [1/1], Batch [191], Loss: 216981.593750
Epoch [1/1], Batch [201], Loss: 226627.203125
Epoch [1/1], Batch [211], Loss: 223876.953125
Epoch [1/1], Batch [221], Loss: 211459.812500
Epoch [1/1], Batch [231], Loss: 225227.515625
Epoch [1/1], Batch [241], Loss: 212936.484375
Epoch [1/1], Batch [251], Loss: 222426.812500
Epoch [1/1], Batch [261], Loss: 230736.750000
Epoch [1/1], Batch [271], Loss: 218732.265625
Epoch [1/1], Batch [281], Loss: 225486.921875
Epoch [1/1], Batch [291], Loss: 221621.375000
Epoch [1/1], Batch [301], Loss: 229748.390625
Epoch [1/1], Batch [311], Loss: 223917.343750
Epoch [1/1], Batch [321], Loss: 219385.843750
Epoch [1/1], Batch [331], Loss: 225391.093750
Epoch [1/1], Batch [341], Loss: 218401.078125
Epoch [1/1], Batch [351], Loss: 216210.015625
Epoch [1/1], Batch [361], Loss: 225730.937500
Epoch [1/1], Batch [371], Loss: 226563.593750
Epoch [1/1], Batch [381], Loss: 220359.187500
Epoch [1/1], Batch [391], Loss: 211906.093750
Epoch [1/1], Batch [401], Loss: 216489.437500
Epoch [1/1], Batch [411], Loss: 223640.093750
Epoch [1/1], Batch [421], Loss: 226962.390625
Epoch [1/1], Batch [431], Loss: 214252.421875
Epoch [1/1], Batch [441], Loss: 215828.203125
Epoch [1/1], Batch [451], Loss: 224841.750000
Epoch [1/1], Batch [461], Loss: 209599.156250
Epoch [1/1], Batch [471], Loss: 226900.843750
Epoch [1/1], Batch [481], Loss: 225356.093750
Epoch [1/1], Batch [491], Loss: 227789.984375
Epoch [1/1], Batch [501], Loss: 222634.515625
Epoch [1/1], Batch [511], Loss: 208977.906250
Epoch [1/1], Batch [521], Loss: 219570.109375
Epoch [1/1], Batch [531], Loss: 222194.343750
Epoch [1/1], Batch [541], Loss: 216830.406250
Epoch [1/1], Batch [551], Loss: 207163.171875
Epoch [1/1], Batch [561], Loss: 214685.968750
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 221474.8561
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 217554.9839
Elapsed time: 3673.68 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 223000.9916
Elapsed time: 3688.24 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 259469.218750
Epoch [1/1], Batch [11], Loss: 254679.187500
Epoch [1/1], Batch [21], Loss: 253646.156250
Epoch [1/1], Batch [31], Loss: 254707.531250
Epoch [1/1], Batch [41], Loss: 251277.218750
Epoch [1/1], Batch [51], Loss: 255655.781250
Epoch [1/1], Batch [61], Loss: 259072.703125
Epoch [1/1], Batch [71], Loss: 252088.375000
Epoch [1/1], Batch [81], Loss: 258384.156250
Epoch [1/1], Batch [91], Loss: 251403.671875
Epoch [1/1], Batch [101], Loss: 256656.375000
Epoch [1/1], Batch [111], Loss: 251403.390625
Epoch [1/1], Batch [121], Loss: 269133.437500
Epoch [1/1], Batch [131], Loss: 256492.765625
Epoch [1/1], Batch [141], Loss: 254813.593750
Epoch [1/1], Batch [151], Loss: 243755.093750
Epoch [1/1], Batch [161], Loss: 261327.562500
Epoch [1/1], Batch [171], Loss: 246890.390625
Epoch [1/1], Batch [181], Loss: 246242.531250
Epoch [1/1], Batch [191], Loss: 246740.593750
Epoch [1/1], Batch [201], Loss: 242524.500000
Epoch [1/1], Batch [211], Loss: 235458.906250
Epoch [1/1], Batch [221], Loss: 251452.687500
Epoch [1/1], Batch [231], Loss: 250098.296875
Epoch [1/1], Batch [241], Loss: 266180.875000
Epoch [1/1], Batch [251], Loss: 260038.078125
Epoch [1/1], Batch [261], Loss: 247718.468750
Epoch [1/1], Batch [271], Loss: 258543.296875
Epoch [1/1], Batch [281], Loss: 249761.640625
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 250914.2630
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 241865.1624
Elapsed time: 3895.29 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 245564.3845
Elapsed time: 3903.37 seconds

Training complete!
Totoal elapsed time: 3903.37 seconds
CUDA is available!
