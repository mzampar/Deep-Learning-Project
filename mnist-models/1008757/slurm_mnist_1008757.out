Starting job 1008757
Training with:
    architecture = [32, 32, 32, 32],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = False,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 398994.687500
Epoch [1/1], Batch [11], Loss: 139212.906250
Epoch [1/1], Batch [21], Loss: 93300.273438
Epoch [1/1], Batch [31], Loss: 77459.812500
Epoch [1/1], Batch [41], Loss: 66869.218750
Epoch [1/1], Batch [51], Loss: 62894.437500
Epoch [1/1], Batch [61], Loss: 59367.898438
Epoch [1/1], Batch [71], Loss: 57032.851562
Epoch [1/1], Batch [81], Loss: 53880.343750
Epoch [1/1], Batch [91], Loss: 51298.507812
Epoch [1/1], Batch [101], Loss: 50634.351562
Epoch [1/1], Batch [111], Loss: 49528.460938
Epoch [1/1], Batch [121], Loss: 49385.371094
Epoch [1/1], Batch [131], Loss: 49180.007812
Epoch [1/1], Batch [141], Loss: 48470.531250
Epoch [1/1], Batch [151], Loss: 50666.746094
Epoch [1/1], Batch [161], Loss: 48505.250000
Epoch [1/1], Batch [171], Loss: 47864.640625
Epoch [1/1], Batch [181], Loss: 47385.097656
Epoch [1/1], Batch [191], Loss: 46396.890625
Epoch [1/1], Batch [201], Loss: 48524.691406
Epoch [1/1], Batch [211], Loss: 47474.828125
Epoch [1/1], Batch [221], Loss: 44993.503906
Epoch [1/1], Batch [231], Loss: 46530.531250
Epoch [1/1], Batch [241], Loss: 46203.328125
Epoch [1/1], Batch [251], Loss: 46618.628906
Epoch [1/1], Batch [261], Loss: 43661.273438
Epoch [1/1], Batch [271], Loss: 45423.574219
Epoch [1/1], Batch [281], Loss: 45543.414062
Epoch [1/1], Batch [291], Loss: 45249.296875
Epoch [1/1], Batch [301], Loss: 46515.363281
Epoch [1/1], Batch [311], Loss: 45923.050781
Epoch [1/1], Batch [321], Loss: 45752.843750
Epoch [1/1], Batch [331], Loss: 45970.847656
Epoch [1/1], Batch [341], Loss: 46305.058594
Epoch [1/1], Batch [351], Loss: 47768.261719
Epoch [1/1], Batch [361], Loss: 45622.328125
Epoch [1/1], Batch [371], Loss: 45272.867188
Epoch [1/1], Batch [381], Loss: 41723.218750
Epoch [1/1], Batch [391], Loss: 42747.492188
Epoch [1/1], Batch [401], Loss: 44333.023438
Epoch [1/1], Batch [411], Loss: 43969.093750
Epoch [1/1], Batch [421], Loss: 45437.562500
Epoch [1/1], Batch [431], Loss: 44010.847656
Epoch [1/1], Batch [441], Loss: 46693.070312
Epoch [1/1], Batch [451], Loss: 44321.726562
Epoch [1/1], Batch [461], Loss: 44445.417969
Epoch [1/1], Batch [471], Loss: 43813.632812
Epoch [1/1], Batch [481], Loss: 43874.996094
Epoch [1/1], Batch [491], Loss: 43946.992188
Epoch [1/1], Batch [501], Loss: 43784.976562
Epoch [1/1], Batch [511], Loss: 44364.503906
Epoch [1/1], Batch [521], Loss: 44150.378906
Epoch [1/1], Batch [531], Loss: 42672.398438
Epoch [1/1], Batch [541], Loss: 43711.464844
Epoch [1/1], Batch [551], Loss: 43567.644531
Epoch [1/1], Batch [561], Loss: 42867.476562
Epoch [1/1], Batch [571], Loss: 44349.320312
Epoch [1/1], Batch [581], Loss: 43340.523438
Epoch [1/1], Batch [591], Loss: 45997.835938
Epoch [1/1], Batch [601], Loss: 44983.632812
Epoch [1/1], Batch [611], Loss: 42096.031250
Epoch [1/1], Batch [621], Loss: 42187.265625
Epoch [1/1], Batch [631], Loss: 42866.578125
Epoch [1/1], Batch [641], Loss: 43246.695312
Epoch [1/1], Batch [651], Loss: 43247.203125
Epoch [1/1], Batch [661], Loss: 42327.265625
Epoch [1/1], Batch [671], Loss: 43914.285156
Epoch [1/1], Batch [681], Loss: 43292.492188
Epoch [1/1], Batch [691], Loss: 41811.359375
Epoch [1/1], Batch [701], Loss: 41843.144531
Epoch [1/1], Batch [711], Loss: 42104.777344
Epoch [1/1], Batch [721], Loss: 41075.078125
Epoch [1/1], Batch [731], Loss: 42263.582031
Epoch [1/1], Batch [741], Loss: 41314.234375
Epoch [1/1], Batch [751], Loss: 42458.648438
Epoch [1/1], Batch [761], Loss: 41721.625000
Epoch [1/1], Batch [771], Loss: 41384.601562
Epoch [1/1], Batch [781], Loss: 40251.890625
Epoch [1/1], Batch [791], Loss: 44363.945312
Epoch [1/1], Batch [801], Loss: 43517.386719
Epoch [1/1], Batch [811], Loss: 42765.609375
Epoch [1/1], Batch [821], Loss: 43335.953125
Epoch [1/1], Batch [831], Loss: 42714.125000
Epoch [1/1], Batch [841], Loss: 41967.378906
Epoch [1/1], Batch [851], Loss: 39958.679688
Epoch [1/1], Batch [861], Loss: 43073.015625
Epoch [1/1], Batch [871], Loss: 40494.117188
Epoch [1/1], Batch [881], Loss: 40050.726562
Epoch [1/1], Batch [891], Loss: 41128.046875
Epoch [1/1], Batch [901], Loss: 40666.820312
Epoch [1/1], Batch [911], Loss: 41476.460938
Epoch [1/1], Batch [921], Loss: 41696.984375
Epoch [1/1], Batch [931], Loss: 38965.402344
Epoch [1/1], Batch [941], Loss: 39643.945312
Epoch [1/1], Batch [951], Loss: 40152.558594
Epoch [1/1], Batch [961], Loss: 41480.539062
Epoch [1/1], Batch [971], Loss: 40284.312500
Epoch [1/1], Batch [981], Loss: 41334.925781
Epoch [1/1], Batch [991], Loss: 38113.351562
Epoch [1/1], Batch [1001], Loss: 40801.835938
Epoch [1/1], Batch [1011], Loss: 41408.468750
Epoch [1/1], Batch [1021], Loss: 42027.296875
Epoch [1/1], Batch [1031], Loss: 40316.542969
Epoch [1/1], Batch [1041], Loss: 40121.750000
Epoch [1/1], Batch [1051], Loss: 40490.875000
Epoch [1/1], Batch [1061], Loss: 42551.328125
Epoch [1/1], Batch [1071], Loss: 40969.742188
Epoch [1/1], Batch [1081], Loss: 39129.308594
Epoch [1/1], Batch [1091], Loss: 41266.593750
Epoch [1/1], Batch [1101], Loss: 38819.781250
Epoch [1/1], Batch [1111], Loss: 40833.070312
Epoch [1/1], Batch [1121], Loss: 40604.929688
Epoch [1/1], Batch [1131], Loss: 39017.476562
Epoch [1/1], Batch [1141], Loss: 41290.773438
Epoch [1/1], Batch [1151], Loss: 39865.984375
Epoch [1/1], Batch [1161], Loss: 43523.890625
Epoch [1/1], Batch [1171], Loss: 39810.066406
Epoch [1/1], Batch [1181], Loss: 41767.710938
Epoch [1/1], Batch [1191], Loss: 36972.878906
Epoch [1/1], Batch [1201], Loss: 37572.824219
Epoch [1/1], Batch [1211], Loss: 40240.078125
Epoch [1/1], Batch [1221], Loss: 39640.812500
Epoch [1/1], Batch [1231], Loss: 39275.226562
Epoch [1/1], Batch [1241], Loss: 39352.421875
Epoch [1/1], Batch [1251], Loss: 39445.003906
Epoch [1/1], Batch [1261], Loss: 40451.335938
Epoch [1/1], Batch [1271], Loss: 40196.578125
Epoch [1/1], Batch [1281], Loss: 40663.671875
Epoch [1/1], Batch [1291], Loss: 39543.929688
Epoch [1/1], Batch [1301], Loss: 39410.503906
Epoch [1/1], Batch [1311], Loss: 37249.535156
Epoch [1/1], Batch [1321], Loss: 39609.750000
Epoch [1/1], Batch [1331], Loss: 39903.171875
Epoch [1/1], Batch [1341], Loss: 38951.570312
Epoch [1/1], Batch [1351], Loss: 41477.945312
Epoch [1/1], Batch [1361], Loss: 40180.460938
Epoch [1/1], Batch [1371], Loss: 36582.859375
Epoch [1/1], Batch [1381], Loss: 40902.796875
Epoch [1/1], Batch [1391], Loss: 39647.953125
Epoch [1/1], Batch [1401], Loss: 39317.417969
Epoch [1/1], Batch [1411], Loss: 39583.257812
Epoch [1/1], Batch [1421], Loss: 39513.453125
Epoch [1/1], Batch [1431], Loss: 39098.160156
Epoch [1/1], Batch [1441], Loss: 39584.558594
Epoch [1/1], Batch [1451], Loss: 40217.824219
Epoch [1/1], Batch [1461], Loss: 39390.464844
Epoch [1/1], Batch [1471], Loss: 38197.664062
Epoch [1/1], Batch [1481], Loss: 39745.953125
Epoch [1/1], Batch [1491], Loss: 40321.378906
Epoch [1/1], Batch [1501], Loss: 41521.367188
Epoch [1/1], Batch [1511], Loss: 41096.664062
Epoch [1/1], Batch [1521], Loss: 40172.367188
Epoch [1/1], Batch [1531], Loss: 39304.042969
Epoch [1/1], Batch [1541], Loss: 37279.140625
Epoch [1/1], Batch [1551], Loss: 40568.250000
Epoch [1/1], Batch [1561], Loss: 40411.621094
Epoch [1/1], Batch [1571], Loss: 39668.910156
Epoch [1/1], Batch [1581], Loss: 38789.882812
Epoch [1/1], Batch [1591], Loss: 40940.960938
Epoch [1/1], Batch [1601], Loss: 39987.371094
Epoch [1/1], Batch [1611], Loss: 39478.707031
Epoch [1/1], Batch [1621], Loss: 40319.164062
Epoch [1/1], Batch [1631], Loss: 39884.539062
Epoch [1/1], Batch [1641], Loss: 38480.929688
Epoch [1/1], Batch [1651], Loss: 37639.390625
Epoch [1/1], Batch [1661], Loss: 39127.796875
Epoch [1/1], Batch [1671], Loss: 40331.109375
Epoch [1/1], Batch [1681], Loss: 39056.468750
Epoch [1/1], Batch [1691], Loss: 39271.566406
Epoch [1/1], Batch [1701], Loss: 37756.628906
Epoch [1/1], Batch [1711], Loss: 38005.863281
Epoch [1/1], Batch [1721], Loss: 38721.683594
Epoch [1/1], Batch [1731], Loss: 39222.718750
Epoch [1/1], Batch [1741], Loss: 38972.164062
Epoch [1/1], Batch [1751], Loss: 39627.113281
Epoch [1/1], Batch [1761], Loss: 37561.703125
Epoch [1/1], Batch [1771], Loss: 39054.648438
Epoch [1/1], Batch [1781], Loss: 38325.261719
Epoch [1/1], Batch [1791], Loss: 39875.007812
Epoch [1/1], Batch [1801], Loss: 36860.828125
Epoch [1/1], Batch [1811], Loss: 38192.566406
Epoch [1/1], Batch [1821], Loss: 39067.410156
Epoch [1/1], Batch [1831], Loss: 38252.484375
Epoch [1/1], Batch [1841], Loss: 38714.734375
Epoch [1/1], Batch [1851], Loss: 37247.109375
Epoch [1/1], Batch [1861], Loss: 40428.289062
Epoch [1/1], Batch [1871], Loss: 40063.859375
Epoch [1/1], Batch [1881], Loss: 37412.468750
Epoch [1/1], Batch [1891], Loss: 39171.429688
Epoch [1/1], Batch [1901], Loss: 38703.453125
Epoch [1/1], Batch [1911], Loss: 37987.078125
Epoch [1/1], Batch [1921], Loss: 38724.921875
Epoch [1/1], Batch [1931], Loss: 39133.769531
Epoch [1/1], Batch [1941], Loss: 40052.710938
Epoch [1/1], Batch [1951], Loss: 37710.746094
Epoch [1/1], Batch [1961], Loss: 40752.140625
Epoch [1/1], Batch [1971], Loss: 36141.558594
Epoch [1/1], Batch [1981], Loss: 37767.421875
Epoch [1/1], Batch [1991], Loss: 38548.429688
Epoch [1/1], Batch [2001], Loss: 38814.445312
Epoch [1/1], Batch [2011], Loss: 40516.492188
Epoch [1/1], Batch [2021], Loss: 37611.078125
Epoch [1/1], Batch [2031], Loss: 39035.578125
Epoch [1/1], Batch [2041], Loss: 40343.375000
Epoch [1/1], Batch [2051], Loss: 36581.695312
Epoch [1/1], Batch [2061], Loss: 36694.476562
Epoch [1/1], Batch [2071], Loss: 37182.382812
Epoch [1/1], Batch [2081], Loss: 38603.062500
Epoch [1/1], Batch [2091], Loss: 36747.828125
Epoch [1/1], Batch [2101], Loss: 38683.656250
Epoch [1/1], Batch [2111], Loss: 38198.734375
Epoch [1/1], Batch [2121], Loss: 36678.015625
Epoch [1/1], Batch [2131], Loss: 37307.367188
Epoch [1/1], Batch [2141], Loss: 40129.390625
Epoch [1/1], Batch [2151], Loss: 38068.757812
Epoch [1/1], Batch [2161], Loss: 37384.320312
Epoch [1/1], Batch [2171], Loss: 38140.625000
Epoch [1/1], Batch [2181], Loss: 36908.816406
Epoch [1/1], Batch [2191], Loss: 36623.046875
Epoch [1/1], Batch [2201], Loss: 39272.226562
Epoch [1/1], Batch [2211], Loss: 37276.179688
Epoch [1/1], Batch [2221], Loss: 37314.574219
Epoch [1/1], Batch [2231], Loss: 38169.859375
Epoch [1/1], Batch [2241], Loss: 37909.867188
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 43215.1682
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 37608.9471
Elapsed time: 527.13 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 37899.5596
Elapsed time: 549.37 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 78612.117188
Epoch [1/1], Batch [11], Loss: 74663.000000
Epoch [1/1], Batch [21], Loss: 70328.804688
Epoch [1/1], Batch [31], Loss: 67338.609375
Epoch [1/1], Batch [41], Loss: 67059.593750
Epoch [1/1], Batch [51], Loss: 61489.289062
Epoch [1/1], Batch [61], Loss: 63486.140625
Epoch [1/1], Batch [71], Loss: 66805.093750
Epoch [1/1], Batch [81], Loss: 64215.089844
Epoch [1/1], Batch [91], Loss: 61897.085938
Epoch [1/1], Batch [101], Loss: 64345.070312
Epoch [1/1], Batch [111], Loss: 60743.894531
Epoch [1/1], Batch [121], Loss: 62144.089844
Epoch [1/1], Batch [131], Loss: 61978.914062
Epoch [1/1], Batch [141], Loss: 64269.718750
Epoch [1/1], Batch [151], Loss: 59396.074219
Epoch [1/1], Batch [161], Loss: 65870.796875
Epoch [1/1], Batch [171], Loss: 66590.054688
Epoch [1/1], Batch [181], Loss: 58136.250000
Epoch [1/1], Batch [191], Loss: 60063.648438
Epoch [1/1], Batch [201], Loss: 61434.218750
Epoch [1/1], Batch [211], Loss: 60768.289062
Epoch [1/1], Batch [221], Loss: 60027.226562
Epoch [1/1], Batch [231], Loss: 61165.570312
Epoch [1/1], Batch [241], Loss: 57533.132812
Epoch [1/1], Batch [251], Loss: 56322.789062
Epoch [1/1], Batch [261], Loss: 59737.835938
Epoch [1/1], Batch [271], Loss: 62566.796875
Epoch [1/1], Batch [281], Loss: 62096.812500
Epoch [1/1], Batch [291], Loss: 60711.136719
Epoch [1/1], Batch [301], Loss: 58645.988281
Epoch [1/1], Batch [311], Loss: 59054.414062
Epoch [1/1], Batch [321], Loss: 62563.746094
Epoch [1/1], Batch [331], Loss: 57242.718750
Epoch [1/1], Batch [341], Loss: 59184.953125
Epoch [1/1], Batch [351], Loss: 61036.097656
Epoch [1/1], Batch [361], Loss: 62277.039062
Epoch [1/1], Batch [371], Loss: 61129.335938
Epoch [1/1], Batch [381], Loss: 60470.617188
Epoch [1/1], Batch [391], Loss: 57701.804688
Epoch [1/1], Batch [401], Loss: 57453.417969
Epoch [1/1], Batch [411], Loss: 59105.066406
Epoch [1/1], Batch [421], Loss: 60661.578125
Epoch [1/1], Batch [431], Loss: 62598.734375
Epoch [1/1], Batch [441], Loss: 57800.152344
Epoch [1/1], Batch [451], Loss: 56161.902344
Epoch [1/1], Batch [461], Loss: 60831.027344
Epoch [1/1], Batch [471], Loss: 60868.531250
Epoch [1/1], Batch [481], Loss: 60867.132812
Epoch [1/1], Batch [491], Loss: 59789.312500
Epoch [1/1], Batch [501], Loss: 60898.460938
Epoch [1/1], Batch [511], Loss: 60049.921875
Epoch [1/1], Batch [521], Loss: 58426.585938
Epoch [1/1], Batch [531], Loss: 58968.976562
Epoch [1/1], Batch [541], Loss: 57442.062500
Epoch [1/1], Batch [551], Loss: 58403.468750
Epoch [1/1], Batch [561], Loss: 59963.984375
Epoch [1/1], Batch [571], Loss: 58520.386719
Epoch [1/1], Batch [581], Loss: 58535.285156
Epoch [1/1], Batch [591], Loss: 58139.320312
Epoch [1/1], Batch [601], Loss: 55818.160156
Epoch [1/1], Batch [611], Loss: 58343.359375
Epoch [1/1], Batch [621], Loss: 57940.953125
Epoch [1/1], Batch [631], Loss: 56613.765625
Epoch [1/1], Batch [641], Loss: 56325.609375
Epoch [1/1], Batch [651], Loss: 61199.519531
Epoch [1/1], Batch [661], Loss: 56950.312500
Epoch [1/1], Batch [671], Loss: 57824.273438
Epoch [1/1], Batch [681], Loss: 55287.812500
Epoch [1/1], Batch [691], Loss: 56236.656250
Epoch [1/1], Batch [701], Loss: 58589.890625
Epoch [1/1], Batch [711], Loss: 58657.382812
Epoch [1/1], Batch [721], Loss: 57448.140625
Epoch [1/1], Batch [731], Loss: 56524.246094
Epoch [1/1], Batch [741], Loss: 57400.636719
Epoch [1/1], Batch [751], Loss: 56794.710938
Epoch [1/1], Batch [761], Loss: 56898.902344
Epoch [1/1], Batch [771], Loss: 56136.375000
Epoch [1/1], Batch [781], Loss: 58031.117188
Epoch [1/1], Batch [791], Loss: 55785.238281
Epoch [1/1], Batch [801], Loss: 53016.171875
Epoch [1/1], Batch [811], Loss: 58663.757812
Epoch [1/1], Batch [821], Loss: 59579.562500
Epoch [1/1], Batch [831], Loss: 58164.156250
Epoch [1/1], Batch [841], Loss: 57578.328125
Epoch [1/1], Batch [851], Loss: 57222.527344
Epoch [1/1], Batch [861], Loss: 58338.765625
Epoch [1/1], Batch [871], Loss: 55966.156250
Epoch [1/1], Batch [881], Loss: 56617.578125
Epoch [1/1], Batch [891], Loss: 58347.835938
Epoch [1/1], Batch [901], Loss: 60601.742188
Epoch [1/1], Batch [911], Loss: 60704.335938
Epoch [1/1], Batch [921], Loss: 58176.250000
Epoch [1/1], Batch [931], Loss: 55858.976562
Epoch [1/1], Batch [941], Loss: 57671.234375
Epoch [1/1], Batch [951], Loss: 57749.125000
Epoch [1/1], Batch [961], Loss: 59928.027344
Epoch [1/1], Batch [971], Loss: 57095.464844
Epoch [1/1], Batch [981], Loss: 58670.257812
Epoch [1/1], Batch [991], Loss: 57717.539062
Epoch [1/1], Batch [1001], Loss: 54569.957031
Epoch [1/1], Batch [1011], Loss: 58079.167969
Epoch [1/1], Batch [1021], Loss: 58478.199219
Epoch [1/1], Batch [1031], Loss: 57277.136719
Epoch [1/1], Batch [1041], Loss: 56313.843750
Epoch [1/1], Batch [1051], Loss: 58182.351562
Epoch [1/1], Batch [1061], Loss: 54009.875000
Epoch [1/1], Batch [1071], Loss: 55939.195312
Epoch [1/1], Batch [1081], Loss: 53541.875000
Epoch [1/1], Batch [1091], Loss: 57786.328125
Epoch [1/1], Batch [1101], Loss: 54301.796875
Epoch [1/1], Batch [1111], Loss: 54349.609375
Epoch [1/1], Batch [1121], Loss: 58804.757812
Epoch [1/1], Batch [1131], Loss: 58793.339844
Epoch [1/1], Batch [1141], Loss: 54095.875000
Epoch [1/1], Batch [1151], Loss: 60459.257812
Epoch [1/1], Batch [1161], Loss: 56492.304688
Epoch [1/1], Batch [1171], Loss: 54638.531250
Epoch [1/1], Batch [1181], Loss: 57561.421875
Epoch [1/1], Batch [1191], Loss: 54445.140625
Epoch [1/1], Batch [1201], Loss: 55781.562500
Epoch [1/1], Batch [1211], Loss: 57069.011719
Epoch [1/1], Batch [1221], Loss: 61129.652344
Epoch [1/1], Batch [1231], Loss: 55149.347656
Epoch [1/1], Batch [1241], Loss: 56407.003906
Epoch [1/1], Batch [1251], Loss: 58097.343750
Epoch [1/1], Batch [1261], Loss: 55657.781250
Epoch [1/1], Batch [1271], Loss: 54504.558594
Epoch [1/1], Batch [1281], Loss: 56923.781250
Epoch [1/1], Batch [1291], Loss: 55678.765625
Epoch [1/1], Batch [1301], Loss: 57236.304688
Epoch [1/1], Batch [1311], Loss: 54627.921875
Epoch [1/1], Batch [1321], Loss: 54586.000000
Epoch [1/1], Batch [1331], Loss: 54909.324219
Epoch [1/1], Batch [1341], Loss: 56593.601562
Epoch [1/1], Batch [1351], Loss: 55269.285156
Epoch [1/1], Batch [1361], Loss: 54755.953125
Epoch [1/1], Batch [1371], Loss: 54723.507812
Epoch [1/1], Batch [1381], Loss: 53640.519531
Epoch [1/1], Batch [1391], Loss: 53088.992188
Epoch [1/1], Batch [1401], Loss: 53733.632812
Epoch [1/1], Batch [1411], Loss: 55666.527344
Epoch [1/1], Batch [1421], Loss: 57422.125000
Epoch [1/1], Batch [1431], Loss: 52341.382812
Epoch [1/1], Batch [1441], Loss: 59138.351562
Epoch [1/1], Batch [1451], Loss: 57072.589844
Epoch [1/1], Batch [1461], Loss: 54131.769531
Epoch [1/1], Batch [1471], Loss: 57004.363281
Epoch [1/1], Batch [1481], Loss: 56905.062500
Epoch [1/1], Batch [1491], Loss: 55077.164062
Epoch [1/1], Batch [1501], Loss: 58226.406250
Epoch [1/1], Batch [1511], Loss: 56070.273438
Epoch [1/1], Batch [1521], Loss: 56164.207031
Epoch [1/1], Batch [1531], Loss: 54285.039062
Epoch [1/1], Batch [1541], Loss: 54812.054688
Epoch [1/1], Batch [1551], Loss: 56676.691406
Epoch [1/1], Batch [1561], Loss: 55706.023438
Epoch [1/1], Batch [1571], Loss: 56360.835938
Epoch [1/1], Batch [1581], Loss: 57939.699219
Epoch [1/1], Batch [1591], Loss: 56095.375000
Epoch [1/1], Batch [1601], Loss: 50313.281250
Epoch [1/1], Batch [1611], Loss: 55885.613281
Epoch [1/1], Batch [1621], Loss: 52568.136719
Epoch [1/1], Batch [1631], Loss: 55584.542969
Epoch [1/1], Batch [1641], Loss: 55400.007812
Epoch [1/1], Batch [1651], Loss: 55023.226562
Epoch [1/1], Batch [1661], Loss: 53379.152344
Epoch [1/1], Batch [1671], Loss: 57596.140625
Epoch [1/1], Batch [1681], Loss: 55230.714844
Epoch [1/1], Batch [1691], Loss: 53323.296875
Epoch [1/1], Batch [1701], Loss: 55163.273438
Epoch [1/1], Batch [1711], Loss: 54465.851562
Epoch [1/1], Batch [1721], Loss: 54474.730469
Epoch [1/1], Batch [1731], Loss: 55107.554688
Epoch [1/1], Batch [1741], Loss: 54960.648438
Epoch [1/1], Batch [1751], Loss: 53606.734375
Epoch [1/1], Batch [1761], Loss: 55926.335938
Epoch [1/1], Batch [1771], Loss: 55254.023438
Epoch [1/1], Batch [1781], Loss: 54305.441406
Epoch [1/1], Batch [1791], Loss: 56341.234375
Epoch [1/1], Batch [1801], Loss: 54581.140625
Epoch [1/1], Batch [1811], Loss: 54905.531250
Epoch [1/1], Batch [1821], Loss: 56421.042969
Epoch [1/1], Batch [1831], Loss: 54331.730469
Epoch [1/1], Batch [1841], Loss: 56846.640625
Epoch [1/1], Batch [1851], Loss: 50540.726562
Epoch [1/1], Batch [1861], Loss: 56334.992188
Epoch [1/1], Batch [1871], Loss: 54987.390625
Epoch [1/1], Batch [1881], Loss: 56285.894531
Epoch [1/1], Batch [1891], Loss: 56237.550781
Epoch [1/1], Batch [1901], Loss: 54164.363281
Epoch [1/1], Batch [1911], Loss: 56414.507812
Epoch [1/1], Batch [1921], Loss: 53996.191406
Epoch [1/1], Batch [1931], Loss: 57295.742188
Epoch [1/1], Batch [1941], Loss: 53360.816406
Epoch [1/1], Batch [1951], Loss: 55156.398438
Epoch [1/1], Batch [1961], Loss: 55848.062500
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 57946.8051
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 54267.1974
Elapsed time: 1202.87 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 55353.0474
Elapsed time: 1229.13 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 81117.335938
Epoch [1/1], Batch [11], Loss: 76175.812500
Epoch [1/1], Batch [21], Loss: 79427.828125
Epoch [1/1], Batch [31], Loss: 79237.867188
Epoch [1/1], Batch [41], Loss: 77096.773438
Epoch [1/1], Batch [51], Loss: 77494.765625
Epoch [1/1], Batch [61], Loss: 78118.507812
Epoch [1/1], Batch [71], Loss: 78396.171875
Epoch [1/1], Batch [81], Loss: 81190.515625
Epoch [1/1], Batch [91], Loss: 75451.140625
Epoch [1/1], Batch [101], Loss: 78761.328125
Epoch [1/1], Batch [111], Loss: 79258.046875
Epoch [1/1], Batch [121], Loss: 78275.843750
Epoch [1/1], Batch [131], Loss: 78480.898438
Epoch [1/1], Batch [141], Loss: 77416.062500
Epoch [1/1], Batch [151], Loss: 73754.796875
Epoch [1/1], Batch [161], Loss: 76071.296875
Epoch [1/1], Batch [171], Loss: 80132.000000
Epoch [1/1], Batch [181], Loss: 78638.734375
Epoch [1/1], Batch [191], Loss: 78168.953125
Epoch [1/1], Batch [201], Loss: 76119.656250
Epoch [1/1], Batch [211], Loss: 75344.296875
Epoch [1/1], Batch [221], Loss: 76103.570312
Epoch [1/1], Batch [231], Loss: 76376.046875
Epoch [1/1], Batch [241], Loss: 72695.171875
Epoch [1/1], Batch [251], Loss: 78581.554688
Epoch [1/1], Batch [261], Loss: 76409.656250
Epoch [1/1], Batch [271], Loss: 78139.156250
Epoch [1/1], Batch [281], Loss: 80038.585938
Epoch [1/1], Batch [291], Loss: 77464.078125
Epoch [1/1], Batch [301], Loss: 74824.929688
Epoch [1/1], Batch [311], Loss: 79457.515625
Epoch [1/1], Batch [321], Loss: 76905.742188
Epoch [1/1], Batch [331], Loss: 73744.703125
Epoch [1/1], Batch [341], Loss: 77303.265625
Epoch [1/1], Batch [351], Loss: 80518.195312
Epoch [1/1], Batch [361], Loss: 73360.703125
Epoch [1/1], Batch [371], Loss: 77121.367188
Epoch [1/1], Batch [381], Loss: 75227.492188
Epoch [1/1], Batch [391], Loss: 78126.585938
Epoch [1/1], Batch [401], Loss: 73903.890625
Epoch [1/1], Batch [411], Loss: 74284.515625
Epoch [1/1], Batch [421], Loss: 77492.789062
Epoch [1/1], Batch [431], Loss: 77911.382812
Epoch [1/1], Batch [441], Loss: 78262.109375
Epoch [1/1], Batch [451], Loss: 74970.664062
Epoch [1/1], Batch [461], Loss: 71454.242188
Epoch [1/1], Batch [471], Loss: 74246.078125
Epoch [1/1], Batch [481], Loss: 71272.390625
Epoch [1/1], Batch [491], Loss: 75610.953125
Epoch [1/1], Batch [501], Loss: 79179.992188
Epoch [1/1], Batch [511], Loss: 77548.656250
Epoch [1/1], Batch [521], Loss: 73394.500000
Epoch [1/1], Batch [531], Loss: 75663.937500
Epoch [1/1], Batch [541], Loss: 78394.078125
Epoch [1/1], Batch [551], Loss: 76011.609375
Epoch [1/1], Batch [561], Loss: 73969.265625
Epoch [1/1], Batch [571], Loss: 73705.546875
Epoch [1/1], Batch [581], Loss: 75504.703125
Epoch [1/1], Batch [591], Loss: 73965.226562
Epoch [1/1], Batch [601], Loss: 77838.898438
Epoch [1/1], Batch [611], Loss: 75944.296875
Epoch [1/1], Batch [621], Loss: 76579.468750
Epoch [1/1], Batch [631], Loss: 73023.632812
Epoch [1/1], Batch [641], Loss: 77417.156250
Epoch [1/1], Batch [651], Loss: 74683.421875
Epoch [1/1], Batch [661], Loss: 75118.570312
Epoch [1/1], Batch [671], Loss: 72081.234375
Epoch [1/1], Batch [681], Loss: 74362.453125
Epoch [1/1], Batch [691], Loss: 72151.250000
Epoch [1/1], Batch [701], Loss: 75213.406250
Epoch [1/1], Batch [711], Loss: 78291.187500
Epoch [1/1], Batch [721], Loss: 76227.515625
Epoch [1/1], Batch [731], Loss: 71473.218750
Epoch [1/1], Batch [741], Loss: 69424.148438
Epoch [1/1], Batch [751], Loss: 76770.414062
Epoch [1/1], Batch [761], Loss: 72943.601562
Epoch [1/1], Batch [771], Loss: 77880.171875
Epoch [1/1], Batch [781], Loss: 75264.015625
Epoch [1/1], Batch [791], Loss: 73995.843750
Epoch [1/1], Batch [801], Loss: 74984.250000
Epoch [1/1], Batch [811], Loss: 76875.109375
Epoch [1/1], Batch [821], Loss: 76381.656250
Epoch [1/1], Batch [831], Loss: 72947.031250
Epoch [1/1], Batch [841], Loss: 73340.640625
Epoch [1/1], Batch [851], Loss: 74338.570312
Epoch [1/1], Batch [861], Loss: 74228.140625
Epoch [1/1], Batch [871], Loss: 71765.984375
Epoch [1/1], Batch [881], Loss: 73583.500000
Epoch [1/1], Batch [891], Loss: 74886.210938
Epoch [1/1], Batch [901], Loss: 73945.343750
Epoch [1/1], Batch [911], Loss: 75297.109375
Epoch [1/1], Batch [921], Loss: 79815.765625
Epoch [1/1], Batch [931], Loss: 75642.929688
Epoch [1/1], Batch [941], Loss: 76471.156250
Epoch [1/1], Batch [951], Loss: 78432.937500
Epoch [1/1], Batch [961], Loss: 76428.976562
Epoch [1/1], Batch [971], Loss: 75743.906250
Epoch [1/1], Batch [981], Loss: 73401.960938
Epoch [1/1], Batch [991], Loss: 74734.664062
Epoch [1/1], Batch [1001], Loss: 73096.625000
Epoch [1/1], Batch [1011], Loss: 71296.218750
Epoch [1/1], Batch [1021], Loss: 73171.492188
Epoch [1/1], Batch [1031], Loss: 72128.617188
Epoch [1/1], Batch [1041], Loss: 73278.468750
Epoch [1/1], Batch [1051], Loss: 75565.328125
Epoch [1/1], Batch [1061], Loss: 75059.000000
Epoch [1/1], Batch [1071], Loss: 76260.960938
Epoch [1/1], Batch [1081], Loss: 72123.351562
Epoch [1/1], Batch [1091], Loss: 74768.984375
Epoch [1/1], Batch [1101], Loss: 75498.281250
Epoch [1/1], Batch [1111], Loss: 74003.203125
Epoch [1/1], Batch [1121], Loss: 72206.015625
Epoch [1/1], Batch [1131], Loss: 75547.953125
Epoch [1/1], Batch [1141], Loss: 74243.890625
Epoch [1/1], Batch [1151], Loss: 75779.453125
Epoch [1/1], Batch [1161], Loss: 71746.171875
Epoch [1/1], Batch [1171], Loss: 75701.296875
Epoch [1/1], Batch [1181], Loss: 73624.546875
Epoch [1/1], Batch [1191], Loss: 77081.171875
Epoch [1/1], Batch [1201], Loss: 69403.851562
Epoch [1/1], Batch [1211], Loss: 72633.265625
Epoch [1/1], Batch [1221], Loss: 73603.984375
Epoch [1/1], Batch [1231], Loss: 73391.968750
Epoch [1/1], Batch [1241], Loss: 69889.671875
Epoch [1/1], Batch [1251], Loss: 76301.671875
Epoch [1/1], Batch [1261], Loss: 74635.390625
Epoch [1/1], Batch [1271], Loss: 77607.718750
Epoch [1/1], Batch [1281], Loss: 74422.015625
Epoch [1/1], Batch [1291], Loss: 72567.742188
Epoch [1/1], Batch [1301], Loss: 76586.070312
Epoch [1/1], Batch [1311], Loss: 76077.531250
Epoch [1/1], Batch [1321], Loss: 77380.437500
Epoch [1/1], Batch [1331], Loss: 73024.382812
Epoch [1/1], Batch [1341], Loss: 71416.328125
Epoch [1/1], Batch [1351], Loss: 73053.187500
Epoch [1/1], Batch [1361], Loss: 75950.562500
Epoch [1/1], Batch [1371], Loss: 74933.609375
Epoch [1/1], Batch [1381], Loss: 73039.445312
Epoch [1/1], Batch [1391], Loss: 71082.507812
Epoch [1/1], Batch [1401], Loss: 75896.156250
Epoch [1/1], Batch [1411], Loss: 69067.023438
Epoch [1/1], Batch [1421], Loss: 71823.703125
Epoch [1/1], Batch [1431], Loss: 78653.726562
Epoch [1/1], Batch [1441], Loss: 75827.992188
Epoch [1/1], Batch [1451], Loss: 75231.656250
Epoch [1/1], Batch [1461], Loss: 72043.031250
Epoch [1/1], Batch [1471], Loss: 70036.218750
Epoch [1/1], Batch [1481], Loss: 74423.882812
Epoch [1/1], Batch [1491], Loss: 74384.531250
Epoch [1/1], Batch [1501], Loss: 73597.398438
Epoch [1/1], Batch [1511], Loss: 72248.625000
Epoch [1/1], Batch [1521], Loss: 74793.398438
Epoch [1/1], Batch [1531], Loss: 69142.171875
Epoch [1/1], Batch [1541], Loss: 72430.187500
Epoch [1/1], Batch [1551], Loss: 75584.484375
Epoch [1/1], Batch [1561], Loss: 69293.585938
Epoch [1/1], Batch [1571], Loss: 71774.992188
Epoch [1/1], Batch [1581], Loss: 72019.156250
Epoch [1/1], Batch [1591], Loss: 73962.343750
Epoch [1/1], Batch [1601], Loss: 75448.000000
Epoch [1/1], Batch [1611], Loss: 73369.117188
Epoch [1/1], Batch [1621], Loss: 69431.906250
Epoch [1/1], Batch [1631], Loss: 75145.687500
Epoch [1/1], Batch [1641], Loss: 72176.593750
Epoch [1/1], Batch [1651], Loss: 72005.890625
Epoch [1/1], Batch [1661], Loss: 74991.671875
Epoch [1/1], Batch [1671], Loss: 71759.476562
Epoch [1/1], Batch [1681], Loss: 71265.625000
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 74783.1714
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 71040.9379
Elapsed time: 1950.63 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 73241.3099
Elapsed time: 1979.05 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 97518.960938
Epoch [1/1], Batch [11], Loss: 95492.984375
Epoch [1/1], Batch [21], Loss: 97298.210938
Epoch [1/1], Batch [31], Loss: 95298.296875
Epoch [1/1], Batch [41], Loss: 95452.656250
Epoch [1/1], Batch [51], Loss: 91209.421875
Epoch [1/1], Batch [61], Loss: 90771.039062
Epoch [1/1], Batch [71], Loss: 96177.687500
Epoch [1/1], Batch [81], Loss: 86942.765625
Epoch [1/1], Batch [91], Loss: 98363.414062
Epoch [1/1], Batch [101], Loss: 92179.851562
Epoch [1/1], Batch [111], Loss: 93666.875000
Epoch [1/1], Batch [121], Loss: 95705.625000
Epoch [1/1], Batch [131], Loss: 94844.218750
Epoch [1/1], Batch [141], Loss: 96977.546875
Epoch [1/1], Batch [151], Loss: 94697.210938
Epoch [1/1], Batch [161], Loss: 94872.570312
Epoch [1/1], Batch [171], Loss: 98389.578125
Epoch [1/1], Batch [181], Loss: 102786.437500
Epoch [1/1], Batch [191], Loss: 96462.250000
Epoch [1/1], Batch [201], Loss: 90347.421875
Epoch [1/1], Batch [211], Loss: 93920.750000
Epoch [1/1], Batch [221], Loss: 95320.312500
Epoch [1/1], Batch [231], Loss: 92639.421875
Epoch [1/1], Batch [241], Loss: 100607.578125
Epoch [1/1], Batch [251], Loss: 95438.390625
Epoch [1/1], Batch [261], Loss: 97424.062500
Epoch [1/1], Batch [271], Loss: 96618.578125
Epoch [1/1], Batch [281], Loss: 95859.679688
Epoch [1/1], Batch [291], Loss: 95656.906250
Epoch [1/1], Batch [301], Loss: 94573.273438
Epoch [1/1], Batch [311], Loss: 95120.593750
Epoch [1/1], Batch [321], Loss: 92888.421875
Epoch [1/1], Batch [331], Loss: 90932.765625
Epoch [1/1], Batch [341], Loss: 86613.015625
Epoch [1/1], Batch [351], Loss: 96521.617188
Epoch [1/1], Batch [361], Loss: 95226.015625
Epoch [1/1], Batch [371], Loss: 98582.937500
Epoch [1/1], Batch [381], Loss: 94388.171875
Epoch [1/1], Batch [391], Loss: 92884.484375
Epoch [1/1], Batch [401], Loss: 92792.890625
Epoch [1/1], Batch [411], Loss: 92520.140625
Epoch [1/1], Batch [421], Loss: 96430.195312
Epoch [1/1], Batch [431], Loss: 89856.921875
Epoch [1/1], Batch [441], Loss: 93948.203125
Epoch [1/1], Batch [451], Loss: 92887.937500
Epoch [1/1], Batch [461], Loss: 88872.578125
Epoch [1/1], Batch [471], Loss: 91949.773438
Epoch [1/1], Batch [481], Loss: 97816.710938
Epoch [1/1], Batch [491], Loss: 96371.914062
Epoch [1/1], Batch [501], Loss: 89190.515625
Epoch [1/1], Batch [511], Loss: 93950.414062
Epoch [1/1], Batch [521], Loss: 91249.625000
Epoch [1/1], Batch [531], Loss: 92007.546875
Epoch [1/1], Batch [541], Loss: 92870.023438
Epoch [1/1], Batch [551], Loss: 89146.960938
Epoch [1/1], Batch [561], Loss: 97741.382812
Epoch [1/1], Batch [571], Loss: 90366.085938
Epoch [1/1], Batch [581], Loss: 90949.328125
Epoch [1/1], Batch [591], Loss: 92461.187500
Epoch [1/1], Batch [601], Loss: 94097.609375
Epoch [1/1], Batch [611], Loss: 90336.429688
Epoch [1/1], Batch [621], Loss: 92729.531250
Epoch [1/1], Batch [631], Loss: 93087.390625
Epoch [1/1], Batch [641], Loss: 93432.476562
Epoch [1/1], Batch [651], Loss: 96550.414062
Epoch [1/1], Batch [661], Loss: 93520.140625
Epoch [1/1], Batch [671], Loss: 93254.156250
Epoch [1/1], Batch [681], Loss: 92634.421875
Epoch [1/1], Batch [691], Loss: 99964.625000
Epoch [1/1], Batch [701], Loss: 92281.414062
Epoch [1/1], Batch [711], Loss: 96110.625000
Epoch [1/1], Batch [721], Loss: 91772.156250
Epoch [1/1], Batch [731], Loss: 87047.507812
Epoch [1/1], Batch [741], Loss: 89455.828125
Epoch [1/1], Batch [751], Loss: 87432.500000
Epoch [1/1], Batch [761], Loss: 86263.687500
Epoch [1/1], Batch [771], Loss: 91223.125000
Epoch [1/1], Batch [781], Loss: 89852.343750
Epoch [1/1], Batch [791], Loss: 92511.656250
Epoch [1/1], Batch [801], Loss: 90958.703125
Epoch [1/1], Batch [811], Loss: 93829.085938
Epoch [1/1], Batch [821], Loss: 92860.593750
Epoch [1/1], Batch [831], Loss: 89893.054688
Epoch [1/1], Batch [841], Loss: 86930.828125
Epoch [1/1], Batch [851], Loss: 96957.054688
Epoch [1/1], Batch [861], Loss: 91065.968750
Epoch [1/1], Batch [871], Loss: 91755.625000
Epoch [1/1], Batch [881], Loss: 92513.046875
Epoch [1/1], Batch [891], Loss: 90374.000000
Epoch [1/1], Batch [901], Loss: 93039.875000
Epoch [1/1], Batch [911], Loss: 91348.375000
Epoch [1/1], Batch [921], Loss: 91374.804688
Epoch [1/1], Batch [931], Loss: 91567.828125
Epoch [1/1], Batch [941], Loss: 95078.445312
Epoch [1/1], Batch [951], Loss: 93484.851562
Epoch [1/1], Batch [961], Loss: 91495.046875
Epoch [1/1], Batch [971], Loss: 92302.734375
Epoch [1/1], Batch [981], Loss: 90168.250000
Epoch [1/1], Batch [991], Loss: 99263.414062
Epoch [1/1], Batch [1001], Loss: 87876.304688
Epoch [1/1], Batch [1011], Loss: 92740.445312
Epoch [1/1], Batch [1021], Loss: 89573.468750
Epoch [1/1], Batch [1031], Loss: 92916.281250
Epoch [1/1], Batch [1041], Loss: 93115.656250
Epoch [1/1], Batch [1051], Loss: 96780.578125
Epoch [1/1], Batch [1061], Loss: 92855.218750
Epoch [1/1], Batch [1071], Loss: 91354.171875
Epoch [1/1], Batch [1081], Loss: 89277.125000
Epoch [1/1], Batch [1091], Loss: 90756.429688
Epoch [1/1], Batch [1101], Loss: 93904.281250
Epoch [1/1], Batch [1111], Loss: 87831.921875
Epoch [1/1], Batch [1121], Loss: 91915.585938
Epoch [1/1], Batch [1131], Loss: 92437.234375
Epoch [1/1], Batch [1141], Loss: 97697.656250
Epoch [1/1], Batch [1151], Loss: 91039.398438
Epoch [1/1], Batch [1161], Loss: 95798.734375
Epoch [1/1], Batch [1171], Loss: 93482.343750
Epoch [1/1], Batch [1181], Loss: 89732.515625
Epoch [1/1], Batch [1191], Loss: 91906.343750
Epoch [1/1], Batch [1201], Loss: 94908.781250
Epoch [1/1], Batch [1211], Loss: 84181.296875
Epoch [1/1], Batch [1221], Loss: 92166.460938
Epoch [1/1], Batch [1231], Loss: 89672.812500
Epoch [1/1], Batch [1241], Loss: 95239.296875
Epoch [1/1], Batch [1251], Loss: 87755.914062
Epoch [1/1], Batch [1261], Loss: 91168.179688
Epoch [1/1], Batch [1271], Loss: 90537.453125
Epoch [1/1], Batch [1281], Loss: 89413.007812
Epoch [1/1], Batch [1291], Loss: 96722.328125
Epoch [1/1], Batch [1301], Loss: 93514.398438
Epoch [1/1], Batch [1311], Loss: 93751.515625
Epoch [1/1], Batch [1321], Loss: 89858.679688
Epoch [1/1], Batch [1331], Loss: 86827.945312
Epoch [1/1], Batch [1341], Loss: 91290.578125
Epoch [1/1], Batch [1351], Loss: 83974.484375
Epoch [1/1], Batch [1361], Loss: 98020.796875
Epoch [1/1], Batch [1371], Loss: 94718.132812
Epoch [1/1], Batch [1381], Loss: 87164.734375
Epoch [1/1], Batch [1391], Loss: 91052.250000
Epoch [1/1], Batch [1401], Loss: 88778.367188
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 92754.5320
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 89202.9993
Elapsed time: 2719.03 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 93240.5284
Elapsed time: 2747.66 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 122002.546875
Epoch [1/1], Batch [11], Loss: 109083.609375
Epoch [1/1], Batch [21], Loss: 116319.015625
Epoch [1/1], Batch [31], Loss: 116067.531250
Epoch [1/1], Batch [41], Loss: 118137.546875
Epoch [1/1], Batch [51], Loss: 113358.906250
Epoch [1/1], Batch [61], Loss: 124838.234375
Epoch [1/1], Batch [71], Loss: 117506.406250
Epoch [1/1], Batch [81], Loss: 108472.265625
Epoch [1/1], Batch [91], Loss: 115916.046875
Epoch [1/1], Batch [101], Loss: 109789.523438
Epoch [1/1], Batch [111], Loss: 115056.968750
Epoch [1/1], Batch [121], Loss: 112597.921875
Epoch [1/1], Batch [131], Loss: 111545.000000
Epoch [1/1], Batch [141], Loss: 115286.250000
Epoch [1/1], Batch [151], Loss: 104977.281250
Epoch [1/1], Batch [161], Loss: 112670.531250
Epoch [1/1], Batch [171], Loss: 109296.812500
Epoch [1/1], Batch [181], Loss: 115006.273438
Epoch [1/1], Batch [191], Loss: 111569.320312
Epoch [1/1], Batch [201], Loss: 120891.171875
Epoch [1/1], Batch [211], Loss: 111682.656250
Epoch [1/1], Batch [221], Loss: 116408.968750
Epoch [1/1], Batch [231], Loss: 118889.687500
Epoch [1/1], Batch [241], Loss: 111221.234375
Epoch [1/1], Batch [251], Loss: 110964.085938
Epoch [1/1], Batch [261], Loss: 114813.578125
Epoch [1/1], Batch [271], Loss: 113241.312500
Epoch [1/1], Batch [281], Loss: 114198.125000
Epoch [1/1], Batch [291], Loss: 114217.734375
Epoch [1/1], Batch [301], Loss: 113207.234375
Epoch [1/1], Batch [311], Loss: 114225.343750
Epoch [1/1], Batch [321], Loss: 119684.359375
Epoch [1/1], Batch [331], Loss: 114241.656250
Epoch [1/1], Batch [341], Loss: 111177.671875
Epoch [1/1], Batch [351], Loss: 116144.562500
Epoch [1/1], Batch [361], Loss: 109435.875000
Epoch [1/1], Batch [371], Loss: 118813.445312
Epoch [1/1], Batch [381], Loss: 116606.867188
Epoch [1/1], Batch [391], Loss: 110041.937500
Epoch [1/1], Batch [401], Loss: 110163.164062
Epoch [1/1], Batch [411], Loss: 114694.015625
Epoch [1/1], Batch [421], Loss: 113378.421875
Epoch [1/1], Batch [431], Loss: 107424.765625
Epoch [1/1], Batch [441], Loss: 117045.062500
Epoch [1/1], Batch [451], Loss: 113543.703125
Epoch [1/1], Batch [461], Loss: 107135.843750
Epoch [1/1], Batch [471], Loss: 106834.375000
Epoch [1/1], Batch [481], Loss: 109010.000000
Epoch [1/1], Batch [491], Loss: 114192.531250
Epoch [1/1], Batch [501], Loss: 110000.515625
Epoch [1/1], Batch [511], Loss: 109733.546875
Epoch [1/1], Batch [521], Loss: 106000.367188
Epoch [1/1], Batch [531], Loss: 110661.015625
Epoch [1/1], Batch [541], Loss: 112836.000000
Epoch [1/1], Batch [551], Loss: 108708.828125
Epoch [1/1], Batch [561], Loss: 109582.125000
Epoch [1/1], Batch [571], Loss: 111718.546875
Epoch [1/1], Batch [581], Loss: 118384.046875
Epoch [1/1], Batch [591], Loss: 108438.484375
Epoch [1/1], Batch [601], Loss: 113874.226562
Epoch [1/1], Batch [611], Loss: 113714.742188
Epoch [1/1], Batch [621], Loss: 112242.328125
Epoch [1/1], Batch [631], Loss: 106375.156250
Epoch [1/1], Batch [641], Loss: 113781.390625
Epoch [1/1], Batch [651], Loss: 110148.476562
Epoch [1/1], Batch [661], Loss: 115234.312500
Epoch [1/1], Batch [671], Loss: 109324.156250
Epoch [1/1], Batch [681], Loss: 110844.734375
Epoch [1/1], Batch [691], Loss: 110309.554688
Epoch [1/1], Batch [701], Loss: 108626.515625
Epoch [1/1], Batch [711], Loss: 105133.718750
Epoch [1/1], Batch [721], Loss: 112334.453125
Epoch [1/1], Batch [731], Loss: 105679.234375
Epoch [1/1], Batch [741], Loss: 118214.203125
Epoch [1/1], Batch [751], Loss: 111082.367188
Epoch [1/1], Batch [761], Loss: 106560.093750
Epoch [1/1], Batch [771], Loss: 116131.679688
Epoch [1/1], Batch [781], Loss: 111710.390625
Epoch [1/1], Batch [791], Loss: 108817.812500
Epoch [1/1], Batch [801], Loss: 104894.976562
Epoch [1/1], Batch [811], Loss: 108752.054688
Epoch [1/1], Batch [821], Loss: 108755.921875
Epoch [1/1], Batch [831], Loss: 110202.093750
Epoch [1/1], Batch [841], Loss: 112282.281250
Epoch [1/1], Batch [851], Loss: 105861.164062
Epoch [1/1], Batch [861], Loss: 115636.578125
Epoch [1/1], Batch [871], Loss: 105089.015625
Epoch [1/1], Batch [881], Loss: 112764.281250
Epoch [1/1], Batch [891], Loss: 107387.492188
Epoch [1/1], Batch [901], Loss: 112616.640625
Epoch [1/1], Batch [911], Loss: 109424.312500
Epoch [1/1], Batch [921], Loss: 106772.796875
Epoch [1/1], Batch [931], Loss: 114373.406250
Epoch [1/1], Batch [941], Loss: 111389.960938
Epoch [1/1], Batch [951], Loss: 110663.046875
Epoch [1/1], Batch [961], Loss: 106440.117188
Epoch [1/1], Batch [971], Loss: 113830.382812
Epoch [1/1], Batch [981], Loss: 109540.093750
Epoch [1/1], Batch [991], Loss: 111390.648438
Epoch [1/1], Batch [1001], Loss: 108162.023438
Epoch [1/1], Batch [1011], Loss: 109442.359375
Epoch [1/1], Batch [1021], Loss: 113726.078125
Epoch [1/1], Batch [1031], Loss: 109805.406250
Epoch [1/1], Batch [1041], Loss: 104679.718750
Epoch [1/1], Batch [1051], Loss: 108477.445312
Epoch [1/1], Batch [1061], Loss: 106847.843750
Epoch [1/1], Batch [1071], Loss: 108099.109375
Epoch [1/1], Batch [1081], Loss: 113042.281250
Epoch [1/1], Batch [1091], Loss: 107010.671875
Epoch [1/1], Batch [1101], Loss: 111344.062500
Epoch [1/1], Batch [1111], Loss: 111237.820312
Epoch [1/1], Batch [1121], Loss: 109722.476562
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 111734.4401
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 107452.3493
Elapsed time: 3457.31 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 113211.1477
Elapsed time: 3484.10 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 129445.875000
Epoch [1/1], Batch [11], Loss: 134024.046875
Epoch [1/1], Batch [21], Loss: 133777.781250
Epoch [1/1], Batch [31], Loss: 133329.562500
Epoch [1/1], Batch [41], Loss: 133586.406250
Epoch [1/1], Batch [51], Loss: 135791.015625
Epoch [1/1], Batch [61], Loss: 136386.484375
Epoch [1/1], Batch [71], Loss: 129128.562500
Epoch [1/1], Batch [81], Loss: 130770.953125
Epoch [1/1], Batch [91], Loss: 138076.500000
Epoch [1/1], Batch [101], Loss: 132430.500000
Epoch [1/1], Batch [111], Loss: 131872.937500
Epoch [1/1], Batch [121], Loss: 129940.265625
Epoch [1/1], Batch [131], Loss: 135575.718750
Epoch [1/1], Batch [141], Loss: 130087.078125
Epoch [1/1], Batch [151], Loss: 129753.984375
Epoch [1/1], Batch [161], Loss: 132566.593750
Epoch [1/1], Batch [171], Loss: 135284.437500
Epoch [1/1], Batch [181], Loss: 134688.765625
Epoch [1/1], Batch [191], Loss: 134108.406250
Epoch [1/1], Batch [201], Loss: 135163.875000
Epoch [1/1], Batch [211], Loss: 134263.062500
Epoch [1/1], Batch [221], Loss: 129192.929688
Epoch [1/1], Batch [231], Loss: 128195.828125
Epoch [1/1], Batch [241], Loss: 126514.195312
Epoch [1/1], Batch [251], Loss: 135935.890625
Epoch [1/1], Batch [261], Loss: 138254.968750
Epoch [1/1], Batch [271], Loss: 131032.718750
Epoch [1/1], Batch [281], Loss: 126577.101562
Epoch [1/1], Batch [291], Loss: 134786.562500
Epoch [1/1], Batch [301], Loss: 130740.609375
Epoch [1/1], Batch [311], Loss: 137687.156250
Epoch [1/1], Batch [321], Loss: 137888.562500
Epoch [1/1], Batch [331], Loss: 130977.046875
Epoch [1/1], Batch [341], Loss: 133441.109375
Epoch [1/1], Batch [351], Loss: 135631.921875
Epoch [1/1], Batch [361], Loss: 134315.562500
Epoch [1/1], Batch [371], Loss: 134169.281250
Epoch [1/1], Batch [381], Loss: 133100.250000
Epoch [1/1], Batch [391], Loss: 129718.164062
Epoch [1/1], Batch [401], Loss: 131893.718750
Epoch [1/1], Batch [411], Loss: 130240.171875
Epoch [1/1], Batch [421], Loss: 131890.921875
Epoch [1/1], Batch [431], Loss: 130992.062500
Epoch [1/1], Batch [441], Loss: 128940.976562
Epoch [1/1], Batch [451], Loss: 126676.578125
Epoch [1/1], Batch [461], Loss: 126722.609375
Epoch [1/1], Batch [471], Loss: 124913.484375
Epoch [1/1], Batch [481], Loss: 130910.328125
Epoch [1/1], Batch [491], Loss: 131263.750000
Epoch [1/1], Batch [501], Loss: 134224.906250
Epoch [1/1], Batch [511], Loss: 123349.804688
Epoch [1/1], Batch [521], Loss: 127037.359375
Epoch [1/1], Batch [531], Loss: 131590.750000
Epoch [1/1], Batch [541], Loss: 133987.343750
Epoch [1/1], Batch [551], Loss: 126077.187500
Epoch [1/1], Batch [561], Loss: 131246.750000
Epoch [1/1], Batch [571], Loss: 134686.156250
Epoch [1/1], Batch [581], Loss: 134395.656250
Epoch [1/1], Batch [591], Loss: 131150.000000
Epoch [1/1], Batch [601], Loss: 130506.187500
Epoch [1/1], Batch [611], Loss: 127597.328125
Epoch [1/1], Batch [621], Loss: 124373.851562
Epoch [1/1], Batch [631], Loss: 129082.218750
Epoch [1/1], Batch [641], Loss: 131418.531250
Epoch [1/1], Batch [651], Loss: 140393.703125
Epoch [1/1], Batch [661], Loss: 124977.625000
Epoch [1/1], Batch [671], Loss: 131437.750000
Epoch [1/1], Batch [681], Loss: 135930.812500
Epoch [1/1], Batch [691], Loss: 137921.203125
Epoch [1/1], Batch [701], Loss: 135103.218750
Epoch [1/1], Batch [711], Loss: 125326.054688
Epoch [1/1], Batch [721], Loss: 136247.218750
Epoch [1/1], Batch [731], Loss: 126453.421875
Epoch [1/1], Batch [741], Loss: 124479.671875
Epoch [1/1], Batch [751], Loss: 139603.609375
Epoch [1/1], Batch [761], Loss: 134031.531250
Epoch [1/1], Batch [771], Loss: 132895.812500
Epoch [1/1], Batch [781], Loss: 127222.093750
Epoch [1/1], Batch [791], Loss: 128267.828125
Epoch [1/1], Batch [801], Loss: 128241.828125
Epoch [1/1], Batch [811], Loss: 131328.859375
Epoch [1/1], Batch [821], Loss: 135497.875000
Epoch [1/1], Batch [831], Loss: 133734.031250
Epoch [1/1], Batch [841], Loss: 137968.671875
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 131240.7384
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 129195.9943
Elapsed time: 4140.88 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 135497.7028
Elapsed time: 4163.99 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 161592.953125
Epoch [1/1], Batch [11], Loss: 142740.812500
Epoch [1/1], Batch [21], Loss: 146778.656250
Epoch [1/1], Batch [31], Loss: 158818.328125
Epoch [1/1], Batch [41], Loss: 154102.218750
Epoch [1/1], Batch [51], Loss: 153077.625000
Epoch [1/1], Batch [61], Loss: 152216.750000
Epoch [1/1], Batch [71], Loss: 149606.656250
Epoch [1/1], Batch [81], Loss: 153986.015625
Epoch [1/1], Batch [91], Loss: 152336.078125
Epoch [1/1], Batch [101], Loss: 153829.031250
Epoch [1/1], Batch [111], Loss: 143179.187500
Epoch [1/1], Batch [121], Loss: 152734.218750
Epoch [1/1], Batch [131], Loss: 153357.812500
Epoch [1/1], Batch [141], Loss: 152225.218750
Epoch [1/1], Batch [151], Loss: 152861.781250
Epoch [1/1], Batch [161], Loss: 148411.468750
Epoch [1/1], Batch [171], Loss: 141924.781250
Epoch [1/1], Batch [181], Loss: 152329.000000
Epoch [1/1], Batch [191], Loss: 144120.453125
Epoch [1/1], Batch [201], Loss: 150915.390625
Epoch [1/1], Batch [211], Loss: 159103.203125
Epoch [1/1], Batch [221], Loss: 149017.750000
Epoch [1/1], Batch [231], Loss: 150468.906250
Epoch [1/1], Batch [241], Loss: 141644.109375
Epoch [1/1], Batch [251], Loss: 152437.750000
Epoch [1/1], Batch [261], Loss: 150539.500000
Epoch [1/1], Batch [271], Loss: 142118.781250
Epoch [1/1], Batch [281], Loss: 154432.484375
Epoch [1/1], Batch [291], Loss: 150930.812500
Epoch [1/1], Batch [301], Loss: 152960.375000
Epoch [1/1], Batch [311], Loss: 146915.234375
Epoch [1/1], Batch [321], Loss: 142903.656250
Epoch [1/1], Batch [331], Loss: 156351.406250
Epoch [1/1], Batch [341], Loss: 154565.031250
Epoch [1/1], Batch [351], Loss: 151706.937500
Epoch [1/1], Batch [361], Loss: 152727.750000
Epoch [1/1], Batch [371], Loss: 146904.734375
Epoch [1/1], Batch [381], Loss: 152870.281250
Epoch [1/1], Batch [391], Loss: 154846.312500
Epoch [1/1], Batch [401], Loss: 149844.734375
Epoch [1/1], Batch [411], Loss: 147708.437500
Epoch [1/1], Batch [421], Loss: 150764.781250
Epoch [1/1], Batch [431], Loss: 152293.406250
Epoch [1/1], Batch [441], Loss: 154037.187500
Epoch [1/1], Batch [451], Loss: 154659.890625
Epoch [1/1], Batch [461], Loss: 149732.968750
Epoch [1/1], Batch [471], Loss: 153132.093750
Epoch [1/1], Batch [481], Loss: 152289.906250
Epoch [1/1], Batch [491], Loss: 150557.937500
Epoch [1/1], Batch [501], Loss: 155360.218750
Epoch [1/1], Batch [511], Loss: 151998.734375
Epoch [1/1], Batch [521], Loss: 152338.218750
Epoch [1/1], Batch [531], Loss: 148165.093750
Epoch [1/1], Batch [541], Loss: 148153.593750
Epoch [1/1], Batch [551], Loss: 155184.093750
Epoch [1/1], Batch [561], Loss: 143216.656250
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 150539.9920
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 144533.8906
Elapsed time: 4705.00 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 153602.0197
Elapsed time: 4722.44 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 178156.656250
Epoch [1/1], Batch [11], Loss: 169203.781250
Epoch [1/1], Batch [21], Loss: 172600.187500
Epoch [1/1], Batch [31], Loss: 165953.406250
Epoch [1/1], Batch [41], Loss: 168669.296875
Epoch [1/1], Batch [51], Loss: 170065.593750
Epoch [1/1], Batch [61], Loss: 174838.437500
Epoch [1/1], Batch [71], Loss: 182813.437500
Epoch [1/1], Batch [81], Loss: 177679.500000
Epoch [1/1], Batch [91], Loss: 177949.937500
Epoch [1/1], Batch [101], Loss: 173388.718750
Epoch [1/1], Batch [111], Loss: 183885.140625
Epoch [1/1], Batch [121], Loss: 172141.468750
Epoch [1/1], Batch [131], Loss: 165992.296875
Epoch [1/1], Batch [141], Loss: 172562.687500
Epoch [1/1], Batch [151], Loss: 169147.296875
Epoch [1/1], Batch [161], Loss: 172075.546875
Epoch [1/1], Batch [171], Loss: 177036.984375
Epoch [1/1], Batch [181], Loss: 176231.875000
Epoch [1/1], Batch [191], Loss: 174723.578125
Epoch [1/1], Batch [201], Loss: 164739.484375
Epoch [1/1], Batch [211], Loss: 174877.812500
Epoch [1/1], Batch [221], Loss: 177212.265625
Epoch [1/1], Batch [231], Loss: 171748.531250
Epoch [1/1], Batch [241], Loss: 173515.843750
Epoch [1/1], Batch [251], Loss: 168108.531250
Epoch [1/1], Batch [261], Loss: 179199.171875
Epoch [1/1], Batch [271], Loss: 167833.750000
Epoch [1/1], Batch [281], Loss: 173627.531250
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 172343.6449
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 161988.1057
Elapsed time: 5029.29 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 167812.2850
Elapsed time: 5039.01 seconds

Training complete!
Totoal elapsed time: 5039.01 seconds
CUDA is available!
