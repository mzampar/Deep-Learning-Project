Starting job 1010037
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = True,
    transpose = True,
    use_lstm_output = True,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 357313.375000
Epoch [1/1], Batch [11], Loss: 136965.421875
Epoch [1/1], Batch [21], Loss: 104450.375000
Epoch [1/1], Batch [31], Loss: 93842.531250
Epoch [1/1], Batch [41], Loss: 89885.492188
Epoch [1/1], Batch [51], Loss: 91175.968750
Epoch [1/1], Batch [61], Loss: 89186.265625
Epoch [1/1], Batch [71], Loss: 86397.953125
Epoch [1/1], Batch [81], Loss: 86402.437500
Epoch [1/1], Batch [91], Loss: 87061.531250
Epoch [1/1], Batch [101], Loss: 82718.468750
Epoch [1/1], Batch [111], Loss: 85175.320312
Epoch [1/1], Batch [121], Loss: 82897.234375
Epoch [1/1], Batch [131], Loss: 78552.554688
Epoch [1/1], Batch [141], Loss: 74445.304688
Epoch [1/1], Batch [151], Loss: 63634.476562
Epoch [1/1], Batch [161], Loss: 60089.625000
Epoch [1/1], Batch [171], Loss: 56686.750000
Epoch [1/1], Batch [181], Loss: 53501.683594
Epoch [1/1], Batch [191], Loss: 52252.917969
Epoch [1/1], Batch [201], Loss: 54039.492188
Epoch [1/1], Batch [211], Loss: 55958.597656
Epoch [1/1], Batch [221], Loss: 49884.734375
Epoch [1/1], Batch [231], Loss: 54034.617188
Epoch [1/1], Batch [241], Loss: 52400.871094
Epoch [1/1], Batch [251], Loss: 50063.257812
Epoch [1/1], Batch [261], Loss: 48257.425781
Epoch [1/1], Batch [271], Loss: 48662.578125
Epoch [1/1], Batch [281], Loss: 50443.226562
Epoch [1/1], Batch [291], Loss: 46266.222656
Epoch [1/1], Batch [301], Loss: 49952.914062
Epoch [1/1], Batch [311], Loss: 49314.953125
Epoch [1/1], Batch [321], Loss: 45918.539062
Epoch [1/1], Batch [331], Loss: 49137.742188
Epoch [1/1], Batch [341], Loss: 48059.320312
Epoch [1/1], Batch [351], Loss: 47379.296875
Epoch [1/1], Batch [361], Loss: 47030.855469
Epoch [1/1], Batch [371], Loss: 45549.453125
Epoch [1/1], Batch [381], Loss: 45489.382812
Epoch [1/1], Batch [391], Loss: 45611.382812
Epoch [1/1], Batch [401], Loss: 46412.015625
Epoch [1/1], Batch [411], Loss: 46314.593750
Epoch [1/1], Batch [421], Loss: 47641.351562
Epoch [1/1], Batch [431], Loss: 45662.832031
Epoch [1/1], Batch [441], Loss: 44837.976562
Epoch [1/1], Batch [451], Loss: 45318.601562
Epoch [1/1], Batch [461], Loss: 46576.257812
Epoch [1/1], Batch [471], Loss: 47153.750000
Epoch [1/1], Batch [481], Loss: 47057.253906
Epoch [1/1], Batch [491], Loss: 48209.996094
Epoch [1/1], Batch [501], Loss: 45706.949219
Epoch [1/1], Batch [511], Loss: 46475.214844
Epoch [1/1], Batch [521], Loss: 46465.070312
Epoch [1/1], Batch [531], Loss: 46427.699219
Epoch [1/1], Batch [541], Loss: 45901.914062
Epoch [1/1], Batch [551], Loss: 44109.207031
Epoch [1/1], Batch [561], Loss: 43607.804688
Epoch [1/1], Batch [571], Loss: 47563.179688
Epoch [1/1], Batch [581], Loss: 45250.902344
Epoch [1/1], Batch [591], Loss: 45898.859375
Epoch [1/1], Batch [601], Loss: 44254.585938
Epoch [1/1], Batch [611], Loss: 45723.394531
Epoch [1/1], Batch [621], Loss: 43330.535156
Epoch [1/1], Batch [631], Loss: 46428.097656
Epoch [1/1], Batch [641], Loss: 43635.453125
Epoch [1/1], Batch [651], Loss: 44571.714844
Epoch [1/1], Batch [661], Loss: 45648.296875
Epoch [1/1], Batch [671], Loss: 45794.179688
Epoch [1/1], Batch [681], Loss: 46786.996094
Epoch [1/1], Batch [691], Loss: 47829.304688
Epoch [1/1], Batch [701], Loss: 43596.511719
Epoch [1/1], Batch [711], Loss: 43842.792969
Epoch [1/1], Batch [721], Loss: 46162.320312
Epoch [1/1], Batch [731], Loss: 42649.058594
Epoch [1/1], Batch [741], Loss: 44554.210938
Epoch [1/1], Batch [751], Loss: 44212.851562
Epoch [1/1], Batch [761], Loss: 43739.484375
Epoch [1/1], Batch [771], Loss: 43096.578125
Epoch [1/1], Batch [781], Loss: 42573.253906
Epoch [1/1], Batch [791], Loss: 43269.453125
Epoch [1/1], Batch [801], Loss: 44579.136719
Epoch [1/1], Batch [811], Loss: 45210.296875
Epoch [1/1], Batch [821], Loss: 44023.089844
Epoch [1/1], Batch [831], Loss: 41680.789062
Epoch [1/1], Batch [841], Loss: 43882.117188
Epoch [1/1], Batch [851], Loss: 42881.539062
Epoch [1/1], Batch [861], Loss: 45217.652344
Epoch [1/1], Batch [871], Loss: 41668.117188
Epoch [1/1], Batch [881], Loss: 42412.218750
Epoch [1/1], Batch [891], Loss: 44180.156250
Epoch [1/1], Batch [901], Loss: 41132.859375
Epoch [1/1], Batch [911], Loss: 42216.218750
Epoch [1/1], Batch [921], Loss: 44042.593750
Epoch [1/1], Batch [931], Loss: 41602.250000
Epoch [1/1], Batch [941], Loss: 43675.035156
Epoch [1/1], Batch [951], Loss: 44870.851562
Epoch [1/1], Batch [961], Loss: 42113.113281
Epoch [1/1], Batch [971], Loss: 43695.781250
Epoch [1/1], Batch [981], Loss: 41729.191406
Epoch [1/1], Batch [991], Loss: 44047.015625
Epoch [1/1], Batch [1001], Loss: 41565.089844
Epoch [1/1], Batch [1011], Loss: 44029.691406
Epoch [1/1], Batch [1021], Loss: 43863.351562
Epoch [1/1], Batch [1031], Loss: 42248.390625
Epoch [1/1], Batch [1041], Loss: 43673.230469
Epoch [1/1], Batch [1051], Loss: 41086.105469
Epoch [1/1], Batch [1061], Loss: 42750.234375
Epoch [1/1], Batch [1071], Loss: 42946.957031
Epoch [1/1], Batch [1081], Loss: 45109.523438
Epoch [1/1], Batch [1091], Loss: 43212.207031
Epoch [1/1], Batch [1101], Loss: 44035.773438
Epoch [1/1], Batch [1111], Loss: 41271.265625
Epoch [1/1], Batch [1121], Loss: 42002.445312
Epoch [1/1], Batch [1131], Loss: 45369.726562
Epoch [1/1], Batch [1141], Loss: 40555.375000
Epoch [1/1], Batch [1151], Loss: 43405.960938
Epoch [1/1], Batch [1161], Loss: 42677.179688
Epoch [1/1], Batch [1171], Loss: 41243.148438
Epoch [1/1], Batch [1181], Loss: 41509.812500
Epoch [1/1], Batch [1191], Loss: 41910.203125
Epoch [1/1], Batch [1201], Loss: 40789.292969
Epoch [1/1], Batch [1211], Loss: 42819.125000
Epoch [1/1], Batch [1221], Loss: 42617.757812
Epoch [1/1], Batch [1231], Loss: 41950.289062
Epoch [1/1], Batch [1241], Loss: 39689.273438
Epoch [1/1], Batch [1251], Loss: 42552.343750
Epoch [1/1], Batch [1261], Loss: 42655.093750
Epoch [1/1], Batch [1271], Loss: 42048.703125
Epoch [1/1], Batch [1281], Loss: 40951.937500
Epoch [1/1], Batch [1291], Loss: 44214.828125
Epoch [1/1], Batch [1301], Loss: 41573.500000
Epoch [1/1], Batch [1311], Loss: 42642.035156
Epoch [1/1], Batch [1321], Loss: 42895.468750
Epoch [1/1], Batch [1331], Loss: 38583.718750
Epoch [1/1], Batch [1341], Loss: 43681.546875
Epoch [1/1], Batch [1351], Loss: 42320.292969
Epoch [1/1], Batch [1361], Loss: 42208.195312
Epoch [1/1], Batch [1371], Loss: 39478.250000
Epoch [1/1], Batch [1381], Loss: 42317.085938
Epoch [1/1], Batch [1391], Loss: 42127.363281
Epoch [1/1], Batch [1401], Loss: 40857.722656
Epoch [1/1], Batch [1411], Loss: 43355.343750
Epoch [1/1], Batch [1421], Loss: 42303.906250
Epoch [1/1], Batch [1431], Loss: 43494.582031
Epoch [1/1], Batch [1441], Loss: 41281.851562
Epoch [1/1], Batch [1451], Loss: 42855.765625
Epoch [1/1], Batch [1461], Loss: 43191.531250
Epoch [1/1], Batch [1471], Loss: 41045.984375
Epoch [1/1], Batch [1481], Loss: 40608.773438
Epoch [1/1], Batch [1491], Loss: 41731.671875
Epoch [1/1], Batch [1501], Loss: 41333.429688
Epoch [1/1], Batch [1511], Loss: 40389.003906
Epoch [1/1], Batch [1521], Loss: 42659.695312
Epoch [1/1], Batch [1531], Loss: 42358.515625
Epoch [1/1], Batch [1541], Loss: 38445.429688
Epoch [1/1], Batch [1551], Loss: 42547.230469
Epoch [1/1], Batch [1561], Loss: 42759.035156
Epoch [1/1], Batch [1571], Loss: 41883.953125
Epoch [1/1], Batch [1581], Loss: 41050.230469
Epoch [1/1], Batch [1591], Loss: 40914.511719
Epoch [1/1], Batch [1601], Loss: 42684.726562
Epoch [1/1], Batch [1611], Loss: 41948.093750
Epoch [1/1], Batch [1621], Loss: 41895.359375
Epoch [1/1], Batch [1631], Loss: 40363.878906
Epoch [1/1], Batch [1641], Loss: 40336.687500
Epoch [1/1], Batch [1651], Loss: 41740.730469
Epoch [1/1], Batch [1661], Loss: 41647.945312
Epoch [1/1], Batch [1671], Loss: 41885.890625
Epoch [1/1], Batch [1681], Loss: 41707.726562
Epoch [1/1], Batch [1691], Loss: 44360.351562
Epoch [1/1], Batch [1701], Loss: 43389.105469
Epoch [1/1], Batch [1711], Loss: 40657.148438
Epoch [1/1], Batch [1721], Loss: 43663.671875
Epoch [1/1], Batch [1731], Loss: 39187.031250
Epoch [1/1], Batch [1741], Loss: 39886.613281
Epoch [1/1], Batch [1751], Loss: 40844.039062
Epoch [1/1], Batch [1761], Loss: 41307.648438
Epoch [1/1], Batch [1771], Loss: 41306.007812
Epoch [1/1], Batch [1781], Loss: 40699.976562
Epoch [1/1], Batch [1791], Loss: 43303.070312
Epoch [1/1], Batch [1801], Loss: 42361.238281
Epoch [1/1], Batch [1811], Loss: 37967.382812
Epoch [1/1], Batch [1821], Loss: 40617.500000
Epoch [1/1], Batch [1831], Loss: 39295.933594
Epoch [1/1], Batch [1841], Loss: 41542.082031
Epoch [1/1], Batch [1851], Loss: 41984.726562
Epoch [1/1], Batch [1861], Loss: 40543.703125
Epoch [1/1], Batch [1871], Loss: 39738.312500
Epoch [1/1], Batch [1881], Loss: 40808.464844
Epoch [1/1], Batch [1891], Loss: 40145.078125
Epoch [1/1], Batch [1901], Loss: 41489.781250
Epoch [1/1], Batch [1911], Loss: 40491.050781
Epoch [1/1], Batch [1921], Loss: 41150.796875
Epoch [1/1], Batch [1931], Loss: 42438.859375
Epoch [1/1], Batch [1941], Loss: 41373.523438
Epoch [1/1], Batch [1951], Loss: 40455.382812
Epoch [1/1], Batch [1961], Loss: 40561.042969
Epoch [1/1], Batch [1971], Loss: 39720.800781
Epoch [1/1], Batch [1981], Loss: 39467.453125
Epoch [1/1], Batch [1991], Loss: 40601.253906
Epoch [1/1], Batch [2001], Loss: 43334.015625
Epoch [1/1], Batch [2011], Loss: 40971.562500
Epoch [1/1], Batch [2021], Loss: 39514.421875
Epoch [1/1], Batch [2031], Loss: 40311.101562
Epoch [1/1], Batch [2041], Loss: 38743.460938
Epoch [1/1], Batch [2051], Loss: 39065.765625
Epoch [1/1], Batch [2061], Loss: 38529.265625
Epoch [1/1], Batch [2071], Loss: 39286.488281
Epoch [1/1], Batch [2081], Loss: 39506.125000
Epoch [1/1], Batch [2091], Loss: 40726.136719
Epoch [1/1], Batch [2101], Loss: 39963.101562
Epoch [1/1], Batch [2111], Loss: 41053.265625
Epoch [1/1], Batch [2121], Loss: 40097.921875
Epoch [1/1], Batch [2131], Loss: 39200.296875
Epoch [1/1], Batch [2141], Loss: 39427.171875
Epoch [1/1], Batch [2151], Loss: 41089.761719
Epoch [1/1], Batch [2161], Loss: 38779.015625
Epoch [1/1], Batch [2171], Loss: 38866.285156
Epoch [1/1], Batch [2181], Loss: 40818.972656
Epoch [1/1], Batch [2191], Loss: 41841.953125
Epoch [1/1], Batch [2201], Loss: 38873.562500
Epoch [1/1], Batch [2211], Loss: 40636.484375
Epoch [1/1], Batch [2221], Loss: 39124.742188
Epoch [1/1], Batch [2231], Loss: 39889.460938
Epoch [1/1], Batch [2241], Loss: 39668.648438
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 46810.8756
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 39805.0024
Elapsed time: 488.34 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 40319.9943
Elapsed time: 509.04 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 83082.210938
Epoch [1/1], Batch [11], Loss: 116957.710938
Epoch [1/1], Batch [21], Loss: 96686.640625
Epoch [1/1], Batch [31], Loss: 89312.421875
Epoch [1/1], Batch [41], Loss: 80511.343750
Epoch [1/1], Batch [51], Loss: 80405.312500
Epoch [1/1], Batch [61], Loss: 80123.296875
Epoch [1/1], Batch [71], Loss: 78612.312500
Epoch [1/1], Batch [81], Loss: 75402.234375
Epoch [1/1], Batch [91], Loss: 74232.250000
Epoch [1/1], Batch [101], Loss: 76125.304688
Epoch [1/1], Batch [111], Loss: 70955.609375
Epoch [1/1], Batch [121], Loss: 75951.406250
Epoch [1/1], Batch [131], Loss: 70899.492188
Epoch [1/1], Batch [141], Loss: 72826.328125
Epoch [1/1], Batch [151], Loss: 71717.265625
Epoch [1/1], Batch [161], Loss: 73048.609375
Epoch [1/1], Batch [171], Loss: 73135.468750
Epoch [1/1], Batch [181], Loss: 72515.085938
Epoch [1/1], Batch [191], Loss: 70833.179688
Epoch [1/1], Batch [201], Loss: 70313.164062
Epoch [1/1], Batch [211], Loss: 70137.570312
Epoch [1/1], Batch [221], Loss: 71833.531250
Epoch [1/1], Batch [231], Loss: 70637.000000
Epoch [1/1], Batch [241], Loss: 69692.398438
Epoch [1/1], Batch [251], Loss: 72787.390625
Epoch [1/1], Batch [261], Loss: 68651.734375
Epoch [1/1], Batch [271], Loss: 69277.500000
Epoch [1/1], Batch [281], Loss: 70702.218750
Epoch [1/1], Batch [291], Loss: 71387.156250
Epoch [1/1], Batch [301], Loss: 73867.718750
Epoch [1/1], Batch [311], Loss: 70137.015625
Epoch [1/1], Batch [321], Loss: 68792.984375
Epoch [1/1], Batch [331], Loss: 64471.902344
Epoch [1/1], Batch [341], Loss: 70503.851562
Epoch [1/1], Batch [351], Loss: 64948.472656
Epoch [1/1], Batch [361], Loss: 68591.031250
Epoch [1/1], Batch [371], Loss: 65381.718750
Epoch [1/1], Batch [381], Loss: 66514.078125
Epoch [1/1], Batch [391], Loss: 70138.437500
Epoch [1/1], Batch [401], Loss: 70824.937500
Epoch [1/1], Batch [411], Loss: 65521.097656
Epoch [1/1], Batch [421], Loss: 67340.796875
Epoch [1/1], Batch [431], Loss: 67487.390625
Epoch [1/1], Batch [441], Loss: 69689.914062
Epoch [1/1], Batch [451], Loss: 67531.562500
Epoch [1/1], Batch [461], Loss: 67111.898438
Epoch [1/1], Batch [471], Loss: 65096.335938
Epoch [1/1], Batch [481], Loss: 63907.367188
Epoch [1/1], Batch [491], Loss: 66653.820312
Epoch [1/1], Batch [501], Loss: 64911.117188
Epoch [1/1], Batch [511], Loss: 63633.503906
Epoch [1/1], Batch [521], Loss: 62619.984375
Epoch [1/1], Batch [531], Loss: 65388.937500
Epoch [1/1], Batch [541], Loss: 66429.500000
Epoch [1/1], Batch [551], Loss: 66326.281250
Epoch [1/1], Batch [561], Loss: 67872.093750
Epoch [1/1], Batch [571], Loss: 66327.406250
Epoch [1/1], Batch [581], Loss: 64914.851562
Epoch [1/1], Batch [591], Loss: 65963.632812
Epoch [1/1], Batch [601], Loss: 65416.226562
Epoch [1/1], Batch [611], Loss: 66197.031250
Epoch [1/1], Batch [621], Loss: 65874.289062
Epoch [1/1], Batch [631], Loss: 67390.757812
Epoch [1/1], Batch [641], Loss: 63422.062500
Epoch [1/1], Batch [651], Loss: 65433.859375
Epoch [1/1], Batch [661], Loss: 66054.828125
Epoch [1/1], Batch [671], Loss: 66972.312500
Epoch [1/1], Batch [681], Loss: 64287.714844
Epoch [1/1], Batch [691], Loss: 63529.992188
Epoch [1/1], Batch [701], Loss: 66398.265625
Epoch [1/1], Batch [711], Loss: 65788.078125
Epoch [1/1], Batch [721], Loss: 64507.300781
Epoch [1/1], Batch [731], Loss: 63399.058594
Epoch [1/1], Batch [741], Loss: 65807.562500
Epoch [1/1], Batch [751], Loss: 66065.343750
Epoch [1/1], Batch [761], Loss: 65854.015625
Epoch [1/1], Batch [771], Loss: 64519.003906
Epoch [1/1], Batch [781], Loss: 64746.183594
Epoch [1/1], Batch [791], Loss: 65269.906250
Epoch [1/1], Batch [801], Loss: 61128.910156
Epoch [1/1], Batch [811], Loss: 64301.007812
Epoch [1/1], Batch [821], Loss: 63769.890625
Epoch [1/1], Batch [831], Loss: 57627.472656
Epoch [1/1], Batch [841], Loss: 64805.347656
Epoch [1/1], Batch [851], Loss: 64507.003906
Epoch [1/1], Batch [861], Loss: 65028.015625
Epoch [1/1], Batch [871], Loss: 63841.730469
Epoch [1/1], Batch [881], Loss: 61451.000000
Epoch [1/1], Batch [891], Loss: 63622.031250
Epoch [1/1], Batch [901], Loss: 64768.250000
Epoch [1/1], Batch [911], Loss: 63431.656250
Epoch [1/1], Batch [921], Loss: 63818.781250
Epoch [1/1], Batch [931], Loss: 63802.492188
Epoch [1/1], Batch [941], Loss: 67114.843750
Epoch [1/1], Batch [951], Loss: 61988.781250
Epoch [1/1], Batch [961], Loss: 58964.308594
Epoch [1/1], Batch [971], Loss: 63674.164062
Epoch [1/1], Batch [981], Loss: 64019.171875
Epoch [1/1], Batch [991], Loss: 66866.515625
Epoch [1/1], Batch [1001], Loss: 63408.054688
Epoch [1/1], Batch [1011], Loss: 61717.097656
Epoch [1/1], Batch [1021], Loss: 60263.671875
Epoch [1/1], Batch [1031], Loss: 61965.242188
Epoch [1/1], Batch [1041], Loss: 63812.953125
Epoch [1/1], Batch [1051], Loss: 62533.281250
Epoch [1/1], Batch [1061], Loss: 61505.054688
Epoch [1/1], Batch [1071], Loss: 62633.148438
Epoch [1/1], Batch [1081], Loss: 61440.636719
Epoch [1/1], Batch [1091], Loss: 62049.152344
Epoch [1/1], Batch [1101], Loss: 63031.753906
Epoch [1/1], Batch [1111], Loss: 61965.289062
Epoch [1/1], Batch [1121], Loss: 64614.617188
Epoch [1/1], Batch [1131], Loss: 64496.640625
Epoch [1/1], Batch [1141], Loss: 61820.015625
Epoch [1/1], Batch [1151], Loss: 59654.722656
Epoch [1/1], Batch [1161], Loss: 62202.718750
Epoch [1/1], Batch [1171], Loss: 60865.113281
Epoch [1/1], Batch [1181], Loss: 62816.601562
Epoch [1/1], Batch [1191], Loss: 63437.109375
Epoch [1/1], Batch [1201], Loss: 63977.093750
Epoch [1/1], Batch [1211], Loss: 65085.808594
Epoch [1/1], Batch [1221], Loss: 60429.488281
Epoch [1/1], Batch [1231], Loss: 59660.128906
Epoch [1/1], Batch [1241], Loss: 60290.835938
Epoch [1/1], Batch [1251], Loss: 60729.843750
Epoch [1/1], Batch [1261], Loss: 62279.625000
Epoch [1/1], Batch [1271], Loss: 62238.039062
Epoch [1/1], Batch [1281], Loss: 63407.960938
Epoch [1/1], Batch [1291], Loss: 62613.761719
Epoch [1/1], Batch [1301], Loss: 60737.632812
Epoch [1/1], Batch [1311], Loss: 59408.394531
Epoch [1/1], Batch [1321], Loss: 61611.398438
Epoch [1/1], Batch [1331], Loss: 61689.992188
Epoch [1/1], Batch [1341], Loss: 61991.765625
Epoch [1/1], Batch [1351], Loss: 60913.335938
Epoch [1/1], Batch [1361], Loss: 63031.507812
Epoch [1/1], Batch [1371], Loss: 63575.132812
Epoch [1/1], Batch [1381], Loss: 61688.996094
Epoch [1/1], Batch [1391], Loss: 62201.515625
Epoch [1/1], Batch [1401], Loss: 61998.835938
Epoch [1/1], Batch [1411], Loss: 59754.578125
Epoch [1/1], Batch [1421], Loss: 62293.835938
Epoch [1/1], Batch [1431], Loss: 60867.507812
Epoch [1/1], Batch [1441], Loss: 62134.515625
Epoch [1/1], Batch [1451], Loss: 63886.546875
Epoch [1/1], Batch [1461], Loss: 63889.714844
Epoch [1/1], Batch [1471], Loss: 60347.578125
Epoch [1/1], Batch [1481], Loss: 61885.437500
Epoch [1/1], Batch [1491], Loss: 60592.871094
Epoch [1/1], Batch [1501], Loss: 62640.179688
Epoch [1/1], Batch [1511], Loss: 62150.097656
Epoch [1/1], Batch [1521], Loss: 60970.945312
Epoch [1/1], Batch [1531], Loss: 61771.320312
Epoch [1/1], Batch [1541], Loss: 64127.257812
Epoch [1/1], Batch [1551], Loss: 61221.726562
Epoch [1/1], Batch [1561], Loss: 61712.250000
Epoch [1/1], Batch [1571], Loss: 63453.015625
Epoch [1/1], Batch [1581], Loss: 62235.890625
Epoch [1/1], Batch [1591], Loss: 62753.722656
Epoch [1/1], Batch [1601], Loss: 58666.828125
Epoch [1/1], Batch [1611], Loss: 58274.164062
Epoch [1/1], Batch [1621], Loss: 57111.261719
Epoch [1/1], Batch [1631], Loss: 59338.945312
Epoch [1/1], Batch [1641], Loss: 60200.351562
Epoch [1/1], Batch [1651], Loss: 59357.328125
Epoch [1/1], Batch [1661], Loss: 62113.968750
Epoch [1/1], Batch [1671], Loss: 61605.468750
Epoch [1/1], Batch [1681], Loss: 61501.750000
Epoch [1/1], Batch [1691], Loss: 62373.460938
Epoch [1/1], Batch [1701], Loss: 59174.242188
Epoch [1/1], Batch [1711], Loss: 61249.621094
Epoch [1/1], Batch [1721], Loss: 61174.078125
Epoch [1/1], Batch [1731], Loss: 64219.351562
Epoch [1/1], Batch [1741], Loss: 58750.656250
Epoch [1/1], Batch [1751], Loss: 59717.062500
Epoch [1/1], Batch [1761], Loss: 58965.097656
Epoch [1/1], Batch [1771], Loss: 63289.335938
Epoch [1/1], Batch [1781], Loss: 62327.710938
Epoch [1/1], Batch [1791], Loss: 58489.359375
Epoch [1/1], Batch [1801], Loss: 61690.117188
Epoch [1/1], Batch [1811], Loss: 61577.617188
Epoch [1/1], Batch [1821], Loss: 63163.902344
Epoch [1/1], Batch [1831], Loss: 61685.460938
Epoch [1/1], Batch [1841], Loss: 64258.695312
Epoch [1/1], Batch [1851], Loss: 59069.898438
Epoch [1/1], Batch [1861], Loss: 59196.109375
Epoch [1/1], Batch [1871], Loss: 59859.742188
Epoch [1/1], Batch [1881], Loss: 59852.402344
Epoch [1/1], Batch [1891], Loss: 59149.054688
Epoch [1/1], Batch [1901], Loss: 60158.589844
Epoch [1/1], Batch [1911], Loss: 63256.453125
Epoch [1/1], Batch [1921], Loss: 62602.941406
Epoch [1/1], Batch [1931], Loss: 58965.960938
Epoch [1/1], Batch [1941], Loss: 61826.656250
Epoch [1/1], Batch [1951], Loss: 61672.554688
Epoch [1/1], Batch [1961], Loss: 57299.835938
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 65236.3449
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 59512.3600
Elapsed time: 1130.55 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 60241.6825
Elapsed time: 1155.58 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 92686.554688
Epoch [1/1], Batch [11], Loss: 88516.250000
Epoch [1/1], Batch [21], Loss: 84517.632812
Epoch [1/1], Batch [31], Loss: 90007.632812
Epoch [1/1], Batch [41], Loss: 85665.914062
Epoch [1/1], Batch [51], Loss: 93837.546875
Epoch [1/1], Batch [61], Loss: 87097.703125
Epoch [1/1], Batch [71], Loss: 85868.203125
Epoch [1/1], Batch [81], Loss: 88496.859375
Epoch [1/1], Batch [91], Loss: 88286.929688
Epoch [1/1], Batch [101], Loss: 83668.609375
Epoch [1/1], Batch [111], Loss: 87778.937500
Epoch [1/1], Batch [121], Loss: 83177.890625
Epoch [1/1], Batch [131], Loss: 89626.375000
Epoch [1/1], Batch [141], Loss: 83264.648438
Epoch [1/1], Batch [151], Loss: 85400.734375
Epoch [1/1], Batch [161], Loss: 86831.632812
Epoch [1/1], Batch [171], Loss: 86691.687500
Epoch [1/1], Batch [181], Loss: 85087.281250
Epoch [1/1], Batch [191], Loss: 86262.976562
Epoch [1/1], Batch [201], Loss: 88094.625000
Epoch [1/1], Batch [211], Loss: 86808.101562
Epoch [1/1], Batch [221], Loss: 85440.203125
Epoch [1/1], Batch [231], Loss: 83387.718750
Epoch [1/1], Batch [241], Loss: 82127.484375
Epoch [1/1], Batch [251], Loss: 79958.476562
Epoch [1/1], Batch [261], Loss: 87921.046875
Epoch [1/1], Batch [271], Loss: 83054.562500
Epoch [1/1], Batch [281], Loss: 83505.937500
Epoch [1/1], Batch [291], Loss: 84157.164062
Epoch [1/1], Batch [301], Loss: 85745.812500
Epoch [1/1], Batch [311], Loss: 88416.125000
Epoch [1/1], Batch [321], Loss: 88592.351562
Epoch [1/1], Batch [331], Loss: 86261.343750
Epoch [1/1], Batch [341], Loss: 86430.375000
Epoch [1/1], Batch [351], Loss: 85808.609375
Epoch [1/1], Batch [361], Loss: 80798.289062
Epoch [1/1], Batch [371], Loss: 82094.859375
Epoch [1/1], Batch [381], Loss: 84017.359375
Epoch [1/1], Batch [391], Loss: 88213.242188
Epoch [1/1], Batch [401], Loss: 82420.234375
Epoch [1/1], Batch [411], Loss: 81784.179688
Epoch [1/1], Batch [421], Loss: 81022.835938
Epoch [1/1], Batch [431], Loss: 86552.765625
Epoch [1/1], Batch [441], Loss: 81430.875000
Epoch [1/1], Batch [451], Loss: 81254.921875
Epoch [1/1], Batch [461], Loss: 84981.757812
Epoch [1/1], Batch [471], Loss: 83594.304688
Epoch [1/1], Batch [481], Loss: 80670.062500
Epoch [1/1], Batch [491], Loss: 82415.875000
Epoch [1/1], Batch [501], Loss: 83820.804688
Epoch [1/1], Batch [511], Loss: 82409.515625
Epoch [1/1], Batch [521], Loss: 84512.367188
Epoch [1/1], Batch [531], Loss: 83551.343750
Epoch [1/1], Batch [541], Loss: 83805.187500
Epoch [1/1], Batch [551], Loss: 86777.343750
Epoch [1/1], Batch [561], Loss: 80004.140625
Epoch [1/1], Batch [571], Loss: 82553.671875
Epoch [1/1], Batch [581], Loss: 84290.343750
Epoch [1/1], Batch [591], Loss: 81596.531250
Epoch [1/1], Batch [601], Loss: 85856.328125
Epoch [1/1], Batch [611], Loss: 78758.875000
Epoch [1/1], Batch [621], Loss: 79592.875000
Epoch [1/1], Batch [631], Loss: 84159.000000
Epoch [1/1], Batch [641], Loss: 81768.218750
Epoch [1/1], Batch [651], Loss: 83067.546875
Epoch [1/1], Batch [661], Loss: 81310.578125
Epoch [1/1], Batch [671], Loss: 84658.351562
Epoch [1/1], Batch [681], Loss: 84449.796875
Epoch [1/1], Batch [691], Loss: 80468.890625
Epoch [1/1], Batch [701], Loss: 85016.609375
Epoch [1/1], Batch [711], Loss: 78039.351562
Epoch [1/1], Batch [721], Loss: 83521.046875
Epoch [1/1], Batch [731], Loss: 78377.765625
Epoch [1/1], Batch [741], Loss: 85845.132812
Epoch [1/1], Batch [751], Loss: 87462.390625
Epoch [1/1], Batch [761], Loss: 81504.093750
Epoch [1/1], Batch [771], Loss: 86958.640625
Epoch [1/1], Batch [781], Loss: 77911.453125
Epoch [1/1], Batch [791], Loss: 86945.734375
Epoch [1/1], Batch [801], Loss: 82709.632812
Epoch [1/1], Batch [811], Loss: 83099.820312
Epoch [1/1], Batch [821], Loss: 79373.593750
Epoch [1/1], Batch [831], Loss: 80740.718750
Epoch [1/1], Batch [841], Loss: 83433.875000
Epoch [1/1], Batch [851], Loss: 81579.984375
Epoch [1/1], Batch [861], Loss: 83368.593750
Epoch [1/1], Batch [871], Loss: 82039.593750
Epoch [1/1], Batch [881], Loss: 80020.140625
Epoch [1/1], Batch [891], Loss: 83719.867188
Epoch [1/1], Batch [901], Loss: 83075.500000
Epoch [1/1], Batch [911], Loss: 83846.015625
Epoch [1/1], Batch [921], Loss: 79068.632812
Epoch [1/1], Batch [931], Loss: 80566.304688
Epoch [1/1], Batch [941], Loss: 80617.843750
Epoch [1/1], Batch [951], Loss: 84453.367188
Epoch [1/1], Batch [961], Loss: 80832.726562
Epoch [1/1], Batch [971], Loss: 81435.695312
Epoch [1/1], Batch [981], Loss: 82279.062500
Epoch [1/1], Batch [991], Loss: 86611.843750
Epoch [1/1], Batch [1001], Loss: 80550.210938
Epoch [1/1], Batch [1011], Loss: 81507.781250
Epoch [1/1], Batch [1021], Loss: 80556.796875
Epoch [1/1], Batch [1031], Loss: 79939.812500
Epoch [1/1], Batch [1041], Loss: 78474.445312
Epoch [1/1], Batch [1051], Loss: 83312.625000
Epoch [1/1], Batch [1061], Loss: 78943.734375
Epoch [1/1], Batch [1071], Loss: 80653.468750
Epoch [1/1], Batch [1081], Loss: 80531.656250
Epoch [1/1], Batch [1091], Loss: 79446.421875
Epoch [1/1], Batch [1101], Loss: 79145.070312
Epoch [1/1], Batch [1111], Loss: 80748.812500
Epoch [1/1], Batch [1121], Loss: 80378.929688
Epoch [1/1], Batch [1131], Loss: 86603.015625
Epoch [1/1], Batch [1141], Loss: 81731.015625
Epoch [1/1], Batch [1151], Loss: 81347.875000
Epoch [1/1], Batch [1161], Loss: 83315.656250
Epoch [1/1], Batch [1171], Loss: 78827.312500
Epoch [1/1], Batch [1181], Loss: 77554.078125
Epoch [1/1], Batch [1191], Loss: 78495.218750
Epoch [1/1], Batch [1201], Loss: 78790.531250
Epoch [1/1], Batch [1211], Loss: 79550.718750
Epoch [1/1], Batch [1221], Loss: 79005.554688
Epoch [1/1], Batch [1231], Loss: 78885.710938
Epoch [1/1], Batch [1241], Loss: 80916.859375
Epoch [1/1], Batch [1251], Loss: 77832.773438
Epoch [1/1], Batch [1261], Loss: 80959.382812
Epoch [1/1], Batch [1271], Loss: 77798.406250
Epoch [1/1], Batch [1281], Loss: 79676.687500
Epoch [1/1], Batch [1291], Loss: 82449.304688
Epoch [1/1], Batch [1301], Loss: 81429.257812
Epoch [1/1], Batch [1311], Loss: 79852.304688
Epoch [1/1], Batch [1321], Loss: 80966.734375
Epoch [1/1], Batch [1331], Loss: 82306.609375
Epoch [1/1], Batch [1341], Loss: 79801.906250
Epoch [1/1], Batch [1351], Loss: 80770.875000
Epoch [1/1], Batch [1361], Loss: 76260.960938
Epoch [1/1], Batch [1371], Loss: 84439.554688
Epoch [1/1], Batch [1381], Loss: 83486.828125
Epoch [1/1], Batch [1391], Loss: 79387.414062
Epoch [1/1], Batch [1401], Loss: 81045.507812
Epoch [1/1], Batch [1411], Loss: 74786.437500
Epoch [1/1], Batch [1421], Loss: 80663.085938
Epoch [1/1], Batch [1431], Loss: 87704.937500
Epoch [1/1], Batch [1441], Loss: 79626.312500
Epoch [1/1], Batch [1451], Loss: 84135.796875
Epoch [1/1], Batch [1461], Loss: 82258.023438
Epoch [1/1], Batch [1471], Loss: 79233.476562
Epoch [1/1], Batch [1481], Loss: 78951.843750
Epoch [1/1], Batch [1491], Loss: 80273.914062
Epoch [1/1], Batch [1501], Loss: 77005.039062
Epoch [1/1], Batch [1511], Loss: 77954.898438
Epoch [1/1], Batch [1521], Loss: 79812.359375
Epoch [1/1], Batch [1531], Loss: 77886.750000
Epoch [1/1], Batch [1541], Loss: 77930.546875
Epoch [1/1], Batch [1551], Loss: 79431.593750
Epoch [1/1], Batch [1561], Loss: 82312.062500
Epoch [1/1], Batch [1571], Loss: 78394.625000
Epoch [1/1], Batch [1581], Loss: 80289.796875
Epoch [1/1], Batch [1591], Loss: 78313.187500
Epoch [1/1], Batch [1601], Loss: 84367.265625
Epoch [1/1], Batch [1611], Loss: 82819.500000
Epoch [1/1], Batch [1621], Loss: 82634.804688
Epoch [1/1], Batch [1631], Loss: 80551.976562
Epoch [1/1], Batch [1641], Loss: 78180.921875
Epoch [1/1], Batch [1651], Loss: 81484.625000
Epoch [1/1], Batch [1661], Loss: 79080.000000
Epoch [1/1], Batch [1671], Loss: 78830.828125
Epoch [1/1], Batch [1681], Loss: 78406.937500
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 82475.4792
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 78148.2315
Elapsed time: 1852.22 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 80792.4499
Elapsed time: 1879.50 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 107751.093750
Epoch [1/1], Batch [11], Loss: 107317.898438
Epoch [1/1], Batch [21], Loss: 103890.015625
Epoch [1/1], Batch [31], Loss: 108982.281250
Epoch [1/1], Batch [41], Loss: 109633.015625
Epoch [1/1], Batch [51], Loss: 105720.031250
Epoch [1/1], Batch [61], Loss: 106582.210938
Epoch [1/1], Batch [71], Loss: 105939.281250
Epoch [1/1], Batch [81], Loss: 104468.875000
Epoch [1/1], Batch [91], Loss: 102141.031250
Epoch [1/1], Batch [101], Loss: 108941.515625
Epoch [1/1], Batch [111], Loss: 103494.273438
Epoch [1/1], Batch [121], Loss: 110182.406250
Epoch [1/1], Batch [131], Loss: 101162.843750
Epoch [1/1], Batch [141], Loss: 101171.421875
Epoch [1/1], Batch [151], Loss: 102338.304688
Epoch [1/1], Batch [161], Loss: 105912.898438
Epoch [1/1], Batch [171], Loss: 103327.734375
Epoch [1/1], Batch [181], Loss: 106115.687500
Epoch [1/1], Batch [191], Loss: 99476.312500
Epoch [1/1], Batch [201], Loss: 103459.320312
Epoch [1/1], Batch [211], Loss: 106226.250000
Epoch [1/1], Batch [221], Loss: 104702.992188
Epoch [1/1], Batch [231], Loss: 101413.000000
Epoch [1/1], Batch [241], Loss: 102213.625000
Epoch [1/1], Batch [251], Loss: 101431.234375
Epoch [1/1], Batch [261], Loss: 103311.070312
Epoch [1/1], Batch [271], Loss: 100017.171875
Epoch [1/1], Batch [281], Loss: 105376.828125
Epoch [1/1], Batch [291], Loss: 103454.968750
Epoch [1/1], Batch [301], Loss: 105828.000000
Epoch [1/1], Batch [311], Loss: 104251.296875
Epoch [1/1], Batch [321], Loss: 103739.828125
Epoch [1/1], Batch [331], Loss: 103729.421875
Epoch [1/1], Batch [341], Loss: 105285.625000
Epoch [1/1], Batch [351], Loss: 104763.843750
Epoch [1/1], Batch [361], Loss: 108393.039062
Epoch [1/1], Batch [371], Loss: 105051.867188
Epoch [1/1], Batch [381], Loss: 103297.140625
Epoch [1/1], Batch [391], Loss: 103863.984375
Epoch [1/1], Batch [401], Loss: 104470.843750
Epoch [1/1], Batch [411], Loss: 104218.976562
Epoch [1/1], Batch [421], Loss: 104578.468750
Epoch [1/1], Batch [431], Loss: 99206.578125
Epoch [1/1], Batch [441], Loss: 104232.906250
Epoch [1/1], Batch [451], Loss: 101857.656250
Epoch [1/1], Batch [461], Loss: 106122.093750
Epoch [1/1], Batch [471], Loss: 102048.585938
Epoch [1/1], Batch [481], Loss: 100099.367188
Epoch [1/1], Batch [491], Loss: 103545.015625
Epoch [1/1], Batch [501], Loss: 101081.937500
Epoch [1/1], Batch [511], Loss: 107426.718750
Epoch [1/1], Batch [521], Loss: 105033.664062
Epoch [1/1], Batch [531], Loss: 107005.171875
Epoch [1/1], Batch [541], Loss: 106193.437500
Epoch [1/1], Batch [551], Loss: 103207.476562
Epoch [1/1], Batch [561], Loss: 101540.039062
Epoch [1/1], Batch [571], Loss: 101951.617188
Epoch [1/1], Batch [581], Loss: 107039.015625
Epoch [1/1], Batch [591], Loss: 102286.601562
Epoch [1/1], Batch [601], Loss: 104980.218750
Epoch [1/1], Batch [611], Loss: 106278.281250
Epoch [1/1], Batch [621], Loss: 105887.328125
Epoch [1/1], Batch [631], Loss: 103914.546875
Epoch [1/1], Batch [641], Loss: 107251.843750
Epoch [1/1], Batch [651], Loss: 97190.789062
Epoch [1/1], Batch [661], Loss: 102590.734375
Epoch [1/1], Batch [671], Loss: 100825.890625
Epoch [1/1], Batch [681], Loss: 95077.203125
Epoch [1/1], Batch [691], Loss: 102081.546875
Epoch [1/1], Batch [701], Loss: 102171.398438
Epoch [1/1], Batch [711], Loss: 103788.351562
Epoch [1/1], Batch [721], Loss: 92593.078125
Epoch [1/1], Batch [731], Loss: 100159.093750
Epoch [1/1], Batch [741], Loss: 103198.593750
Epoch [1/1], Batch [751], Loss: 104617.921875
Epoch [1/1], Batch [761], Loss: 100057.171875
Epoch [1/1], Batch [771], Loss: 100985.507812
Epoch [1/1], Batch [781], Loss: 104263.718750
Epoch [1/1], Batch [791], Loss: 100317.828125
Epoch [1/1], Batch [801], Loss: 105464.000000
Epoch [1/1], Batch [811], Loss: 101253.929688
Epoch [1/1], Batch [821], Loss: 101632.671875
Epoch [1/1], Batch [831], Loss: 103364.414062
Epoch [1/1], Batch [841], Loss: 100034.898438
Epoch [1/1], Batch [851], Loss: 108329.546875
Epoch [1/1], Batch [861], Loss: 99708.453125
Epoch [1/1], Batch [871], Loss: 103452.937500
Epoch [1/1], Batch [881], Loss: 100511.109375
Epoch [1/1], Batch [891], Loss: 104011.914062
Epoch [1/1], Batch [901], Loss: 101446.195312
Epoch [1/1], Batch [911], Loss: 98575.226562
Epoch [1/1], Batch [921], Loss: 98723.296875
Epoch [1/1], Batch [931], Loss: 102395.531250
Epoch [1/1], Batch [941], Loss: 96942.117188
Epoch [1/1], Batch [951], Loss: 101032.953125
Epoch [1/1], Batch [961], Loss: 99370.234375
Epoch [1/1], Batch [971], Loss: 99689.164062
Epoch [1/1], Batch [981], Loss: 103946.437500
Epoch [1/1], Batch [991], Loss: 103051.109375
Epoch [1/1], Batch [1001], Loss: 97971.265625
Epoch [1/1], Batch [1011], Loss: 98668.671875
Epoch [1/1], Batch [1021], Loss: 96079.843750
Epoch [1/1], Batch [1031], Loss: 104932.156250
Epoch [1/1], Batch [1041], Loss: 101386.171875
Epoch [1/1], Batch [1051], Loss: 101381.000000
Epoch [1/1], Batch [1061], Loss: 100093.445312
Epoch [1/1], Batch [1071], Loss: 105157.687500
Epoch [1/1], Batch [1081], Loss: 102547.984375
Epoch [1/1], Batch [1091], Loss: 101567.757812
Epoch [1/1], Batch [1101], Loss: 96394.398438
Epoch [1/1], Batch [1111], Loss: 106904.625000
Epoch [1/1], Batch [1121], Loss: 102640.382812
Epoch [1/1], Batch [1131], Loss: 100196.023438
Epoch [1/1], Batch [1141], Loss: 102906.734375
Epoch [1/1], Batch [1151], Loss: 110307.140625
Epoch [1/1], Batch [1161], Loss: 100511.046875
Epoch [1/1], Batch [1171], Loss: 103320.921875
Epoch [1/1], Batch [1181], Loss: 96680.375000
Epoch [1/1], Batch [1191], Loss: 96718.929688
Epoch [1/1], Batch [1201], Loss: 101646.164062
Epoch [1/1], Batch [1211], Loss: 101287.968750
Epoch [1/1], Batch [1221], Loss: 102158.921875
Epoch [1/1], Batch [1231], Loss: 98694.125000
Epoch [1/1], Batch [1241], Loss: 99805.593750
Epoch [1/1], Batch [1251], Loss: 104096.898438
Epoch [1/1], Batch [1261], Loss: 98844.656250
Epoch [1/1], Batch [1271], Loss: 99948.296875
Epoch [1/1], Batch [1281], Loss: 100338.421875
Epoch [1/1], Batch [1291], Loss: 99770.921875
Epoch [1/1], Batch [1301], Loss: 99746.976562
Epoch [1/1], Batch [1311], Loss: 97974.234375
Epoch [1/1], Batch [1321], Loss: 101440.828125
Epoch [1/1], Batch [1331], Loss: 98792.976562
Epoch [1/1], Batch [1341], Loss: 100734.812500
Epoch [1/1], Batch [1351], Loss: 100678.390625
Epoch [1/1], Batch [1361], Loss: 97348.937500
Epoch [1/1], Batch [1371], Loss: 98641.656250
Epoch [1/1], Batch [1381], Loss: 97283.343750
Epoch [1/1], Batch [1391], Loss: 100326.328125
Epoch [1/1], Batch [1401], Loss: 102206.265625
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 102629.7938
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 98498.7613
Elapsed time: 2596.86 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 100837.5227
Elapsed time: 2624.43 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 125907.359375
Epoch [1/1], Batch [11], Loss: 128315.859375
Epoch [1/1], Batch [21], Loss: 119139.781250
Epoch [1/1], Batch [31], Loss: 132105.203125
Epoch [1/1], Batch [41], Loss: 129297.640625
Epoch [1/1], Batch [51], Loss: 129036.835938
Epoch [1/1], Batch [61], Loss: 125052.890625
Epoch [1/1], Batch [71], Loss: 125878.203125
Epoch [1/1], Batch [81], Loss: 120082.539062
Epoch [1/1], Batch [91], Loss: 129875.890625
Epoch [1/1], Batch [101], Loss: 124135.062500
Epoch [1/1], Batch [111], Loss: 128927.375000
Epoch [1/1], Batch [121], Loss: 122282.296875
Epoch [1/1], Batch [131], Loss: 124251.992188
Epoch [1/1], Batch [141], Loss: 129477.203125
Epoch [1/1], Batch [151], Loss: 126738.500000
Epoch [1/1], Batch [161], Loss: 122156.054688
Epoch [1/1], Batch [171], Loss: 126980.585938
Epoch [1/1], Batch [181], Loss: 126324.195312
Epoch [1/1], Batch [191], Loss: 127437.281250
Epoch [1/1], Batch [201], Loss: 132513.687500
Epoch [1/1], Batch [211], Loss: 130154.234375
Epoch [1/1], Batch [221], Loss: 128255.031250
Epoch [1/1], Batch [231], Loss: 121451.632812
Epoch [1/1], Batch [241], Loss: 123196.812500
Epoch [1/1], Batch [251], Loss: 127412.257812
Epoch [1/1], Batch [261], Loss: 128737.093750
Epoch [1/1], Batch [271], Loss: 128630.562500
Epoch [1/1], Batch [281], Loss: 132506.218750
Epoch [1/1], Batch [291], Loss: 121945.796875
Epoch [1/1], Batch [301], Loss: 125838.937500
Epoch [1/1], Batch [311], Loss: 120798.343750
Epoch [1/1], Batch [321], Loss: 126753.804688
Epoch [1/1], Batch [331], Loss: 122290.093750
Epoch [1/1], Batch [341], Loss: 122580.296875
Epoch [1/1], Batch [351], Loss: 121476.109375
Epoch [1/1], Batch [361], Loss: 122107.593750
Epoch [1/1], Batch [371], Loss: 128336.414062
Epoch [1/1], Batch [381], Loss: 129447.890625
Epoch [1/1], Batch [391], Loss: 126594.585938
Epoch [1/1], Batch [401], Loss: 129747.468750
Epoch [1/1], Batch [411], Loss: 123633.664062
Epoch [1/1], Batch [421], Loss: 116838.617188
Epoch [1/1], Batch [431], Loss: 125774.609375
Epoch [1/1], Batch [441], Loss: 130709.585938
Epoch [1/1], Batch [451], Loss: 122292.281250
Epoch [1/1], Batch [461], Loss: 119779.687500
Epoch [1/1], Batch [471], Loss: 120973.781250
Epoch [1/1], Batch [481], Loss: 125106.531250
Epoch [1/1], Batch [491], Loss: 129607.421875
Epoch [1/1], Batch [501], Loss: 125598.789062
Epoch [1/1], Batch [511], Loss: 125653.570312
Epoch [1/1], Batch [521], Loss: 121607.765625
Epoch [1/1], Batch [531], Loss: 122231.593750
Epoch [1/1], Batch [541], Loss: 122809.671875
Epoch [1/1], Batch [551], Loss: 126620.609375
Epoch [1/1], Batch [561], Loss: 130210.406250
Epoch [1/1], Batch [571], Loss: 125185.359375
Epoch [1/1], Batch [581], Loss: 118002.406250
Epoch [1/1], Batch [591], Loss: 122266.320312
Epoch [1/1], Batch [601], Loss: 124827.898438
Epoch [1/1], Batch [611], Loss: 127355.140625
Epoch [1/1], Batch [621], Loss: 128852.320312
Epoch [1/1], Batch [631], Loss: 123770.484375
Epoch [1/1], Batch [641], Loss: 123256.156250
Epoch [1/1], Batch [651], Loss: 119852.734375
Epoch [1/1], Batch [661], Loss: 129755.843750
Epoch [1/1], Batch [671], Loss: 124728.398438
Epoch [1/1], Batch [681], Loss: 127469.320312
Epoch [1/1], Batch [691], Loss: 127264.265625
Epoch [1/1], Batch [701], Loss: 122258.109375
Epoch [1/1], Batch [711], Loss: 120397.789062
Epoch [1/1], Batch [721], Loss: 125143.046875
Epoch [1/1], Batch [731], Loss: 124948.968750
Epoch [1/1], Batch [741], Loss: 124115.515625
Epoch [1/1], Batch [751], Loss: 118232.156250
Epoch [1/1], Batch [761], Loss: 126840.968750
Epoch [1/1], Batch [771], Loss: 123622.617188
Epoch [1/1], Batch [781], Loss: 124830.960938
Epoch [1/1], Batch [791], Loss: 120792.015625
Epoch [1/1], Batch [801], Loss: 122094.601562
Epoch [1/1], Batch [811], Loss: 123877.656250
Epoch [1/1], Batch [821], Loss: 122348.078125
Epoch [1/1], Batch [831], Loss: 122402.156250
Epoch [1/1], Batch [841], Loss: 123510.882812
Epoch [1/1], Batch [851], Loss: 126632.000000
Epoch [1/1], Batch [861], Loss: 122309.414062
Epoch [1/1], Batch [871], Loss: 122387.031250
Epoch [1/1], Batch [881], Loss: 124386.585938
Epoch [1/1], Batch [891], Loss: 122304.562500
Epoch [1/1], Batch [901], Loss: 123098.578125
Epoch [1/1], Batch [911], Loss: 123561.304688
Epoch [1/1], Batch [921], Loss: 121702.648438
Epoch [1/1], Batch [931], Loss: 122070.257812
Epoch [1/1], Batch [941], Loss: 116317.554688
Epoch [1/1], Batch [951], Loss: 118033.718750
Epoch [1/1], Batch [961], Loss: 121553.257812
Epoch [1/1], Batch [971], Loss: 117695.992188
Epoch [1/1], Batch [981], Loss: 114698.187500
Epoch [1/1], Batch [991], Loss: 127237.015625
Epoch [1/1], Batch [1001], Loss: 123886.953125
Epoch [1/1], Batch [1011], Loss: 117219.742188
Epoch [1/1], Batch [1021], Loss: 124897.765625
Epoch [1/1], Batch [1031], Loss: 122514.546875
Epoch [1/1], Batch [1041], Loss: 126578.289062
Epoch [1/1], Batch [1051], Loss: 117987.750000
Epoch [1/1], Batch [1061], Loss: 116846.218750
Epoch [1/1], Batch [1071], Loss: 121811.914062
Epoch [1/1], Batch [1081], Loss: 120672.375000
Epoch [1/1], Batch [1091], Loss: 124576.468750
Epoch [1/1], Batch [1101], Loss: 119050.101562
Epoch [1/1], Batch [1111], Loss: 119252.523438
Epoch [1/1], Batch [1121], Loss: 120736.234375
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 123456.4879
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 118055.5286
Elapsed time: 3305.25 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 123910.7428
Elapsed time: 3331.18 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 146735.125000
Epoch [1/1], Batch [11], Loss: 147471.687500
Epoch [1/1], Batch [21], Loss: 136820.171875
Epoch [1/1], Batch [31], Loss: 150590.640625
Epoch [1/1], Batch [41], Loss: 148708.437500
Epoch [1/1], Batch [51], Loss: 144136.750000
Epoch [1/1], Batch [61], Loss: 145193.640625
Epoch [1/1], Batch [71], Loss: 143461.718750
Epoch [1/1], Batch [81], Loss: 150031.187500
Epoch [1/1], Batch [91], Loss: 144027.828125
Epoch [1/1], Batch [101], Loss: 138236.734375
Epoch [1/1], Batch [111], Loss: 140956.562500
Epoch [1/1], Batch [121], Loss: 150200.156250
Epoch [1/1], Batch [131], Loss: 147237.312500
Epoch [1/1], Batch [141], Loss: 146457.531250
Epoch [1/1], Batch [151], Loss: 154442.312500
Epoch [1/1], Batch [161], Loss: 146532.312500
Epoch [1/1], Batch [171], Loss: 147105.812500
Epoch [1/1], Batch [181], Loss: 139638.687500
Epoch [1/1], Batch [191], Loss: 136685.937500
Epoch [1/1], Batch [201], Loss: 143197.468750
Epoch [1/1], Batch [211], Loss: 152275.421875
Epoch [1/1], Batch [221], Loss: 148703.546875
Epoch [1/1], Batch [231], Loss: 147213.062500
Epoch [1/1], Batch [241], Loss: 147780.062500
Epoch [1/1], Batch [251], Loss: 147663.359375
Epoch [1/1], Batch [261], Loss: 141089.875000
Epoch [1/1], Batch [271], Loss: 140268.875000
Epoch [1/1], Batch [281], Loss: 149238.812500
Epoch [1/1], Batch [291], Loss: 144905.437500
Epoch [1/1], Batch [301], Loss: 144948.203125
Epoch [1/1], Batch [311], Loss: 147602.125000
Epoch [1/1], Batch [321], Loss: 141033.796875
Epoch [1/1], Batch [331], Loss: 149686.437500
Epoch [1/1], Batch [341], Loss: 147948.812500
Epoch [1/1], Batch [351], Loss: 147458.218750
Epoch [1/1], Batch [361], Loss: 141159.625000
Epoch [1/1], Batch [371], Loss: 145792.187500
Epoch [1/1], Batch [381], Loss: 148508.796875
Epoch [1/1], Batch [391], Loss: 145278.453125
Epoch [1/1], Batch [401], Loss: 142887.531250
Epoch [1/1], Batch [411], Loss: 145908.343750
Epoch [1/1], Batch [421], Loss: 141545.703125
Epoch [1/1], Batch [431], Loss: 141057.781250
Epoch [1/1], Batch [441], Loss: 152185.343750
Epoch [1/1], Batch [451], Loss: 146073.828125
Epoch [1/1], Batch [461], Loss: 139153.593750
Epoch [1/1], Batch [471], Loss: 139935.343750
Epoch [1/1], Batch [481], Loss: 149400.078125
Epoch [1/1], Batch [491], Loss: 145859.125000
Epoch [1/1], Batch [501], Loss: 148908.562500
Epoch [1/1], Batch [511], Loss: 143190.906250
Epoch [1/1], Batch [521], Loss: 141620.437500
Epoch [1/1], Batch [531], Loss: 149872.500000
Epoch [1/1], Batch [541], Loss: 147986.609375
Epoch [1/1], Batch [551], Loss: 150019.562500
Epoch [1/1], Batch [561], Loss: 148722.656250
Epoch [1/1], Batch [571], Loss: 142037.765625
Epoch [1/1], Batch [581], Loss: 148635.578125
Epoch [1/1], Batch [591], Loss: 149953.562500
Epoch [1/1], Batch [601], Loss: 142652.281250
Epoch [1/1], Batch [611], Loss: 146592.390625
Epoch [1/1], Batch [621], Loss: 144584.375000
Epoch [1/1], Batch [631], Loss: 143541.500000
Epoch [1/1], Batch [641], Loss: 143218.421875
Epoch [1/1], Batch [651], Loss: 142446.312500
Epoch [1/1], Batch [661], Loss: 142913.953125
Epoch [1/1], Batch [671], Loss: 145362.062500
Epoch [1/1], Batch [681], Loss: 150608.765625
Epoch [1/1], Batch [691], Loss: 146640.687500
Epoch [1/1], Batch [701], Loss: 147881.656250
Epoch [1/1], Batch [711], Loss: 142341.156250
Epoch [1/1], Batch [721], Loss: 139951.781250
Epoch [1/1], Batch [731], Loss: 142215.000000
Epoch [1/1], Batch [741], Loss: 136363.687500
Epoch [1/1], Batch [751], Loss: 141538.750000
Epoch [1/1], Batch [761], Loss: 146519.531250
Epoch [1/1], Batch [771], Loss: 136498.109375
Epoch [1/1], Batch [781], Loss: 144256.093750
Epoch [1/1], Batch [791], Loss: 145291.062500
Epoch [1/1], Batch [801], Loss: 143659.906250
Epoch [1/1], Batch [811], Loss: 143313.468750
Epoch [1/1], Batch [821], Loss: 140706.140625
Epoch [1/1], Batch [831], Loss: 150899.437500
Epoch [1/1], Batch [841], Loss: 139419.312500
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 145426.5464
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 139099.6959
Elapsed time: 3924.58 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 145237.0068
Elapsed time: 3946.91 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 167541.984375
Epoch [1/1], Batch [11], Loss: 173564.093750
Epoch [1/1], Batch [21], Loss: 174084.906250
Epoch [1/1], Batch [31], Loss: 177501.453125
Epoch [1/1], Batch [41], Loss: 173408.937500
Epoch [1/1], Batch [51], Loss: 170852.515625
Epoch [1/1], Batch [61], Loss: 162050.109375
Epoch [1/1], Batch [71], Loss: 167711.062500
Epoch [1/1], Batch [81], Loss: 164994.984375
Epoch [1/1], Batch [91], Loss: 162114.515625
Epoch [1/1], Batch [101], Loss: 172806.375000
Epoch [1/1], Batch [111], Loss: 176167.703125
Epoch [1/1], Batch [121], Loss: 171339.750000
Epoch [1/1], Batch [131], Loss: 175126.015625
Epoch [1/1], Batch [141], Loss: 170539.562500
Epoch [1/1], Batch [151], Loss: 173459.312500
Epoch [1/1], Batch [161], Loss: 171663.453125
Epoch [1/1], Batch [171], Loss: 167549.375000
Epoch [1/1], Batch [181], Loss: 170268.718750
Epoch [1/1], Batch [191], Loss: 180633.562500
Epoch [1/1], Batch [201], Loss: 164892.984375
Epoch [1/1], Batch [211], Loss: 172017.453125
Epoch [1/1], Batch [221], Loss: 167826.968750
Epoch [1/1], Batch [231], Loss: 170884.437500
Epoch [1/1], Batch [241], Loss: 175411.500000
Epoch [1/1], Batch [251], Loss: 171376.125000
Epoch [1/1], Batch [261], Loss: 159745.906250
Epoch [1/1], Batch [271], Loss: 158720.937500
Epoch [1/1], Batch [281], Loss: 172429.812500
Epoch [1/1], Batch [291], Loss: 169378.359375
Epoch [1/1], Batch [301], Loss: 164785.312500
Epoch [1/1], Batch [311], Loss: 164132.468750
Epoch [1/1], Batch [321], Loss: 168242.750000
Epoch [1/1], Batch [331], Loss: 163148.093750
Epoch [1/1], Batch [341], Loss: 162371.078125
Epoch [1/1], Batch [351], Loss: 171431.093750
Epoch [1/1], Batch [361], Loss: 162499.875000
Epoch [1/1], Batch [371], Loss: 170119.593750
Epoch [1/1], Batch [381], Loss: 162632.906250
Epoch [1/1], Batch [391], Loss: 173271.125000
Epoch [1/1], Batch [401], Loss: 168635.406250
Epoch [1/1], Batch [411], Loss: 171331.156250
Epoch [1/1], Batch [421], Loss: 172115.609375
Epoch [1/1], Batch [431], Loss: 165525.140625
Epoch [1/1], Batch [441], Loss: 166793.562500
Epoch [1/1], Batch [451], Loss: 160521.468750
Epoch [1/1], Batch [461], Loss: 169209.875000
Epoch [1/1], Batch [471], Loss: 170148.984375
Epoch [1/1], Batch [481], Loss: 170115.687500
Epoch [1/1], Batch [491], Loss: 163137.812500
Epoch [1/1], Batch [501], Loss: 170216.265625
Epoch [1/1], Batch [511], Loss: 173738.031250
Epoch [1/1], Batch [521], Loss: 162162.906250
Epoch [1/1], Batch [531], Loss: 166526.921875
Epoch [1/1], Batch [541], Loss: 166763.437500
Epoch [1/1], Batch [551], Loss: 158580.562500
Epoch [1/1], Batch [561], Loss: 167261.968750
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 168145.0399
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 161840.7831
Elapsed time: 4432.93 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 168143.0826
Elapsed time: 4449.82 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 193662.687500
Epoch [1/1], Batch [11], Loss: 192178.843750
Epoch [1/1], Batch [21], Loss: 182678.421875
Epoch [1/1], Batch [31], Loss: 184773.000000
Epoch [1/1], Batch [41], Loss: 195075.656250
Epoch [1/1], Batch [51], Loss: 190498.078125
Epoch [1/1], Batch [61], Loss: 192175.531250
Epoch [1/1], Batch [71], Loss: 190771.156250
Epoch [1/1], Batch [81], Loss: 196613.703125
Epoch [1/1], Batch [91], Loss: 194485.281250
Epoch [1/1], Batch [101], Loss: 194497.328125
Epoch [1/1], Batch [111], Loss: 194306.093750
Epoch [1/1], Batch [121], Loss: 195513.750000
Epoch [1/1], Batch [131], Loss: 185681.859375
Epoch [1/1], Batch [141], Loss: 192127.125000
Epoch [1/1], Batch [151], Loss: 191537.453125
Epoch [1/1], Batch [161], Loss: 197839.218750
Epoch [1/1], Batch [171], Loss: 193269.234375
Epoch [1/1], Batch [181], Loss: 192210.406250
Epoch [1/1], Batch [191], Loss: 197913.515625
Epoch [1/1], Batch [201], Loss: 194657.828125
Epoch [1/1], Batch [211], Loss: 188602.953125
Epoch [1/1], Batch [221], Loss: 194776.437500
Epoch [1/1], Batch [231], Loss: 194278.375000
Epoch [1/1], Batch [241], Loss: 186409.218750
Epoch [1/1], Batch [251], Loss: 184267.359375
Epoch [1/1], Batch [261], Loss: 190479.937500
Epoch [1/1], Batch [271], Loss: 190575.843750
Epoch [1/1], Batch [281], Loss: 207225.437500
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 191426.1827
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 182374.5706
Elapsed time: 4750.16 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 188440.3000
Elapsed time: 4759.57 seconds

Training complete!
Totoal elapsed time: 4759.57 seconds
CUDA is available!
