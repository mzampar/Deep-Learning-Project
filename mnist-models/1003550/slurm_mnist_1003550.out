Starting job 1003550
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = True,
    scheduler = False,
    bias = False,
    transpose = True,
    use_lstm_output = True,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 397651.187500
Epoch [1/1], Batch [11], Loss: 139851.484375
Epoch [1/1], Batch [21], Loss: 103349.000000
Epoch [1/1], Batch [31], Loss: 95041.898438
Epoch [1/1], Batch [41], Loss: 90795.781250
Epoch [1/1], Batch [51], Loss: 89079.867188
Epoch [1/1], Batch [61], Loss: 91083.296875
Epoch [1/1], Batch [71], Loss: 83339.429688
Epoch [1/1], Batch [81], Loss: 79646.335938
Epoch [1/1], Batch [91], Loss: 79339.734375
Epoch [1/1], Batch [101], Loss: 78177.164062
Epoch [1/1], Batch [111], Loss: 75201.234375
Epoch [1/1], Batch [121], Loss: 73642.000000
Epoch [1/1], Batch [131], Loss: 74623.828125
Epoch [1/1], Batch [141], Loss: 70934.343750
Epoch [1/1], Batch [151], Loss: 74116.523438
Epoch [1/1], Batch [161], Loss: 74140.546875
Epoch [1/1], Batch [171], Loss: 72467.765625
Epoch [1/1], Batch [181], Loss: 74700.273438
Epoch [1/1], Batch [191], Loss: 71993.265625
Epoch [1/1], Batch [201], Loss: 70232.515625
Epoch [1/1], Batch [211], Loss: 68400.031250
Epoch [1/1], Batch [221], Loss: 70565.875000
Epoch [1/1], Batch [231], Loss: 70241.312500
Epoch [1/1], Batch [241], Loss: 71804.812500
Epoch [1/1], Batch [251], Loss: 68786.093750
Epoch [1/1], Batch [261], Loss: 69027.515625
Epoch [1/1], Batch [271], Loss: 71809.570312
Epoch [1/1], Batch [281], Loss: 72882.750000
Epoch [1/1], Batch [291], Loss: 71534.867188
Epoch [1/1], Batch [301], Loss: 68296.703125
Epoch [1/1], Batch [311], Loss: 70585.570312
Epoch [1/1], Batch [321], Loss: 68307.046875
Epoch [1/1], Batch [331], Loss: 68006.171875
Epoch [1/1], Batch [341], Loss: 67923.554688
Epoch [1/1], Batch [351], Loss: 66644.562500
Epoch [1/1], Batch [361], Loss: 67567.078125
Epoch [1/1], Batch [371], Loss: 65094.234375
Epoch [1/1], Batch [381], Loss: 66503.679688
Epoch [1/1], Batch [391], Loss: 69589.843750
Epoch [1/1], Batch [401], Loss: 70126.398438
Epoch [1/1], Batch [411], Loss: 70294.765625
Epoch [1/1], Batch [421], Loss: 68101.937500
Epoch [1/1], Batch [431], Loss: 70697.078125
Epoch [1/1], Batch [441], Loss: 65716.453125
Epoch [1/1], Batch [451], Loss: 69077.507812
Epoch [1/1], Batch [461], Loss: 69574.601562
Epoch [1/1], Batch [471], Loss: 67489.906250
Epoch [1/1], Batch [481], Loss: 70220.296875
Epoch [1/1], Batch [491], Loss: 69434.054688
Epoch [1/1], Batch [501], Loss: 65759.062500
Epoch [1/1], Batch [511], Loss: 69394.859375
Epoch [1/1], Batch [521], Loss: 67179.781250
Epoch [1/1], Batch [531], Loss: 67701.382812
Epoch [1/1], Batch [541], Loss: 69566.640625
Epoch [1/1], Batch [551], Loss: 64288.765625
Epoch [1/1], Batch [561], Loss: 70240.062500
Epoch [1/1], Batch [571], Loss: 69731.375000
Epoch [1/1], Batch [581], Loss: 68140.859375
Epoch [1/1], Batch [591], Loss: 69925.351562
Epoch [1/1], Batch [601], Loss: 67630.625000
Epoch [1/1], Batch [611], Loss: 69959.406250
Epoch [1/1], Batch [621], Loss: 68945.828125
Epoch [1/1], Batch [631], Loss: 68622.960938
Epoch [1/1], Batch [641], Loss: 68566.140625
Epoch [1/1], Batch [651], Loss: 70460.515625
Epoch [1/1], Batch [661], Loss: 67474.609375
Epoch [1/1], Batch [671], Loss: 68594.375000
Epoch [1/1], Batch [681], Loss: 68706.710938
Epoch [1/1], Batch [691], Loss: 67501.421875
Epoch [1/1], Batch [701], Loss: 67522.750000
Epoch [1/1], Batch [711], Loss: 67018.140625
Epoch [1/1], Batch [721], Loss: 69828.765625
Epoch [1/1], Batch [731], Loss: 68955.906250
Epoch [1/1], Batch [741], Loss: 67798.812500
Epoch [1/1], Batch [751], Loss: 66280.843750
Epoch [1/1], Batch [761], Loss: 66892.031250
Epoch [1/1], Batch [771], Loss: 69387.718750
Epoch [1/1], Batch [781], Loss: 67667.171875
Epoch [1/1], Batch [791], Loss: 69871.328125
Epoch [1/1], Batch [801], Loss: 66603.625000
Epoch [1/1], Batch [811], Loss: 66765.203125
Epoch [1/1], Batch [821], Loss: 67176.476562
Epoch [1/1], Batch [831], Loss: 70266.273438
Epoch [1/1], Batch [841], Loss: 67389.875000
Epoch [1/1], Batch [851], Loss: 65909.367188
Epoch [1/1], Batch [861], Loss: 68616.421875
Epoch [1/1], Batch [871], Loss: 70792.289062
Epoch [1/1], Batch [881], Loss: 68523.515625
Epoch [1/1], Batch [891], Loss: 68962.773438
Epoch [1/1], Batch [901], Loss: 66183.046875
Epoch [1/1], Batch [911], Loss: 67895.546875
Epoch [1/1], Batch [921], Loss: 65180.683594
Epoch [1/1], Batch [931], Loss: 66551.304688
Epoch [1/1], Batch [941], Loss: 68095.203125
Epoch [1/1], Batch [951], Loss: 68794.500000
Epoch [1/1], Batch [961], Loss: 69190.296875
Epoch [1/1], Batch [971], Loss: 69934.867188
Epoch [1/1], Batch [981], Loss: 69138.734375
Epoch [1/1], Batch [991], Loss: 67870.781250
Epoch [1/1], Batch [1001], Loss: 64140.593750
Epoch [1/1], Batch [1011], Loss: 66181.234375
Epoch [1/1], Batch [1021], Loss: 68164.250000
Epoch [1/1], Batch [1031], Loss: 66877.703125
Epoch [1/1], Batch [1041], Loss: 67021.375000
Epoch [1/1], Batch [1051], Loss: 69061.515625
Epoch [1/1], Batch [1061], Loss: 66001.757812
Epoch [1/1], Batch [1071], Loss: 66675.984375
Epoch [1/1], Batch [1081], Loss: 64829.171875
Epoch [1/1], Batch [1091], Loss: 66075.304688
Epoch [1/1], Batch [1101], Loss: 68511.671875
Epoch [1/1], Batch [1111], Loss: 68933.500000
Epoch [1/1], Batch [1121], Loss: 67584.468750
Epoch [1/1], Batch [1131], Loss: 65463.328125
Epoch [1/1], Batch [1141], Loss: 68034.359375
Epoch [1/1], Batch [1151], Loss: 67648.937500
Epoch [1/1], Batch [1161], Loss: 68977.734375
Epoch [1/1], Batch [1171], Loss: 66825.671875
Epoch [1/1], Batch [1181], Loss: 65693.351562
Epoch [1/1], Batch [1191], Loss: 68020.585938
Epoch [1/1], Batch [1201], Loss: 65902.757812
Epoch [1/1], Batch [1211], Loss: 65580.593750
Epoch [1/1], Batch [1221], Loss: 70715.906250
Epoch [1/1], Batch [1231], Loss: 68137.734375
Epoch [1/1], Batch [1241], Loss: 65525.937500
Epoch [1/1], Batch [1251], Loss: 67743.015625
Epoch [1/1], Batch [1261], Loss: 68199.718750
Epoch [1/1], Batch [1271], Loss: 65621.734375
Epoch [1/1], Batch [1281], Loss: 66282.578125
Epoch [1/1], Batch [1291], Loss: 69747.031250
Epoch [1/1], Batch [1301], Loss: 67258.179688
Epoch [1/1], Batch [1311], Loss: 68505.320312
Epoch [1/1], Batch [1321], Loss: 67310.640625
Epoch [1/1], Batch [1331], Loss: 66847.953125
Epoch [1/1], Batch [1341], Loss: 67474.226562
Epoch [1/1], Batch [1351], Loss: 68814.015625
Epoch [1/1], Batch [1361], Loss: 68905.429688
Epoch [1/1], Batch [1371], Loss: 70315.406250
Epoch [1/1], Batch [1381], Loss: 64965.511719
Epoch [1/1], Batch [1391], Loss: 68920.023438
Epoch [1/1], Batch [1401], Loss: 66342.187500
Epoch [1/1], Batch [1411], Loss: 67367.335938
Epoch [1/1], Batch [1421], Loss: 69169.187500
Epoch [1/1], Batch [1431], Loss: 66949.765625
Epoch [1/1], Batch [1441], Loss: 67544.734375
Epoch [1/1], Batch [1451], Loss: 67230.914062
Epoch [1/1], Batch [1461], Loss: 67441.609375
Epoch [1/1], Batch [1471], Loss: 67148.531250
Epoch [1/1], Batch [1481], Loss: 67303.460938
Epoch [1/1], Batch [1491], Loss: 67138.328125
Epoch [1/1], Batch [1501], Loss: 68011.062500
Epoch [1/1], Batch [1511], Loss: 66893.640625
Epoch [1/1], Batch [1521], Loss: 70657.054688
Epoch [1/1], Batch [1531], Loss: 67167.117188
Epoch [1/1], Batch [1541], Loss: 67067.304688
Epoch [1/1], Batch [1551], Loss: 65035.394531
Epoch [1/1], Batch [1561], Loss: 68677.539062
Epoch [1/1], Batch [1571], Loss: 69519.296875
Epoch [1/1], Batch [1581], Loss: 68698.578125
Epoch [1/1], Batch [1591], Loss: 69146.507812
Epoch [1/1], Batch [1601], Loss: 66976.796875
Epoch [1/1], Batch [1611], Loss: 65964.000000
Epoch [1/1], Batch [1621], Loss: 69971.703125
Epoch [1/1], Batch [1631], Loss: 69235.187500
Epoch [1/1], Batch [1641], Loss: 67429.812500
Epoch [1/1], Batch [1651], Loss: 68195.085938
Epoch [1/1], Batch [1661], Loss: 66159.750000
Epoch [1/1], Batch [1671], Loss: 66283.406250
Epoch [1/1], Batch [1681], Loss: 68209.562500
Epoch [1/1], Batch [1691], Loss: 66692.390625
Epoch [1/1], Batch [1701], Loss: 67588.031250
Epoch [1/1], Batch [1711], Loss: 66348.351562
Epoch [1/1], Batch [1721], Loss: 66664.546875
Epoch [1/1], Batch [1731], Loss: 65047.136719
Epoch [1/1], Batch [1741], Loss: 67220.148438
Epoch [1/1], Batch [1751], Loss: 64282.187500
Epoch [1/1], Batch [1761], Loss: 64594.160156
Epoch [1/1], Batch [1771], Loss: 67113.875000
Epoch [1/1], Batch [1781], Loss: 66241.132812
Epoch [1/1], Batch [1791], Loss: 66691.093750
Epoch [1/1], Batch [1801], Loss: 65285.652344
Epoch [1/1], Batch [1811], Loss: 64611.597656
Epoch [1/1], Batch [1821], Loss: 66648.953125
Epoch [1/1], Batch [1831], Loss: 69274.820312
Epoch [1/1], Batch [1841], Loss: 68485.437500
Epoch [1/1], Batch [1851], Loss: 68892.687500
Epoch [1/1], Batch [1861], Loss: 66849.960938
Epoch [1/1], Batch [1871], Loss: 67396.250000
Epoch [1/1], Batch [1881], Loss: 65715.203125
Epoch [1/1], Batch [1891], Loss: 64998.742188
Epoch [1/1], Batch [1901], Loss: 66153.531250
Epoch [1/1], Batch [1911], Loss: 69113.203125
Epoch [1/1], Batch [1921], Loss: 67058.960938
Epoch [1/1], Batch [1931], Loss: 68887.406250
Epoch [1/1], Batch [1941], Loss: 65675.406250
Epoch [1/1], Batch [1951], Loss: 68919.351562
Epoch [1/1], Batch [1961], Loss: 66967.742188
Epoch [1/1], Batch [1971], Loss: 65325.859375
Epoch [1/1], Batch [1981], Loss: 67002.203125
Epoch [1/1], Batch [1991], Loss: 70219.851562
Epoch [1/1], Batch [2001], Loss: 65497.769531
Epoch [1/1], Batch [2011], Loss: 68495.109375
Epoch [1/1], Batch [2021], Loss: 69350.304688
Epoch [1/1], Batch [2031], Loss: 68398.718750
Epoch [1/1], Batch [2041], Loss: 68104.585938
Epoch [1/1], Batch [2051], Loss: 66115.687500
Epoch [1/1], Batch [2061], Loss: 66685.890625
Epoch [1/1], Batch [2071], Loss: 67497.164062
Epoch [1/1], Batch [2081], Loss: 67429.882812
Epoch [1/1], Batch [2091], Loss: 65397.796875
Epoch [1/1], Batch [2101], Loss: 64919.609375
Epoch [1/1], Batch [2111], Loss: 74134.078125
Epoch [1/1], Batch [2121], Loss: 66102.773438
Epoch [1/1], Batch [2131], Loss: 67305.937500
Epoch [1/1], Batch [2141], Loss: 65918.476562
Epoch [1/1], Batch [2151], Loss: 66483.882812
Epoch [1/1], Batch [2161], Loss: 69933.109375
Epoch [1/1], Batch [2171], Loss: 67825.664062
Epoch [1/1], Batch [2181], Loss: 67312.656250
Epoch [1/1], Batch [2191], Loss: 67867.312500
Epoch [1/1], Batch [2201], Loss: 66551.421875
Epoch [1/1], Batch [2211], Loss: 67274.078125
Epoch [1/1], Batch [2221], Loss: 68680.859375
Epoch [1/1], Batch [2231], Loss: 63709.953125
Epoch [1/1], Batch [2241], Loss: 69322.953125
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 69668.6289
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 66422.9852
Elapsed time: 477.36 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 66456.4564
Elapsed time: 496.58 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 130478.765625
Epoch [1/1], Batch [11], Loss: 121283.914062
Epoch [1/1], Batch [21], Loss: 118586.554688
Epoch [1/1], Batch [31], Loss: 115596.156250
Epoch [1/1], Batch [41], Loss: 118291.906250
Epoch [1/1], Batch [51], Loss: 115663.648438
Epoch [1/1], Batch [61], Loss: 117039.125000
Epoch [1/1], Batch [71], Loss: 111970.757812
Epoch [1/1], Batch [81], Loss: 114156.132812
Epoch [1/1], Batch [91], Loss: 113797.585938
Epoch [1/1], Batch [101], Loss: 116390.218750
Epoch [1/1], Batch [111], Loss: 114445.882812
Epoch [1/1], Batch [121], Loss: 111217.390625
Epoch [1/1], Batch [131], Loss: 110524.867188
Epoch [1/1], Batch [141], Loss: 112390.531250
Epoch [1/1], Batch [151], Loss: 113905.421875
Epoch [1/1], Batch [161], Loss: 113252.820312
Epoch [1/1], Batch [171], Loss: 113755.531250
Epoch [1/1], Batch [181], Loss: 114337.703125
Epoch [1/1], Batch [191], Loss: 114986.265625
Epoch [1/1], Batch [201], Loss: 118743.156250
Epoch [1/1], Batch [211], Loss: 116384.398438
Epoch [1/1], Batch [221], Loss: 116399.531250
Epoch [1/1], Batch [231], Loss: 114650.671875
Epoch [1/1], Batch [241], Loss: 115920.046875
Epoch [1/1], Batch [251], Loss: 113464.750000
Epoch [1/1], Batch [261], Loss: 114787.945312
Epoch [1/1], Batch [271], Loss: 113921.156250
Epoch [1/1], Batch [281], Loss: 115795.460938
Epoch [1/1], Batch [291], Loss: 112891.718750
Epoch [1/1], Batch [301], Loss: 111883.109375
Epoch [1/1], Batch [311], Loss: 109417.625000
Epoch [1/1], Batch [321], Loss: 113744.437500
Epoch [1/1], Batch [331], Loss: 116269.812500
Epoch [1/1], Batch [341], Loss: 116954.421875
Epoch [1/1], Batch [351], Loss: 114479.484375
Epoch [1/1], Batch [361], Loss: 117470.000000
Epoch [1/1], Batch [371], Loss: 115691.656250
Epoch [1/1], Batch [381], Loss: 118561.117188
Epoch [1/1], Batch [391], Loss: 115329.859375
Epoch [1/1], Batch [401], Loss: 109922.125000
Epoch [1/1], Batch [411], Loss: 116493.531250
Epoch [1/1], Batch [421], Loss: 111169.500000
Epoch [1/1], Batch [431], Loss: 113533.531250
Epoch [1/1], Batch [441], Loss: 113815.984375
Epoch [1/1], Batch [451], Loss: 112321.148438
Epoch [1/1], Batch [461], Loss: 116427.750000
Epoch [1/1], Batch [471], Loss: 114291.882812
Epoch [1/1], Batch [481], Loss: 112541.265625
Epoch [1/1], Batch [491], Loss: 116949.492188
Epoch [1/1], Batch [501], Loss: 117018.414062
Epoch [1/1], Batch [511], Loss: 110881.390625
Epoch [1/1], Batch [521], Loss: 114487.687500
Epoch [1/1], Batch [531], Loss: 110799.445312
Epoch [1/1], Batch [541], Loss: 113071.515625
Epoch [1/1], Batch [551], Loss: 113180.015625
Epoch [1/1], Batch [561], Loss: 112586.390625
Epoch [1/1], Batch [571], Loss: 115628.429688
Epoch [1/1], Batch [581], Loss: 111413.742188
Epoch [1/1], Batch [591], Loss: 115453.140625
Epoch [1/1], Batch [601], Loss: 112653.687500
Epoch [1/1], Batch [611], Loss: 112650.960938
Epoch [1/1], Batch [621], Loss: 116834.320312
Epoch [1/1], Batch [631], Loss: 113380.437500
Epoch [1/1], Batch [641], Loss: 114149.468750
Epoch [1/1], Batch [651], Loss: 112519.765625
Epoch [1/1], Batch [661], Loss: 111178.156250
Epoch [1/1], Batch [671], Loss: 112132.929688
Epoch [1/1], Batch [681], Loss: 118658.507812
Epoch [1/1], Batch [691], Loss: 107577.078125
Epoch [1/1], Batch [701], Loss: 108856.632812
Epoch [1/1], Batch [711], Loss: 111097.234375
Epoch [1/1], Batch [721], Loss: 112856.226562
Epoch [1/1], Batch [731], Loss: 114954.421875
Epoch [1/1], Batch [741], Loss: 113833.117188
Epoch [1/1], Batch [751], Loss: 114110.796875
Epoch [1/1], Batch [761], Loss: 113205.437500
Epoch [1/1], Batch [771], Loss: 117093.000000
Epoch [1/1], Batch [781], Loss: 116436.625000
Epoch [1/1], Batch [791], Loss: 120837.656250
Epoch [1/1], Batch [801], Loss: 117265.671875
Epoch [1/1], Batch [811], Loss: 114224.640625
Epoch [1/1], Batch [821], Loss: 116639.937500
Epoch [1/1], Batch [831], Loss: 108847.593750
Epoch [1/1], Batch [841], Loss: 111607.562500
Epoch [1/1], Batch [851], Loss: 108496.015625
Epoch [1/1], Batch [861], Loss: 118858.164062
Epoch [1/1], Batch [871], Loss: 106463.492188
Epoch [1/1], Batch [881], Loss: 111932.101562
Epoch [1/1], Batch [891], Loss: 116446.515625
Epoch [1/1], Batch [901], Loss: 111595.984375
Epoch [1/1], Batch [911], Loss: 113547.812500
Epoch [1/1], Batch [921], Loss: 110657.070312
Epoch [1/1], Batch [931], Loss: 113696.742188
Epoch [1/1], Batch [941], Loss: 114980.453125
Epoch [1/1], Batch [951], Loss: 114024.453125
Epoch [1/1], Batch [961], Loss: 117680.656250
Epoch [1/1], Batch [971], Loss: 109266.773438
Epoch [1/1], Batch [981], Loss: 111979.453125
Epoch [1/1], Batch [991], Loss: 111250.757812
Epoch [1/1], Batch [1001], Loss: 111195.234375
Epoch [1/1], Batch [1011], Loss: 112589.093750
Epoch [1/1], Batch [1021], Loss: 111638.445312
Epoch [1/1], Batch [1031], Loss: 111398.828125
Epoch [1/1], Batch [1041], Loss: 113070.164062
Epoch [1/1], Batch [1051], Loss: 111655.875000
Epoch [1/1], Batch [1061], Loss: 114262.320312
Epoch [1/1], Batch [1071], Loss: 113979.656250
Epoch [1/1], Batch [1081], Loss: 113295.812500
Epoch [1/1], Batch [1091], Loss: 108516.859375
Epoch [1/1], Batch [1101], Loss: 111116.500000
Epoch [1/1], Batch [1111], Loss: 114931.203125
Epoch [1/1], Batch [1121], Loss: 118118.171875
Epoch [1/1], Batch [1131], Loss: 109781.054688
Epoch [1/1], Batch [1141], Loss: 115724.687500
Epoch [1/1], Batch [1151], Loss: 114555.031250
Epoch [1/1], Batch [1161], Loss: 113974.968750
Epoch [1/1], Batch [1171], Loss: 109218.476562
Epoch [1/1], Batch [1181], Loss: 110731.140625
Epoch [1/1], Batch [1191], Loss: 114090.226562
Epoch [1/1], Batch [1201], Loss: 112055.687500
Epoch [1/1], Batch [1211], Loss: 112092.406250
Epoch [1/1], Batch [1221], Loss: 109893.796875
Epoch [1/1], Batch [1231], Loss: 109120.093750
Epoch [1/1], Batch [1241], Loss: 113098.578125
Epoch [1/1], Batch [1251], Loss: 108584.937500
Epoch [1/1], Batch [1261], Loss: 109058.242188
Epoch [1/1], Batch [1271], Loss: 114683.437500
Epoch [1/1], Batch [1281], Loss: 114143.304688
Epoch [1/1], Batch [1291], Loss: 110292.085938
Epoch [1/1], Batch [1301], Loss: 109237.164062
Epoch [1/1], Batch [1311], Loss: 110313.984375
Epoch [1/1], Batch [1321], Loss: 115849.695312
Epoch [1/1], Batch [1331], Loss: 122052.031250
Epoch [1/1], Batch [1341], Loss: 115749.609375
Epoch [1/1], Batch [1351], Loss: 112534.593750
Epoch [1/1], Batch [1361], Loss: 112318.718750
Epoch [1/1], Batch [1371], Loss: 110860.718750
Epoch [1/1], Batch [1381], Loss: 114108.703125
Epoch [1/1], Batch [1391], Loss: 114817.828125
Epoch [1/1], Batch [1401], Loss: 115858.468750
Epoch [1/1], Batch [1411], Loss: 115376.257812
Epoch [1/1], Batch [1421], Loss: 113676.812500
Epoch [1/1], Batch [1431], Loss: 112726.484375
Epoch [1/1], Batch [1441], Loss: 115314.335938
Epoch [1/1], Batch [1451], Loss: 114502.015625
Epoch [1/1], Batch [1461], Loss: 116078.187500
Epoch [1/1], Batch [1471], Loss: 108140.820312
Epoch [1/1], Batch [1481], Loss: 110877.218750
Epoch [1/1], Batch [1491], Loss: 114238.843750
Epoch [1/1], Batch [1501], Loss: 111124.593750
Epoch [1/1], Batch [1511], Loss: 115377.843750
Epoch [1/1], Batch [1521], Loss: 115243.562500
Epoch [1/1], Batch [1531], Loss: 113479.148438
Epoch [1/1], Batch [1541], Loss: 111583.468750
Epoch [1/1], Batch [1551], Loss: 110914.101562
Epoch [1/1], Batch [1561], Loss: 115235.812500
Epoch [1/1], Batch [1571], Loss: 112836.875000
Epoch [1/1], Batch [1581], Loss: 116145.812500
Epoch [1/1], Batch [1591], Loss: 118908.640625
Epoch [1/1], Batch [1601], Loss: 112722.156250
Epoch [1/1], Batch [1611], Loss: 108900.031250
Epoch [1/1], Batch [1621], Loss: 113881.476562
Epoch [1/1], Batch [1631], Loss: 115189.148438
Epoch [1/1], Batch [1641], Loss: 117739.554688
Epoch [1/1], Batch [1651], Loss: 112133.476562
Epoch [1/1], Batch [1661], Loss: 109766.250000
Epoch [1/1], Batch [1671], Loss: 110234.671875
Epoch [1/1], Batch [1681], Loss: 112695.757812
Epoch [1/1], Batch [1691], Loss: 112439.921875
Epoch [1/1], Batch [1701], Loss: 112469.484375
Epoch [1/1], Batch [1711], Loss: 110426.648438
Epoch [1/1], Batch [1721], Loss: 112536.046875
Epoch [1/1], Batch [1731], Loss: 116623.921875
Epoch [1/1], Batch [1741], Loss: 112531.210938
Epoch [1/1], Batch [1751], Loss: 113141.484375
Epoch [1/1], Batch [1761], Loss: 115915.976562
Epoch [1/1], Batch [1771], Loss: 115609.453125
Epoch [1/1], Batch [1781], Loss: 115864.820312
Epoch [1/1], Batch [1791], Loss: 107775.937500
Epoch [1/1], Batch [1801], Loss: 111867.335938
Epoch [1/1], Batch [1811], Loss: 111594.765625
Epoch [1/1], Batch [1821], Loss: 108951.218750
Epoch [1/1], Batch [1831], Loss: 119169.078125
Epoch [1/1], Batch [1841], Loss: 111352.484375
Epoch [1/1], Batch [1851], Loss: 114231.570312
Epoch [1/1], Batch [1861], Loss: 118236.398438
Epoch [1/1], Batch [1871], Loss: 111643.000000
Epoch [1/1], Batch [1881], Loss: 113422.484375
Epoch [1/1], Batch [1891], Loss: 114288.859375
Epoch [1/1], Batch [1901], Loss: 106991.679688
Epoch [1/1], Batch [1911], Loss: 111612.250000
Epoch [1/1], Batch [1921], Loss: 110709.765625
Epoch [1/1], Batch [1931], Loss: 111458.671875
Epoch [1/1], Batch [1941], Loss: 111842.593750
Epoch [1/1], Batch [1951], Loss: 115336.953125
Epoch [1/1], Batch [1961], Loss: 106981.234375
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 113300.4793
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 111862.0826
Elapsed time: 1102.02 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 112171.5457
Elapsed time: 1125.08 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 155999.937500
Epoch [1/1], Batch [11], Loss: 164696.781250
Epoch [1/1], Batch [21], Loss: 161939.093750
Epoch [1/1], Batch [31], Loss: 156414.734375
Epoch [1/1], Batch [41], Loss: 154599.312500
Epoch [1/1], Batch [51], Loss: 161142.781250
Epoch [1/1], Batch [61], Loss: 161137.765625
Epoch [1/1], Batch [71], Loss: 161587.125000
Epoch [1/1], Batch [81], Loss: 158179.656250
Epoch [1/1], Batch [91], Loss: 159638.312500
Epoch [1/1], Batch [101], Loss: 154133.468750
Epoch [1/1], Batch [111], Loss: 158543.437500
Epoch [1/1], Batch [121], Loss: 160532.109375
Epoch [1/1], Batch [131], Loss: 163014.875000
Epoch [1/1], Batch [141], Loss: 156764.234375
Epoch [1/1], Batch [151], Loss: 157321.062500
Epoch [1/1], Batch [161], Loss: 166972.093750
Epoch [1/1], Batch [171], Loss: 157697.265625
Epoch [1/1], Batch [181], Loss: 163428.000000
Epoch [1/1], Batch [191], Loss: 155128.343750
Epoch [1/1], Batch [201], Loss: 161567.296875
Epoch [1/1], Batch [211], Loss: 152122.812500
Epoch [1/1], Batch [221], Loss: 162312.500000
Epoch [1/1], Batch [231], Loss: 162177.750000
Epoch [1/1], Batch [241], Loss: 168144.687500
Epoch [1/1], Batch [251], Loss: 158977.718750
Epoch [1/1], Batch [261], Loss: 160349.750000
Epoch [1/1], Batch [271], Loss: 158425.171875
Epoch [1/1], Batch [281], Loss: 161189.312500
Epoch [1/1], Batch [291], Loss: 159197.781250
Epoch [1/1], Batch [301], Loss: 160923.875000
Epoch [1/1], Batch [311], Loss: 156300.578125
Epoch [1/1], Batch [321], Loss: 154011.156250
Epoch [1/1], Batch [331], Loss: 167057.593750
Epoch [1/1], Batch [341], Loss: 161465.765625
Epoch [1/1], Batch [351], Loss: 159767.531250
Epoch [1/1], Batch [361], Loss: 153266.437500
Epoch [1/1], Batch [371], Loss: 156459.343750
Epoch [1/1], Batch [381], Loss: 160653.156250
Epoch [1/1], Batch [391], Loss: 163447.125000
Epoch [1/1], Batch [401], Loss: 153979.906250
Epoch [1/1], Batch [411], Loss: 161641.156250
Epoch [1/1], Batch [421], Loss: 161027.531250
Epoch [1/1], Batch [431], Loss: 155507.875000
Epoch [1/1], Batch [441], Loss: 158163.343750
Epoch [1/1], Batch [451], Loss: 168937.812500
Epoch [1/1], Batch [461], Loss: 160294.640625
Epoch [1/1], Batch [471], Loss: 156731.468750
Epoch [1/1], Batch [481], Loss: 165128.406250
Epoch [1/1], Batch [491], Loss: 157153.218750
Epoch [1/1], Batch [501], Loss: 155550.109375
Epoch [1/1], Batch [511], Loss: 161652.046875
Epoch [1/1], Batch [521], Loss: 158089.828125
Epoch [1/1], Batch [531], Loss: 157199.031250
Epoch [1/1], Batch [541], Loss: 152673.437500
Epoch [1/1], Batch [551], Loss: 159432.593750
Epoch [1/1], Batch [561], Loss: 161930.875000
Epoch [1/1], Batch [571], Loss: 161317.906250
Epoch [1/1], Batch [581], Loss: 154775.796875
Epoch [1/1], Batch [591], Loss: 157425.125000
Epoch [1/1], Batch [601], Loss: 161327.875000
Epoch [1/1], Batch [611], Loss: 157915.531250
Epoch [1/1], Batch [621], Loss: 158423.781250
Epoch [1/1], Batch [631], Loss: 152871.656250
Epoch [1/1], Batch [641], Loss: 161641.031250
Epoch [1/1], Batch [651], Loss: 165408.156250
Epoch [1/1], Batch [661], Loss: 162600.468750
Epoch [1/1], Batch [671], Loss: 163354.515625
Epoch [1/1], Batch [681], Loss: 157142.281250
Epoch [1/1], Batch [691], Loss: 164050.859375
Epoch [1/1], Batch [701], Loss: 153419.140625
Epoch [1/1], Batch [711], Loss: 163478.593750
Epoch [1/1], Batch [721], Loss: 158907.437500
Epoch [1/1], Batch [731], Loss: 160826.781250
Epoch [1/1], Batch [741], Loss: 156574.250000
Epoch [1/1], Batch [751], Loss: 157375.937500
Epoch [1/1], Batch [761], Loss: 163515.500000
Epoch [1/1], Batch [771], Loss: 159213.421875
Epoch [1/1], Batch [781], Loss: 163957.515625
Epoch [1/1], Batch [791], Loss: 161509.750000
Epoch [1/1], Batch [801], Loss: 161394.234375
Epoch [1/1], Batch [811], Loss: 164949.359375
Epoch [1/1], Batch [821], Loss: 156415.234375
Epoch [1/1], Batch [831], Loss: 154538.796875
Epoch [1/1], Batch [841], Loss: 157429.437500
Epoch [1/1], Batch [851], Loss: 158211.703125
Epoch [1/1], Batch [861], Loss: 160380.406250
Epoch [1/1], Batch [871], Loss: 158644.812500
Epoch [1/1], Batch [881], Loss: 154222.437500
Epoch [1/1], Batch [891], Loss: 164996.671875
Epoch [1/1], Batch [901], Loss: 156294.937500
Epoch [1/1], Batch [911], Loss: 160533.468750
Epoch [1/1], Batch [921], Loss: 157120.984375
Epoch [1/1], Batch [931], Loss: 159653.328125
Epoch [1/1], Batch [941], Loss: 155882.140625
Epoch [1/1], Batch [951], Loss: 165327.609375
Epoch [1/1], Batch [961], Loss: 153815.140625
Epoch [1/1], Batch [971], Loss: 160713.906250
Epoch [1/1], Batch [981], Loss: 167076.406250
Epoch [1/1], Batch [991], Loss: 161490.968750
Epoch [1/1], Batch [1001], Loss: 160892.000000
Epoch [1/1], Batch [1011], Loss: 163407.390625
Epoch [1/1], Batch [1021], Loss: 158065.156250
Epoch [1/1], Batch [1031], Loss: 163357.625000
Epoch [1/1], Batch [1041], Loss: 159907.765625
Epoch [1/1], Batch [1051], Loss: 161194.125000
Epoch [1/1], Batch [1061], Loss: 160625.125000
Epoch [1/1], Batch [1071], Loss: 165687.703125
Epoch [1/1], Batch [1081], Loss: 159622.312500
Epoch [1/1], Batch [1091], Loss: 163096.171875
Epoch [1/1], Batch [1101], Loss: 159778.171875
Epoch [1/1], Batch [1111], Loss: 165335.453125
Epoch [1/1], Batch [1121], Loss: 154887.625000
Epoch [1/1], Batch [1131], Loss: 159601.218750
Epoch [1/1], Batch [1141], Loss: 162643.703125
Epoch [1/1], Batch [1151], Loss: 161221.125000
Epoch [1/1], Batch [1161], Loss: 157723.734375
Epoch [1/1], Batch [1171], Loss: 164561.343750
Epoch [1/1], Batch [1181], Loss: 167546.625000
Epoch [1/1], Batch [1191], Loss: 164669.687500
Epoch [1/1], Batch [1201], Loss: 155990.531250
Epoch [1/1], Batch [1211], Loss: 154833.468750
Epoch [1/1], Batch [1221], Loss: 157065.968750
Epoch [1/1], Batch [1231], Loss: 156608.000000
Epoch [1/1], Batch [1241], Loss: 159292.718750
Epoch [1/1], Batch [1251], Loss: 163880.968750
Epoch [1/1], Batch [1261], Loss: 153776.468750
Epoch [1/1], Batch [1271], Loss: 153489.843750
Epoch [1/1], Batch [1281], Loss: 157357.656250
Epoch [1/1], Batch [1291], Loss: 164200.109375
Epoch [1/1], Batch [1301], Loss: 161274.312500
Epoch [1/1], Batch [1311], Loss: 163505.343750
Epoch [1/1], Batch [1321], Loss: 162589.765625
Epoch [1/1], Batch [1331], Loss: 155098.984375
Epoch [1/1], Batch [1341], Loss: 157688.953125
Epoch [1/1], Batch [1351], Loss: 163374.703125
Epoch [1/1], Batch [1361], Loss: 152510.343750
Epoch [1/1], Batch [1371], Loss: 158437.265625
Epoch [1/1], Batch [1381], Loss: 158424.078125
Epoch [1/1], Batch [1391], Loss: 157947.359375
Epoch [1/1], Batch [1401], Loss: 160525.281250
Epoch [1/1], Batch [1411], Loss: 153552.906250
Epoch [1/1], Batch [1421], Loss: 161604.531250
Epoch [1/1], Batch [1431], Loss: 164187.750000
Epoch [1/1], Batch [1441], Loss: 159629.812500
Epoch [1/1], Batch [1451], Loss: 159076.000000
Epoch [1/1], Batch [1461], Loss: 160551.687500
Epoch [1/1], Batch [1471], Loss: 154213.531250
Epoch [1/1], Batch [1481], Loss: 157523.687500
Epoch [1/1], Batch [1491], Loss: 159208.437500
Epoch [1/1], Batch [1501], Loss: 154697.187500
Epoch [1/1], Batch [1511], Loss: 157418.828125
Epoch [1/1], Batch [1521], Loss: 160692.156250
Epoch [1/1], Batch [1531], Loss: 159912.937500
Epoch [1/1], Batch [1541], Loss: 151994.328125
Epoch [1/1], Batch [1551], Loss: 156733.625000
Epoch [1/1], Batch [1561], Loss: 160259.906250
Epoch [1/1], Batch [1571], Loss: 160412.156250
Epoch [1/1], Batch [1581], Loss: 158802.953125
Epoch [1/1], Batch [1591], Loss: 168704.437500
Epoch [1/1], Batch [1601], Loss: 160066.703125
Epoch [1/1], Batch [1611], Loss: 160390.296875
Epoch [1/1], Batch [1621], Loss: 152405.781250
Epoch [1/1], Batch [1631], Loss: 160236.843750
Epoch [1/1], Batch [1641], Loss: 159893.062500
Epoch [1/1], Batch [1651], Loss: 165000.796875
Epoch [1/1], Batch [1661], Loss: 158046.906250
Epoch [1/1], Batch [1671], Loss: 155065.781250
Epoch [1/1], Batch [1681], Loss: 165335.984375
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 158901.4229
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 156079.4944
Elapsed time: 1800.73 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 157260.0592
Elapsed time: 1825.68 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 201016.171875
Epoch [1/1], Batch [11], Loss: 210026.187500
Epoch [1/1], Batch [21], Loss: 201493.593750
Epoch [1/1], Batch [31], Loss: 204315.750000
Epoch [1/1], Batch [41], Loss: 207308.515625
Epoch [1/1], Batch [51], Loss: 210726.859375
Epoch [1/1], Batch [61], Loss: 202752.234375
Epoch [1/1], Batch [71], Loss: 214971.031250
Epoch [1/1], Batch [81], Loss: 205807.343750
Epoch [1/1], Batch [91], Loss: 211235.203125
Epoch [1/1], Batch [101], Loss: 200725.218750
Epoch [1/1], Batch [111], Loss: 200844.625000
Epoch [1/1], Batch [121], Loss: 209727.187500
Epoch [1/1], Batch [131], Loss: 200635.750000
Epoch [1/1], Batch [141], Loss: 210109.890625
Epoch [1/1], Batch [151], Loss: 209908.937500
Epoch [1/1], Batch [161], Loss: 208661.156250
Epoch [1/1], Batch [171], Loss: 209589.656250
Epoch [1/1], Batch [181], Loss: 210298.000000
Epoch [1/1], Batch [191], Loss: 212270.250000
Epoch [1/1], Batch [201], Loss: 198775.531250
Epoch [1/1], Batch [211], Loss: 199588.609375
Epoch [1/1], Batch [221], Loss: 195181.984375
Epoch [1/1], Batch [231], Loss: 201806.750000
Epoch [1/1], Batch [241], Loss: 211161.609375
Epoch [1/1], Batch [251], Loss: 206006.406250
Epoch [1/1], Batch [261], Loss: 202783.031250
Epoch [1/1], Batch [271], Loss: 190160.312500
Epoch [1/1], Batch [281], Loss: 201708.562500
Epoch [1/1], Batch [291], Loss: 207661.937500
Epoch [1/1], Batch [301], Loss: 207526.500000
Epoch [1/1], Batch [311], Loss: 210106.000000
Epoch [1/1], Batch [321], Loss: 202040.000000
Epoch [1/1], Batch [331], Loss: 201288.937500
Epoch [1/1], Batch [341], Loss: 200381.515625
Epoch [1/1], Batch [351], Loss: 201327.750000
Epoch [1/1], Batch [361], Loss: 209269.500000
Epoch [1/1], Batch [371], Loss: 207772.140625
Epoch [1/1], Batch [381], Loss: 209704.625000
Epoch [1/1], Batch [391], Loss: 200200.859375
Epoch [1/1], Batch [401], Loss: 201155.781250
Epoch [1/1], Batch [411], Loss: 195918.828125
Epoch [1/1], Batch [421], Loss: 207024.765625
Epoch [1/1], Batch [431], Loss: 196326.562500
Epoch [1/1], Batch [441], Loss: 213780.359375
Epoch [1/1], Batch [451], Loss: 204518.156250
Epoch [1/1], Batch [461], Loss: 207314.921875
Epoch [1/1], Batch [471], Loss: 198746.281250
Epoch [1/1], Batch [481], Loss: 208878.156250
Epoch [1/1], Batch [491], Loss: 207516.156250
Epoch [1/1], Batch [501], Loss: 208945.968750
Epoch [1/1], Batch [511], Loss: 205974.484375
Epoch [1/1], Batch [521], Loss: 203783.906250
Epoch [1/1], Batch [531], Loss: 200502.468750
Epoch [1/1], Batch [541], Loss: 208869.312500
Epoch [1/1], Batch [551], Loss: 197188.031250
Epoch [1/1], Batch [561], Loss: 204924.093750
Epoch [1/1], Batch [571], Loss: 206926.781250
Epoch [1/1], Batch [581], Loss: 213124.750000
Epoch [1/1], Batch [591], Loss: 198307.234375
Epoch [1/1], Batch [601], Loss: 210708.937500
Epoch [1/1], Batch [611], Loss: 206687.843750
Epoch [1/1], Batch [621], Loss: 195919.765625
Epoch [1/1], Batch [631], Loss: 209957.718750
Epoch [1/1], Batch [641], Loss: 207949.812500
Epoch [1/1], Batch [651], Loss: 210411.281250
Epoch [1/1], Batch [661], Loss: 202762.343750
Epoch [1/1], Batch [671], Loss: 209131.078125
Epoch [1/1], Batch [681], Loss: 205657.000000
Epoch [1/1], Batch [691], Loss: 199924.921875
Epoch [1/1], Batch [701], Loss: 202665.906250
Epoch [1/1], Batch [711], Loss: 198977.625000
Epoch [1/1], Batch [721], Loss: 198404.343750
Epoch [1/1], Batch [731], Loss: 203655.187500
Epoch [1/1], Batch [741], Loss: 205649.281250
Epoch [1/1], Batch [751], Loss: 210920.203125
Epoch [1/1], Batch [761], Loss: 205943.812500
Epoch [1/1], Batch [771], Loss: 200285.046875
Epoch [1/1], Batch [781], Loss: 199863.218750
Epoch [1/1], Batch [791], Loss: 205151.937500
Epoch [1/1], Batch [801], Loss: 201439.046875
Epoch [1/1], Batch [811], Loss: 211717.125000
Epoch [1/1], Batch [821], Loss: 200818.031250
Epoch [1/1], Batch [831], Loss: 207713.156250
Epoch [1/1], Batch [841], Loss: 201880.718750
Epoch [1/1], Batch [851], Loss: 206086.062500
Epoch [1/1], Batch [861], Loss: 200222.546875
Epoch [1/1], Batch [871], Loss: 203274.375000
Epoch [1/1], Batch [881], Loss: 205468.031250
Epoch [1/1], Batch [891], Loss: 215222.875000
Epoch [1/1], Batch [901], Loss: 209762.500000
Epoch [1/1], Batch [911], Loss: 203273.718750
Epoch [1/1], Batch [921], Loss: 204504.562500
Epoch [1/1], Batch [931], Loss: 202021.500000
Epoch [1/1], Batch [941], Loss: 208706.500000
Epoch [1/1], Batch [951], Loss: 200503.500000
Epoch [1/1], Batch [961], Loss: 207041.625000
Epoch [1/1], Batch [971], Loss: 210400.375000
Epoch [1/1], Batch [981], Loss: 202730.531250
Epoch [1/1], Batch [991], Loss: 206374.781250
Epoch [1/1], Batch [1001], Loss: 213089.406250
Epoch [1/1], Batch [1011], Loss: 201541.343750
Epoch [1/1], Batch [1021], Loss: 196393.609375
Epoch [1/1], Batch [1031], Loss: 200150.531250
Epoch [1/1], Batch [1041], Loss: 208733.703125
Epoch [1/1], Batch [1051], Loss: 208608.984375
Epoch [1/1], Batch [1061], Loss: 206614.343750
Epoch [1/1], Batch [1071], Loss: 204420.734375
Epoch [1/1], Batch [1081], Loss: 204853.781250
Epoch [1/1], Batch [1091], Loss: 208336.437500
Epoch [1/1], Batch [1101], Loss: 202802.406250
Epoch [1/1], Batch [1111], Loss: 199129.859375
Epoch [1/1], Batch [1121], Loss: 207067.578125
Epoch [1/1], Batch [1131], Loss: 207435.968750
Epoch [1/1], Batch [1141], Loss: 199780.437500
Epoch [1/1], Batch [1151], Loss: 194854.781250
Epoch [1/1], Batch [1161], Loss: 201220.031250
Epoch [1/1], Batch [1171], Loss: 196817.000000
Epoch [1/1], Batch [1181], Loss: 204918.109375
Epoch [1/1], Batch [1191], Loss: 205658.687500
Epoch [1/1], Batch [1201], Loss: 200911.421875
Epoch [1/1], Batch [1211], Loss: 211607.453125
Epoch [1/1], Batch [1221], Loss: 206788.984375
Epoch [1/1], Batch [1231], Loss: 206845.281250
Epoch [1/1], Batch [1241], Loss: 203082.765625
Epoch [1/1], Batch [1251], Loss: 205274.312500
Epoch [1/1], Batch [1261], Loss: 203774.765625
Epoch [1/1], Batch [1271], Loss: 206144.828125
Epoch [1/1], Batch [1281], Loss: 204071.562500
Epoch [1/1], Batch [1291], Loss: 196528.875000
Epoch [1/1], Batch [1301], Loss: 206737.062500
Epoch [1/1], Batch [1311], Loss: 195502.218750
Epoch [1/1], Batch [1321], Loss: 204971.453125
Epoch [1/1], Batch [1331], Loss: 195283.250000
Epoch [1/1], Batch [1341], Loss: 211015.250000
Epoch [1/1], Batch [1351], Loss: 202792.953125
Epoch [1/1], Batch [1361], Loss: 203699.937500
Epoch [1/1], Batch [1371], Loss: 214126.781250
Epoch [1/1], Batch [1381], Loss: 203966.406250
Epoch [1/1], Batch [1391], Loss: 204566.656250
Epoch [1/1], Batch [1401], Loss: 208415.390625
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 204170.4470
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 202432.3426
Elapsed time: 2526.74 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 204516.3596
Elapsed time: 2551.98 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 253231.750000
Epoch [1/1], Batch [11], Loss: 245772.765625
Epoch [1/1], Batch [21], Loss: 242603.625000
Epoch [1/1], Batch [31], Loss: 246780.718750
Epoch [1/1], Batch [41], Loss: 240951.593750
Epoch [1/1], Batch [51], Loss: 254321.140625
Epoch [1/1], Batch [61], Loss: 239267.593750
Epoch [1/1], Batch [71], Loss: 240686.140625
Epoch [1/1], Batch [81], Loss: 248196.656250
Epoch [1/1], Batch [91], Loss: 246999.062500
Epoch [1/1], Batch [101], Loss: 250343.140625
Epoch [1/1], Batch [111], Loss: 247687.093750
Epoch [1/1], Batch [121], Loss: 250607.031250
Epoch [1/1], Batch [131], Loss: 247043.562500
Epoch [1/1], Batch [141], Loss: 251778.093750
Epoch [1/1], Batch [151], Loss: 249429.656250
Epoch [1/1], Batch [161], Loss: 246041.203125
Epoch [1/1], Batch [171], Loss: 250319.218750
Epoch [1/1], Batch [181], Loss: 239295.906250
Epoch [1/1], Batch [191], Loss: 248785.312500
Epoch [1/1], Batch [201], Loss: 242054.875000
Epoch [1/1], Batch [211], Loss: 253500.750000
Epoch [1/1], Batch [221], Loss: 248661.406250
Epoch [1/1], Batch [231], Loss: 246894.812500
Epoch [1/1], Batch [241], Loss: 252650.468750
Epoch [1/1], Batch [251], Loss: 258112.187500
Epoch [1/1], Batch [261], Loss: 245012.531250
Epoch [1/1], Batch [271], Loss: 245603.296875
Epoch [1/1], Batch [281], Loss: 247960.203125
Epoch [1/1], Batch [291], Loss: 246687.937500
Epoch [1/1], Batch [301], Loss: 245310.437500
Epoch [1/1], Batch [311], Loss: 248981.312500
Epoch [1/1], Batch [321], Loss: 249494.875000
Epoch [1/1], Batch [331], Loss: 246389.906250
Epoch [1/1], Batch [341], Loss: 250275.343750
Epoch [1/1], Batch [351], Loss: 248270.031250
Epoch [1/1], Batch [361], Loss: 256252.625000
Epoch [1/1], Batch [371], Loss: 251118.343750
Epoch [1/1], Batch [381], Loss: 241904.640625
Epoch [1/1], Batch [391], Loss: 245668.656250
Epoch [1/1], Batch [401], Loss: 226512.031250
Epoch [1/1], Batch [411], Loss: 247605.718750
Epoch [1/1], Batch [421], Loss: 243659.796875
Epoch [1/1], Batch [431], Loss: 263148.468750
Epoch [1/1], Batch [441], Loss: 254494.515625
Epoch [1/1], Batch [451], Loss: 243225.687500
Epoch [1/1], Batch [461], Loss: 249245.078125
Epoch [1/1], Batch [471], Loss: 257639.718750
Epoch [1/1], Batch [481], Loss: 241864.125000
Epoch [1/1], Batch [491], Loss: 237191.328125
Epoch [1/1], Batch [501], Loss: 247724.781250
Epoch [1/1], Batch [511], Loss: 239671.234375
Epoch [1/1], Batch [521], Loss: 252463.750000
Epoch [1/1], Batch [531], Loss: 255740.906250
Epoch [1/1], Batch [541], Loss: 248969.546875
Epoch [1/1], Batch [551], Loss: 245730.125000
Epoch [1/1], Batch [561], Loss: 239205.015625
Epoch [1/1], Batch [571], Loss: 248520.484375
Epoch [1/1], Batch [581], Loss: 247067.828125
Epoch [1/1], Batch [591], Loss: 252198.718750
Epoch [1/1], Batch [601], Loss: 261011.531250
Epoch [1/1], Batch [611], Loss: 244825.281250
Epoch [1/1], Batch [621], Loss: 251723.281250
Epoch [1/1], Batch [631], Loss: 252719.062500
Epoch [1/1], Batch [641], Loss: 252445.578125
Epoch [1/1], Batch [651], Loss: 254527.281250
Epoch [1/1], Batch [661], Loss: 248196.875000
Epoch [1/1], Batch [671], Loss: 246911.390625
Epoch [1/1], Batch [681], Loss: 241998.312500
Epoch [1/1], Batch [691], Loss: 249942.593750
Epoch [1/1], Batch [701], Loss: 246057.546875
Epoch [1/1], Batch [711], Loss: 245447.593750
Epoch [1/1], Batch [721], Loss: 237146.593750
Epoch [1/1], Batch [731], Loss: 244725.484375
Epoch [1/1], Batch [741], Loss: 247434.046875
Epoch [1/1], Batch [751], Loss: 242509.812500
Epoch [1/1], Batch [761], Loss: 242891.750000
Epoch [1/1], Batch [771], Loss: 242299.578125
Epoch [1/1], Batch [781], Loss: 241076.468750
Epoch [1/1], Batch [791], Loss: 243982.593750
Epoch [1/1], Batch [801], Loss: 239411.953125
Epoch [1/1], Batch [811], Loss: 247228.187500
Epoch [1/1], Batch [821], Loss: 240083.765625
Epoch [1/1], Batch [831], Loss: 249894.468750
Epoch [1/1], Batch [841], Loss: 244770.781250
Epoch [1/1], Batch [851], Loss: 237703.453125
Epoch [1/1], Batch [861], Loss: 243281.203125
Epoch [1/1], Batch [871], Loss: 257115.828125
Epoch [1/1], Batch [881], Loss: 242436.796875
Epoch [1/1], Batch [891], Loss: 251831.531250
Epoch [1/1], Batch [901], Loss: 252622.140625
Epoch [1/1], Batch [911], Loss: 247013.484375
Epoch [1/1], Batch [921], Loss: 245839.125000
Epoch [1/1], Batch [931], Loss: 252951.625000
Epoch [1/1], Batch [941], Loss: 242949.000000
Epoch [1/1], Batch [951], Loss: 243498.078125
Epoch [1/1], Batch [961], Loss: 228937.031250
Epoch [1/1], Batch [971], Loss: 247656.937500
Epoch [1/1], Batch [981], Loss: 247522.078125
Epoch [1/1], Batch [991], Loss: 238863.281250
Epoch [1/1], Batch [1001], Loss: 246915.859375
Epoch [1/1], Batch [1011], Loss: 258950.328125
Epoch [1/1], Batch [1021], Loss: 239757.187500
Epoch [1/1], Batch [1031], Loss: 234021.703125
Epoch [1/1], Batch [1041], Loss: 244853.359375
Epoch [1/1], Batch [1051], Loss: 245425.562500
Epoch [1/1], Batch [1061], Loss: 245401.937500
Epoch [1/1], Batch [1071], Loss: 246484.312500
Epoch [1/1], Batch [1081], Loss: 251340.515625
Epoch [1/1], Batch [1091], Loss: 248916.500000
Epoch [1/1], Batch [1101], Loss: 245346.578125
Epoch [1/1], Batch [1111], Loss: 246741.140625
Epoch [1/1], Batch [1121], Loss: 240949.515625
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 246948.0623
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 246390.1294
Elapsed time: 3222.13 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 248192.5146
Elapsed time: 3245.77 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 281875.281250
Epoch [1/1], Batch [11], Loss: 292213.375000
Epoch [1/1], Batch [21], Loss: 291841.250000
Epoch [1/1], Batch [31], Loss: 286366.812500
Epoch [1/1], Batch [41], Loss: 287784.812500
Epoch [1/1], Batch [51], Loss: 285176.312500
Epoch [1/1], Batch [61], Loss: 302954.281250
Epoch [1/1], Batch [71], Loss: 286956.750000
Epoch [1/1], Batch [81], Loss: 286498.250000
Epoch [1/1], Batch [91], Loss: 274924.312500
Epoch [1/1], Batch [101], Loss: 295362.000000
Epoch [1/1], Batch [111], Loss: 287238.093750
Epoch [1/1], Batch [121], Loss: 289755.937500
Epoch [1/1], Batch [131], Loss: 283060.312500
Epoch [1/1], Batch [141], Loss: 282859.843750
Epoch [1/1], Batch [151], Loss: 293366.000000
Epoch [1/1], Batch [161], Loss: 287687.437500
Epoch [1/1], Batch [171], Loss: 287007.500000
Epoch [1/1], Batch [181], Loss: 285669.437500
Epoch [1/1], Batch [191], Loss: 275186.125000
Epoch [1/1], Batch [201], Loss: 278256.093750
Epoch [1/1], Batch [211], Loss: 281313.000000
Epoch [1/1], Batch [221], Loss: 291904.312500
Epoch [1/1], Batch [231], Loss: 293761.593750
Epoch [1/1], Batch [241], Loss: 279599.468750
Epoch [1/1], Batch [251], Loss: 278784.812500
Epoch [1/1], Batch [261], Loss: 291779.000000
Epoch [1/1], Batch [271], Loss: 277788.968750
Epoch [1/1], Batch [281], Loss: 286835.281250
Epoch [1/1], Batch [291], Loss: 289180.000000
Epoch [1/1], Batch [301], Loss: 291168.531250
Epoch [1/1], Batch [311], Loss: 284756.750000
Epoch [1/1], Batch [321], Loss: 287119.656250
Epoch [1/1], Batch [331], Loss: 288701.968750
Epoch [1/1], Batch [341], Loss: 288095.625000
Epoch [1/1], Batch [351], Loss: 282966.312500
Epoch [1/1], Batch [361], Loss: 304641.062500
Epoch [1/1], Batch [371], Loss: 293747.656250
Epoch [1/1], Batch [381], Loss: 284709.281250
Epoch [1/1], Batch [391], Loss: 269501.875000
Epoch [1/1], Batch [401], Loss: 288509.687500
Epoch [1/1], Batch [411], Loss: 292196.718750
Epoch [1/1], Batch [421], Loss: 287148.906250
Epoch [1/1], Batch [431], Loss: 291124.687500
Epoch [1/1], Batch [441], Loss: 290146.343750
Epoch [1/1], Batch [451], Loss: 277928.656250
Epoch [1/1], Batch [461], Loss: 292851.250000
Epoch [1/1], Batch [471], Loss: 286995.312500
Epoch [1/1], Batch [481], Loss: 290121.000000
Epoch [1/1], Batch [491], Loss: 283632.937500
Epoch [1/1], Batch [501], Loss: 293693.093750
Epoch [1/1], Batch [511], Loss: 293123.687500
Epoch [1/1], Batch [521], Loss: 289530.968750
Epoch [1/1], Batch [531], Loss: 298677.531250
Epoch [1/1], Batch [541], Loss: 280360.968750
Epoch [1/1], Batch [551], Loss: 276615.406250
Epoch [1/1], Batch [561], Loss: 290445.312500
Epoch [1/1], Batch [571], Loss: 289187.562500
Epoch [1/1], Batch [581], Loss: 280302.093750
Epoch [1/1], Batch [591], Loss: 295969.562500
Epoch [1/1], Batch [601], Loss: 291096.062500
Epoch [1/1], Batch [611], Loss: 290986.375000
Epoch [1/1], Batch [621], Loss: 276362.062500
Epoch [1/1], Batch [631], Loss: 295630.468750
Epoch [1/1], Batch [641], Loss: 296108.187500
Epoch [1/1], Batch [651], Loss: 286491.000000
Epoch [1/1], Batch [661], Loss: 295412.093750
Epoch [1/1], Batch [671], Loss: 294811.531250
Epoch [1/1], Batch [681], Loss: 281588.500000
Epoch [1/1], Batch [691], Loss: 297092.937500
Epoch [1/1], Batch [701], Loss: 289802.125000
Epoch [1/1], Batch [711], Loss: 301650.687500
Epoch [1/1], Batch [721], Loss: 293719.812500
Epoch [1/1], Batch [731], Loss: 289587.625000
Epoch [1/1], Batch [741], Loss: 301082.125000
Epoch [1/1], Batch [751], Loss: 289559.812500
Epoch [1/1], Batch [761], Loss: 282646.812500
Epoch [1/1], Batch [771], Loss: 285018.312500
Epoch [1/1], Batch [781], Loss: 299350.250000
Epoch [1/1], Batch [791], Loss: 291552.062500
Epoch [1/1], Batch [801], Loss: 286873.156250
Epoch [1/1], Batch [811], Loss: 281510.593750
Epoch [1/1], Batch [821], Loss: 290854.906250
Epoch [1/1], Batch [831], Loss: 289582.812500
Epoch [1/1], Batch [841], Loss: 296061.468750
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 288299.7676
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 286967.8213
Elapsed time: 3858.12 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 290242.0136
Elapsed time: 3878.56 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 319966.187500
Epoch [1/1], Batch [11], Loss: 328553.687500
Epoch [1/1], Batch [21], Loss: 336156.437500
Epoch [1/1], Batch [31], Loss: 332608.625000
Epoch [1/1], Batch [41], Loss: 328704.750000
Epoch [1/1], Batch [51], Loss: 346483.375000
Epoch [1/1], Batch [61], Loss: 319145.343750
Epoch [1/1], Batch [71], Loss: 340627.062500
Epoch [1/1], Batch [81], Loss: 320252.718750
Epoch [1/1], Batch [91], Loss: 320899.187500
Epoch [1/1], Batch [101], Loss: 324296.031250
Epoch [1/1], Batch [111], Loss: 337896.500000
Epoch [1/1], Batch [121], Loss: 326624.937500
Epoch [1/1], Batch [131], Loss: 331545.843750
Epoch [1/1], Batch [141], Loss: 336565.062500
Epoch [1/1], Batch [151], Loss: 334336.562500
Epoch [1/1], Batch [161], Loss: 335405.906250
Epoch [1/1], Batch [171], Loss: 330438.625000
Epoch [1/1], Batch [181], Loss: 325609.375000
Epoch [1/1], Batch [191], Loss: 326651.562500
Epoch [1/1], Batch [201], Loss: 339348.593750
Epoch [1/1], Batch [211], Loss: 344941.937500
Epoch [1/1], Batch [221], Loss: 331883.375000
Epoch [1/1], Batch [231], Loss: 311772.937500
Epoch [1/1], Batch [241], Loss: 323642.125000
Epoch [1/1], Batch [251], Loss: 335412.437500
Epoch [1/1], Batch [261], Loss: 337140.000000
Epoch [1/1], Batch [271], Loss: 337126.375000
Epoch [1/1], Batch [281], Loss: 331659.625000
Epoch [1/1], Batch [291], Loss: 338937.437500
Epoch [1/1], Batch [301], Loss: 307410.062500
Epoch [1/1], Batch [311], Loss: 339283.093750
Epoch [1/1], Batch [321], Loss: 338106.343750
Epoch [1/1], Batch [331], Loss: 324459.718750
Epoch [1/1], Batch [341], Loss: 330297.843750
Epoch [1/1], Batch [351], Loss: 333948.093750
Epoch [1/1], Batch [361], Loss: 344775.093750
Epoch [1/1], Batch [371], Loss: 330995.531250
Epoch [1/1], Batch [381], Loss: 320876.875000
Epoch [1/1], Batch [391], Loss: 329254.843750
Epoch [1/1], Batch [401], Loss: 327505.062500
Epoch [1/1], Batch [411], Loss: 332900.843750
Epoch [1/1], Batch [421], Loss: 329103.125000
Epoch [1/1], Batch [431], Loss: 333887.968750
Epoch [1/1], Batch [441], Loss: 360291.906250
Epoch [1/1], Batch [451], Loss: 328344.562500
Epoch [1/1], Batch [461], Loss: 332505.281250
Epoch [1/1], Batch [471], Loss: 321887.562500
Epoch [1/1], Batch [481], Loss: 341108.375000
Epoch [1/1], Batch [491], Loss: 319417.750000
Epoch [1/1], Batch [501], Loss: 330022.375000
Epoch [1/1], Batch [511], Loss: 337612.500000
Epoch [1/1], Batch [521], Loss: 339250.500000
Epoch [1/1], Batch [531], Loss: 329831.875000
Epoch [1/1], Batch [541], Loss: 329598.406250
Epoch [1/1], Batch [551], Loss: 319612.687500
Epoch [1/1], Batch [561], Loss: 331190.218750
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 330052.0634
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 323727.7798
Elapsed time: 4369.46 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 326040.9906
Elapsed time: 4384.88 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 383881.437500
Epoch [1/1], Batch [11], Loss: 373371.343750
Epoch [1/1], Batch [21], Loss: 379589.656250
Epoch [1/1], Batch [31], Loss: 381099.812500
Epoch [1/1], Batch [41], Loss: 368497.531250
Epoch [1/1], Batch [51], Loss: 383486.750000
Epoch [1/1], Batch [61], Loss: 362800.437500
Epoch [1/1], Batch [71], Loss: 366113.593750
Epoch [1/1], Batch [81], Loss: 370988.937500
Epoch [1/1], Batch [91], Loss: 368332.875000
Epoch [1/1], Batch [101], Loss: 378053.437500
Epoch [1/1], Batch [111], Loss: 376104.250000
Epoch [1/1], Batch [121], Loss: 375642.781250
Epoch [1/1], Batch [131], Loss: 373330.406250
Epoch [1/1], Batch [141], Loss: 373918.250000
Epoch [1/1], Batch [151], Loss: 373686.125000
Epoch [1/1], Batch [161], Loss: 378382.531250
Epoch [1/1], Batch [171], Loss: 361040.406250
Epoch [1/1], Batch [181], Loss: 376751.937500
Epoch [1/1], Batch [191], Loss: 374120.812500
Epoch [1/1], Batch [201], Loss: 375378.375000
Epoch [1/1], Batch [211], Loss: 364651.375000
Epoch [1/1], Batch [221], Loss: 380086.875000
Epoch [1/1], Batch [231], Loss: 368400.718750
Epoch [1/1], Batch [241], Loss: 373783.281250
Epoch [1/1], Batch [251], Loss: 361801.343750
Epoch [1/1], Batch [261], Loss: 380184.468750
Epoch [1/1], Batch [271], Loss: 378766.750000
Epoch [1/1], Batch [281], Loss: 371147.593750
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 371960.3429
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 360303.9744
Elapsed time: 4688.12 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 359563.2756
Elapsed time: 4696.68 seconds

Training with sequence length 10.
Traceback (most recent call last):
  File "/orfeo/cephfs/home/dssc/mzampar/Deep-Learning-Project/train/mnist_train.py", line 174, in <module>
    train_dataset = MnistSequenceDataset(
  File "/orfeo/cephfs/home/dssc/mzampar/Deep-Learning-Project/train/utils.py", line 90, in __init__
    all_sequences = np.stack(all_sequences, axis=1)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/numpy/core/shape_base.py", line 445, in stack
    raise ValueError('need at least one array to stack')
ValueError: need at least one array to stack
srun: error: gpu001: task 0: Exited with exit code 1
CUDA is available!
Traceback (most recent call last):
  File "/u/dssc/mzampar/Deep-Learning-Project/display/mnist_generate_gif.py", line 200, in <module>
    state_dict = th.load(out_folder + f"/{model_name}", map_location=th.device('cpu'), weights_only=True)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/serialization.py", line 1319, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/serialization.py", line 659, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/serialization.py", line 640, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/u/dssc/mzampar/Deep-Learning-Project/mnist-models/1003550/model_1003550.pth'
rm: cannot remove '*.gif': No such file or directory
