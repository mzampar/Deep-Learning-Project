Starting job 1008755
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = True,
    bias = True,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 345837.875000
Epoch [1/1], Batch [11], Loss: 111687.906250
Epoch [1/1], Batch [21], Loss: 86842.375000
Epoch [1/1], Batch [31], Loss: 81843.046875
Epoch [1/1], Batch [41], Loss: 70629.437500
Epoch [1/1], Batch [51], Loss: 70358.375000
Epoch [1/1], Batch [61], Loss: 68320.390625
Epoch [1/1], Batch [71], Loss: 68487.585938
Epoch [1/1], Batch [81], Loss: 69240.578125
Epoch [1/1], Batch [91], Loss: 73669.453125
Epoch [1/1], Batch [101], Loss: 72724.625000
Epoch [1/1], Batch [111], Loss: 70748.179688
Epoch [1/1], Batch [121], Loss: 69454.632812
Epoch [1/1], Batch [131], Loss: 69842.859375
Epoch [1/1], Batch [141], Loss: 72307.218750
Epoch [1/1], Batch [151], Loss: 68756.507812
Epoch [1/1], Batch [161], Loss: 68470.296875
Epoch [1/1], Batch [171], Loss: 69769.234375
Epoch [1/1], Batch [181], Loss: 69890.546875
Epoch [1/1], Batch [191], Loss: 69941.625000
Epoch [1/1], Batch [201], Loss: 68101.515625
Epoch [1/1], Batch [211], Loss: 68614.953125
Epoch [1/1], Batch [221], Loss: 65098.011719
Epoch [1/1], Batch [231], Loss: 69561.000000
Epoch [1/1], Batch [241], Loss: 69674.453125
Epoch [1/1], Batch [251], Loss: 69841.039062
Epoch [1/1], Batch [261], Loss: 67329.437500
Epoch [1/1], Batch [271], Loss: 66986.156250
Epoch [1/1], Batch [281], Loss: 70227.789062
Epoch [1/1], Batch [291], Loss: 68509.039062
Epoch [1/1], Batch [301], Loss: 64479.238281
Epoch [1/1], Batch [311], Loss: 67403.468750
Epoch [1/1], Batch [321], Loss: 67118.093750
Epoch [1/1], Batch [331], Loss: 68614.914062
Epoch [1/1], Batch [341], Loss: 67376.203125
Epoch [1/1], Batch [351], Loss: 67381.906250
Epoch [1/1], Batch [361], Loss: 69186.562500
Epoch [1/1], Batch [371], Loss: 67042.343750
Epoch [1/1], Batch [381], Loss: 69370.531250
Epoch [1/1], Batch [391], Loss: 72388.906250
Epoch [1/1], Batch [401], Loss: 67496.421875
Epoch [1/1], Batch [411], Loss: 69909.242188
Epoch [1/1], Batch [421], Loss: 69341.507812
Epoch [1/1], Batch [431], Loss: 66509.085938
Epoch [1/1], Batch [441], Loss: 68360.242188
Epoch [1/1], Batch [451], Loss: 70656.234375
Epoch [1/1], Batch [461], Loss: 63341.714844
Epoch [1/1], Batch [471], Loss: 66690.023438
Epoch [1/1], Batch [481], Loss: 66176.109375
Epoch [1/1], Batch [491], Loss: 65059.492188
Epoch [1/1], Batch [501], Loss: 69703.945312
Epoch [1/1], Batch [511], Loss: 67683.445312
Epoch [1/1], Batch [521], Loss: 68314.015625
Epoch [1/1], Batch [531], Loss: 68331.687500
Epoch [1/1], Batch [541], Loss: 68529.000000
Epoch [1/1], Batch [551], Loss: 68296.593750
Epoch [1/1], Batch [561], Loss: 65135.812500
Epoch [1/1], Batch [571], Loss: 70777.546875
Epoch [1/1], Batch [581], Loss: 68110.437500
Epoch [1/1], Batch [591], Loss: 67034.218750
Epoch [1/1], Batch [601], Loss: 67430.156250
Epoch [1/1], Batch [611], Loss: 68419.703125
Epoch [1/1], Batch [621], Loss: 66127.593750
Epoch [1/1], Batch [631], Loss: 67256.093750
Epoch [1/1], Batch [641], Loss: 67529.671875
Epoch [1/1], Batch [651], Loss: 67244.945312
Epoch [1/1], Batch [661], Loss: 68521.820312
Epoch [1/1], Batch [671], Loss: 67191.898438
Epoch [1/1], Batch [681], Loss: 71953.304688
Epoch [1/1], Batch [691], Loss: 71107.929688
Epoch [1/1], Batch [701], Loss: 66266.421875
Epoch [1/1], Batch [711], Loss: 67709.046875
Epoch [1/1], Batch [721], Loss: 68549.437500
Epoch [1/1], Batch [731], Loss: 68901.585938
Epoch [1/1], Batch [741], Loss: 67052.515625
Epoch [1/1], Batch [751], Loss: 69681.820312
Epoch [1/1], Batch [761], Loss: 68034.171875
Epoch [1/1], Batch [771], Loss: 65250.894531
Epoch [1/1], Batch [781], Loss: 66461.570312
Epoch [1/1], Batch [791], Loss: 69643.031250
Epoch [1/1], Batch [801], Loss: 65822.171875
Epoch [1/1], Batch [811], Loss: 64539.281250
Epoch [1/1], Batch [821], Loss: 67138.750000
Epoch [1/1], Batch [831], Loss: 65786.984375
Epoch [1/1], Batch [841], Loss: 67331.187500
Epoch [1/1], Batch [851], Loss: 67989.578125
Epoch [1/1], Batch [861], Loss: 64321.453125
Epoch [1/1], Batch [871], Loss: 71016.703125
Epoch [1/1], Batch [881], Loss: 67983.546875
Epoch [1/1], Batch [891], Loss: 71421.414062
Epoch [1/1], Batch [901], Loss: 65713.406250
Epoch [1/1], Batch [911], Loss: 68908.093750
Epoch [1/1], Batch [921], Loss: 64186.878906
Epoch [1/1], Batch [931], Loss: 66917.093750
Epoch [1/1], Batch [941], Loss: 68449.148438
Epoch [1/1], Batch [951], Loss: 67444.703125
Epoch [1/1], Batch [961], Loss: 65899.453125
Epoch [1/1], Batch [971], Loss: 68155.000000
Epoch [1/1], Batch [981], Loss: 67694.968750
Epoch [1/1], Batch [991], Loss: 65525.828125
Epoch [1/1], Batch [1001], Loss: 67382.453125
Epoch [1/1], Batch [1011], Loss: 67497.507812
Epoch [1/1], Batch [1021], Loss: 67889.937500
Epoch [1/1], Batch [1031], Loss: 63468.476562
Epoch [1/1], Batch [1041], Loss: 69506.726562
Epoch [1/1], Batch [1051], Loss: 68242.171875
Epoch [1/1], Batch [1061], Loss: 69687.250000
Epoch [1/1], Batch [1071], Loss: 70484.796875
Epoch [1/1], Batch [1081], Loss: 67941.421875
Epoch [1/1], Batch [1091], Loss: 67092.648438
Epoch [1/1], Batch [1101], Loss: 66066.703125
Epoch [1/1], Batch [1111], Loss: 66321.109375
Epoch [1/1], Batch [1121], Loss: 67097.812500
Epoch [1/1], Batch [1131], Loss: 66652.468750
Epoch [1/1], Batch [1141], Loss: 68018.437500
Epoch [1/1], Batch [1151], Loss: 68064.015625
Epoch [1/1], Batch [1161], Loss: 65707.718750
Epoch [1/1], Batch [1171], Loss: 67514.218750
Epoch [1/1], Batch [1181], Loss: 67579.593750
Epoch [1/1], Batch [1191], Loss: 66128.562500
Epoch [1/1], Batch [1201], Loss: 65979.671875
Epoch [1/1], Batch [1211], Loss: 69699.750000
Epoch [1/1], Batch [1221], Loss: 65570.093750
Epoch [1/1], Batch [1231], Loss: 66931.187500
Epoch [1/1], Batch [1241], Loss: 67479.421875
Epoch [1/1], Batch [1251], Loss: 66871.656250
Epoch [1/1], Batch [1261], Loss: 67209.546875
Epoch [1/1], Batch [1271], Loss: 67608.304688
Epoch [1/1], Batch [1281], Loss: 67432.875000
Epoch [1/1], Batch [1291], Loss: 68098.015625
Epoch [1/1], Batch [1301], Loss: 66501.656250
Epoch [1/1], Batch [1311], Loss: 68194.859375
Epoch [1/1], Batch [1321], Loss: 66244.828125
Epoch [1/1], Batch [1331], Loss: 68987.859375
Epoch [1/1], Batch [1341], Loss: 67489.742188
Epoch [1/1], Batch [1351], Loss: 69743.085938
Epoch [1/1], Batch [1361], Loss: 64538.808594
Epoch [1/1], Batch [1371], Loss: 66977.070312
Epoch [1/1], Batch [1381], Loss: 65847.679688
Epoch [1/1], Batch [1391], Loss: 66881.281250
Epoch [1/1], Batch [1401], Loss: 68740.078125
Epoch [1/1], Batch [1411], Loss: 67331.703125
Epoch [1/1], Batch [1421], Loss: 68269.164062
Epoch [1/1], Batch [1431], Loss: 67827.539062
Epoch [1/1], Batch [1441], Loss: 66387.804688
Epoch [1/1], Batch [1451], Loss: 68341.500000
Epoch [1/1], Batch [1461], Loss: 68751.945312
Epoch [1/1], Batch [1471], Loss: 69545.671875
Epoch [1/1], Batch [1481], Loss: 68119.960938
Epoch [1/1], Batch [1491], Loss: 67802.117188
Epoch [1/1], Batch [1501], Loss: 68952.109375
Epoch [1/1], Batch [1511], Loss: 68636.453125
Epoch [1/1], Batch [1521], Loss: 66762.875000
Epoch [1/1], Batch [1531], Loss: 67232.390625
Epoch [1/1], Batch [1541], Loss: 69739.531250
Epoch [1/1], Batch [1551], Loss: 68282.562500
Epoch [1/1], Batch [1561], Loss: 64165.828125
Epoch [1/1], Batch [1571], Loss: 67611.281250
Epoch [1/1], Batch [1581], Loss: 66853.023438
Epoch [1/1], Batch [1591], Loss: 65919.585938
Epoch [1/1], Batch [1601], Loss: 67728.453125
Epoch [1/1], Batch [1611], Loss: 69148.734375
Epoch [1/1], Batch [1621], Loss: 66728.875000
Epoch [1/1], Batch [1631], Loss: 67258.593750
Epoch [1/1], Batch [1641], Loss: 69995.109375
Epoch [1/1], Batch [1651], Loss: 66856.671875
Epoch [1/1], Batch [1661], Loss: 69927.289062
Epoch [1/1], Batch [1671], Loss: 69671.257812
Epoch [1/1], Batch [1681], Loss: 66958.125000
Epoch [1/1], Batch [1691], Loss: 68496.953125
Epoch [1/1], Batch [1701], Loss: 65780.843750
Epoch [1/1], Batch [1711], Loss: 67898.382812
Epoch [1/1], Batch [1721], Loss: 65781.375000
Epoch [1/1], Batch [1731], Loss: 65250.718750
Epoch [1/1], Batch [1741], Loss: 69096.093750
Epoch [1/1], Batch [1751], Loss: 65667.500000
Epoch [1/1], Batch [1761], Loss: 66170.640625
Epoch [1/1], Batch [1771], Loss: 67521.000000
Epoch [1/1], Batch [1781], Loss: 69923.867188
Epoch [1/1], Batch [1791], Loss: 65064.773438
Epoch [1/1], Batch [1801], Loss: 69266.726562
Epoch [1/1], Batch [1811], Loss: 67474.515625
Epoch [1/1], Batch [1821], Loss: 68892.859375
Epoch [1/1], Batch [1831], Loss: 67520.687500
Epoch [1/1], Batch [1841], Loss: 66084.453125
Epoch [1/1], Batch [1851], Loss: 68042.453125
Epoch [1/1], Batch [1861], Loss: 64537.554688
Epoch [1/1], Batch [1871], Loss: 69038.328125
Epoch [1/1], Batch [1881], Loss: 67073.109375
Epoch [1/1], Batch [1891], Loss: 64810.332031
Epoch [1/1], Batch [1901], Loss: 65767.703125
Epoch [1/1], Batch [1911], Loss: 66587.171875
Epoch [1/1], Batch [1921], Loss: 67262.546875
Epoch [1/1], Batch [1931], Loss: 68971.617188
Epoch [1/1], Batch [1941], Loss: 66201.210938
Epoch [1/1], Batch [1951], Loss: 65661.515625
Epoch [1/1], Batch [1961], Loss: 66126.210938
Epoch [1/1], Batch [1971], Loss: 67408.968750
Epoch [1/1], Batch [1981], Loss: 65733.375000
Epoch [1/1], Batch [1991], Loss: 67736.179688
Epoch [1/1], Batch [2001], Loss: 66834.437500
Epoch [1/1], Batch [2011], Loss: 65097.414062
Epoch [1/1], Batch [2021], Loss: 66464.093750
Epoch [1/1], Batch [2031], Loss: 66903.515625
Epoch [1/1], Batch [2041], Loss: 64397.937500
Epoch [1/1], Batch [2051], Loss: 65203.480469
Epoch [1/1], Batch [2061], Loss: 69065.125000
Epoch [1/1], Batch [2071], Loss: 65606.062500
Epoch [1/1], Batch [2081], Loss: 66460.664062
Epoch [1/1], Batch [2091], Loss: 67542.835938
Epoch [1/1], Batch [2101], Loss: 70827.531250
Epoch [1/1], Batch [2111], Loss: 68773.062500
Epoch [1/1], Batch [2121], Loss: 66238.148438
Epoch [1/1], Batch [2131], Loss: 65615.757812
Epoch [1/1], Batch [2141], Loss: 64411.605469
Epoch [1/1], Batch [2151], Loss: 67010.390625
Epoch [1/1], Batch [2161], Loss: 68249.781250
Epoch [1/1], Batch [2171], Loss: 68785.859375
Epoch [1/1], Batch [2181], Loss: 66891.671875
Epoch [1/1], Batch [2191], Loss: 64873.414062
Epoch [1/1], Batch [2201], Loss: 67096.476562
Epoch [1/1], Batch [2211], Loss: 69741.359375
Epoch [1/1], Batch [2221], Loss: 68831.890625
Epoch [1/1], Batch [2231], Loss: 66881.140625
Epoch [1/1], Batch [2241], Loss: 67437.062500
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 68355.2313
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 67575.2117
Elapsed time: 496.68 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 67938.5496
Elapsed time: 517.46 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 123972.609375
Epoch [1/1], Batch [11], Loss: 121249.218750
Epoch [1/1], Batch [21], Loss: 120928.406250
Epoch [1/1], Batch [31], Loss: 118567.500000
Epoch [1/1], Batch [41], Loss: 116706.171875
Epoch [1/1], Batch [51], Loss: 109918.890625
Epoch [1/1], Batch [61], Loss: 114170.953125
Epoch [1/1], Batch [71], Loss: 113540.656250
Epoch [1/1], Batch [81], Loss: 115428.687500
Epoch [1/1], Batch [91], Loss: 114045.593750
Epoch [1/1], Batch [101], Loss: 111701.187500
Epoch [1/1], Batch [111], Loss: 113870.921875
Epoch [1/1], Batch [121], Loss: 118300.156250
Epoch [1/1], Batch [131], Loss: 116494.953125
Epoch [1/1], Batch [141], Loss: 110539.695312
Epoch [1/1], Batch [151], Loss: 114476.765625
Epoch [1/1], Batch [161], Loss: 111105.109375
Epoch [1/1], Batch [171], Loss: 113453.359375
Epoch [1/1], Batch [181], Loss: 114368.406250
Epoch [1/1], Batch [191], Loss: 112991.109375
Epoch [1/1], Batch [201], Loss: 113158.968750
Epoch [1/1], Batch [211], Loss: 113626.921875
Epoch [1/1], Batch [221], Loss: 111587.031250
Epoch [1/1], Batch [231], Loss: 108941.843750
Epoch [1/1], Batch [241], Loss: 116923.312500
Epoch [1/1], Batch [251], Loss: 114824.679688
Epoch [1/1], Batch [261], Loss: 112340.664062
Epoch [1/1], Batch [271], Loss: 115076.000000
Epoch [1/1], Batch [281], Loss: 116743.656250
Epoch [1/1], Batch [291], Loss: 117935.593750
Epoch [1/1], Batch [301], Loss: 107731.468750
Epoch [1/1], Batch [311], Loss: 112276.875000
Epoch [1/1], Batch [321], Loss: 116923.554688
Epoch [1/1], Batch [331], Loss: 111985.968750
Epoch [1/1], Batch [341], Loss: 116966.406250
Epoch [1/1], Batch [351], Loss: 113497.250000
Epoch [1/1], Batch [361], Loss: 113910.953125
Epoch [1/1], Batch [371], Loss: 110580.906250
Epoch [1/1], Batch [381], Loss: 110021.968750
Epoch [1/1], Batch [391], Loss: 111265.562500
Epoch [1/1], Batch [401], Loss: 115308.343750
Epoch [1/1], Batch [411], Loss: 113129.929688
Epoch [1/1], Batch [421], Loss: 112705.773438
Epoch [1/1], Batch [431], Loss: 114550.703125
Epoch [1/1], Batch [441], Loss: 120937.171875
Epoch [1/1], Batch [451], Loss: 116773.515625
Epoch [1/1], Batch [461], Loss: 111132.265625
Epoch [1/1], Batch [471], Loss: 117619.578125
Epoch [1/1], Batch [481], Loss: 115623.937500
Epoch [1/1], Batch [491], Loss: 113297.906250
Epoch [1/1], Batch [501], Loss: 109519.062500
Epoch [1/1], Batch [511], Loss: 114399.796875
Epoch [1/1], Batch [521], Loss: 113459.484375
Epoch [1/1], Batch [531], Loss: 114575.625000
Epoch [1/1], Batch [541], Loss: 112695.546875
Epoch [1/1], Batch [551], Loss: 115316.289062
Epoch [1/1], Batch [561], Loss: 112030.265625
Epoch [1/1], Batch [571], Loss: 111981.781250
Epoch [1/1], Batch [581], Loss: 109166.609375
Epoch [1/1], Batch [591], Loss: 116188.921875
Epoch [1/1], Batch [601], Loss: 116110.679688
Epoch [1/1], Batch [611], Loss: 112627.093750
Epoch [1/1], Batch [621], Loss: 111611.429688
Epoch [1/1], Batch [631], Loss: 115420.960938
Epoch [1/1], Batch [641], Loss: 113217.789062
Epoch [1/1], Batch [651], Loss: 111890.031250
Epoch [1/1], Batch [661], Loss: 108347.296875
Epoch [1/1], Batch [671], Loss: 112777.953125
Epoch [1/1], Batch [681], Loss: 112891.101562
Epoch [1/1], Batch [691], Loss: 111535.687500
Epoch [1/1], Batch [701], Loss: 113412.250000
Epoch [1/1], Batch [711], Loss: 112652.968750
Epoch [1/1], Batch [721], Loss: 114478.468750
Epoch [1/1], Batch [731], Loss: 111502.289062
Epoch [1/1], Batch [741], Loss: 113282.773438
Epoch [1/1], Batch [751], Loss: 119216.648438
Epoch [1/1], Batch [761], Loss: 114842.859375
Epoch [1/1], Batch [771], Loss: 110422.500000
Epoch [1/1], Batch [781], Loss: 113433.757812
Epoch [1/1], Batch [791], Loss: 111683.500000
Epoch [1/1], Batch [801], Loss: 113055.765625
Epoch [1/1], Batch [811], Loss: 116109.687500
Epoch [1/1], Batch [821], Loss: 117716.273438
Epoch [1/1], Batch [831], Loss: 118770.976562
Epoch [1/1], Batch [841], Loss: 108309.515625
Epoch [1/1], Batch [851], Loss: 111273.781250
Epoch [1/1], Batch [861], Loss: 118843.078125
Epoch [1/1], Batch [871], Loss: 118200.671875
Epoch [1/1], Batch [881], Loss: 111058.500000
Epoch [1/1], Batch [891], Loss: 115185.468750
Epoch [1/1], Batch [901], Loss: 112861.156250
Epoch [1/1], Batch [911], Loss: 109731.468750
Epoch [1/1], Batch [921], Loss: 116007.046875
Epoch [1/1], Batch [931], Loss: 111301.734375
Epoch [1/1], Batch [941], Loss: 114627.218750
Epoch [1/1], Batch [951], Loss: 113881.632812
Epoch [1/1], Batch [961], Loss: 116540.382812
Epoch [1/1], Batch [971], Loss: 115384.046875
Epoch [1/1], Batch [981], Loss: 116149.234375
Epoch [1/1], Batch [991], Loss: 109297.906250
Epoch [1/1], Batch [1001], Loss: 111900.289062
Epoch [1/1], Batch [1011], Loss: 117431.578125
Epoch [1/1], Batch [1021], Loss: 114038.429688
Epoch [1/1], Batch [1031], Loss: 116855.578125
Epoch [1/1], Batch [1041], Loss: 110154.015625
Epoch [1/1], Batch [1051], Loss: 115418.437500
Epoch [1/1], Batch [1061], Loss: 116592.539062
Epoch [1/1], Batch [1071], Loss: 113473.375000
Epoch [1/1], Batch [1081], Loss: 116513.937500
Epoch [1/1], Batch [1091], Loss: 114226.187500
Epoch [1/1], Batch [1101], Loss: 113782.953125
Epoch [1/1], Batch [1111], Loss: 114585.539062
Epoch [1/1], Batch [1121], Loss: 112734.843750
Epoch [1/1], Batch [1131], Loss: 110687.062500
Epoch [1/1], Batch [1141], Loss: 114826.914062
Epoch [1/1], Batch [1151], Loss: 115107.093750
Epoch [1/1], Batch [1161], Loss: 115959.609375
Epoch [1/1], Batch [1171], Loss: 109057.242188
Epoch [1/1], Batch [1181], Loss: 117507.382812
Epoch [1/1], Batch [1191], Loss: 111200.687500
Epoch [1/1], Batch [1201], Loss: 112657.062500
Epoch [1/1], Batch [1211], Loss: 109940.953125
Epoch [1/1], Batch [1221], Loss: 111223.203125
Epoch [1/1], Batch [1231], Loss: 112674.062500
Epoch [1/1], Batch [1241], Loss: 110980.140625
Epoch [1/1], Batch [1251], Loss: 110656.851562
Epoch [1/1], Batch [1261], Loss: 117070.781250
Epoch [1/1], Batch [1271], Loss: 120469.085938
Epoch [1/1], Batch [1281], Loss: 110671.492188
Epoch [1/1], Batch [1291], Loss: 116795.921875
Epoch [1/1], Batch [1301], Loss: 113054.585938
Epoch [1/1], Batch [1311], Loss: 111929.906250
Epoch [1/1], Batch [1321], Loss: 111349.960938
Epoch [1/1], Batch [1331], Loss: 114566.679688
Epoch [1/1], Batch [1341], Loss: 110815.500000
Epoch [1/1], Batch [1351], Loss: 113315.140625
Epoch [1/1], Batch [1361], Loss: 112582.757812
Epoch [1/1], Batch [1371], Loss: 112564.109375
Epoch [1/1], Batch [1381], Loss: 114511.656250
Epoch [1/1], Batch [1391], Loss: 114277.062500
Epoch [1/1], Batch [1401], Loss: 115229.570312
Epoch [1/1], Batch [1411], Loss: 112953.328125
Epoch [1/1], Batch [1421], Loss: 112806.312500
Epoch [1/1], Batch [1431], Loss: 110812.625000
Epoch [1/1], Batch [1441], Loss: 109818.757812
Epoch [1/1], Batch [1451], Loss: 111733.875000
Epoch [1/1], Batch [1461], Loss: 113568.460938
Epoch [1/1], Batch [1471], Loss: 112747.031250
Epoch [1/1], Batch [1481], Loss: 118591.890625
Epoch [1/1], Batch [1491], Loss: 113168.734375
Epoch [1/1], Batch [1501], Loss: 110656.476562
Epoch [1/1], Batch [1511], Loss: 106499.226562
Epoch [1/1], Batch [1521], Loss: 113287.125000
Epoch [1/1], Batch [1531], Loss: 111021.593750
Epoch [1/1], Batch [1541], Loss: 113560.632812
Epoch [1/1], Batch [1551], Loss: 112252.421875
Epoch [1/1], Batch [1561], Loss: 108565.328125
Epoch [1/1], Batch [1571], Loss: 111823.039062
Epoch [1/1], Batch [1581], Loss: 113957.718750
Epoch [1/1], Batch [1591], Loss: 117419.062500
Epoch [1/1], Batch [1601], Loss: 116994.171875
Epoch [1/1], Batch [1611], Loss: 111907.218750
Epoch [1/1], Batch [1621], Loss: 110560.437500
Epoch [1/1], Batch [1631], Loss: 113694.812500
Epoch [1/1], Batch [1641], Loss: 117673.507812
Epoch [1/1], Batch [1651], Loss: 112560.812500
Epoch [1/1], Batch [1661], Loss: 109625.796875
Epoch [1/1], Batch [1671], Loss: 110606.015625
Epoch [1/1], Batch [1681], Loss: 107380.515625
Epoch [1/1], Batch [1691], Loss: 113425.656250
Epoch [1/1], Batch [1701], Loss: 108149.460938
Epoch [1/1], Batch [1711], Loss: 111443.875000
Epoch [1/1], Batch [1721], Loss: 114233.718750
Epoch [1/1], Batch [1731], Loss: 113033.812500
Epoch [1/1], Batch [1741], Loss: 112355.976562
Epoch [1/1], Batch [1751], Loss: 115026.539062
Epoch [1/1], Batch [1761], Loss: 108367.140625
Epoch [1/1], Batch [1771], Loss: 113433.960938
Epoch [1/1], Batch [1781], Loss: 113208.250000
Epoch [1/1], Batch [1791], Loss: 113672.046875
Epoch [1/1], Batch [1801], Loss: 112546.257812
Epoch [1/1], Batch [1811], Loss: 108300.562500
Epoch [1/1], Batch [1821], Loss: 112608.437500
Epoch [1/1], Batch [1831], Loss: 111937.750000
Epoch [1/1], Batch [1841], Loss: 114030.343750
Epoch [1/1], Batch [1851], Loss: 109042.656250
Epoch [1/1], Batch [1861], Loss: 110710.781250
Epoch [1/1], Batch [1871], Loss: 115656.484375
Epoch [1/1], Batch [1881], Loss: 110705.265625
Epoch [1/1], Batch [1891], Loss: 113422.085938
Epoch [1/1], Batch [1901], Loss: 109670.562500
Epoch [1/1], Batch [1911], Loss: 113569.445312
Epoch [1/1], Batch [1921], Loss: 110735.093750
Epoch [1/1], Batch [1931], Loss: 111068.093750
Epoch [1/1], Batch [1941], Loss: 113577.875000
Epoch [1/1], Batch [1951], Loss: 108770.359375
Epoch [1/1], Batch [1961], Loss: 116212.453125
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 113210.3855
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 111787.9548
Elapsed time: 1153.21 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 112943.0381
Elapsed time: 1178.23 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 158018.156250
Epoch [1/1], Batch [11], Loss: 156800.468750
Epoch [1/1], Batch [21], Loss: 152265.796875
Epoch [1/1], Batch [31], Loss: 158549.218750
Epoch [1/1], Batch [41], Loss: 166719.484375
Epoch [1/1], Batch [51], Loss: 154817.062500
Epoch [1/1], Batch [61], Loss: 156394.359375
Epoch [1/1], Batch [71], Loss: 155143.515625
Epoch [1/1], Batch [81], Loss: 164028.156250
Epoch [1/1], Batch [91], Loss: 155104.203125
Epoch [1/1], Batch [101], Loss: 155999.750000
Epoch [1/1], Batch [111], Loss: 160617.281250
Epoch [1/1], Batch [121], Loss: 155655.796875
Epoch [1/1], Batch [131], Loss: 162416.500000
Epoch [1/1], Batch [141], Loss: 158170.531250
Epoch [1/1], Batch [151], Loss: 158989.687500
Epoch [1/1], Batch [161], Loss: 159962.531250
Epoch [1/1], Batch [171], Loss: 154095.781250
Epoch [1/1], Batch [181], Loss: 152706.875000
Epoch [1/1], Batch [191], Loss: 157877.406250
Epoch [1/1], Batch [201], Loss: 156200.421875
Epoch [1/1], Batch [211], Loss: 157070.375000
Epoch [1/1], Batch [221], Loss: 161606.375000
Epoch [1/1], Batch [231], Loss: 156383.953125
Epoch [1/1], Batch [241], Loss: 158230.640625
Epoch [1/1], Batch [251], Loss: 158172.843750
Epoch [1/1], Batch [261], Loss: 160223.046875
Epoch [1/1], Batch [271], Loss: 159237.531250
Epoch [1/1], Batch [281], Loss: 160073.046875
Epoch [1/1], Batch [291], Loss: 160565.906250
Epoch [1/1], Batch [301], Loss: 161636.828125
Epoch [1/1], Batch [311], Loss: 155376.343750
Epoch [1/1], Batch [321], Loss: 161320.078125
Epoch [1/1], Batch [331], Loss: 159897.328125
Epoch [1/1], Batch [341], Loss: 162574.031250
Epoch [1/1], Batch [351], Loss: 161967.921875
Epoch [1/1], Batch [361], Loss: 166253.562500
Epoch [1/1], Batch [371], Loss: 161229.781250
Epoch [1/1], Batch [381], Loss: 150483.625000
Epoch [1/1], Batch [391], Loss: 160522.156250
Epoch [1/1], Batch [401], Loss: 159164.593750
Epoch [1/1], Batch [411], Loss: 162997.328125
Epoch [1/1], Batch [421], Loss: 160764.187500
Epoch [1/1], Batch [431], Loss: 155972.437500
Epoch [1/1], Batch [441], Loss: 159421.312500
Epoch [1/1], Batch [451], Loss: 159540.015625
Epoch [1/1], Batch [461], Loss: 158208.218750
Epoch [1/1], Batch [471], Loss: 157314.781250
Epoch [1/1], Batch [481], Loss: 163900.281250
Epoch [1/1], Batch [491], Loss: 160883.062500
Epoch [1/1], Batch [501], Loss: 161598.437500
Epoch [1/1], Batch [511], Loss: 160968.125000
Epoch [1/1], Batch [521], Loss: 156902.562500
Epoch [1/1], Batch [531], Loss: 149175.281250
Epoch [1/1], Batch [541], Loss: 159296.531250
Epoch [1/1], Batch [551], Loss: 157663.921875
Epoch [1/1], Batch [561], Loss: 157434.468750
Epoch [1/1], Batch [571], Loss: 154426.218750
Epoch [1/1], Batch [581], Loss: 150811.500000
Epoch [1/1], Batch [591], Loss: 157154.750000
Epoch [1/1], Batch [601], Loss: 157487.953125
Epoch [1/1], Batch [611], Loss: 166644.500000
Epoch [1/1], Batch [621], Loss: 165090.828125
Epoch [1/1], Batch [631], Loss: 161306.000000
Epoch [1/1], Batch [641], Loss: 157274.031250
Epoch [1/1], Batch [651], Loss: 153654.562500
Epoch [1/1], Batch [661], Loss: 159624.031250
Epoch [1/1], Batch [671], Loss: 160145.937500
Epoch [1/1], Batch [681], Loss: 151828.875000
Epoch [1/1], Batch [691], Loss: 159012.937500
Epoch [1/1], Batch [701], Loss: 167934.687500
Epoch [1/1], Batch [711], Loss: 159331.312500
Epoch [1/1], Batch [721], Loss: 157435.562500
Epoch [1/1], Batch [731], Loss: 170533.750000
Epoch [1/1], Batch [741], Loss: 156939.375000
Epoch [1/1], Batch [751], Loss: 162413.937500
Epoch [1/1], Batch [761], Loss: 152895.078125
Epoch [1/1], Batch [771], Loss: 165810.015625
Epoch [1/1], Batch [781], Loss: 167147.156250
Epoch [1/1], Batch [791], Loss: 151621.406250
Epoch [1/1], Batch [801], Loss: 148957.171875
Epoch [1/1], Batch [811], Loss: 156530.500000
Epoch [1/1], Batch [821], Loss: 164408.109375
Epoch [1/1], Batch [831], Loss: 157434.953125
Epoch [1/1], Batch [841], Loss: 164149.921875
Epoch [1/1], Batch [851], Loss: 159496.859375
Epoch [1/1], Batch [861], Loss: 154655.625000
Epoch [1/1], Batch [871], Loss: 149734.265625
Epoch [1/1], Batch [881], Loss: 161948.218750
Epoch [1/1], Batch [891], Loss: 166363.781250
Epoch [1/1], Batch [901], Loss: 161609.468750
Epoch [1/1], Batch [911], Loss: 162543.687500
Epoch [1/1], Batch [921], Loss: 158889.218750
Epoch [1/1], Batch [931], Loss: 165398.984375
Epoch [1/1], Batch [941], Loss: 160741.625000
Epoch [1/1], Batch [951], Loss: 158674.281250
Epoch [1/1], Batch [961], Loss: 165219.937500
Epoch [1/1], Batch [971], Loss: 152637.750000
Epoch [1/1], Batch [981], Loss: 158906.312500
Epoch [1/1], Batch [991], Loss: 159596.687500
Epoch [1/1], Batch [1001], Loss: 155563.218750
Epoch [1/1], Batch [1011], Loss: 161730.234375
Epoch [1/1], Batch [1021], Loss: 157849.843750
Epoch [1/1], Batch [1031], Loss: 162454.468750
Epoch [1/1], Batch [1041], Loss: 161421.765625
Epoch [1/1], Batch [1051], Loss: 150230.000000
Epoch [1/1], Batch [1061], Loss: 160441.687500
Epoch [1/1], Batch [1071], Loss: 162214.953125
Epoch [1/1], Batch [1081], Loss: 156111.687500
Epoch [1/1], Batch [1091], Loss: 155988.843750
Epoch [1/1], Batch [1101], Loss: 158308.359375
Epoch [1/1], Batch [1111], Loss: 159532.828125
Epoch [1/1], Batch [1121], Loss: 160469.125000
Epoch [1/1], Batch [1131], Loss: 148643.031250
Epoch [1/1], Batch [1141], Loss: 159461.468750
Epoch [1/1], Batch [1151], Loss: 160499.953125
Epoch [1/1], Batch [1161], Loss: 160114.437500
Epoch [1/1], Batch [1171], Loss: 154257.625000
Epoch [1/1], Batch [1181], Loss: 160998.281250
Epoch [1/1], Batch [1191], Loss: 153069.125000
Epoch [1/1], Batch [1201], Loss: 162354.562500
Epoch [1/1], Batch [1211], Loss: 161166.250000
Epoch [1/1], Batch [1221], Loss: 154891.265625
Epoch [1/1], Batch [1231], Loss: 164643.718750
Epoch [1/1], Batch [1241], Loss: 163574.734375
Epoch [1/1], Batch [1251], Loss: 157263.156250
Epoch [1/1], Batch [1261], Loss: 161432.218750
Epoch [1/1], Batch [1271], Loss: 152927.671875
Epoch [1/1], Batch [1281], Loss: 158834.031250
Epoch [1/1], Batch [1291], Loss: 155262.375000
Epoch [1/1], Batch [1301], Loss: 160859.453125
Epoch [1/1], Batch [1311], Loss: 158193.062500
Epoch [1/1], Batch [1321], Loss: 159656.093750
Epoch [1/1], Batch [1331], Loss: 162727.875000
Epoch [1/1], Batch [1341], Loss: 159511.593750
Epoch [1/1], Batch [1351], Loss: 158862.812500
Epoch [1/1], Batch [1361], Loss: 159951.656250
Epoch [1/1], Batch [1371], Loss: 154854.125000
Epoch [1/1], Batch [1381], Loss: 157685.859375
Epoch [1/1], Batch [1391], Loss: 155269.437500
Epoch [1/1], Batch [1401], Loss: 159615.937500
Epoch [1/1], Batch [1411], Loss: 159807.984375
Epoch [1/1], Batch [1421], Loss: 153773.859375
Epoch [1/1], Batch [1431], Loss: 150487.328125
Epoch [1/1], Batch [1441], Loss: 161072.312500
Epoch [1/1], Batch [1451], Loss: 158447.140625
Epoch [1/1], Batch [1461], Loss: 158688.375000
Epoch [1/1], Batch [1471], Loss: 156156.875000
Epoch [1/1], Batch [1481], Loss: 156323.156250
Epoch [1/1], Batch [1491], Loss: 161848.687500
Epoch [1/1], Batch [1501], Loss: 150499.453125
Epoch [1/1], Batch [1511], Loss: 157637.093750
Epoch [1/1], Batch [1521], Loss: 159042.359375
Epoch [1/1], Batch [1531], Loss: 156648.515625
Epoch [1/1], Batch [1541], Loss: 153895.031250
Epoch [1/1], Batch [1551], Loss: 161297.296875
Epoch [1/1], Batch [1561], Loss: 158656.750000
Epoch [1/1], Batch [1571], Loss: 154813.375000
Epoch [1/1], Batch [1581], Loss: 156117.734375
Epoch [1/1], Batch [1591], Loss: 163240.578125
Epoch [1/1], Batch [1601], Loss: 151746.890625
Epoch [1/1], Batch [1611], Loss: 162281.453125
Epoch [1/1], Batch [1621], Loss: 160148.296875
Epoch [1/1], Batch [1631], Loss: 156749.546875
Epoch [1/1], Batch [1641], Loss: 165069.296875
Epoch [1/1], Batch [1651], Loss: 157040.875000
Epoch [1/1], Batch [1661], Loss: 156792.562500
Epoch [1/1], Batch [1671], Loss: 159904.625000
Epoch [1/1], Batch [1681], Loss: 156334.500000
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 158805.1569
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 156552.0696
Elapsed time: 1992.45 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 157675.9971
Elapsed time: 2019.74 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 211960.000000
Epoch [1/1], Batch [11], Loss: 199190.187500
Epoch [1/1], Batch [21], Loss: 206502.875000
Epoch [1/1], Batch [31], Loss: 203376.890625
Epoch [1/1], Batch [41], Loss: 197172.937500
Epoch [1/1], Batch [51], Loss: 205346.515625
Epoch [1/1], Batch [61], Loss: 206090.734375
Epoch [1/1], Batch [71], Loss: 210071.625000
Epoch [1/1], Batch [81], Loss: 209511.687500
Epoch [1/1], Batch [91], Loss: 213721.062500
Epoch [1/1], Batch [101], Loss: 192775.359375
Epoch [1/1], Batch [111], Loss: 193842.765625
Epoch [1/1], Batch [121], Loss: 212117.593750
Epoch [1/1], Batch [131], Loss: 210688.312500
Epoch [1/1], Batch [141], Loss: 204570.468750
Epoch [1/1], Batch [151], Loss: 203223.890625
Epoch [1/1], Batch [161], Loss: 210651.250000
Epoch [1/1], Batch [171], Loss: 195777.828125
Epoch [1/1], Batch [181], Loss: 193683.187500
Epoch [1/1], Batch [191], Loss: 199029.718750
Epoch [1/1], Batch [201], Loss: 203409.875000
Epoch [1/1], Batch [211], Loss: 209548.296875
Epoch [1/1], Batch [221], Loss: 202617.515625
Epoch [1/1], Batch [231], Loss: 206270.562500
Epoch [1/1], Batch [241], Loss: 204123.593750
Epoch [1/1], Batch [251], Loss: 208177.453125
Epoch [1/1], Batch [261], Loss: 201313.968750
Epoch [1/1], Batch [271], Loss: 200756.250000
Epoch [1/1], Batch [281], Loss: 202593.203125
Epoch [1/1], Batch [291], Loss: 205031.484375
Epoch [1/1], Batch [301], Loss: 209790.265625
Epoch [1/1], Batch [311], Loss: 208989.562500
Epoch [1/1], Batch [321], Loss: 206504.875000
Epoch [1/1], Batch [331], Loss: 208429.156250
Epoch [1/1], Batch [341], Loss: 204914.375000
Epoch [1/1], Batch [351], Loss: 206979.765625
Epoch [1/1], Batch [361], Loss: 208126.562500
Epoch [1/1], Batch [371], Loss: 204266.437500
Epoch [1/1], Batch [381], Loss: 202829.812500
Epoch [1/1], Batch [391], Loss: 206980.453125
Epoch [1/1], Batch [401], Loss: 208747.765625
Epoch [1/1], Batch [411], Loss: 201339.984375
Epoch [1/1], Batch [421], Loss: 205217.437500
Epoch [1/1], Batch [431], Loss: 202888.468750
Epoch [1/1], Batch [441], Loss: 207693.078125
Epoch [1/1], Batch [451], Loss: 199736.406250
Epoch [1/1], Batch [461], Loss: 204927.000000
Epoch [1/1], Batch [471], Loss: 207439.984375
Epoch [1/1], Batch [481], Loss: 204556.156250
Epoch [1/1], Batch [491], Loss: 209418.921875
Epoch [1/1], Batch [501], Loss: 193813.187500
Epoch [1/1], Batch [511], Loss: 205923.984375
Epoch [1/1], Batch [521], Loss: 210085.375000
Epoch [1/1], Batch [531], Loss: 205516.421875
Epoch [1/1], Batch [541], Loss: 210668.343750
Epoch [1/1], Batch [551], Loss: 196880.796875
Epoch [1/1], Batch [561], Loss: 215055.250000
Epoch [1/1], Batch [571], Loss: 198470.531250
Epoch [1/1], Batch [581], Loss: 204414.093750
Epoch [1/1], Batch [591], Loss: 195507.578125
Epoch [1/1], Batch [601], Loss: 199503.843750
Epoch [1/1], Batch [611], Loss: 202957.000000
Epoch [1/1], Batch [621], Loss: 201273.031250
Epoch [1/1], Batch [631], Loss: 203043.031250
Epoch [1/1], Batch [641], Loss: 204989.781250
Epoch [1/1], Batch [651], Loss: 201538.156250
Epoch [1/1], Batch [661], Loss: 208180.250000
Epoch [1/1], Batch [671], Loss: 206144.796875
Epoch [1/1], Batch [681], Loss: 213405.156250
Epoch [1/1], Batch [691], Loss: 211681.578125
Epoch [1/1], Batch [701], Loss: 207529.406250
Epoch [1/1], Batch [711], Loss: 196507.937500
Epoch [1/1], Batch [721], Loss: 203306.781250
Epoch [1/1], Batch [731], Loss: 199721.843750
Epoch [1/1], Batch [741], Loss: 206262.718750
Epoch [1/1], Batch [751], Loss: 200872.937500
Epoch [1/1], Batch [761], Loss: 201958.375000
Epoch [1/1], Batch [771], Loss: 200967.906250
Epoch [1/1], Batch [781], Loss: 204537.671875
Epoch [1/1], Batch [791], Loss: 208483.937500
Epoch [1/1], Batch [801], Loss: 199189.968750
Epoch [1/1], Batch [811], Loss: 206215.000000
Epoch [1/1], Batch [821], Loss: 205907.015625
Epoch [1/1], Batch [831], Loss: 204195.953125
Epoch [1/1], Batch [841], Loss: 202594.906250
Epoch [1/1], Batch [851], Loss: 201605.265625
Epoch [1/1], Batch [861], Loss: 207646.906250
Epoch [1/1], Batch [871], Loss: 202478.562500
Epoch [1/1], Batch [881], Loss: 195031.218750
Epoch [1/1], Batch [891], Loss: 205116.843750
Epoch [1/1], Batch [901], Loss: 207872.671875
Epoch [1/1], Batch [911], Loss: 205015.125000
Epoch [1/1], Batch [921], Loss: 205383.281250
Epoch [1/1], Batch [931], Loss: 203439.578125
Epoch [1/1], Batch [941], Loss: 204091.062500
Epoch [1/1], Batch [951], Loss: 210506.484375
Epoch [1/1], Batch [961], Loss: 206904.375000
Epoch [1/1], Batch [971], Loss: 201360.093750
Epoch [1/1], Batch [981], Loss: 202539.687500
Epoch [1/1], Batch [991], Loss: 205447.062500
Epoch [1/1], Batch [1001], Loss: 204531.625000
Epoch [1/1], Batch [1011], Loss: 210345.015625
Epoch [1/1], Batch [1021], Loss: 203010.296875
Epoch [1/1], Batch [1031], Loss: 205573.000000
Epoch [1/1], Batch [1041], Loss: 206291.265625
Epoch [1/1], Batch [1051], Loss: 209247.265625
Epoch [1/1], Batch [1061], Loss: 204158.250000
Epoch [1/1], Batch [1071], Loss: 199120.468750
Epoch [1/1], Batch [1081], Loss: 206321.468750
Epoch [1/1], Batch [1091], Loss: 206259.734375
Epoch [1/1], Batch [1101], Loss: 207822.078125
Epoch [1/1], Batch [1111], Loss: 203331.562500
Epoch [1/1], Batch [1121], Loss: 203054.437500
Epoch [1/1], Batch [1131], Loss: 210143.093750
Epoch [1/1], Batch [1141], Loss: 202199.921875
Epoch [1/1], Batch [1151], Loss: 203241.000000
Epoch [1/1], Batch [1161], Loss: 204413.562500
Epoch [1/1], Batch [1171], Loss: 206079.468750
Epoch [1/1], Batch [1181], Loss: 203834.093750
Epoch [1/1], Batch [1191], Loss: 206506.750000
Epoch [1/1], Batch [1201], Loss: 200892.718750
Epoch [1/1], Batch [1211], Loss: 212782.640625
Epoch [1/1], Batch [1221], Loss: 213258.093750
Epoch [1/1], Batch [1231], Loss: 204191.125000
Epoch [1/1], Batch [1241], Loss: 209186.156250
Epoch [1/1], Batch [1251], Loss: 203032.500000
Epoch [1/1], Batch [1261], Loss: 206994.437500
Epoch [1/1], Batch [1271], Loss: 202734.546875
Epoch [1/1], Batch [1281], Loss: 205006.562500
Epoch [1/1], Batch [1291], Loss: 204264.937500
Epoch [1/1], Batch [1301], Loss: 200794.468750
Epoch [1/1], Batch [1311], Loss: 199130.281250
Epoch [1/1], Batch [1321], Loss: 210691.312500
Epoch [1/1], Batch [1331], Loss: 196549.859375
Epoch [1/1], Batch [1341], Loss: 210072.562500
Epoch [1/1], Batch [1351], Loss: 207580.562500
Epoch [1/1], Batch [1361], Loss: 207885.031250
Epoch [1/1], Batch [1371], Loss: 210144.250000
Epoch [1/1], Batch [1381], Loss: 197335.500000
Epoch [1/1], Batch [1391], Loss: 206335.625000
Epoch [1/1], Batch [1401], Loss: 205161.812500
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 204135.8951
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 201731.7987
Elapsed time: 2773.62 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 202672.4119
Elapsed time: 2801.13 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 239945.093750
Epoch [1/1], Batch [11], Loss: 241952.390625
Epoch [1/1], Batch [21], Loss: 243451.484375
Epoch [1/1], Batch [31], Loss: 243094.312500
Epoch [1/1], Batch [41], Loss: 248198.406250
Epoch [1/1], Batch [51], Loss: 251630.281250
Epoch [1/1], Batch [61], Loss: 247499.828125
Epoch [1/1], Batch [71], Loss: 237845.859375
Epoch [1/1], Batch [81], Loss: 246719.562500
Epoch [1/1], Batch [91], Loss: 247541.781250
Epoch [1/1], Batch [101], Loss: 244966.031250
Epoch [1/1], Batch [111], Loss: 255810.000000
Epoch [1/1], Batch [121], Loss: 246356.468750
Epoch [1/1], Batch [131], Loss: 248214.687500
Epoch [1/1], Batch [141], Loss: 253110.359375
Epoch [1/1], Batch [151], Loss: 254171.875000
Epoch [1/1], Batch [161], Loss: 248526.312500
Epoch [1/1], Batch [171], Loss: 258797.531250
Epoch [1/1], Batch [181], Loss: 245222.734375
Epoch [1/1], Batch [191], Loss: 244440.593750
Epoch [1/1], Batch [201], Loss: 236926.000000
Epoch [1/1], Batch [211], Loss: 254399.796875
Epoch [1/1], Batch [221], Loss: 251478.421875
Epoch [1/1], Batch [231], Loss: 254022.875000
Epoch [1/1], Batch [241], Loss: 240771.187500
Epoch [1/1], Batch [251], Loss: 252955.125000
Epoch [1/1], Batch [261], Loss: 249091.343750
Epoch [1/1], Batch [271], Loss: 248883.906250
Epoch [1/1], Batch [281], Loss: 235528.796875
Epoch [1/1], Batch [291], Loss: 249342.156250
Epoch [1/1], Batch [301], Loss: 258418.406250
Epoch [1/1], Batch [311], Loss: 242994.062500
Epoch [1/1], Batch [321], Loss: 251278.468750
Epoch [1/1], Batch [331], Loss: 236254.781250
Epoch [1/1], Batch [341], Loss: 245245.406250
Epoch [1/1], Batch [351], Loss: 253417.531250
Epoch [1/1], Batch [361], Loss: 249834.140625
Epoch [1/1], Batch [371], Loss: 245600.640625
Epoch [1/1], Batch [381], Loss: 249368.750000
Epoch [1/1], Batch [391], Loss: 249812.343750
Epoch [1/1], Batch [401], Loss: 241791.937500
Epoch [1/1], Batch [411], Loss: 250686.265625
Epoch [1/1], Batch [421], Loss: 251627.156250
Epoch [1/1], Batch [431], Loss: 257296.718750
Epoch [1/1], Batch [441], Loss: 248800.109375
Epoch [1/1], Batch [451], Loss: 245671.625000
Epoch [1/1], Batch [461], Loss: 239072.718750
Epoch [1/1], Batch [471], Loss: 252102.203125
Epoch [1/1], Batch [481], Loss: 242949.906250
Epoch [1/1], Batch [491], Loss: 240711.562500
Epoch [1/1], Batch [501], Loss: 256044.437500
Epoch [1/1], Batch [511], Loss: 245375.843750
Epoch [1/1], Batch [521], Loss: 246486.765625
Epoch [1/1], Batch [531], Loss: 238974.281250
Epoch [1/1], Batch [541], Loss: 249203.656250
Epoch [1/1], Batch [551], Loss: 249108.875000
Epoch [1/1], Batch [561], Loss: 247512.703125
Epoch [1/1], Batch [571], Loss: 245802.546875
Epoch [1/1], Batch [581], Loss: 251527.906250
Epoch [1/1], Batch [591], Loss: 246392.718750
Epoch [1/1], Batch [601], Loss: 247450.687500
Epoch [1/1], Batch [611], Loss: 253628.359375
Epoch [1/1], Batch [621], Loss: 244586.234375
Epoch [1/1], Batch [631], Loss: 243649.031250
Epoch [1/1], Batch [641], Loss: 246838.203125
Epoch [1/1], Batch [651], Loss: 246329.296875
Epoch [1/1], Batch [661], Loss: 247567.546875
Epoch [1/1], Batch [671], Loss: 254922.125000
Epoch [1/1], Batch [681], Loss: 246771.078125
Epoch [1/1], Batch [691], Loss: 249896.812500
Epoch [1/1], Batch [701], Loss: 245159.484375
Epoch [1/1], Batch [711], Loss: 241588.343750
Epoch [1/1], Batch [721], Loss: 239408.140625
Epoch [1/1], Batch [731], Loss: 248556.187500
Epoch [1/1], Batch [741], Loss: 257996.375000
Epoch [1/1], Batch [751], Loss: 245454.312500
Epoch [1/1], Batch [761], Loss: 248179.906250
Epoch [1/1], Batch [771], Loss: 239582.906250
Epoch [1/1], Batch [781], Loss: 237720.781250
Epoch [1/1], Batch [791], Loss: 245711.687500
Epoch [1/1], Batch [801], Loss: 260347.437500
Epoch [1/1], Batch [811], Loss: 251951.468750
Epoch [1/1], Batch [821], Loss: 245779.109375
Epoch [1/1], Batch [831], Loss: 237858.718750
Epoch [1/1], Batch [841], Loss: 242832.437500
Epoch [1/1], Batch [851], Loss: 244373.109375
Epoch [1/1], Batch [861], Loss: 252121.125000
Epoch [1/1], Batch [871], Loss: 243760.796875
Epoch [1/1], Batch [881], Loss: 242956.937500
Epoch [1/1], Batch [891], Loss: 243013.125000
Epoch [1/1], Batch [901], Loss: 245569.234375
Epoch [1/1], Batch [911], Loss: 253432.312500
Epoch [1/1], Batch [921], Loss: 238892.093750
Epoch [1/1], Batch [931], Loss: 245654.625000
Epoch [1/1], Batch [941], Loss: 250129.812500
Epoch [1/1], Batch [951], Loss: 246596.953125
Epoch [1/1], Batch [961], Loss: 252790.062500
Epoch [1/1], Batch [971], Loss: 251643.562500
Epoch [1/1], Batch [981], Loss: 255613.109375
Epoch [1/1], Batch [991], Loss: 244631.984375
Epoch [1/1], Batch [1001], Loss: 240072.609375
Epoch [1/1], Batch [1011], Loss: 245290.250000
Epoch [1/1], Batch [1021], Loss: 253145.031250
Epoch [1/1], Batch [1031], Loss: 244515.062500
Epoch [1/1], Batch [1041], Loss: 257072.593750
Epoch [1/1], Batch [1051], Loss: 241874.609375
Epoch [1/1], Batch [1061], Loss: 246991.125000
Epoch [1/1], Batch [1071], Loss: 244404.562500
Epoch [1/1], Batch [1081], Loss: 234568.484375
Epoch [1/1], Batch [1091], Loss: 253144.140625
Epoch [1/1], Batch [1101], Loss: 246694.390625
Epoch [1/1], Batch [1111], Loss: 235156.843750
Epoch [1/1], Batch [1121], Loss: 249513.281250
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 246983.8107
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 245891.4454
Elapsed time: 3550.98 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 244578.2120
Elapsed time: 3576.88 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 290897.250000
Epoch [1/1], Batch [11], Loss: 301023.093750
Epoch [1/1], Batch [21], Loss: 284885.000000
Epoch [1/1], Batch [31], Loss: 298332.125000
Epoch [1/1], Batch [41], Loss: 279513.656250
Epoch [1/1], Batch [51], Loss: 280090.656250
Epoch [1/1], Batch [61], Loss: 279384.187500
Epoch [1/1], Batch [71], Loss: 280326.000000
Epoch [1/1], Batch [81], Loss: 283422.875000
Epoch [1/1], Batch [91], Loss: 290434.000000
Epoch [1/1], Batch [101], Loss: 289971.781250
Epoch [1/1], Batch [111], Loss: 291384.812500
Epoch [1/1], Batch [121], Loss: 292218.812500
Epoch [1/1], Batch [131], Loss: 291272.312500
Epoch [1/1], Batch [141], Loss: 287572.468750
Epoch [1/1], Batch [151], Loss: 284414.875000
Epoch [1/1], Batch [161], Loss: 284889.718750
Epoch [1/1], Batch [171], Loss: 281054.125000
Epoch [1/1], Batch [181], Loss: 286127.218750
Epoch [1/1], Batch [191], Loss: 295501.437500
Epoch [1/1], Batch [201], Loss: 283035.187500
Epoch [1/1], Batch [211], Loss: 281996.125000
Epoch [1/1], Batch [221], Loss: 288477.625000
Epoch [1/1], Batch [231], Loss: 301031.062500
Epoch [1/1], Batch [241], Loss: 294038.687500
Epoch [1/1], Batch [251], Loss: 277558.125000
Epoch [1/1], Batch [261], Loss: 280944.375000
Epoch [1/1], Batch [271], Loss: 290450.625000
Epoch [1/1], Batch [281], Loss: 289151.375000
Epoch [1/1], Batch [291], Loss: 285178.750000
Epoch [1/1], Batch [301], Loss: 288101.781250
Epoch [1/1], Batch [311], Loss: 293525.562500
Epoch [1/1], Batch [321], Loss: 279515.250000
Epoch [1/1], Batch [331], Loss: 281194.187500
Epoch [1/1], Batch [341], Loss: 281338.500000
Epoch [1/1], Batch [351], Loss: 303379.437500
Epoch [1/1], Batch [361], Loss: 279179.750000
Epoch [1/1], Batch [371], Loss: 284232.375000
Epoch [1/1], Batch [381], Loss: 292493.250000
Epoch [1/1], Batch [391], Loss: 288236.125000
Epoch [1/1], Batch [401], Loss: 282650.156250
Epoch [1/1], Batch [411], Loss: 289741.312500
Epoch [1/1], Batch [421], Loss: 282831.625000
Epoch [1/1], Batch [431], Loss: 293276.500000
Epoch [1/1], Batch [441], Loss: 279831.156250
Epoch [1/1], Batch [451], Loss: 289881.812500
Epoch [1/1], Batch [461], Loss: 288167.750000
Epoch [1/1], Batch [471], Loss: 288685.656250
Epoch [1/1], Batch [481], Loss: 296850.375000
Epoch [1/1], Batch [491], Loss: 280116.812500
Epoch [1/1], Batch [501], Loss: 284212.062500
Epoch [1/1], Batch [511], Loss: 273894.000000
Epoch [1/1], Batch [521], Loss: 289242.000000
Epoch [1/1], Batch [531], Loss: 285884.187500
Epoch [1/1], Batch [541], Loss: 280029.187500
Epoch [1/1], Batch [551], Loss: 298930.187500
Epoch [1/1], Batch [561], Loss: 296538.312500
Epoch [1/1], Batch [571], Loss: 296213.187500
Epoch [1/1], Batch [581], Loss: 284983.687500
Epoch [1/1], Batch [591], Loss: 297652.781250
Epoch [1/1], Batch [601], Loss: 282118.156250
Epoch [1/1], Batch [611], Loss: 300876.531250
Epoch [1/1], Batch [621], Loss: 276025.125000
Epoch [1/1], Batch [631], Loss: 287613.125000
Epoch [1/1], Batch [641], Loss: 283912.187500
Epoch [1/1], Batch [651], Loss: 292410.250000
Epoch [1/1], Batch [661], Loss: 284784.750000
Epoch [1/1], Batch [671], Loss: 292933.093750
Epoch [1/1], Batch [681], Loss: 292399.281250
Epoch [1/1], Batch [691], Loss: 288996.781250
Epoch [1/1], Batch [701], Loss: 287607.468750
Epoch [1/1], Batch [711], Loss: 294775.187500
Epoch [1/1], Batch [721], Loss: 308317.062500
Epoch [1/1], Batch [731], Loss: 286108.437500
Epoch [1/1], Batch [741], Loss: 295272.687500
Epoch [1/1], Batch [751], Loss: 285775.000000
Epoch [1/1], Batch [761], Loss: 295240.562500
Epoch [1/1], Batch [771], Loss: 286121.656250
Epoch [1/1], Batch [781], Loss: 296067.781250
Epoch [1/1], Batch [791], Loss: 285180.437500
Epoch [1/1], Batch [801], Loss: 274225.781250
Epoch [1/1], Batch [811], Loss: 277712.500000
Epoch [1/1], Batch [821], Loss: 286694.875000
Epoch [1/1], Batch [831], Loss: 283065.875000
Epoch [1/1], Batch [841], Loss: 288793.625000
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 288114.0553
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 284871.1656
Elapsed time: 4187.90 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 286966.8353
Elapsed time: 4210.26 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 323505.375000
Epoch [1/1], Batch [11], Loss: 314887.250000
Epoch [1/1], Batch [21], Loss: 322607.875000
Epoch [1/1], Batch [31], Loss: 332754.250000
Epoch [1/1], Batch [41], Loss: 336584.687500
Epoch [1/1], Batch [51], Loss: 330043.125000
Epoch [1/1], Batch [61], Loss: 330338.843750
Epoch [1/1], Batch [71], Loss: 326094.687500
Epoch [1/1], Batch [81], Loss: 338962.187500
Epoch [1/1], Batch [91], Loss: 331778.437500
Epoch [1/1], Batch [101], Loss: 329821.562500
Epoch [1/1], Batch [111], Loss: 321528.875000
Epoch [1/1], Batch [121], Loss: 331853.093750
Epoch [1/1], Batch [131], Loss: 339182.625000
Epoch [1/1], Batch [141], Loss: 323626.250000
Epoch [1/1], Batch [151], Loss: 324796.031250
Epoch [1/1], Batch [161], Loss: 323992.375000
Epoch [1/1], Batch [171], Loss: 323603.312500
Epoch [1/1], Batch [181], Loss: 333662.906250
Epoch [1/1], Batch [191], Loss: 344806.718750
Epoch [1/1], Batch [201], Loss: 329840.875000
Epoch [1/1], Batch [211], Loss: 323708.312500
Epoch [1/1], Batch [221], Loss: 330651.093750
Epoch [1/1], Batch [231], Loss: 327097.437500
Epoch [1/1], Batch [241], Loss: 330661.750000
Epoch [1/1], Batch [251], Loss: 334245.937500
Epoch [1/1], Batch [261], Loss: 336883.125000
Epoch [1/1], Batch [271], Loss: 336331.718750
Epoch [1/1], Batch [281], Loss: 341160.500000
Epoch [1/1], Batch [291], Loss: 333377.875000
Epoch [1/1], Batch [301], Loss: 340395.000000
Epoch [1/1], Batch [311], Loss: 342752.562500
Epoch [1/1], Batch [321], Loss: 323904.250000
Epoch [1/1], Batch [331], Loss: 330767.843750
Epoch [1/1], Batch [341], Loss: 328481.187500
Epoch [1/1], Batch [351], Loss: 335838.500000
Epoch [1/1], Batch [361], Loss: 324058.750000
Epoch [1/1], Batch [371], Loss: 334906.968750
Epoch [1/1], Batch [381], Loss: 330673.312500
Epoch [1/1], Batch [391], Loss: 342696.781250
Epoch [1/1], Batch [401], Loss: 331197.875000
Epoch [1/1], Batch [411], Loss: 330667.937500
Epoch [1/1], Batch [421], Loss: 319331.000000
Epoch [1/1], Batch [431], Loss: 326224.343750
Epoch [1/1], Batch [441], Loss: 330133.937500
Epoch [1/1], Batch [451], Loss: 331108.250000
Epoch [1/1], Batch [461], Loss: 324738.687500
Epoch [1/1], Batch [471], Loss: 317008.343750
Epoch [1/1], Batch [481], Loss: 337950.031250
Epoch [1/1], Batch [491], Loss: 324781.750000
Epoch [1/1], Batch [501], Loss: 333559.000000
Epoch [1/1], Batch [511], Loss: 335414.562500
Epoch [1/1], Batch [521], Loss: 333759.156250
Epoch [1/1], Batch [531], Loss: 332918.625000
Epoch [1/1], Batch [541], Loss: 326122.593750
Epoch [1/1], Batch [551], Loss: 323022.875000
Epoch [1/1], Batch [561], Loss: 323178.000000
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 329640.5974
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 325382.9407
Elapsed time: 4709.16 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 327616.7884
Elapsed time: 4726.03 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 370083.750000
Epoch [1/1], Batch [11], Loss: 357032.625000
Epoch [1/1], Batch [21], Loss: 382217.593750
Epoch [1/1], Batch [31], Loss: 367310.750000
Epoch [1/1], Batch [41], Loss: 377474.312500
Epoch [1/1], Batch [51], Loss: 377088.062500
Epoch [1/1], Batch [61], Loss: 360699.187500
Epoch [1/1], Batch [71], Loss: 369314.968750
Epoch [1/1], Batch [81], Loss: 370331.875000
Epoch [1/1], Batch [91], Loss: 375802.218750
Epoch [1/1], Batch [101], Loss: 367964.375000
Epoch [1/1], Batch [111], Loss: 373100.875000
Epoch [1/1], Batch [121], Loss: 384336.375000
Epoch [1/1], Batch [131], Loss: 373511.718750
Epoch [1/1], Batch [141], Loss: 380500.625000
Epoch [1/1], Batch [151], Loss: 367302.062500
Epoch [1/1], Batch [161], Loss: 379584.250000
Epoch [1/1], Batch [171], Loss: 372141.187500
Epoch [1/1], Batch [181], Loss: 379544.500000
Epoch [1/1], Batch [191], Loss: 365442.562500
Epoch [1/1], Batch [201], Loss: 365454.187500
Epoch [1/1], Batch [211], Loss: 368459.875000
Epoch [1/1], Batch [221], Loss: 374890.156250
Epoch [1/1], Batch [231], Loss: 377084.562500
Epoch [1/1], Batch [241], Loss: 369510.500000
Epoch [1/1], Batch [251], Loss: 364835.531250
Epoch [1/1], Batch [261], Loss: 367621.031250
Epoch [1/1], Batch [271], Loss: 369276.062500
Epoch [1/1], Batch [281], Loss: 365949.562500
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 371006.0465
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 358480.9368
Elapsed time: 5034.29 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 362069.0168
Elapsed time: 5043.68 seconds

Training complete!
Totoal elapsed time: 5043.68 seconds
CUDA is available!
