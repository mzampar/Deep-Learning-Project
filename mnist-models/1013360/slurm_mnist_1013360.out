Starting job 1013360
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = True,
    transpose = True,
    use_lstm_output = False,
    scheduler = True,
    initial_lr = 0.05,
    gamma = 0.95.

CUDA is available!
Using learning rate scheduler with initial_lr = 0.05 and gamma = 0.95.
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 0.770947
Epoch [1/1], Batch [11], Loss: 0.175985
Epoch [1/1], Batch [21], Loss: 0.166452
Epoch [1/1], Batch [31], Loss: 0.162747
Epoch [1/1], Batch [41], Loss: 0.169369
Epoch [1/1], Batch [51], Loss: 0.165738
Epoch [1/1], Batch [61], Loss: 0.167775
Epoch [1/1], Batch [71], Loss: 0.162382
Epoch [1/1], Batch [81], Loss: 0.157109
Epoch [1/1], Batch [91], Loss: 0.140414
Epoch [1/1], Batch [101], Loss: 0.118092
Epoch [1/1], Batch [111], Loss: 0.106077
Epoch [1/1], Batch [121], Loss: 0.107432
Epoch [1/1], Batch [131], Loss: 0.103975
Epoch [1/1], Batch [141], Loss: 0.104488
Epoch [1/1], Batch [151], Loss: 0.100332
Epoch [1/1], Batch [161], Loss: 0.097362
Epoch [1/1], Batch [171], Loss: 0.099624
Epoch [1/1], Batch [181], Loss: 0.094040
Epoch [1/1], Batch [191], Loss: 0.097058
Epoch [1/1], Batch [201], Loss: 0.093203
Epoch [1/1], Batch [211], Loss: 0.094730
Epoch [1/1], Batch [221], Loss: 0.096991
Epoch [1/1], Batch [231], Loss: 0.093695
Epoch [1/1], Batch [241], Loss: 0.095775
Epoch [1/1], Batch [251], Loss: 0.090874
Epoch [1/1], Batch [261], Loss: 0.094784
Epoch [1/1], Batch [271], Loss: 0.091110
Epoch [1/1], Batch [281], Loss: 0.092301
Epoch [1/1], Batch [291], Loss: 0.089845
Epoch [1/1], Batch [301], Loss: 0.094188
Epoch [1/1], Batch [311], Loss: 0.091085
Epoch [1/1], Batch [321], Loss: 0.090455
Epoch [1/1], Batch [331], Loss: 0.095199
Epoch [1/1], Batch [341], Loss: 0.093970
Epoch [1/1], Batch [351], Loss: 0.089809
Epoch [1/1], Batch [361], Loss: 0.090250
Epoch [1/1], Batch [371], Loss: 0.092968
Epoch [1/1], Batch [381], Loss: 0.088706
Epoch [1/1], Batch [391], Loss: 0.092678
Epoch [1/1], Batch [401], Loss: 0.092110
Epoch [1/1], Batch [411], Loss: 0.091911
Epoch [1/1], Batch [421], Loss: 0.095004
Epoch [1/1], Batch [431], Loss: 0.089842
Epoch [1/1], Batch [441], Loss: 0.089628
Epoch [1/1], Batch [451], Loss: 0.093558
Epoch [1/1], Batch [461], Loss: 0.094828
Epoch [1/1], Batch [471], Loss: 0.091008
Epoch [1/1], Batch [481], Loss: 0.090438
Epoch [1/1], Batch [491], Loss: 0.094152
Epoch [1/1], Batch [501], Loss: 0.091405
Epoch [1/1], Batch [511], Loss: 0.089888
Epoch [1/1], Batch [521], Loss: 0.088999
Epoch [1/1], Batch [531], Loss: 0.088813
Epoch [1/1], Batch [541], Loss: 0.089617
Epoch [1/1], Batch [551], Loss: 0.092041
Epoch [1/1], Batch [561], Loss: 0.092842
Epoch [1/1], Batch [571], Loss: 0.087961
Epoch [1/1], Batch [581], Loss: 0.092722
Epoch [1/1], Batch [591], Loss: 0.084622
Epoch [1/1], Batch [601], Loss: 0.087718
Epoch [1/1], Batch [611], Loss: 0.086678
Epoch [1/1], Batch [621], Loss: 0.091028
Epoch [1/1], Batch [631], Loss: 0.089818
Epoch [1/1], Batch [641], Loss: 0.088765
Epoch [1/1], Batch [651], Loss: 0.089008
Epoch [1/1], Batch [661], Loss: 0.091957
Epoch [1/1], Batch [671], Loss: 0.090686
Epoch [1/1], Batch [681], Loss: 0.089272
Epoch [1/1], Batch [691], Loss: 0.090141
Epoch [1/1], Batch [701], Loss: 0.087408
Epoch [1/1], Batch [711], Loss: 0.090596
Epoch [1/1], Batch [721], Loss: 0.089156
Epoch [1/1], Batch [731], Loss: 0.085628
Epoch [1/1], Batch [741], Loss: 0.087987
Epoch [1/1], Batch [751], Loss: 0.087934
Epoch [1/1], Batch [761], Loss: 0.087614
Epoch [1/1], Batch [771], Loss: 0.089066
Epoch [1/1], Batch [781], Loss: 0.088309
Epoch [1/1], Batch [791], Loss: 0.089872
Epoch [1/1], Batch [801], Loss: 0.088043
Epoch [1/1], Batch [811], Loss: 0.086821
Epoch [1/1], Batch [821], Loss: 0.086550
Epoch [1/1], Batch [831], Loss: 0.085532
Epoch [1/1], Batch [841], Loss: 0.084232
Epoch [1/1], Batch [851], Loss: 0.088817
Epoch [1/1], Batch [861], Loss: 0.082477
Epoch [1/1], Batch [871], Loss: 0.090304
Epoch [1/1], Batch [881], Loss: 0.089335
Epoch [1/1], Batch [891], Loss: 0.091590
Epoch [1/1], Batch [901], Loss: 0.089041
Epoch [1/1], Batch [911], Loss: 0.092375
Epoch [1/1], Batch [921], Loss: 0.081618
Epoch [1/1], Batch [931], Loss: 0.087105
Epoch [1/1], Batch [941], Loss: 0.087237
Epoch [1/1], Batch [951], Loss: 0.088952
Epoch [1/1], Batch [961], Loss: 0.087027
Epoch [1/1], Batch [971], Loss: 0.088599
Epoch [1/1], Batch [981], Loss: 0.087083
Epoch [1/1], Batch [991], Loss: 0.084463
Epoch [1/1], Batch [1001], Loss: 0.087258
Epoch [1/1], Batch [1011], Loss: 0.082082
Epoch [1/1], Batch [1021], Loss: 0.091963
Epoch [1/1], Batch [1031], Loss: 0.086375
Epoch [1/1], Batch [1041], Loss: 0.089522
Epoch [1/1], Batch [1051], Loss: 0.090097
Epoch [1/1], Batch [1061], Loss: 0.082421
Epoch [1/1], Batch [1071], Loss: 0.086951
Epoch [1/1], Batch [1081], Loss: 0.083936
Epoch [1/1], Batch [1091], Loss: 0.086378
Epoch [1/1], Batch [1101], Loss: 0.086120
Epoch [1/1], Batch [1111], Loss: 0.081988
Epoch [1/1], Batch [1121], Loss: 0.082765
Epoch [1/1], Batch [1131], Loss: 0.087573
Epoch [1/1], Batch [1141], Loss: 0.084272
Epoch [1/1], Batch [1151], Loss: 0.085028
Epoch [1/1], Batch [1161], Loss: 0.088602
Epoch [1/1], Batch [1171], Loss: 0.083370
Epoch [1/1], Batch [1181], Loss: 0.087199
Epoch [1/1], Batch [1191], Loss: 0.083988
Epoch [1/1], Batch [1201], Loss: 0.088788
Epoch [1/1], Batch [1211], Loss: 0.085437
Epoch [1/1], Batch [1221], Loss: 0.085701
Epoch [1/1], Batch [1231], Loss: 0.085699
Epoch [1/1], Batch [1241], Loss: 0.086064
Epoch [1/1], Batch [1251], Loss: 0.087867
Epoch [1/1], Batch [1261], Loss: 0.080583
Epoch [1/1], Batch [1271], Loss: 0.088169
Epoch [1/1], Batch [1281], Loss: 0.086887
Epoch [1/1], Batch [1291], Loss: 0.084952
Epoch [1/1], Batch [1301], Loss: 0.084981
Epoch [1/1], Batch [1311], Loss: 0.089002
Epoch [1/1], Batch [1321], Loss: 0.081212
Epoch [1/1], Batch [1331], Loss: 0.085323
Epoch [1/1], Batch [1341], Loss: 0.082055
Epoch [1/1], Batch [1351], Loss: 0.080137
Epoch [1/1], Batch [1361], Loss: 0.082813
Epoch [1/1], Batch [1371], Loss: 0.084249
Epoch [1/1], Batch [1381], Loss: 0.084463
Epoch [1/1], Batch [1391], Loss: 0.081178
Epoch [1/1], Batch [1401], Loss: 0.080643
Epoch [1/1], Batch [1411], Loss: 0.086056
Epoch [1/1], Batch [1421], Loss: 0.081554
Epoch [1/1], Batch [1431], Loss: 0.085554
Epoch [1/1], Batch [1441], Loss: 0.087184
Epoch [1/1], Batch [1451], Loss: 0.083356
Epoch [1/1], Batch [1461], Loss: 0.083616
Epoch [1/1], Batch [1471], Loss: 0.084370
Epoch [1/1], Batch [1481], Loss: 0.080974
Epoch [1/1], Batch [1491], Loss: 0.085261
Epoch [1/1], Batch [1501], Loss: 0.084759
Epoch [1/1], Batch [1511], Loss: 0.081718
Epoch [1/1], Batch [1521], Loss: 0.085494
Epoch [1/1], Batch [1531], Loss: 0.085959
Epoch [1/1], Batch [1541], Loss: 0.086136
Epoch [1/1], Batch [1551], Loss: 0.085047
Epoch [1/1], Batch [1561], Loss: 0.082071
Epoch [1/1], Batch [1571], Loss: 0.085963
Epoch [1/1], Batch [1581], Loss: 0.085091
Epoch [1/1], Batch [1591], Loss: 0.087206
Epoch [1/1], Batch [1601], Loss: 0.084447
Epoch [1/1], Batch [1611], Loss: 0.082273
Epoch [1/1], Batch [1621], Loss: 0.085338
Epoch [1/1], Batch [1631], Loss: 0.086278
Epoch [1/1], Batch [1641], Loss: 0.083986
Epoch [1/1], Batch [1651], Loss: 0.081958
Epoch [1/1], Batch [1661], Loss: 0.084000
Epoch [1/1], Batch [1671], Loss: 0.083794
Epoch [1/1], Batch [1681], Loss: 0.085548
Epoch [1/1], Batch [1691], Loss: 0.082373
Epoch [1/1], Batch [1701], Loss: 0.079613
Epoch [1/1], Batch [1711], Loss: 0.083730
Epoch [1/1], Batch [1721], Loss: 0.084521
Epoch [1/1], Batch [1731], Loss: 0.085287
Epoch [1/1], Batch [1741], Loss: 0.089088
Epoch [1/1], Batch [1751], Loss: 0.086582
Epoch [1/1], Batch [1761], Loss: 0.081420
Epoch [1/1], Batch [1771], Loss: 0.083983
Epoch [1/1], Batch [1781], Loss: 0.082909
Epoch [1/1], Batch [1791], Loss: 0.083326
Epoch [1/1], Batch [1801], Loss: 0.085857
Epoch [1/1], Batch [1811], Loss: 0.081287
Epoch [1/1], Batch [1821], Loss: 0.084528
Epoch [1/1], Batch [1831], Loss: 0.082518
Epoch [1/1], Batch [1841], Loss: 0.086353
Epoch [1/1], Batch [1851], Loss: 0.081917
Epoch [1/1], Batch [1861], Loss: 0.078439
Epoch [1/1], Batch [1871], Loss: 0.082582
Epoch [1/1], Batch [1881], Loss: 0.083969
Epoch [1/1], Batch [1891], Loss: 0.084503
Epoch [1/1], Batch [1901], Loss: 0.085433
Epoch [1/1], Batch [1911], Loss: 0.082971
Epoch [1/1], Batch [1921], Loss: 0.084537
Epoch [1/1], Batch [1931], Loss: 0.084269
Epoch [1/1], Batch [1941], Loss: 0.083551
Epoch [1/1], Batch [1951], Loss: 0.081088
Epoch [1/1], Batch [1961], Loss: 0.082011
Epoch [1/1], Batch [1971], Loss: 0.086814
Epoch [1/1], Batch [1981], Loss: 0.078636
Epoch [1/1], Batch [1991], Loss: 0.085032
Epoch [1/1], Batch [2001], Loss: 0.081522
Epoch [1/1], Batch [2011], Loss: 0.080512
Epoch [1/1], Batch [2021], Loss: 0.082590
Epoch [1/1], Batch [2031], Loss: 0.087084
Epoch [1/1], Batch [2041], Loss: 0.082436
Epoch [1/1], Batch [2051], Loss: 0.085218
Epoch [1/1], Batch [2061], Loss: 0.083113
Epoch [1/1], Batch [2071], Loss: 0.083744
Epoch [1/1], Batch [2081], Loss: 0.085057
Epoch [1/1], Batch [2091], Loss: 0.084475
Epoch [1/1], Batch [2101], Loss: 0.083595
Epoch [1/1], Batch [2111], Loss: 0.084944
Epoch [1/1], Batch [2121], Loss: 0.080110
Epoch [1/1], Batch [2131], Loss: 0.081821
Epoch [1/1], Batch [2141], Loss: 0.084553
Epoch [1/1], Batch [2151], Loss: 0.080457
Epoch [1/1], Batch [2161], Loss: 0.082157
Epoch [1/1], Batch [2171], Loss: 0.080128
Epoch [1/1], Batch [2181], Loss: 0.078639
Epoch [1/1], Batch [2191], Loss: 0.082975
Epoch [1/1], Batch [2201], Loss: 0.080426
Epoch [1/1], Batch [2211], Loss: 0.082946
Epoch [1/1], Batch [2221], Loss: 0.077115
Epoch [1/1], Batch [2231], Loss: 0.082912
Epoch [1/1], Batch [2241], Loss: 0.081865
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 0.0906
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 0.0817
Elapsed time: 501.60 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 0.0823
Elapsed time: 522.84 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 0.103520
Epoch [1/1], Batch [11], Loss: 0.103024
Epoch [1/1], Batch [21], Loss: 0.102076
Epoch [1/1], Batch [31], Loss: 0.091125
Epoch [1/1], Batch [41], Loss: 0.098352
Epoch [1/1], Batch [51], Loss: 0.090154
Epoch [1/1], Batch [61], Loss: 0.092341
Epoch [1/1], Batch [71], Loss: 0.092267
Epoch [1/1], Batch [81], Loss: 0.090471
Epoch [1/1], Batch [91], Loss: 0.091385
Epoch [1/1], Batch [101], Loss: 0.091529
Epoch [1/1], Batch [111], Loss: 0.094487
Epoch [1/1], Batch [121], Loss: 0.089260
Epoch [1/1], Batch [131], Loss: 0.090565
Epoch [1/1], Batch [141], Loss: 0.090973
Epoch [1/1], Batch [151], Loss: 0.089803
Epoch [1/1], Batch [161], Loss: 0.088474
Epoch [1/1], Batch [171], Loss: 0.090297
Epoch [1/1], Batch [181], Loss: 0.085172
Epoch [1/1], Batch [191], Loss: 0.089228
Epoch [1/1], Batch [201], Loss: 0.088196
Epoch [1/1], Batch [211], Loss: 0.086703
Epoch [1/1], Batch [221], Loss: 0.089998
Epoch [1/1], Batch [231], Loss: 0.085208
Epoch [1/1], Batch [241], Loss: 0.085984
Epoch [1/1], Batch [251], Loss: 0.090875
Epoch [1/1], Batch [261], Loss: 0.089491
Epoch [1/1], Batch [271], Loss: 0.092990
Epoch [1/1], Batch [281], Loss: 0.092954
Epoch [1/1], Batch [291], Loss: 0.085176
Epoch [1/1], Batch [301], Loss: 0.089929
Epoch [1/1], Batch [311], Loss: 0.087001
Epoch [1/1], Batch [321], Loss: 0.086428
Epoch [1/1], Batch [331], Loss: 0.091862
Epoch [1/1], Batch [341], Loss: 0.086421
Epoch [1/1], Batch [351], Loss: 0.085810
Epoch [1/1], Batch [361], Loss: 0.086886
Epoch [1/1], Batch [371], Loss: 0.087609
Epoch [1/1], Batch [381], Loss: 0.089631
Epoch [1/1], Batch [391], Loss: 0.087875
Epoch [1/1], Batch [401], Loss: 0.087384
Epoch [1/1], Batch [411], Loss: 0.087618
Epoch [1/1], Batch [421], Loss: 0.087194
Epoch [1/1], Batch [431], Loss: 0.085876
Epoch [1/1], Batch [441], Loss: 0.088026
Epoch [1/1], Batch [451], Loss: 0.088921
Epoch [1/1], Batch [461], Loss: 0.087624
Epoch [1/1], Batch [471], Loss: 0.089319
Epoch [1/1], Batch [481], Loss: 0.087374
Epoch [1/1], Batch [491], Loss: 0.087321
Epoch [1/1], Batch [501], Loss: 0.083418
Epoch [1/1], Batch [511], Loss: 0.089655
Epoch [1/1], Batch [521], Loss: 0.087785
Epoch [1/1], Batch [531], Loss: 0.088020
Epoch [1/1], Batch [541], Loss: 0.088371
Epoch [1/1], Batch [551], Loss: 0.088306
Epoch [1/1], Batch [561], Loss: 0.084088
Epoch [1/1], Batch [571], Loss: 0.089023
Epoch [1/1], Batch [581], Loss: 0.090917
Epoch [1/1], Batch [591], Loss: 0.086624
Epoch [1/1], Batch [601], Loss: 0.085577
Epoch [1/1], Batch [611], Loss: 0.085631
Epoch [1/1], Batch [621], Loss: 0.087956
Epoch [1/1], Batch [631], Loss: 0.085921
Epoch [1/1], Batch [641], Loss: 0.091164
Epoch [1/1], Batch [651], Loss: 0.085165
Epoch [1/1], Batch [661], Loss: 0.087502
Epoch [1/1], Batch [671], Loss: 0.084446
Epoch [1/1], Batch [681], Loss: 0.081669
Epoch [1/1], Batch [691], Loss: 0.087799
Epoch [1/1], Batch [701], Loss: 0.085534
Epoch [1/1], Batch [711], Loss: 0.093174
Epoch [1/1], Batch [721], Loss: 0.081670
Epoch [1/1], Batch [731], Loss: 0.086307
Epoch [1/1], Batch [741], Loss: 0.088186
Epoch [1/1], Batch [751], Loss: 0.084117
Epoch [1/1], Batch [761], Loss: 0.082043
Epoch [1/1], Batch [771], Loss: 0.091639
Epoch [1/1], Batch [781], Loss: 0.084435
Epoch [1/1], Batch [791], Loss: 0.084939
Epoch [1/1], Batch [801], Loss: 0.088096
Epoch [1/1], Batch [811], Loss: 0.084563
Epoch [1/1], Batch [821], Loss: 0.089518
Epoch [1/1], Batch [831], Loss: 0.085511
Epoch [1/1], Batch [841], Loss: 0.086395
Epoch [1/1], Batch [851], Loss: 0.086626
Epoch [1/1], Batch [861], Loss: 0.080556
Epoch [1/1], Batch [871], Loss: 0.087761
Epoch [1/1], Batch [881], Loss: 0.085082
Epoch [1/1], Batch [891], Loss: 0.079005
Epoch [1/1], Batch [901], Loss: 0.083859
Epoch [1/1], Batch [911], Loss: 0.082392
Epoch [1/1], Batch [921], Loss: 0.086226
Epoch [1/1], Batch [931], Loss: 0.087565
Epoch [1/1], Batch [941], Loss: 0.086699
Epoch [1/1], Batch [951], Loss: 0.084325
Epoch [1/1], Batch [961], Loss: 0.082737
Epoch [1/1], Batch [971], Loss: 0.089394
Epoch [1/1], Batch [981], Loss: 0.085427
Epoch [1/1], Batch [991], Loss: 0.088981
Epoch [1/1], Batch [1001], Loss: 0.085301
Epoch [1/1], Batch [1011], Loss: 0.082886
Epoch [1/1], Batch [1021], Loss: 0.081149
Epoch [1/1], Batch [1031], Loss: 0.084152
Epoch [1/1], Batch [1041], Loss: 0.084412
Epoch [1/1], Batch [1051], Loss: 0.088070
Epoch [1/1], Batch [1061], Loss: 0.085137
Epoch [1/1], Batch [1071], Loss: 0.084556
Epoch [1/1], Batch [1081], Loss: 0.082555
Epoch [1/1], Batch [1091], Loss: 0.084650
Epoch [1/1], Batch [1101], Loss: 0.080349
Epoch [1/1], Batch [1111], Loss: 0.084118
Epoch [1/1], Batch [1121], Loss: 0.086131
Epoch [1/1], Batch [1131], Loss: 0.083298
Epoch [1/1], Batch [1141], Loss: 0.085754
Epoch [1/1], Batch [1151], Loss: 0.080050
Epoch [1/1], Batch [1161], Loss: 0.082917
Epoch [1/1], Batch [1171], Loss: 0.082947
Epoch [1/1], Batch [1181], Loss: 0.083347
Epoch [1/1], Batch [1191], Loss: 0.087727
Epoch [1/1], Batch [1201], Loss: 0.087635
Epoch [1/1], Batch [1211], Loss: 0.084181
Epoch [1/1], Batch [1221], Loss: 0.083086
Epoch [1/1], Batch [1231], Loss: 0.085272
Epoch [1/1], Batch [1241], Loss: 0.083209
Epoch [1/1], Batch [1251], Loss: 0.082527
Epoch [1/1], Batch [1261], Loss: 0.083784
Epoch [1/1], Batch [1271], Loss: 0.081947
Epoch [1/1], Batch [1281], Loss: 0.083061
Epoch [1/1], Batch [1291], Loss: 0.084173
Epoch [1/1], Batch [1301], Loss: 0.082961
Epoch [1/1], Batch [1311], Loss: 0.085767
Epoch [1/1], Batch [1321], Loss: 0.082922
Epoch [1/1], Batch [1331], Loss: 0.088844
Epoch [1/1], Batch [1341], Loss: 0.081684
Epoch [1/1], Batch [1351], Loss: 0.089215
Epoch [1/1], Batch [1361], Loss: 0.083177
Epoch [1/1], Batch [1371], Loss: 0.084071
Epoch [1/1], Batch [1381], Loss: 0.083175
Epoch [1/1], Batch [1391], Loss: 0.080674
Epoch [1/1], Batch [1401], Loss: 0.081640
Epoch [1/1], Batch [1411], Loss: 0.082531
Epoch [1/1], Batch [1421], Loss: 0.087653
Epoch [1/1], Batch [1431], Loss: 0.082833
Epoch [1/1], Batch [1441], Loss: 0.082681
Epoch [1/1], Batch [1451], Loss: 0.086611
Epoch [1/1], Batch [1461], Loss: 0.084477
Epoch [1/1], Batch [1471], Loss: 0.079468
Epoch [1/1], Batch [1481], Loss: 0.083703
Epoch [1/1], Batch [1491], Loss: 0.085165
Epoch [1/1], Batch [1501], Loss: 0.087985
Epoch [1/1], Batch [1511], Loss: 0.084104
Epoch [1/1], Batch [1521], Loss: 0.084750
Epoch [1/1], Batch [1531], Loss: 0.083419
Epoch [1/1], Batch [1541], Loss: 0.081616
Epoch [1/1], Batch [1551], Loss: 0.082218
Epoch [1/1], Batch [1561], Loss: 0.085668
Epoch [1/1], Batch [1571], Loss: 0.090702
Epoch [1/1], Batch [1581], Loss: 0.083731
Epoch [1/1], Batch [1591], Loss: 0.082424
Epoch [1/1], Batch [1601], Loss: 0.084704
Epoch [1/1], Batch [1611], Loss: 0.085440
Epoch [1/1], Batch [1621], Loss: 0.086104
Epoch [1/1], Batch [1631], Loss: 0.079267
Epoch [1/1], Batch [1641], Loss: 0.086477
Epoch [1/1], Batch [1651], Loss: 0.083332
Epoch [1/1], Batch [1661], Loss: 0.080624
Epoch [1/1], Batch [1671], Loss: 0.081633
Epoch [1/1], Batch [1681], Loss: 0.081412
Epoch [1/1], Batch [1691], Loss: 0.081142
Epoch [1/1], Batch [1701], Loss: 0.081218
Epoch [1/1], Batch [1711], Loss: 0.079542
Epoch [1/1], Batch [1721], Loss: 0.085519
Epoch [1/1], Batch [1731], Loss: 0.086730
Epoch [1/1], Batch [1741], Loss: 0.082751
Epoch [1/1], Batch [1751], Loss: 0.080674
Epoch [1/1], Batch [1761], Loss: 0.084966
Epoch [1/1], Batch [1771], Loss: 0.082788
Epoch [1/1], Batch [1781], Loss: 0.087355
Epoch [1/1], Batch [1791], Loss: 0.083844
Epoch [1/1], Batch [1801], Loss: 0.080755
Epoch [1/1], Batch [1811], Loss: 0.084213
Epoch [1/1], Batch [1821], Loss: 0.081392
Epoch [1/1], Batch [1831], Loss: 0.088340
Epoch [1/1], Batch [1841], Loss: 0.085587
Epoch [1/1], Batch [1851], Loss: 0.081869
Epoch [1/1], Batch [1861], Loss: 0.088284
Epoch [1/1], Batch [1871], Loss: 0.084872
Epoch [1/1], Batch [1881], Loss: 0.079782
Epoch [1/1], Batch [1891], Loss: 0.081205
Epoch [1/1], Batch [1901], Loss: 0.080839
Epoch [1/1], Batch [1911], Loss: 0.084613
Epoch [1/1], Batch [1921], Loss: 0.083915
Epoch [1/1], Batch [1931], Loss: 0.081342
Epoch [1/1], Batch [1941], Loss: 0.080934
Epoch [1/1], Batch [1951], Loss: 0.081196
Epoch [1/1], Batch [1961], Loss: 0.086706
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 0.0862
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 0.0828
Elapsed time: 1159.52 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 0.0834
Elapsed time: 1185.00 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 0.089365
Epoch [1/1], Batch [11], Loss: 0.094627
Epoch [1/1], Batch [21], Loss: 0.087381
Epoch [1/1], Batch [31], Loss: 0.090593
Epoch [1/1], Batch [41], Loss: 0.088189
Epoch [1/1], Batch [51], Loss: 0.085687
Epoch [1/1], Batch [61], Loss: 0.087450
Epoch [1/1], Batch [71], Loss: 0.091706
Epoch [1/1], Batch [81], Loss: 0.092581
Epoch [1/1], Batch [91], Loss: 0.085590
Epoch [1/1], Batch [101], Loss: 0.085873
Epoch [1/1], Batch [111], Loss: 0.092044
Epoch [1/1], Batch [121], Loss: 0.089187
Epoch [1/1], Batch [131], Loss: 0.092364
Epoch [1/1], Batch [141], Loss: 0.090156
Epoch [1/1], Batch [151], Loss: 0.089166
Epoch [1/1], Batch [161], Loss: 0.088724
Epoch [1/1], Batch [171], Loss: 0.086872
Epoch [1/1], Batch [181], Loss: 0.089703
Epoch [1/1], Batch [191], Loss: 0.091008
Epoch [1/1], Batch [201], Loss: 0.088974
Epoch [1/1], Batch [211], Loss: 0.087802
Epoch [1/1], Batch [221], Loss: 0.091268
Epoch [1/1], Batch [231], Loss: 0.084032
Epoch [1/1], Batch [241], Loss: 0.085371
Epoch [1/1], Batch [251], Loss: 0.090344
Epoch [1/1], Batch [261], Loss: 0.086849
Epoch [1/1], Batch [271], Loss: 0.087967
Epoch [1/1], Batch [281], Loss: 0.090380
Epoch [1/1], Batch [291], Loss: 0.088018
Epoch [1/1], Batch [301], Loss: 0.084856
Epoch [1/1], Batch [311], Loss: 0.087724
Epoch [1/1], Batch [321], Loss: 0.088811
Epoch [1/1], Batch [331], Loss: 0.087716
Epoch [1/1], Batch [341], Loss: 0.089391
Epoch [1/1], Batch [351], Loss: 0.086123
Epoch [1/1], Batch [361], Loss: 0.086440
Epoch [1/1], Batch [371], Loss: 0.087314
Epoch [1/1], Batch [381], Loss: 0.089585
Epoch [1/1], Batch [391], Loss: 0.090229
Epoch [1/1], Batch [401], Loss: 0.089063
Epoch [1/1], Batch [411], Loss: 0.084924
Epoch [1/1], Batch [421], Loss: 0.089306
Epoch [1/1], Batch [431], Loss: 0.088287
Epoch [1/1], Batch [441], Loss: 0.085348
Epoch [1/1], Batch [451], Loss: 0.087462
Epoch [1/1], Batch [461], Loss: 0.090284
Epoch [1/1], Batch [471], Loss: 0.087267
Epoch [1/1], Batch [481], Loss: 0.089594
Epoch [1/1], Batch [491], Loss: 0.087766
Epoch [1/1], Batch [501], Loss: 0.086396
Epoch [1/1], Batch [511], Loss: 0.089566
Epoch [1/1], Batch [521], Loss: 0.086683
Epoch [1/1], Batch [531], Loss: 0.084378
Epoch [1/1], Batch [541], Loss: 0.082654
Epoch [1/1], Batch [551], Loss: 0.089636
Epoch [1/1], Batch [561], Loss: 0.086396
Epoch [1/1], Batch [571], Loss: 0.085435
Epoch [1/1], Batch [581], Loss: 0.087415
Epoch [1/1], Batch [591], Loss: 0.086963
Epoch [1/1], Batch [601], Loss: 0.085342
Epoch [1/1], Batch [611], Loss: 0.091160
Epoch [1/1], Batch [621], Loss: 0.088729
Epoch [1/1], Batch [631], Loss: 0.087937
Epoch [1/1], Batch [641], Loss: 0.087703
Epoch [1/1], Batch [651], Loss: 0.083958
Epoch [1/1], Batch [661], Loss: 0.085488
Epoch [1/1], Batch [671], Loss: 0.088131
Epoch [1/1], Batch [681], Loss: 0.087856
Epoch [1/1], Batch [691], Loss: 0.083332
Epoch [1/1], Batch [701], Loss: 0.087124
Epoch [1/1], Batch [711], Loss: 0.082950
Epoch [1/1], Batch [721], Loss: 0.091282
Epoch [1/1], Batch [731], Loss: 0.090466
Epoch [1/1], Batch [741], Loss: 0.082987
Epoch [1/1], Batch [751], Loss: 0.085187
Epoch [1/1], Batch [761], Loss: 0.086417
Epoch [1/1], Batch [771], Loss: 0.085152
Epoch [1/1], Batch [781], Loss: 0.085759
Epoch [1/1], Batch [791], Loss: 0.084217
Epoch [1/1], Batch [801], Loss: 0.085863
Epoch [1/1], Batch [811], Loss: 0.083774
Epoch [1/1], Batch [821], Loss: 0.087200
Epoch [1/1], Batch [831], Loss: 0.090138
Epoch [1/1], Batch [841], Loss: 0.086915
Epoch [1/1], Batch [851], Loss: 0.092461
Epoch [1/1], Batch [861], Loss: 0.088253
Epoch [1/1], Batch [871], Loss: 0.085842
Epoch [1/1], Batch [881], Loss: 0.086273
Epoch [1/1], Batch [891], Loss: 0.083960
Epoch [1/1], Batch [901], Loss: 0.084400
Epoch [1/1], Batch [911], Loss: 0.091529
Epoch [1/1], Batch [921], Loss: 0.087679
Epoch [1/1], Batch [931], Loss: 0.084932
Epoch [1/1], Batch [941], Loss: 0.086875
Epoch [1/1], Batch [951], Loss: 0.085660
Epoch [1/1], Batch [961], Loss: 0.087651
Epoch [1/1], Batch [971], Loss: 0.089436
Epoch [1/1], Batch [981], Loss: 0.088974
Epoch [1/1], Batch [991], Loss: 0.087629
Epoch [1/1], Batch [1001], Loss: 0.088564
Epoch [1/1], Batch [1011], Loss: 0.087952
Epoch [1/1], Batch [1021], Loss: 0.089796
Epoch [1/1], Batch [1031], Loss: 0.087854
Epoch [1/1], Batch [1041], Loss: 0.087197
Epoch [1/1], Batch [1051], Loss: 0.085869
Epoch [1/1], Batch [1061], Loss: 0.085526
Epoch [1/1], Batch [1071], Loss: 0.084966
Epoch [1/1], Batch [1081], Loss: 0.089439
Epoch [1/1], Batch [1091], Loss: 0.088773
Epoch [1/1], Batch [1101], Loss: 0.087906
Epoch [1/1], Batch [1111], Loss: 0.086385
Epoch [1/1], Batch [1121], Loss: 0.085501
Epoch [1/1], Batch [1131], Loss: 0.087760
Epoch [1/1], Batch [1141], Loss: 0.088098
Epoch [1/1], Batch [1151], Loss: 0.084507
Epoch [1/1], Batch [1161], Loss: 0.087740
Epoch [1/1], Batch [1171], Loss: 0.086103
Epoch [1/1], Batch [1181], Loss: 0.085999
Epoch [1/1], Batch [1191], Loss: 0.085587
Epoch [1/1], Batch [1201], Loss: 0.085884
Epoch [1/1], Batch [1211], Loss: 0.087514
Epoch [1/1], Batch [1221], Loss: 0.085734
Epoch [1/1], Batch [1231], Loss: 0.086984
Epoch [1/1], Batch [1241], Loss: 0.087510
Epoch [1/1], Batch [1251], Loss: 0.085104
Epoch [1/1], Batch [1261], Loss: 0.088406
Epoch [1/1], Batch [1271], Loss: 0.087397
Epoch [1/1], Batch [1281], Loss: 0.088812
Epoch [1/1], Batch [1291], Loss: 0.087016
Epoch [1/1], Batch [1301], Loss: 0.085826
Epoch [1/1], Batch [1311], Loss: 0.085479
Epoch [1/1], Batch [1321], Loss: 0.086678
Epoch [1/1], Batch [1331], Loss: 0.086620
Epoch [1/1], Batch [1341], Loss: 0.086634
Epoch [1/1], Batch [1351], Loss: 0.087837
Epoch [1/1], Batch [1361], Loss: 0.088988
Epoch [1/1], Batch [1371], Loss: 0.086840
Epoch [1/1], Batch [1381], Loss: 0.089956
Epoch [1/1], Batch [1391], Loss: 0.086441
Epoch [1/1], Batch [1401], Loss: 0.086536
Epoch [1/1], Batch [1411], Loss: 0.086546
Epoch [1/1], Batch [1421], Loss: 0.085863
Epoch [1/1], Batch [1431], Loss: 0.085253
Epoch [1/1], Batch [1441], Loss: 0.079351
Epoch [1/1], Batch [1451], Loss: 0.088631
Epoch [1/1], Batch [1461], Loss: 0.086853
Epoch [1/1], Batch [1471], Loss: 0.085485
Epoch [1/1], Batch [1481], Loss: 0.085050
Epoch [1/1], Batch [1491], Loss: 0.084887
Epoch [1/1], Batch [1501], Loss: 0.085473
Epoch [1/1], Batch [1511], Loss: 0.086332
Epoch [1/1], Batch [1521], Loss: 0.089349
Epoch [1/1], Batch [1531], Loss: 0.084895
Epoch [1/1], Batch [1541], Loss: 0.090932
Epoch [1/1], Batch [1551], Loss: 0.082453
Epoch [1/1], Batch [1561], Loss: 0.086615
Epoch [1/1], Batch [1571], Loss: 0.084538
Epoch [1/1], Batch [1581], Loss: 0.084501
Epoch [1/1], Batch [1591], Loss: 0.087824
Epoch [1/1], Batch [1601], Loss: 0.086700
Epoch [1/1], Batch [1611], Loss: 0.088499
Epoch [1/1], Batch [1621], Loss: 0.087885
Epoch [1/1], Batch [1631], Loss: 0.084268
Epoch [1/1], Batch [1641], Loss: 0.082401
Epoch [1/1], Batch [1651], Loss: 0.087208
Epoch [1/1], Batch [1661], Loss: 0.087204
Epoch [1/1], Batch [1671], Loss: 0.086995
Epoch [1/1], Batch [1681], Loss: 0.083179
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 0.0870
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 0.0852
Elapsed time: 1886.51 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 0.0861
Elapsed time: 1914.41 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 0.092409
Epoch [1/1], Batch [11], Loss: 0.093699
Epoch [1/1], Batch [21], Loss: 0.093458
Epoch [1/1], Batch [31], Loss: 0.091367
Epoch [1/1], Batch [41], Loss: 0.089903
Epoch [1/1], Batch [51], Loss: 0.089957
Epoch [1/1], Batch [61], Loss: 0.090905
Epoch [1/1], Batch [71], Loss: 0.089302
Epoch [1/1], Batch [81], Loss: 0.090708
Epoch [1/1], Batch [91], Loss: 0.090867
Epoch [1/1], Batch [101], Loss: 0.090959
Epoch [1/1], Batch [111], Loss: 0.089167
Epoch [1/1], Batch [121], Loss: 0.089707
Epoch [1/1], Batch [131], Loss: 0.090375
Epoch [1/1], Batch [141], Loss: 0.092218
Epoch [1/1], Batch [151], Loss: 0.094274
Epoch [1/1], Batch [161], Loss: 0.092127
Epoch [1/1], Batch [171], Loss: 0.085548
Epoch [1/1], Batch [181], Loss: 0.083747
Epoch [1/1], Batch [191], Loss: 0.089638
Epoch [1/1], Batch [201], Loss: 0.089315
Epoch [1/1], Batch [211], Loss: 0.090783
Epoch [1/1], Batch [221], Loss: 0.087856
Epoch [1/1], Batch [231], Loss: 0.085878
Epoch [1/1], Batch [241], Loss: 0.091907
Epoch [1/1], Batch [251], Loss: 0.090111
Epoch [1/1], Batch [261], Loss: 0.091524
Epoch [1/1], Batch [271], Loss: 0.091273
Epoch [1/1], Batch [281], Loss: 0.086966
Epoch [1/1], Batch [291], Loss: 0.089059
Epoch [1/1], Batch [301], Loss: 0.091870
Epoch [1/1], Batch [311], Loss: 0.091834
Epoch [1/1], Batch [321], Loss: 0.088975
Epoch [1/1], Batch [331], Loss: 0.087951
Epoch [1/1], Batch [341], Loss: 0.091951
Epoch [1/1], Batch [351], Loss: 0.089153
Epoch [1/1], Batch [361], Loss: 0.085284
Epoch [1/1], Batch [371], Loss: 0.092980
Epoch [1/1], Batch [381], Loss: 0.094857
Epoch [1/1], Batch [391], Loss: 0.088845
Epoch [1/1], Batch [401], Loss: 0.093078
Epoch [1/1], Batch [411], Loss: 0.088984
Epoch [1/1], Batch [421], Loss: 0.090397
Epoch [1/1], Batch [431], Loss: 0.087484
Epoch [1/1], Batch [441], Loss: 0.091175
Epoch [1/1], Batch [451], Loss: 0.090195
Epoch [1/1], Batch [461], Loss: 0.087130
Epoch [1/1], Batch [471], Loss: 0.094704
Epoch [1/1], Batch [481], Loss: 0.091816
Epoch [1/1], Batch [491], Loss: 0.089717
Epoch [1/1], Batch [501], Loss: 0.087426
Epoch [1/1], Batch [511], Loss: 0.090258
Epoch [1/1], Batch [521], Loss: 0.091895
Epoch [1/1], Batch [531], Loss: 0.092246
Epoch [1/1], Batch [541], Loss: 0.089666
Epoch [1/1], Batch [551], Loss: 0.095718
Epoch [1/1], Batch [561], Loss: 0.091171
Epoch [1/1], Batch [571], Loss: 0.090350
Epoch [1/1], Batch [581], Loss: 0.088921
Epoch [1/1], Batch [591], Loss: 0.089013
Epoch [1/1], Batch [601], Loss: 0.090241
Epoch [1/1], Batch [611], Loss: 0.092195
Epoch [1/1], Batch [621], Loss: 0.090817
Epoch [1/1], Batch [631], Loss: 0.092206
Epoch [1/1], Batch [641], Loss: 0.087078
Epoch [1/1], Batch [651], Loss: 0.089919
Epoch [1/1], Batch [661], Loss: 0.087770
Epoch [1/1], Batch [671], Loss: 0.090995
Epoch [1/1], Batch [681], Loss: 0.089456
Epoch [1/1], Batch [691], Loss: 0.091690
Epoch [1/1], Batch [701], Loss: 0.088399
Epoch [1/1], Batch [711], Loss: 0.088271
Epoch [1/1], Batch [721], Loss: 0.094558
Epoch [1/1], Batch [731], Loss: 0.092743
Epoch [1/1], Batch [741], Loss: 0.089377
Epoch [1/1], Batch [751], Loss: 0.087692
Epoch [1/1], Batch [761], Loss: 0.085502
Epoch [1/1], Batch [771], Loss: 0.089717
Epoch [1/1], Batch [781], Loss: 0.093682
Epoch [1/1], Batch [791], Loss: 0.088967
Epoch [1/1], Batch [801], Loss: 0.090925
Epoch [1/1], Batch [811], Loss: 0.090397
Epoch [1/1], Batch [821], Loss: 0.089263
Epoch [1/1], Batch [831], Loss: 0.085011
Epoch [1/1], Batch [841], Loss: 0.089136
Epoch [1/1], Batch [851], Loss: 0.091271
Epoch [1/1], Batch [861], Loss: 0.092345
Epoch [1/1], Batch [871], Loss: 0.087257
Epoch [1/1], Batch [881], Loss: 0.083373
Epoch [1/1], Batch [891], Loss: 0.088714
Epoch [1/1], Batch [901], Loss: 0.088306
Epoch [1/1], Batch [911], Loss: 0.093168
Epoch [1/1], Batch [921], Loss: 0.092376
Epoch [1/1], Batch [931], Loss: 0.087429
Epoch [1/1], Batch [941], Loss: 0.092203
Epoch [1/1], Batch [951], Loss: 0.090381
Epoch [1/1], Batch [961], Loss: 0.089343
Epoch [1/1], Batch [971], Loss: 0.089248
Epoch [1/1], Batch [981], Loss: 0.091215
Epoch [1/1], Batch [991], Loss: 0.090041
Epoch [1/1], Batch [1001], Loss: 0.089487
Epoch [1/1], Batch [1011], Loss: 0.086339
Epoch [1/1], Batch [1021], Loss: 0.084204
Epoch [1/1], Batch [1031], Loss: 0.093322
Epoch [1/1], Batch [1041], Loss: 0.088431
Epoch [1/1], Batch [1051], Loss: 0.089254
Epoch [1/1], Batch [1061], Loss: 0.094604
Epoch [1/1], Batch [1071], Loss: 0.083642
Epoch [1/1], Batch [1081], Loss: 0.087983
Epoch [1/1], Batch [1091], Loss: 0.086056
Epoch [1/1], Batch [1101], Loss: 0.087001
Epoch [1/1], Batch [1111], Loss: 0.090228
Epoch [1/1], Batch [1121], Loss: 0.089024
Epoch [1/1], Batch [1131], Loss: 0.093125
Epoch [1/1], Batch [1141], Loss: 0.084904
Epoch [1/1], Batch [1151], Loss: 0.086967
Epoch [1/1], Batch [1161], Loss: 0.085102
Epoch [1/1], Batch [1171], Loss: 0.092797
Epoch [1/1], Batch [1181], Loss: 0.091355
Epoch [1/1], Batch [1191], Loss: 0.086383
Epoch [1/1], Batch [1201], Loss: 0.090210
Epoch [1/1], Batch [1211], Loss: 0.090468
Epoch [1/1], Batch [1221], Loss: 0.085984
Epoch [1/1], Batch [1231], Loss: 0.088218
Epoch [1/1], Batch [1241], Loss: 0.088004
Epoch [1/1], Batch [1251], Loss: 0.090431
Epoch [1/1], Batch [1261], Loss: 0.089933
Epoch [1/1], Batch [1271], Loss: 0.090535
Epoch [1/1], Batch [1281], Loss: 0.091354
Epoch [1/1], Batch [1291], Loss: 0.089168
Epoch [1/1], Batch [1301], Loss: 0.088762
Epoch [1/1], Batch [1311], Loss: 0.087039
Epoch [1/1], Batch [1321], Loss: 0.087463
Epoch [1/1], Batch [1331], Loss: 0.090017
Epoch [1/1], Batch [1341], Loss: 0.087083
Epoch [1/1], Batch [1351], Loss: 0.089524
Epoch [1/1], Batch [1361], Loss: 0.085511
Epoch [1/1], Batch [1371], Loss: 0.091716
Epoch [1/1], Batch [1381], Loss: 0.095179
Epoch [1/1], Batch [1391], Loss: 0.091986
Epoch [1/1], Batch [1401], Loss: 0.088554
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 0.0895
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 0.0882
Elapsed time: 2640.76 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 0.0891
Elapsed time: 2668.27 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 0.094591
Epoch [1/1], Batch [11], Loss: 0.096874
Epoch [1/1], Batch [21], Loss: 0.088433
Epoch [1/1], Batch [31], Loss: 0.091638
Epoch [1/1], Batch [41], Loss: 0.092886
Epoch [1/1], Batch [51], Loss: 0.091146
Epoch [1/1], Batch [61], Loss: 0.091723
Epoch [1/1], Batch [71], Loss: 0.091966
Epoch [1/1], Batch [81], Loss: 0.094938
Epoch [1/1], Batch [91], Loss: 0.091452
Epoch [1/1], Batch [101], Loss: 0.094933
Epoch [1/1], Batch [111], Loss: 0.093298
Epoch [1/1], Batch [121], Loss: 0.092315
Epoch [1/1], Batch [131], Loss: 0.092377
Epoch [1/1], Batch [141], Loss: 0.091532
Epoch [1/1], Batch [151], Loss: 0.088291
Epoch [1/1], Batch [161], Loss: 0.096986
Epoch [1/1], Batch [171], Loss: 0.094151
Epoch [1/1], Batch [181], Loss: 0.088247
Epoch [1/1], Batch [191], Loss: 0.090987
Epoch [1/1], Batch [201], Loss: 0.090976
Epoch [1/1], Batch [211], Loss: 0.095561
Epoch [1/1], Batch [221], Loss: 0.094722
Epoch [1/1], Batch [231], Loss: 0.095912
Epoch [1/1], Batch [241], Loss: 0.090317
Epoch [1/1], Batch [251], Loss: 0.092434
Epoch [1/1], Batch [261], Loss: 0.095097
Epoch [1/1], Batch [271], Loss: 0.092999
Epoch [1/1], Batch [281], Loss: 0.092788
Epoch [1/1], Batch [291], Loss: 0.091321
Epoch [1/1], Batch [301], Loss: 0.088601
Epoch [1/1], Batch [311], Loss: 0.087759
Epoch [1/1], Batch [321], Loss: 0.093112
Epoch [1/1], Batch [331], Loss: 0.091324
Epoch [1/1], Batch [341], Loss: 0.092997
Epoch [1/1], Batch [351], Loss: 0.095832
Epoch [1/1], Batch [361], Loss: 0.092015
Epoch [1/1], Batch [371], Loss: 0.090846
Epoch [1/1], Batch [381], Loss: 0.087930
Epoch [1/1], Batch [391], Loss: 0.091847
Epoch [1/1], Batch [401], Loss: 0.088914
Epoch [1/1], Batch [411], Loss: 0.094896
Epoch [1/1], Batch [421], Loss: 0.094912
Epoch [1/1], Batch [431], Loss: 0.095409
Epoch [1/1], Batch [441], Loss: 0.090422
Epoch [1/1], Batch [451], Loss: 0.092918
Epoch [1/1], Batch [461], Loss: 0.089366
Epoch [1/1], Batch [471], Loss: 0.090004
Epoch [1/1], Batch [481], Loss: 0.094740
Epoch [1/1], Batch [491], Loss: 0.092904
Epoch [1/1], Batch [501], Loss: 0.094389
Epoch [1/1], Batch [511], Loss: 0.095585
Epoch [1/1], Batch [521], Loss: 0.091889
Epoch [1/1], Batch [531], Loss: 0.090654
Epoch [1/1], Batch [541], Loss: 0.093239
Epoch [1/1], Batch [551], Loss: 0.092259
Epoch [1/1], Batch [561], Loss: 0.089518
Epoch [1/1], Batch [571], Loss: 0.091031
Epoch [1/1], Batch [581], Loss: 0.090688
Epoch [1/1], Batch [591], Loss: 0.094999
Epoch [1/1], Batch [601], Loss: 0.091399
Epoch [1/1], Batch [611], Loss: 0.095218
Epoch [1/1], Batch [621], Loss: 0.097590
Epoch [1/1], Batch [631], Loss: 0.089910
Epoch [1/1], Batch [641], Loss: 0.091362
Epoch [1/1], Batch [651], Loss: 0.094411
Epoch [1/1], Batch [661], Loss: 0.096227
Epoch [1/1], Batch [671], Loss: 0.089486
Epoch [1/1], Batch [681], Loss: 0.090718
Epoch [1/1], Batch [691], Loss: 0.091950
Epoch [1/1], Batch [701], Loss: 0.090828
Epoch [1/1], Batch [711], Loss: 0.087530
Epoch [1/1], Batch [721], Loss: 0.092158
Epoch [1/1], Batch [731], Loss: 0.090002
Epoch [1/1], Batch [741], Loss: 0.091627
Epoch [1/1], Batch [751], Loss: 0.091170
Epoch [1/1], Batch [761], Loss: 0.094197
Epoch [1/1], Batch [771], Loss: 0.090477
Epoch [1/1], Batch [781], Loss: 0.090558
Epoch [1/1], Batch [791], Loss: 0.089214
Epoch [1/1], Batch [801], Loss: 0.091072
Epoch [1/1], Batch [811], Loss: 0.091958
Epoch [1/1], Batch [821], Loss: 0.090767
Epoch [1/1], Batch [831], Loss: 0.097396
Epoch [1/1], Batch [841], Loss: 0.090460
Epoch [1/1], Batch [851], Loss: 0.092496
Epoch [1/1], Batch [861], Loss: 0.097605
Epoch [1/1], Batch [871], Loss: 0.095611
Epoch [1/1], Batch [881], Loss: 0.093717
Epoch [1/1], Batch [891], Loss: 0.093757
Epoch [1/1], Batch [901], Loss: 0.090981
Epoch [1/1], Batch [911], Loss: 0.095806
Epoch [1/1], Batch [921], Loss: 0.091929
Epoch [1/1], Batch [931], Loss: 0.090271
Epoch [1/1], Batch [941], Loss: 0.091780
Epoch [1/1], Batch [951], Loss: 0.092868
Epoch [1/1], Batch [961], Loss: 0.097856
Epoch [1/1], Batch [971], Loss: 0.095707
Epoch [1/1], Batch [981], Loss: 0.090255
Epoch [1/1], Batch [991], Loss: 0.094215
Epoch [1/1], Batch [1001], Loss: 0.096974
Epoch [1/1], Batch [1011], Loss: 0.094693
Epoch [1/1], Batch [1021], Loss: 0.097450
Epoch [1/1], Batch [1031], Loss: 0.092103
Epoch [1/1], Batch [1041], Loss: 0.093558
Epoch [1/1], Batch [1051], Loss: 0.092094
Epoch [1/1], Batch [1061], Loss: 0.094723
Epoch [1/1], Batch [1071], Loss: 0.089661
Epoch [1/1], Batch [1081], Loss: 0.094656
Epoch [1/1], Batch [1091], Loss: 0.091195
Epoch [1/1], Batch [1101], Loss: 0.090908
Epoch [1/1], Batch [1111], Loss: 0.093385
Epoch [1/1], Batch [1121], Loss: 0.094931
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 0.0927
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 0.0921
Elapsed time: 3417.05 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 0.0935
Elapsed time: 3442.92 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 0.096930
Epoch [1/1], Batch [11], Loss: 0.099063
Epoch [1/1], Batch [21], Loss: 0.095330
Epoch [1/1], Batch [31], Loss: 0.093595
Epoch [1/1], Batch [41], Loss: 0.095199
Epoch [1/1], Batch [51], Loss: 0.093793
Epoch [1/1], Batch [61], Loss: 0.097082
Epoch [1/1], Batch [71], Loss: 0.094058
Epoch [1/1], Batch [81], Loss: 0.095859
Epoch [1/1], Batch [91], Loss: 0.097821
Epoch [1/1], Batch [101], Loss: 0.097104
Epoch [1/1], Batch [111], Loss: 0.099677
Epoch [1/1], Batch [121], Loss: 0.098363
Epoch [1/1], Batch [131], Loss: 0.095338
Epoch [1/1], Batch [141], Loss: 0.098303
Epoch [1/1], Batch [151], Loss: 0.096192
Epoch [1/1], Batch [161], Loss: 0.091974
Epoch [1/1], Batch [171], Loss: 0.095740
Epoch [1/1], Batch [181], Loss: 0.095595
Epoch [1/1], Batch [191], Loss: 0.097342
Epoch [1/1], Batch [201], Loss: 0.095638
Epoch [1/1], Batch [211], Loss: 0.092885
Epoch [1/1], Batch [221], Loss: 0.097051
Epoch [1/1], Batch [231], Loss: 0.099014
Epoch [1/1], Batch [241], Loss: 0.094117
Epoch [1/1], Batch [251], Loss: 0.092975
Epoch [1/1], Batch [261], Loss: 0.095578
Epoch [1/1], Batch [271], Loss: 0.094753
Epoch [1/1], Batch [281], Loss: 0.093236
Epoch [1/1], Batch [291], Loss: 0.101449
Epoch [1/1], Batch [301], Loss: 0.099519
Epoch [1/1], Batch [311], Loss: 0.101455
Epoch [1/1], Batch [321], Loss: 0.092224
Epoch [1/1], Batch [331], Loss: 0.096549
Epoch [1/1], Batch [341], Loss: 0.097962
Epoch [1/1], Batch [351], Loss: 0.100027
Epoch [1/1], Batch [361], Loss: 0.097470
Epoch [1/1], Batch [371], Loss: 0.096703
Epoch [1/1], Batch [381], Loss: 0.092218
Epoch [1/1], Batch [391], Loss: 0.095873
Epoch [1/1], Batch [401], Loss: 0.095646
Epoch [1/1], Batch [411], Loss: 0.093111
Epoch [1/1], Batch [421], Loss: 0.095997
Epoch [1/1], Batch [431], Loss: 0.096924
Epoch [1/1], Batch [441], Loss: 0.092664
Epoch [1/1], Batch [451], Loss: 0.101199
Epoch [1/1], Batch [461], Loss: 0.097890
Epoch [1/1], Batch [471], Loss: 0.096467
Epoch [1/1], Batch [481], Loss: 0.093709
Epoch [1/1], Batch [491], Loss: 0.096113
Epoch [1/1], Batch [501], Loss: 0.096236
Epoch [1/1], Batch [511], Loss: 0.094564
Epoch [1/1], Batch [521], Loss: 0.094998
Epoch [1/1], Batch [531], Loss: 0.095717
Epoch [1/1], Batch [541], Loss: 0.096974
Epoch [1/1], Batch [551], Loss: 0.097880
Epoch [1/1], Batch [561], Loss: 0.097694
Epoch [1/1], Batch [571], Loss: 0.097890
Epoch [1/1], Batch [581], Loss: 0.095839
Epoch [1/1], Batch [591], Loss: 0.097410
Epoch [1/1], Batch [601], Loss: 0.100555
Epoch [1/1], Batch [611], Loss: 0.096363
Epoch [1/1], Batch [621], Loss: 0.100475
Epoch [1/1], Batch [631], Loss: 0.093466
Epoch [1/1], Batch [641], Loss: 0.098864
Epoch [1/1], Batch [651], Loss: 0.096426
Epoch [1/1], Batch [661], Loss: 0.096645
Epoch [1/1], Batch [671], Loss: 0.100053
Epoch [1/1], Batch [681], Loss: 0.095822
Epoch [1/1], Batch [691], Loss: 0.093818
Epoch [1/1], Batch [701], Loss: 0.096831
Epoch [1/1], Batch [711], Loss: 0.095703
Epoch [1/1], Batch [721], Loss: 0.099620
Epoch [1/1], Batch [731], Loss: 0.096161
Epoch [1/1], Batch [741], Loss: 0.098824
Epoch [1/1], Batch [751], Loss: 0.099327
Epoch [1/1], Batch [761], Loss: 0.093194
Epoch [1/1], Batch [771], Loss: 0.100193
Epoch [1/1], Batch [781], Loss: 0.098287
Epoch [1/1], Batch [791], Loss: 0.092520
Epoch [1/1], Batch [801], Loss: 0.097175
Epoch [1/1], Batch [811], Loss: 0.099402
Epoch [1/1], Batch [821], Loss: 0.096988
Epoch [1/1], Batch [831], Loss: 0.096735
Epoch [1/1], Batch [841], Loss: 0.102850
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 0.0965
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 0.0965
Elapsed time: 4053.20 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 0.0973
Elapsed time: 4075.48 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 0.101921
Epoch [1/1], Batch [11], Loss: 0.097442
Epoch [1/1], Batch [21], Loss: 0.101900
Epoch [1/1], Batch [31], Loss: 0.099560
Epoch [1/1], Batch [41], Loss: 0.099695
Epoch [1/1], Batch [51], Loss: 0.099337
Epoch [1/1], Batch [61], Loss: 0.100902
Epoch [1/1], Batch [71], Loss: 0.097014
Epoch [1/1], Batch [81], Loss: 0.102020
Epoch [1/1], Batch [91], Loss: 0.102358
Epoch [1/1], Batch [101], Loss: 0.098990
Epoch [1/1], Batch [111], Loss: 0.100239
Epoch [1/1], Batch [121], Loss: 0.102168
Epoch [1/1], Batch [131], Loss: 0.100298
Epoch [1/1], Batch [141], Loss: 0.102192
Epoch [1/1], Batch [151], Loss: 0.099408
Epoch [1/1], Batch [161], Loss: 0.102359
Epoch [1/1], Batch [171], Loss: 0.098771
Epoch [1/1], Batch [181], Loss: 0.099967
Epoch [1/1], Batch [191], Loss: 0.098034
Epoch [1/1], Batch [201], Loss: 0.099756
Epoch [1/1], Batch [211], Loss: 0.099511
Epoch [1/1], Batch [221], Loss: 0.101158
Epoch [1/1], Batch [231], Loss: 0.103167
Epoch [1/1], Batch [241], Loss: 0.101421
Epoch [1/1], Batch [251], Loss: 0.099344
Epoch [1/1], Batch [261], Loss: 0.099626
Epoch [1/1], Batch [271], Loss: 0.100878
Epoch [1/1], Batch [281], Loss: 0.101349
Epoch [1/1], Batch [291], Loss: 0.099486
Epoch [1/1], Batch [301], Loss: 0.106005
Epoch [1/1], Batch [311], Loss: 0.104960
Epoch [1/1], Batch [321], Loss: 0.104742
Epoch [1/1], Batch [331], Loss: 0.105452
Epoch [1/1], Batch [341], Loss: 0.101961
Epoch [1/1], Batch [351], Loss: 0.096953
Epoch [1/1], Batch [361], Loss: 0.102807
Epoch [1/1], Batch [371], Loss: 0.107500
Epoch [1/1], Batch [381], Loss: 0.098382
Epoch [1/1], Batch [391], Loss: 0.099497
Epoch [1/1], Batch [401], Loss: 0.101425
Epoch [1/1], Batch [411], Loss: 0.097604
Epoch [1/1], Batch [421], Loss: 0.099730
Epoch [1/1], Batch [431], Loss: 0.100710
Epoch [1/1], Batch [441], Loss: 0.101730
Epoch [1/1], Batch [451], Loss: 0.097093
Epoch [1/1], Batch [461], Loss: 0.097324
Epoch [1/1], Batch [471], Loss: 0.099449
Epoch [1/1], Batch [481], Loss: 0.095987
Epoch [1/1], Batch [491], Loss: 0.101886
Epoch [1/1], Batch [501], Loss: 0.098544
Epoch [1/1], Batch [511], Loss: 0.102443
Epoch [1/1], Batch [521], Loss: 0.097103
Epoch [1/1], Batch [531], Loss: 0.103700
Epoch [1/1], Batch [541], Loss: 0.101710
Epoch [1/1], Batch [551], Loss: 0.101326
Epoch [1/1], Batch [561], Loss: 0.101804
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 0.1010
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 0.1011
Elapsed time: 4563.02 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 0.1019
Elapsed time: 4579.84 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 0.105001
Epoch [1/1], Batch [11], Loss: 0.102210
Epoch [1/1], Batch [21], Loss: 0.107939
Epoch [1/1], Batch [31], Loss: 0.105557
Epoch [1/1], Batch [41], Loss: 0.101099
Epoch [1/1], Batch [51], Loss: 0.098041
Epoch [1/1], Batch [61], Loss: 0.103762
Epoch [1/1], Batch [71], Loss: 0.111090
Epoch [1/1], Batch [81], Loss: 0.102489
Epoch [1/1], Batch [91], Loss: 0.106700
Epoch [1/1], Batch [101], Loss: 0.105370
Epoch [1/1], Batch [111], Loss: 0.105724
Epoch [1/1], Batch [121], Loss: 0.104155
Epoch [1/1], Batch [131], Loss: 0.104025
Epoch [1/1], Batch [141], Loss: 0.101550
Epoch [1/1], Batch [151], Loss: 0.102248
Epoch [1/1], Batch [161], Loss: 0.099801
Epoch [1/1], Batch [171], Loss: 0.106833
Epoch [1/1], Batch [181], Loss: 0.108742
Epoch [1/1], Batch [191], Loss: 0.107878
Epoch [1/1], Batch [201], Loss: 0.106546
Epoch [1/1], Batch [211], Loss: 0.101431
Epoch [1/1], Batch [221], Loss: 0.105676
Epoch [1/1], Batch [231], Loss: 0.106221
Epoch [1/1], Batch [241], Loss: 0.109088
Epoch [1/1], Batch [251], Loss: 0.107970
Epoch [1/1], Batch [261], Loss: 0.099937
Epoch [1/1], Batch [271], Loss: 0.109709
Epoch [1/1], Batch [281], Loss: 0.108940
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 0.1051
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 0.1062
Elapsed time: 4879.49 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 0.1074
Elapsed time: 4888.88 seconds

Training complete!
Totoal elapsed time: 4888.88 seconds
CUDA is available!
