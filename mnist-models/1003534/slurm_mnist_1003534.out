Starting job 1003534
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = True,
    scheduler = False,
    bias = False,
    transpose = True,
    use_lstm_output = True,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 319588.000000
Epoch [1/1], Batch [11], Loss: 125490.140625
Epoch [1/1], Batch [21], Loss: 103191.843750
Epoch [1/1], Batch [31], Loss: 90397.117188
Epoch [1/1], Batch [41], Loss: 89440.984375
Epoch [1/1], Batch [51], Loss: 87549.984375
Epoch [1/1], Batch [61], Loss: 87618.468750
Epoch [1/1], Batch [71], Loss: 89577.687500
Epoch [1/1], Batch [81], Loss: 89631.843750
Epoch [1/1], Batch [91], Loss: 86314.460938
Epoch [1/1], Batch [101], Loss: 86681.187500
Epoch [1/1], Batch [111], Loss: 83572.765625
Epoch [1/1], Batch [121], Loss: 88060.312500
Epoch [1/1], Batch [131], Loss: 86255.546875
Epoch [1/1], Batch [141], Loss: 83180.546875
Epoch [1/1], Batch [151], Loss: 83184.414062
Epoch [1/1], Batch [161], Loss: 79591.914062
Epoch [1/1], Batch [171], Loss: 75914.101562
Epoch [1/1], Batch [181], Loss: 76212.515625
Epoch [1/1], Batch [191], Loss: 74515.796875
Epoch [1/1], Batch [201], Loss: 71757.125000
Epoch [1/1], Batch [211], Loss: 74603.953125
Epoch [1/1], Batch [221], Loss: 71231.062500
Epoch [1/1], Batch [231], Loss: 71470.468750
Epoch [1/1], Batch [241], Loss: 70191.742188
Epoch [1/1], Batch [251], Loss: 72160.742188
Epoch [1/1], Batch [261], Loss: 72108.078125
Epoch [1/1], Batch [271], Loss: 69812.796875
Epoch [1/1], Batch [281], Loss: 68922.000000
Epoch [1/1], Batch [291], Loss: 70109.156250
Epoch [1/1], Batch [301], Loss: 69541.890625
Epoch [1/1], Batch [311], Loss: 72047.453125
Epoch [1/1], Batch [321], Loss: 67875.585938
Epoch [1/1], Batch [331], Loss: 68905.921875
Epoch [1/1], Batch [341], Loss: 69907.015625
Epoch [1/1], Batch [351], Loss: 67959.187500
Epoch [1/1], Batch [361], Loss: 70032.625000
Epoch [1/1], Batch [371], Loss: 71832.859375
Epoch [1/1], Batch [381], Loss: 72376.765625
Epoch [1/1], Batch [391], Loss: 69768.921875
Epoch [1/1], Batch [401], Loss: 64962.023438
Epoch [1/1], Batch [411], Loss: 69752.757812
Epoch [1/1], Batch [421], Loss: 69483.945312
Epoch [1/1], Batch [431], Loss: 70666.164062
Epoch [1/1], Batch [441], Loss: 70760.875000
Epoch [1/1], Batch [451], Loss: 66237.359375
Epoch [1/1], Batch [461], Loss: 65920.750000
Epoch [1/1], Batch [471], Loss: 71688.000000
Epoch [1/1], Batch [481], Loss: 67174.851562
Epoch [1/1], Batch [491], Loss: 69222.796875
Epoch [1/1], Batch [501], Loss: 70428.140625
Epoch [1/1], Batch [511], Loss: 69813.523438
Epoch [1/1], Batch [521], Loss: 69073.078125
Epoch [1/1], Batch [531], Loss: 66157.460938
Epoch [1/1], Batch [541], Loss: 70438.968750
Epoch [1/1], Batch [551], Loss: 68567.328125
Epoch [1/1], Batch [561], Loss: 68641.515625
Epoch [1/1], Batch [571], Loss: 71066.398438
Epoch [1/1], Batch [581], Loss: 67599.687500
Epoch [1/1], Batch [591], Loss: 70541.812500
Epoch [1/1], Batch [601], Loss: 67612.906250
Epoch [1/1], Batch [611], Loss: 70550.812500
Epoch [1/1], Batch [621], Loss: 68511.554688
Epoch [1/1], Batch [631], Loss: 69339.179688
Epoch [1/1], Batch [641], Loss: 67464.773438
Epoch [1/1], Batch [651], Loss: 67983.265625
Epoch [1/1], Batch [661], Loss: 68981.750000
Epoch [1/1], Batch [671], Loss: 68142.218750
Epoch [1/1], Batch [681], Loss: 67024.812500
Epoch [1/1], Batch [691], Loss: 69601.710938
Epoch [1/1], Batch [701], Loss: 68649.562500
Epoch [1/1], Batch [711], Loss: 68272.039062
Epoch [1/1], Batch [721], Loss: 69680.578125
Epoch [1/1], Batch [731], Loss: 68625.890625
Epoch [1/1], Batch [741], Loss: 65126.195312
Epoch [1/1], Batch [751], Loss: 69240.234375
Epoch [1/1], Batch [761], Loss: 70677.625000
Epoch [1/1], Batch [771], Loss: 67679.578125
Epoch [1/1], Batch [781], Loss: 67955.515625
Epoch [1/1], Batch [791], Loss: 67310.281250
Epoch [1/1], Batch [801], Loss: 67510.523438
Epoch [1/1], Batch [811], Loss: 69399.265625
Epoch [1/1], Batch [821], Loss: 65237.750000
Epoch [1/1], Batch [831], Loss: 68225.109375
Epoch [1/1], Batch [841], Loss: 70218.796875
Epoch [1/1], Batch [851], Loss: 67922.718750
Epoch [1/1], Batch [861], Loss: 67411.328125
Epoch [1/1], Batch [871], Loss: 69147.507812
Epoch [1/1], Batch [881], Loss: 66863.984375
Epoch [1/1], Batch [891], Loss: 68179.187500
Epoch [1/1], Batch [901], Loss: 69082.437500
Epoch [1/1], Batch [911], Loss: 67200.828125
Epoch [1/1], Batch [921], Loss: 66554.820312
Epoch [1/1], Batch [931], Loss: 65871.742188
Epoch [1/1], Batch [941], Loss: 69055.953125
Epoch [1/1], Batch [951], Loss: 66043.007812
Epoch [1/1], Batch [961], Loss: 69285.921875
Epoch [1/1], Batch [971], Loss: 67911.070312
Epoch [1/1], Batch [981], Loss: 65152.324219
Epoch [1/1], Batch [991], Loss: 69071.039062
Epoch [1/1], Batch [1001], Loss: 68634.656250
Epoch [1/1], Batch [1011], Loss: 67403.953125
Epoch [1/1], Batch [1021], Loss: 68213.156250
Epoch [1/1], Batch [1031], Loss: 70449.562500
Epoch [1/1], Batch [1041], Loss: 67841.500000
Epoch [1/1], Batch [1051], Loss: 69095.648438
Epoch [1/1], Batch [1061], Loss: 64224.351562
Epoch [1/1], Batch [1071], Loss: 67211.515625
Epoch [1/1], Batch [1081], Loss: 69522.203125
Epoch [1/1], Batch [1091], Loss: 66820.437500
Epoch [1/1], Batch [1101], Loss: 65642.304688
Epoch [1/1], Batch [1111], Loss: 69696.843750
Epoch [1/1], Batch [1121], Loss: 67316.250000
Epoch [1/1], Batch [1131], Loss: 68845.484375
Epoch [1/1], Batch [1141], Loss: 66351.367188
Epoch [1/1], Batch [1151], Loss: 67503.382812
Epoch [1/1], Batch [1161], Loss: 69834.718750
Epoch [1/1], Batch [1171], Loss: 67635.125000
Epoch [1/1], Batch [1181], Loss: 70547.328125
Epoch [1/1], Batch [1191], Loss: 68753.031250
Epoch [1/1], Batch [1201], Loss: 66909.343750
Epoch [1/1], Batch [1211], Loss: 65842.328125
Epoch [1/1], Batch [1221], Loss: 65132.835938
Epoch [1/1], Batch [1231], Loss: 68526.960938
Epoch [1/1], Batch [1241], Loss: 65382.781250
Epoch [1/1], Batch [1251], Loss: 70200.968750
Epoch [1/1], Batch [1261], Loss: 66566.421875
Epoch [1/1], Batch [1271], Loss: 65748.398438
Epoch [1/1], Batch [1281], Loss: 67740.281250
Epoch [1/1], Batch [1291], Loss: 70221.375000
Epoch [1/1], Batch [1301], Loss: 66944.195312
Epoch [1/1], Batch [1311], Loss: 68836.578125
Epoch [1/1], Batch [1321], Loss: 64309.054688
Epoch [1/1], Batch [1331], Loss: 67979.390625
Epoch [1/1], Batch [1341], Loss: 68951.867188
Epoch [1/1], Batch [1351], Loss: 70835.671875
Epoch [1/1], Batch [1361], Loss: 66977.734375
Epoch [1/1], Batch [1371], Loss: 68691.125000
Epoch [1/1], Batch [1381], Loss: 67765.601562
Epoch [1/1], Batch [1391], Loss: 69241.453125
Epoch [1/1], Batch [1401], Loss: 68044.093750
Epoch [1/1], Batch [1411], Loss: 65433.996094
Epoch [1/1], Batch [1421], Loss: 68580.320312
Epoch [1/1], Batch [1431], Loss: 67227.187500
Epoch [1/1], Batch [1441], Loss: 66889.953125
Epoch [1/1], Batch [1451], Loss: 71697.468750
Epoch [1/1], Batch [1461], Loss: 69762.515625
Epoch [1/1], Batch [1471], Loss: 65751.234375
Epoch [1/1], Batch [1481], Loss: 71080.671875
Epoch [1/1], Batch [1491], Loss: 67838.445312
Epoch [1/1], Batch [1501], Loss: 66112.906250
Epoch [1/1], Batch [1511], Loss: 67027.187500
Epoch [1/1], Batch [1521], Loss: 67540.593750
Epoch [1/1], Batch [1531], Loss: 65272.597656
Epoch [1/1], Batch [1541], Loss: 68457.007812
Epoch [1/1], Batch [1551], Loss: 66996.984375
Epoch [1/1], Batch [1561], Loss: 68451.101562
Epoch [1/1], Batch [1571], Loss: 69595.531250
Epoch [1/1], Batch [1581], Loss: 67550.804688
Epoch [1/1], Batch [1591], Loss: 63554.593750
Epoch [1/1], Batch [1601], Loss: 65785.281250
Epoch [1/1], Batch [1611], Loss: 65408.406250
Epoch [1/1], Batch [1621], Loss: 69339.843750
Epoch [1/1], Batch [1631], Loss: 68787.828125
Epoch [1/1], Batch [1641], Loss: 67554.218750
Epoch [1/1], Batch [1651], Loss: 69988.453125
Epoch [1/1], Batch [1661], Loss: 67034.242188
Epoch [1/1], Batch [1671], Loss: 69354.406250
Epoch [1/1], Batch [1681], Loss: 67534.906250
Epoch [1/1], Batch [1691], Loss: 68053.757812
Epoch [1/1], Batch [1701], Loss: 69729.640625
Epoch [1/1], Batch [1711], Loss: 67909.093750
Epoch [1/1], Batch [1721], Loss: 67133.437500
Epoch [1/1], Batch [1731], Loss: 66114.578125
Epoch [1/1], Batch [1741], Loss: 69443.515625
Epoch [1/1], Batch [1751], Loss: 66989.968750
Epoch [1/1], Batch [1761], Loss: 68460.187500
Epoch [1/1], Batch [1771], Loss: 66857.531250
Epoch [1/1], Batch [1781], Loss: 69137.804688
Epoch [1/1], Batch [1791], Loss: 68160.453125
Epoch [1/1], Batch [1801], Loss: 67138.335938
Epoch [1/1], Batch [1811], Loss: 69575.250000
Epoch [1/1], Batch [1821], Loss: 66529.421875
Epoch [1/1], Batch [1831], Loss: 67982.218750
Epoch [1/1], Batch [1841], Loss: 65431.265625
Epoch [1/1], Batch [1851], Loss: 66314.382812
Epoch [1/1], Batch [1861], Loss: 67109.953125
Epoch [1/1], Batch [1871], Loss: 67022.281250
Epoch [1/1], Batch [1881], Loss: 67210.882812
Epoch [1/1], Batch [1891], Loss: 65862.125000
Epoch [1/1], Batch [1901], Loss: 67422.718750
Epoch [1/1], Batch [1911], Loss: 68533.125000
Epoch [1/1], Batch [1921], Loss: 66648.156250
Epoch [1/1], Batch [1931], Loss: 64766.503906
Epoch [1/1], Batch [1941], Loss: 66423.484375
Epoch [1/1], Batch [1951], Loss: 66225.382812
Epoch [1/1], Batch [1961], Loss: 67443.062500
Epoch [1/1], Batch [1971], Loss: 66697.710938
Epoch [1/1], Batch [1981], Loss: 66664.296875
Epoch [1/1], Batch [1991], Loss: 68956.289062
Epoch [1/1], Batch [2001], Loss: 68357.757812
Epoch [1/1], Batch [2011], Loss: 67467.578125
Epoch [1/1], Batch [2021], Loss: 64141.574219
Epoch [1/1], Batch [2031], Loss: 66600.890625
Epoch [1/1], Batch [2041], Loss: 69108.671875
Epoch [1/1], Batch [2051], Loss: 68323.789062
Epoch [1/1], Batch [2061], Loss: 67914.968750
Epoch [1/1], Batch [2071], Loss: 69481.109375
Epoch [1/1], Batch [2081], Loss: 67080.781250
Epoch [1/1], Batch [2091], Loss: 67410.312500
Epoch [1/1], Batch [2101], Loss: 66861.335938
Epoch [1/1], Batch [2111], Loss: 66414.710938
Epoch [1/1], Batch [2121], Loss: 68662.156250
Epoch [1/1], Batch [2131], Loss: 68356.960938
Epoch [1/1], Batch [2141], Loss: 66401.687500
Epoch [1/1], Batch [2151], Loss: 69857.593750
Epoch [1/1], Batch [2161], Loss: 69864.000000
Epoch [1/1], Batch [2171], Loss: 67189.304688
Epoch [1/1], Batch [2181], Loss: 66244.109375
Epoch [1/1], Batch [2191], Loss: 66656.796875
Epoch [1/1], Batch [2201], Loss: 66010.570312
Epoch [1/1], Batch [2211], Loss: 69895.453125
Epoch [1/1], Batch [2221], Loss: 64203.628906
Epoch [1/1], Batch [2231], Loss: 69444.000000
Epoch [1/1], Batch [2241], Loss: 65124.531250
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 70185.9774
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 68109.9835
Elapsed time: 476.43 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 68342.2395
Elapsed time: 495.72 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 121793.859375
Epoch [1/1], Batch [11], Loss: 112435.820312
Epoch [1/1], Batch [21], Loss: 114354.921875
Epoch [1/1], Batch [31], Loss: 120162.343750
Epoch [1/1], Batch [41], Loss: 113701.531250
Epoch [1/1], Batch [51], Loss: 111478.195312
Epoch [1/1], Batch [61], Loss: 113096.390625
Epoch [1/1], Batch [71], Loss: 114618.406250
Epoch [1/1], Batch [81], Loss: 114236.648438
Epoch [1/1], Batch [91], Loss: 114387.585938
Epoch [1/1], Batch [101], Loss: 119153.500000
Epoch [1/1], Batch [111], Loss: 107096.898438
Epoch [1/1], Batch [121], Loss: 114760.351562
Epoch [1/1], Batch [131], Loss: 114833.187500
Epoch [1/1], Batch [141], Loss: 115695.609375
Epoch [1/1], Batch [151], Loss: 120687.046875
Epoch [1/1], Batch [161], Loss: 115800.265625
Epoch [1/1], Batch [171], Loss: 117910.421875
Epoch [1/1], Batch [181], Loss: 111112.093750
Epoch [1/1], Batch [191], Loss: 113943.406250
Epoch [1/1], Batch [201], Loss: 114336.875000
Epoch [1/1], Batch [211], Loss: 116862.710938
Epoch [1/1], Batch [221], Loss: 112660.039062
Epoch [1/1], Batch [231], Loss: 117179.023438
Epoch [1/1], Batch [241], Loss: 117024.296875
Epoch [1/1], Batch [251], Loss: 112821.507812
Epoch [1/1], Batch [261], Loss: 109062.046875
Epoch [1/1], Batch [271], Loss: 110827.976562
Epoch [1/1], Batch [281], Loss: 114061.609375
Epoch [1/1], Batch [291], Loss: 113507.468750
Epoch [1/1], Batch [301], Loss: 112096.421875
Epoch [1/1], Batch [311], Loss: 112652.117188
Epoch [1/1], Batch [321], Loss: 116752.179688
Epoch [1/1], Batch [331], Loss: 112858.312500
Epoch [1/1], Batch [341], Loss: 112594.281250
Epoch [1/1], Batch [351], Loss: 108757.265625
Epoch [1/1], Batch [361], Loss: 110028.921875
Epoch [1/1], Batch [371], Loss: 111434.421875
Epoch [1/1], Batch [381], Loss: 111648.359375
Epoch [1/1], Batch [391], Loss: 112921.132812
Epoch [1/1], Batch [401], Loss: 112083.531250
Epoch [1/1], Batch [411], Loss: 114742.585938
Epoch [1/1], Batch [421], Loss: 116298.351562
Epoch [1/1], Batch [431], Loss: 115818.250000
Epoch [1/1], Batch [441], Loss: 113538.367188
Epoch [1/1], Batch [451], Loss: 114207.468750
Epoch [1/1], Batch [461], Loss: 110434.140625
Epoch [1/1], Batch [471], Loss: 114575.578125
Epoch [1/1], Batch [481], Loss: 116209.335938
Epoch [1/1], Batch [491], Loss: 111838.781250
Epoch [1/1], Batch [501], Loss: 115334.281250
Epoch [1/1], Batch [511], Loss: 114748.867188
Epoch [1/1], Batch [521], Loss: 115672.281250
Epoch [1/1], Batch [531], Loss: 116810.906250
Epoch [1/1], Batch [541], Loss: 114694.023438
Epoch [1/1], Batch [551], Loss: 113742.007812
Epoch [1/1], Batch [561], Loss: 117467.851562
Epoch [1/1], Batch [571], Loss: 114055.890625
Epoch [1/1], Batch [581], Loss: 110665.382812
Epoch [1/1], Batch [591], Loss: 113141.882812
Epoch [1/1], Batch [601], Loss: 114619.781250
Epoch [1/1], Batch [611], Loss: 109268.343750
Epoch [1/1], Batch [621], Loss: 109849.890625
Epoch [1/1], Batch [631], Loss: 112948.367188
Epoch [1/1], Batch [641], Loss: 113045.281250
Epoch [1/1], Batch [651], Loss: 114582.726562
Epoch [1/1], Batch [661], Loss: 119540.914062
Epoch [1/1], Batch [671], Loss: 112631.890625
Epoch [1/1], Batch [681], Loss: 115021.390625
Epoch [1/1], Batch [691], Loss: 115628.140625
Epoch [1/1], Batch [701], Loss: 112749.398438
Epoch [1/1], Batch [711], Loss: 111374.109375
Epoch [1/1], Batch [721], Loss: 112359.960938
Epoch [1/1], Batch [731], Loss: 116611.671875
Epoch [1/1], Batch [741], Loss: 110104.187500
Epoch [1/1], Batch [751], Loss: 120203.085938
Epoch [1/1], Batch [761], Loss: 111587.585938
Epoch [1/1], Batch [771], Loss: 111613.265625
Epoch [1/1], Batch [781], Loss: 115870.984375
Epoch [1/1], Batch [791], Loss: 112773.765625
Epoch [1/1], Batch [801], Loss: 116204.468750
Epoch [1/1], Batch [811], Loss: 111361.046875
Epoch [1/1], Batch [821], Loss: 110362.593750
Epoch [1/1], Batch [831], Loss: 109970.484375
Epoch [1/1], Batch [841], Loss: 110627.804688
Epoch [1/1], Batch [851], Loss: 112014.515625
Epoch [1/1], Batch [861], Loss: 112413.343750
Epoch [1/1], Batch [871], Loss: 119436.132812
Epoch [1/1], Batch [881], Loss: 113904.656250
Epoch [1/1], Batch [891], Loss: 110070.164062
Epoch [1/1], Batch [901], Loss: 111826.875000
Epoch [1/1], Batch [911], Loss: 109584.453125
Epoch [1/1], Batch [921], Loss: 117568.890625
Epoch [1/1], Batch [931], Loss: 115966.726562
Epoch [1/1], Batch [941], Loss: 116512.750000
Epoch [1/1], Batch [951], Loss: 109722.523438
Epoch [1/1], Batch [961], Loss: 115377.476562
Epoch [1/1], Batch [971], Loss: 111089.734375
Epoch [1/1], Batch [981], Loss: 116808.679688
Epoch [1/1], Batch [991], Loss: 110137.812500
Epoch [1/1], Batch [1001], Loss: 113849.234375
Epoch [1/1], Batch [1011], Loss: 118996.859375
Epoch [1/1], Batch [1021], Loss: 112814.875000
Epoch [1/1], Batch [1031], Loss: 113093.218750
Epoch [1/1], Batch [1041], Loss: 114298.718750
Epoch [1/1], Batch [1051], Loss: 109808.859375
Epoch [1/1], Batch [1061], Loss: 111825.531250
Epoch [1/1], Batch [1071], Loss: 114156.515625
Epoch [1/1], Batch [1081], Loss: 109817.820312
Epoch [1/1], Batch [1091], Loss: 111961.687500
Epoch [1/1], Batch [1101], Loss: 112422.718750
Epoch [1/1], Batch [1111], Loss: 112062.476562
Epoch [1/1], Batch [1121], Loss: 111194.218750
Epoch [1/1], Batch [1131], Loss: 118184.054688
Epoch [1/1], Batch [1141], Loss: 109437.531250
Epoch [1/1], Batch [1151], Loss: 114134.421875
Epoch [1/1], Batch [1161], Loss: 111874.531250
Epoch [1/1], Batch [1171], Loss: 110124.906250
Epoch [1/1], Batch [1181], Loss: 113681.031250
Epoch [1/1], Batch [1191], Loss: 113059.546875
Epoch [1/1], Batch [1201], Loss: 108602.046875
Epoch [1/1], Batch [1211], Loss: 112064.953125
Epoch [1/1], Batch [1221], Loss: 115150.757812
Epoch [1/1], Batch [1231], Loss: 108518.984375
Epoch [1/1], Batch [1241], Loss: 105699.328125
Epoch [1/1], Batch [1251], Loss: 113922.851562
Epoch [1/1], Batch [1261], Loss: 111922.078125
Epoch [1/1], Batch [1271], Loss: 112827.421875
Epoch [1/1], Batch [1281], Loss: 113067.945312
Epoch [1/1], Batch [1291], Loss: 108520.945312
Epoch [1/1], Batch [1301], Loss: 115179.468750
Epoch [1/1], Batch [1311], Loss: 113529.390625
Epoch [1/1], Batch [1321], Loss: 110025.312500
Epoch [1/1], Batch [1331], Loss: 121582.390625
Epoch [1/1], Batch [1341], Loss: 109419.468750
Epoch [1/1], Batch [1351], Loss: 119699.664062
Epoch [1/1], Batch [1361], Loss: 109244.664062
Epoch [1/1], Batch [1371], Loss: 111730.078125
Epoch [1/1], Batch [1381], Loss: 117391.328125
Epoch [1/1], Batch [1391], Loss: 117293.343750
Epoch [1/1], Batch [1401], Loss: 116904.289062
Epoch [1/1], Batch [1411], Loss: 114834.523438
Epoch [1/1], Batch [1421], Loss: 111790.101562
Epoch [1/1], Batch [1431], Loss: 113725.156250
Epoch [1/1], Batch [1441], Loss: 110486.039062
Epoch [1/1], Batch [1451], Loss: 111345.718750
Epoch [1/1], Batch [1461], Loss: 115862.398438
Epoch [1/1], Batch [1471], Loss: 110318.328125
Epoch [1/1], Batch [1481], Loss: 113209.468750
Epoch [1/1], Batch [1491], Loss: 119127.140625
Epoch [1/1], Batch [1501], Loss: 111972.046875
Epoch [1/1], Batch [1511], Loss: 117214.414062
Epoch [1/1], Batch [1521], Loss: 109337.726562
Epoch [1/1], Batch [1531], Loss: 114006.281250
Epoch [1/1], Batch [1541], Loss: 109585.265625
Epoch [1/1], Batch [1551], Loss: 112271.453125
Epoch [1/1], Batch [1561], Loss: 107719.265625
Epoch [1/1], Batch [1571], Loss: 111553.125000
Epoch [1/1], Batch [1581], Loss: 112470.406250
Epoch [1/1], Batch [1591], Loss: 113065.031250
Epoch [1/1], Batch [1601], Loss: 113866.960938
Epoch [1/1], Batch [1611], Loss: 111073.296875
Epoch [1/1], Batch [1621], Loss: 111080.726562
Epoch [1/1], Batch [1631], Loss: 110919.953125
Epoch [1/1], Batch [1641], Loss: 111099.687500
Epoch [1/1], Batch [1651], Loss: 113117.062500
Epoch [1/1], Batch [1661], Loss: 109130.234375
Epoch [1/1], Batch [1671], Loss: 111881.835938
Epoch [1/1], Batch [1681], Loss: 117025.093750
Epoch [1/1], Batch [1691], Loss: 115045.210938
Epoch [1/1], Batch [1701], Loss: 108587.835938
Epoch [1/1], Batch [1711], Loss: 113002.617188
Epoch [1/1], Batch [1721], Loss: 112931.312500
Epoch [1/1], Batch [1731], Loss: 112030.687500
Epoch [1/1], Batch [1741], Loss: 111309.437500
Epoch [1/1], Batch [1751], Loss: 117694.429688
Epoch [1/1], Batch [1761], Loss: 112250.078125
Epoch [1/1], Batch [1771], Loss: 108882.500000
Epoch [1/1], Batch [1781], Loss: 118792.984375
Epoch [1/1], Batch [1791], Loss: 115677.781250
Epoch [1/1], Batch [1801], Loss: 112371.875000
Epoch [1/1], Batch [1811], Loss: 115283.343750
Epoch [1/1], Batch [1821], Loss: 111698.609375
Epoch [1/1], Batch [1831], Loss: 112203.156250
Epoch [1/1], Batch [1841], Loss: 117376.703125
Epoch [1/1], Batch [1851], Loss: 109458.250000
Epoch [1/1], Batch [1861], Loss: 117394.062500
Epoch [1/1], Batch [1871], Loss: 114464.546875
Epoch [1/1], Batch [1881], Loss: 109913.296875
Epoch [1/1], Batch [1891], Loss: 110919.515625
Epoch [1/1], Batch [1901], Loss: 112508.343750
Epoch [1/1], Batch [1911], Loss: 113975.914062
Epoch [1/1], Batch [1921], Loss: 111208.203125
Epoch [1/1], Batch [1931], Loss: 114514.468750
Epoch [1/1], Batch [1941], Loss: 112148.750000
Epoch [1/1], Batch [1951], Loss: 114714.359375
Epoch [1/1], Batch [1961], Loss: 108464.460938
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 113205.9243
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 112189.2577
Elapsed time: 1101.17 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 112903.7204
Elapsed time: 1124.24 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 157316.734375
Epoch [1/1], Batch [11], Loss: 163256.343750
Epoch [1/1], Batch [21], Loss: 155061.625000
Epoch [1/1], Batch [31], Loss: 152970.265625
Epoch [1/1], Batch [41], Loss: 153806.093750
Epoch [1/1], Batch [51], Loss: 158479.093750
Epoch [1/1], Batch [61], Loss: 165313.156250
Epoch [1/1], Batch [71], Loss: 163981.593750
Epoch [1/1], Batch [81], Loss: 158308.859375
Epoch [1/1], Batch [91], Loss: 157839.765625
Epoch [1/1], Batch [101], Loss: 159378.468750
Epoch [1/1], Batch [111], Loss: 161910.093750
Epoch [1/1], Batch [121], Loss: 159548.125000
Epoch [1/1], Batch [131], Loss: 157634.312500
Epoch [1/1], Batch [141], Loss: 156956.234375
Epoch [1/1], Batch [151], Loss: 160832.875000
Epoch [1/1], Batch [161], Loss: 154124.968750
Epoch [1/1], Batch [171], Loss: 161539.281250
Epoch [1/1], Batch [181], Loss: 162983.625000
Epoch [1/1], Batch [191], Loss: 160511.000000
Epoch [1/1], Batch [201], Loss: 159688.687500
Epoch [1/1], Batch [211], Loss: 159242.437500
Epoch [1/1], Batch [221], Loss: 160050.843750
Epoch [1/1], Batch [231], Loss: 155708.562500
Epoch [1/1], Batch [241], Loss: 158014.906250
Epoch [1/1], Batch [251], Loss: 166203.343750
Epoch [1/1], Batch [261], Loss: 156924.531250
Epoch [1/1], Batch [271], Loss: 157764.281250
Epoch [1/1], Batch [281], Loss: 156766.812500
Epoch [1/1], Batch [291], Loss: 158521.031250
Epoch [1/1], Batch [301], Loss: 153895.671875
Epoch [1/1], Batch [311], Loss: 156234.093750
Epoch [1/1], Batch [321], Loss: 166116.359375
Epoch [1/1], Batch [331], Loss: 151899.500000
Epoch [1/1], Batch [341], Loss: 157786.625000
Epoch [1/1], Batch [351], Loss: 161402.406250
Epoch [1/1], Batch [361], Loss: 161248.734375
Epoch [1/1], Batch [371], Loss: 169730.296875
Epoch [1/1], Batch [381], Loss: 156458.343750
Epoch [1/1], Batch [391], Loss: 152641.000000
Epoch [1/1], Batch [401], Loss: 156487.718750
Epoch [1/1], Batch [411], Loss: 157287.656250
Epoch [1/1], Batch [421], Loss: 158523.062500
Epoch [1/1], Batch [431], Loss: 155878.656250
Epoch [1/1], Batch [441], Loss: 149730.250000
Epoch [1/1], Batch [451], Loss: 162426.812500
Epoch [1/1], Batch [461], Loss: 159828.531250
Epoch [1/1], Batch [471], Loss: 149953.093750
Epoch [1/1], Batch [481], Loss: 160990.250000
Epoch [1/1], Batch [491], Loss: 161885.625000
Epoch [1/1], Batch [501], Loss: 152675.000000
Epoch [1/1], Batch [511], Loss: 159607.406250
Epoch [1/1], Batch [521], Loss: 162231.562500
Epoch [1/1], Batch [531], Loss: 157517.062500
Epoch [1/1], Batch [541], Loss: 158470.859375
Epoch [1/1], Batch [551], Loss: 155936.468750
Epoch [1/1], Batch [561], Loss: 158850.953125
Epoch [1/1], Batch [571], Loss: 159933.375000
Epoch [1/1], Batch [581], Loss: 152399.046875
Epoch [1/1], Batch [591], Loss: 156614.500000
Epoch [1/1], Batch [601], Loss: 165773.343750
Epoch [1/1], Batch [611], Loss: 149019.187500
Epoch [1/1], Batch [621], Loss: 155953.359375
Epoch [1/1], Batch [631], Loss: 155495.656250
Epoch [1/1], Batch [641], Loss: 157074.000000
Epoch [1/1], Batch [651], Loss: 159724.984375
Epoch [1/1], Batch [661], Loss: 159768.796875
Epoch [1/1], Batch [671], Loss: 153455.312500
Epoch [1/1], Batch [681], Loss: 156245.625000
Epoch [1/1], Batch [691], Loss: 158142.843750
Epoch [1/1], Batch [701], Loss: 167830.765625
Epoch [1/1], Batch [711], Loss: 149705.687500
Epoch [1/1], Batch [721], Loss: 157097.203125
Epoch [1/1], Batch [731], Loss: 159348.015625
Epoch [1/1], Batch [741], Loss: 158714.046875
Epoch [1/1], Batch [751], Loss: 156688.531250
Epoch [1/1], Batch [761], Loss: 158301.156250
Epoch [1/1], Batch [771], Loss: 161274.015625
Epoch [1/1], Batch [781], Loss: 157541.875000
Epoch [1/1], Batch [791], Loss: 160053.140625
Epoch [1/1], Batch [801], Loss: 157053.546875
Epoch [1/1], Batch [811], Loss: 159340.937500
Epoch [1/1], Batch [821], Loss: 153985.968750
Epoch [1/1], Batch [831], Loss: 157039.234375
Epoch [1/1], Batch [841], Loss: 167614.218750
Epoch [1/1], Batch [851], Loss: 156339.656250
Epoch [1/1], Batch [861], Loss: 156098.062500
Epoch [1/1], Batch [871], Loss: 159617.390625
Epoch [1/1], Batch [881], Loss: 156473.656250
Epoch [1/1], Batch [891], Loss: 167469.937500
Epoch [1/1], Batch [901], Loss: 160525.859375
Epoch [1/1], Batch [911], Loss: 152805.093750
Epoch [1/1], Batch [921], Loss: 154091.421875
Epoch [1/1], Batch [931], Loss: 153585.562500
Epoch [1/1], Batch [941], Loss: 165677.437500
Epoch [1/1], Batch [951], Loss: 163034.046875
Epoch [1/1], Batch [961], Loss: 163119.906250
Epoch [1/1], Batch [971], Loss: 161941.343750
Epoch [1/1], Batch [981], Loss: 158535.562500
Epoch [1/1], Batch [991], Loss: 164940.859375
Epoch [1/1], Batch [1001], Loss: 159574.250000
Epoch [1/1], Batch [1011], Loss: 159419.953125
Epoch [1/1], Batch [1021], Loss: 165579.250000
Epoch [1/1], Batch [1031], Loss: 157036.968750
Epoch [1/1], Batch [1041], Loss: 168591.125000
Epoch [1/1], Batch [1051], Loss: 163947.875000
Epoch [1/1], Batch [1061], Loss: 157425.140625
Epoch [1/1], Batch [1071], Loss: 161360.437500
Epoch [1/1], Batch [1081], Loss: 157473.562500
Epoch [1/1], Batch [1091], Loss: 162898.484375
Epoch [1/1], Batch [1101], Loss: 155991.953125
Epoch [1/1], Batch [1111], Loss: 159688.562500
Epoch [1/1], Batch [1121], Loss: 160859.406250
Epoch [1/1], Batch [1131], Loss: 150053.437500
Epoch [1/1], Batch [1141], Loss: 151658.921875
Epoch [1/1], Batch [1151], Loss: 162890.296875
Epoch [1/1], Batch [1161], Loss: 155488.125000
Epoch [1/1], Batch [1171], Loss: 166805.656250
Epoch [1/1], Batch [1181], Loss: 156070.000000
Epoch [1/1], Batch [1191], Loss: 162156.593750
Epoch [1/1], Batch [1201], Loss: 160187.531250
Epoch [1/1], Batch [1211], Loss: 158836.093750
Epoch [1/1], Batch [1221], Loss: 149404.625000
Epoch [1/1], Batch [1231], Loss: 161953.500000
Epoch [1/1], Batch [1241], Loss: 165807.125000
Epoch [1/1], Batch [1251], Loss: 156322.906250
Epoch [1/1], Batch [1261], Loss: 165876.062500
Epoch [1/1], Batch [1271], Loss: 158269.265625
Epoch [1/1], Batch [1281], Loss: 157599.359375
Epoch [1/1], Batch [1291], Loss: 163165.375000
Epoch [1/1], Batch [1301], Loss: 160234.906250
Epoch [1/1], Batch [1311], Loss: 152114.328125
Epoch [1/1], Batch [1321], Loss: 156561.593750
Epoch [1/1], Batch [1331], Loss: 158987.250000
Epoch [1/1], Batch [1341], Loss: 158507.625000
Epoch [1/1], Batch [1351], Loss: 154851.015625
Epoch [1/1], Batch [1361], Loss: 161364.765625
Epoch [1/1], Batch [1371], Loss: 158698.500000
Epoch [1/1], Batch [1381], Loss: 162906.906250
Epoch [1/1], Batch [1391], Loss: 154511.828125
Epoch [1/1], Batch [1401], Loss: 159166.718750
Epoch [1/1], Batch [1411], Loss: 157785.343750
Epoch [1/1], Batch [1421], Loss: 163090.171875
Epoch [1/1], Batch [1431], Loss: 157369.218750
Epoch [1/1], Batch [1441], Loss: 154760.828125
Epoch [1/1], Batch [1451], Loss: 161139.703125
Epoch [1/1], Batch [1461], Loss: 153507.187500
Epoch [1/1], Batch [1471], Loss: 155367.625000
Epoch [1/1], Batch [1481], Loss: 158194.625000
Epoch [1/1], Batch [1491], Loss: 164742.546875
Epoch [1/1], Batch [1501], Loss: 161049.500000
Epoch [1/1], Batch [1511], Loss: 162998.468750
Epoch [1/1], Batch [1521], Loss: 155293.625000
Epoch [1/1], Batch [1531], Loss: 155058.078125
Epoch [1/1], Batch [1541], Loss: 157144.093750
Epoch [1/1], Batch [1551], Loss: 157093.406250
Epoch [1/1], Batch [1561], Loss: 154416.843750
Epoch [1/1], Batch [1571], Loss: 162947.140625
Epoch [1/1], Batch [1581], Loss: 150519.750000
Epoch [1/1], Batch [1591], Loss: 159286.968750
Epoch [1/1], Batch [1601], Loss: 152463.062500
Epoch [1/1], Batch [1611], Loss: 158809.890625
Epoch [1/1], Batch [1621], Loss: 160603.437500
Epoch [1/1], Batch [1631], Loss: 152286.843750
Epoch [1/1], Batch [1641], Loss: 166394.156250
Epoch [1/1], Batch [1651], Loss: 153003.921875
Epoch [1/1], Batch [1661], Loss: 152496.500000
Epoch [1/1], Batch [1671], Loss: 158851.453125
Epoch [1/1], Batch [1681], Loss: 154275.421875
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 158900.7874
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 157408.1647
Elapsed time: 1800.23 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 158569.0503
Elapsed time: 1825.23 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 196336.406250
Epoch [1/1], Batch [11], Loss: 205372.593750
Epoch [1/1], Batch [21], Loss: 207177.703125
Epoch [1/1], Batch [31], Loss: 204213.343750
Epoch [1/1], Batch [41], Loss: 195786.875000
Epoch [1/1], Batch [51], Loss: 204950.843750
Epoch [1/1], Batch [61], Loss: 205025.265625
Epoch [1/1], Batch [71], Loss: 212768.656250
Epoch [1/1], Batch [81], Loss: 206513.125000
Epoch [1/1], Batch [91], Loss: 198430.968750
Epoch [1/1], Batch [101], Loss: 204727.468750
Epoch [1/1], Batch [111], Loss: 212848.125000
Epoch [1/1], Batch [121], Loss: 196869.812500
Epoch [1/1], Batch [131], Loss: 204538.984375
Epoch [1/1], Batch [141], Loss: 199839.718750
Epoch [1/1], Batch [151], Loss: 207129.968750
Epoch [1/1], Batch [161], Loss: 205659.968750
Epoch [1/1], Batch [171], Loss: 200834.390625
Epoch [1/1], Batch [181], Loss: 196061.171875
Epoch [1/1], Batch [191], Loss: 206361.562500
Epoch [1/1], Batch [201], Loss: 202734.390625
Epoch [1/1], Batch [211], Loss: 197304.171875
Epoch [1/1], Batch [221], Loss: 205803.062500
Epoch [1/1], Batch [231], Loss: 208410.625000
Epoch [1/1], Batch [241], Loss: 209719.531250
Epoch [1/1], Batch [251], Loss: 208642.359375
Epoch [1/1], Batch [261], Loss: 195345.906250
Epoch [1/1], Batch [271], Loss: 210831.796875
Epoch [1/1], Batch [281], Loss: 210044.765625
Epoch [1/1], Batch [291], Loss: 199185.812500
Epoch [1/1], Batch [301], Loss: 196749.593750
Epoch [1/1], Batch [311], Loss: 207961.625000
Epoch [1/1], Batch [321], Loss: 202354.171875
Epoch [1/1], Batch [331], Loss: 208901.375000
Epoch [1/1], Batch [341], Loss: 208557.765625
Epoch [1/1], Batch [351], Loss: 193115.593750
Epoch [1/1], Batch [361], Loss: 209311.359375
Epoch [1/1], Batch [371], Loss: 204268.343750
Epoch [1/1], Batch [381], Loss: 207623.937500
Epoch [1/1], Batch [391], Loss: 204368.265625
Epoch [1/1], Batch [401], Loss: 212976.296875
Epoch [1/1], Batch [411], Loss: 209037.718750
Epoch [1/1], Batch [421], Loss: 202839.406250
Epoch [1/1], Batch [431], Loss: 207097.250000
Epoch [1/1], Batch [441], Loss: 201823.703125
Epoch [1/1], Batch [451], Loss: 203325.859375
Epoch [1/1], Batch [461], Loss: 195939.406250
Epoch [1/1], Batch [471], Loss: 200869.078125
Epoch [1/1], Batch [481], Loss: 201544.781250
Epoch [1/1], Batch [491], Loss: 198889.984375
Epoch [1/1], Batch [501], Loss: 205721.093750
Epoch [1/1], Batch [511], Loss: 208957.828125
Epoch [1/1], Batch [521], Loss: 204054.734375
Epoch [1/1], Batch [531], Loss: 198976.890625
Epoch [1/1], Batch [541], Loss: 196153.093750
Epoch [1/1], Batch [551], Loss: 201264.500000
Epoch [1/1], Batch [561], Loss: 202162.265625
Epoch [1/1], Batch [571], Loss: 202046.234375
Epoch [1/1], Batch [581], Loss: 211232.875000
Epoch [1/1], Batch [591], Loss: 213507.843750
Epoch [1/1], Batch [601], Loss: 206005.187500
Epoch [1/1], Batch [611], Loss: 201009.953125
Epoch [1/1], Batch [621], Loss: 206860.468750
Epoch [1/1], Batch [631], Loss: 205474.781250
Epoch [1/1], Batch [641], Loss: 198422.171875
Epoch [1/1], Batch [651], Loss: 205564.906250
Epoch [1/1], Batch [661], Loss: 200916.328125
Epoch [1/1], Batch [671], Loss: 202921.703125
Epoch [1/1], Batch [681], Loss: 196865.125000
Epoch [1/1], Batch [691], Loss: 208373.187500
Epoch [1/1], Batch [701], Loss: 207852.859375
Epoch [1/1], Batch [711], Loss: 202720.906250
Epoch [1/1], Batch [721], Loss: 197497.406250
Epoch [1/1], Batch [731], Loss: 207145.062500
Epoch [1/1], Batch [741], Loss: 199178.359375
Epoch [1/1], Batch [751], Loss: 202615.656250
Epoch [1/1], Batch [761], Loss: 197605.671875
Epoch [1/1], Batch [771], Loss: 202695.296875
Epoch [1/1], Batch [781], Loss: 206555.218750
Epoch [1/1], Batch [791], Loss: 200186.593750
Epoch [1/1], Batch [801], Loss: 204264.187500
Epoch [1/1], Batch [811], Loss: 202235.625000
Epoch [1/1], Batch [821], Loss: 205988.187500
Epoch [1/1], Batch [831], Loss: 214819.562500
Epoch [1/1], Batch [841], Loss: 209858.218750
Epoch [1/1], Batch [851], Loss: 209796.593750
Epoch [1/1], Batch [861], Loss: 203988.781250
Epoch [1/1], Batch [871], Loss: 210900.375000
Epoch [1/1], Batch [881], Loss: 213404.250000
Epoch [1/1], Batch [891], Loss: 200295.906250
Epoch [1/1], Batch [901], Loss: 208808.359375
Epoch [1/1], Batch [911], Loss: 209655.531250
Epoch [1/1], Batch [921], Loss: 202315.750000
Epoch [1/1], Batch [931], Loss: 204033.718750
Epoch [1/1], Batch [941], Loss: 205777.859375
Epoch [1/1], Batch [951], Loss: 200440.437500
Epoch [1/1], Batch [961], Loss: 206171.375000
Epoch [1/1], Batch [971], Loss: 204261.875000
Epoch [1/1], Batch [981], Loss: 206556.875000
Epoch [1/1], Batch [991], Loss: 196795.000000
Epoch [1/1], Batch [1001], Loss: 208032.156250
Epoch [1/1], Batch [1011], Loss: 206706.781250
Epoch [1/1], Batch [1021], Loss: 201776.187500
Epoch [1/1], Batch [1031], Loss: 199113.406250
Epoch [1/1], Batch [1041], Loss: 207627.062500
Epoch [1/1], Batch [1051], Loss: 198381.359375
Epoch [1/1], Batch [1061], Loss: 207668.500000
Epoch [1/1], Batch [1071], Loss: 203108.828125
Epoch [1/1], Batch [1081], Loss: 204261.656250
Epoch [1/1], Batch [1091], Loss: 204586.218750
Epoch [1/1], Batch [1101], Loss: 205032.703125
Epoch [1/1], Batch [1111], Loss: 203035.781250
Epoch [1/1], Batch [1121], Loss: 204485.250000
Epoch [1/1], Batch [1131], Loss: 199228.343750
Epoch [1/1], Batch [1141], Loss: 207196.671875
Epoch [1/1], Batch [1151], Loss: 208393.187500
Epoch [1/1], Batch [1161], Loss: 203507.312500
Epoch [1/1], Batch [1171], Loss: 207917.640625
Epoch [1/1], Batch [1181], Loss: 208416.546875
Epoch [1/1], Batch [1191], Loss: 196575.859375
Epoch [1/1], Batch [1201], Loss: 202170.765625
Epoch [1/1], Batch [1211], Loss: 207820.968750
Epoch [1/1], Batch [1221], Loss: 202706.687500
Epoch [1/1], Batch [1231], Loss: 201669.125000
Epoch [1/1], Batch [1241], Loss: 210729.546875
Epoch [1/1], Batch [1251], Loss: 205043.328125
Epoch [1/1], Batch [1261], Loss: 206304.125000
Epoch [1/1], Batch [1271], Loss: 195148.687500
Epoch [1/1], Batch [1281], Loss: 197286.281250
Epoch [1/1], Batch [1291], Loss: 206294.375000
Epoch [1/1], Batch [1301], Loss: 206527.218750
Epoch [1/1], Batch [1311], Loss: 202196.125000
Epoch [1/1], Batch [1321], Loss: 206984.125000
Epoch [1/1], Batch [1331], Loss: 196277.000000
Epoch [1/1], Batch [1341], Loss: 198459.843750
Epoch [1/1], Batch [1351], Loss: 196024.062500
Epoch [1/1], Batch [1361], Loss: 208242.937500
Epoch [1/1], Batch [1371], Loss: 209534.562500
Epoch [1/1], Batch [1381], Loss: 207960.093750
Epoch [1/1], Batch [1391], Loss: 202011.406250
Epoch [1/1], Batch [1401], Loss: 208404.031250
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 204253.3549
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 200045.9986
Elapsed time: 2526.66 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 201461.8336
Elapsed time: 2551.89 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 250430.687500
Epoch [1/1], Batch [11], Loss: 243272.812500
Epoch [1/1], Batch [21], Loss: 256784.515625
Epoch [1/1], Batch [31], Loss: 243792.578125
Epoch [1/1], Batch [41], Loss: 242707.578125
Epoch [1/1], Batch [51], Loss: 253985.437500
Epoch [1/1], Batch [61], Loss: 242435.359375
Epoch [1/1], Batch [71], Loss: 254953.390625
Epoch [1/1], Batch [81], Loss: 248855.687500
Epoch [1/1], Batch [91], Loss: 251239.687500
Epoch [1/1], Batch [101], Loss: 252190.968750
Epoch [1/1], Batch [111], Loss: 250657.125000
Epoch [1/1], Batch [121], Loss: 248035.093750
Epoch [1/1], Batch [131], Loss: 246948.218750
Epoch [1/1], Batch [141], Loss: 251247.781250
Epoch [1/1], Batch [151], Loss: 253395.734375
Epoch [1/1], Batch [161], Loss: 253007.437500
Epoch [1/1], Batch [171], Loss: 241491.000000
Epoch [1/1], Batch [181], Loss: 248608.531250
Epoch [1/1], Batch [191], Loss: 246754.281250
Epoch [1/1], Batch [201], Loss: 236206.421875
Epoch [1/1], Batch [211], Loss: 245047.781250
Epoch [1/1], Batch [221], Loss: 243764.531250
Epoch [1/1], Batch [231], Loss: 248390.531250
Epoch [1/1], Batch [241], Loss: 248702.203125
Epoch [1/1], Batch [251], Loss: 254207.031250
Epoch [1/1], Batch [261], Loss: 242174.859375
Epoch [1/1], Batch [271], Loss: 243021.593750
Epoch [1/1], Batch [281], Loss: 252586.156250
Epoch [1/1], Batch [291], Loss: 247444.671875
Epoch [1/1], Batch [301], Loss: 250014.000000
Epoch [1/1], Batch [311], Loss: 258625.000000
Epoch [1/1], Batch [321], Loss: 247913.546875
Epoch [1/1], Batch [331], Loss: 238747.843750
Epoch [1/1], Batch [341], Loss: 254249.750000
Epoch [1/1], Batch [351], Loss: 245857.312500
Epoch [1/1], Batch [361], Loss: 261102.500000
Epoch [1/1], Batch [371], Loss: 256095.031250
Epoch [1/1], Batch [381], Loss: 247499.828125
Epoch [1/1], Batch [391], Loss: 239557.640625
Epoch [1/1], Batch [401], Loss: 253218.218750
Epoch [1/1], Batch [411], Loss: 248201.500000
Epoch [1/1], Batch [421], Loss: 251244.906250
Epoch [1/1], Batch [431], Loss: 237444.921875
Epoch [1/1], Batch [441], Loss: 255181.406250
Epoch [1/1], Batch [451], Loss: 251707.281250
Epoch [1/1], Batch [461], Loss: 254667.187500
Epoch [1/1], Batch [471], Loss: 247970.687500
Epoch [1/1], Batch [481], Loss: 252884.312500
Epoch [1/1], Batch [491], Loss: 247844.531250
Epoch [1/1], Batch [501], Loss: 237788.531250
Epoch [1/1], Batch [511], Loss: 245025.593750
Epoch [1/1], Batch [521], Loss: 251164.531250
Epoch [1/1], Batch [531], Loss: 251642.187500
Epoch [1/1], Batch [541], Loss: 243167.781250
Epoch [1/1], Batch [551], Loss: 239965.312500
Epoch [1/1], Batch [561], Loss: 246362.781250
Epoch [1/1], Batch [571], Loss: 247338.375000
Epoch [1/1], Batch [581], Loss: 247722.218750
Epoch [1/1], Batch [591], Loss: 246190.734375
Epoch [1/1], Batch [601], Loss: 248526.593750
Epoch [1/1], Batch [611], Loss: 248274.859375
Epoch [1/1], Batch [621], Loss: 246418.812500
Epoch [1/1], Batch [631], Loss: 255532.125000
Epoch [1/1], Batch [641], Loss: 255880.812500
Epoch [1/1], Batch [651], Loss: 245154.093750
Epoch [1/1], Batch [661], Loss: 249039.484375
Epoch [1/1], Batch [671], Loss: 250767.281250
Epoch [1/1], Batch [681], Loss: 248414.843750
Epoch [1/1], Batch [691], Loss: 248142.312500
Epoch [1/1], Batch [701], Loss: 245753.437500
Epoch [1/1], Batch [711], Loss: 238190.171875
Epoch [1/1], Batch [721], Loss: 242198.125000
Epoch [1/1], Batch [731], Loss: 247155.062500
Epoch [1/1], Batch [741], Loss: 252540.734375
Epoch [1/1], Batch [751], Loss: 244002.593750
Epoch [1/1], Batch [761], Loss: 249883.343750
Epoch [1/1], Batch [771], Loss: 249848.562500
Epoch [1/1], Batch [781], Loss: 239320.750000
Epoch [1/1], Batch [791], Loss: 233518.125000
Epoch [1/1], Batch [801], Loss: 246001.203125
Epoch [1/1], Batch [811], Loss: 248545.125000
Epoch [1/1], Batch [821], Loss: 234359.281250
Epoch [1/1], Batch [831], Loss: 245870.328125
Epoch [1/1], Batch [841], Loss: 243895.890625
Epoch [1/1], Batch [851], Loss: 253135.906250
Epoch [1/1], Batch [861], Loss: 259546.750000
Epoch [1/1], Batch [871], Loss: 244925.937500
Epoch [1/1], Batch [881], Loss: 247251.468750
Epoch [1/1], Batch [891], Loss: 247500.062500
Epoch [1/1], Batch [901], Loss: 244898.375000
Epoch [1/1], Batch [911], Loss: 246050.500000
Epoch [1/1], Batch [921], Loss: 241370.046875
Epoch [1/1], Batch [931], Loss: 243708.343750
Epoch [1/1], Batch [941], Loss: 251045.281250
Epoch [1/1], Batch [951], Loss: 241584.562500
Epoch [1/1], Batch [961], Loss: 255161.875000
Epoch [1/1], Batch [971], Loss: 243059.031250
Epoch [1/1], Batch [981], Loss: 243740.140625
Epoch [1/1], Batch [991], Loss: 250420.140625
Epoch [1/1], Batch [1001], Loss: 250916.703125
Epoch [1/1], Batch [1011], Loss: 255613.656250
Epoch [1/1], Batch [1021], Loss: 244593.218750
Epoch [1/1], Batch [1031], Loss: 246713.734375
Epoch [1/1], Batch [1041], Loss: 246294.593750
Epoch [1/1], Batch [1051], Loss: 250611.437500
Epoch [1/1], Batch [1061], Loss: 254990.375000
Epoch [1/1], Batch [1071], Loss: 245082.421875
Epoch [1/1], Batch [1081], Loss: 253283.687500
Epoch [1/1], Batch [1091], Loss: 234732.562500
Epoch [1/1], Batch [1101], Loss: 239702.812500
Epoch [1/1], Batch [1111], Loss: 256859.843750
Epoch [1/1], Batch [1121], Loss: 235159.718750
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 247000.7600
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 243848.0314
Elapsed time: 3217.99 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 245860.6038
Elapsed time: 3241.60 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 294820.250000
Epoch [1/1], Batch [11], Loss: 279915.656250
Epoch [1/1], Batch [21], Loss: 283708.812500
Epoch [1/1], Batch [31], Loss: 281090.687500
Epoch [1/1], Batch [41], Loss: 291786.093750
Epoch [1/1], Batch [51], Loss: 300606.937500
Epoch [1/1], Batch [61], Loss: 292194.562500
Epoch [1/1], Batch [71], Loss: 298686.187500
Epoch [1/1], Batch [81], Loss: 302579.812500
Epoch [1/1], Batch [91], Loss: 292807.656250
Epoch [1/1], Batch [101], Loss: 287840.093750
Epoch [1/1], Batch [111], Loss: 301037.218750
Epoch [1/1], Batch [121], Loss: 289988.562500
Epoch [1/1], Batch [131], Loss: 288257.437500
Epoch [1/1], Batch [141], Loss: 292439.625000
Epoch [1/1], Batch [151], Loss: 290946.281250
Epoch [1/1], Batch [161], Loss: 288932.437500
Epoch [1/1], Batch [171], Loss: 295715.875000
Epoch [1/1], Batch [181], Loss: 283509.031250
Epoch [1/1], Batch [191], Loss: 303364.250000
Epoch [1/1], Batch [201], Loss: 301146.625000
Epoch [1/1], Batch [211], Loss: 278255.750000
Epoch [1/1], Batch [221], Loss: 284069.500000
Epoch [1/1], Batch [231], Loss: 294363.437500
Epoch [1/1], Batch [241], Loss: 276362.875000
Epoch [1/1], Batch [251], Loss: 281166.781250
Epoch [1/1], Batch [261], Loss: 292006.312500
Epoch [1/1], Batch [271], Loss: 292729.562500
Epoch [1/1], Batch [281], Loss: 294290.812500
Epoch [1/1], Batch [291], Loss: 282706.375000
Epoch [1/1], Batch [301], Loss: 277370.281250
Epoch [1/1], Batch [311], Loss: 299125.468750
Epoch [1/1], Batch [321], Loss: 293245.500000
Epoch [1/1], Batch [331], Loss: 292850.437500
Epoch [1/1], Batch [341], Loss: 283515.062500
Epoch [1/1], Batch [351], Loss: 285240.437500
Epoch [1/1], Batch [361], Loss: 294900.468750
Epoch [1/1], Batch [371], Loss: 290232.656250
Epoch [1/1], Batch [381], Loss: 283720.562500
Epoch [1/1], Batch [391], Loss: 288310.187500
Epoch [1/1], Batch [401], Loss: 288125.312500
Epoch [1/1], Batch [411], Loss: 283998.875000
Epoch [1/1], Batch [421], Loss: 283623.812500
Epoch [1/1], Batch [431], Loss: 290082.562500
Epoch [1/1], Batch [441], Loss: 299888.656250
Epoch [1/1], Batch [451], Loss: 289256.125000
Epoch [1/1], Batch [461], Loss: 283251.562500
Epoch [1/1], Batch [471], Loss: 291586.312500
Epoch [1/1], Batch [481], Loss: 285329.218750
Epoch [1/1], Batch [491], Loss: 286277.406250
Epoch [1/1], Batch [501], Loss: 273947.750000
Epoch [1/1], Batch [511], Loss: 288257.625000
Epoch [1/1], Batch [521], Loss: 289669.625000
Epoch [1/1], Batch [531], Loss: 284732.875000
Epoch [1/1], Batch [541], Loss: 288147.062500
Epoch [1/1], Batch [551], Loss: 279000.062500
Epoch [1/1], Batch [561], Loss: 293942.343750
Epoch [1/1], Batch [571], Loss: 290436.718750
Epoch [1/1], Batch [581], Loss: 279282.687500
Epoch [1/1], Batch [591], Loss: 295081.875000
Epoch [1/1], Batch [601], Loss: 290036.062500
Epoch [1/1], Batch [611], Loss: 290963.562500
Epoch [1/1], Batch [621], Loss: 291889.687500
Epoch [1/1], Batch [631], Loss: 288921.562500
Epoch [1/1], Batch [641], Loss: 280605.062500
Epoch [1/1], Batch [651], Loss: 279164.406250
Epoch [1/1], Batch [661], Loss: 282130.156250
Epoch [1/1], Batch [671], Loss: 286585.750000
Epoch [1/1], Batch [681], Loss: 294617.937500
Epoch [1/1], Batch [691], Loss: 305119.531250
Epoch [1/1], Batch [701], Loss: 294954.937500
Epoch [1/1], Batch [711], Loss: 287144.187500
Epoch [1/1], Batch [721], Loss: 282319.406250
Epoch [1/1], Batch [731], Loss: 290087.437500
Epoch [1/1], Batch [741], Loss: 286839.875000
Epoch [1/1], Batch [751], Loss: 285844.375000
Epoch [1/1], Batch [761], Loss: 283989.875000
Epoch [1/1], Batch [771], Loss: 286106.781250
Epoch [1/1], Batch [781], Loss: 292226.781250
Epoch [1/1], Batch [791], Loss: 295472.156250
Epoch [1/1], Batch [801], Loss: 284144.812500
Epoch [1/1], Batch [811], Loss: 299747.531250
Epoch [1/1], Batch [821], Loss: 286589.625000
Epoch [1/1], Batch [831], Loss: 290309.500000
Epoch [1/1], Batch [841], Loss: 297551.937500
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 288577.2903
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 282211.9304
Elapsed time: 3823.38 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 283133.1170
Elapsed time: 3843.74 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 321616.937500
Epoch [1/1], Batch [11], Loss: 330039.375000
Epoch [1/1], Batch [21], Loss: 325313.156250
Epoch [1/1], Batch [31], Loss: 322629.593750
Epoch [1/1], Batch [41], Loss: 345212.093750
Epoch [1/1], Batch [51], Loss: 318455.812500
Epoch [1/1], Batch [61], Loss: 331533.375000
Epoch [1/1], Batch [71], Loss: 345859.562500
Epoch [1/1], Batch [81], Loss: 323851.906250
Epoch [1/1], Batch [91], Loss: 331124.312500
Epoch [1/1], Batch [101], Loss: 330318.031250
Epoch [1/1], Batch [111], Loss: 329122.937500
Epoch [1/1], Batch [121], Loss: 347827.187500
Epoch [1/1], Batch [131], Loss: 321158.625000
Epoch [1/1], Batch [141], Loss: 336522.750000
Epoch [1/1], Batch [151], Loss: 319792.437500
Epoch [1/1], Batch [161], Loss: 325637.000000
Epoch [1/1], Batch [171], Loss: 333841.500000
Epoch [1/1], Batch [181], Loss: 333096.500000
Epoch [1/1], Batch [191], Loss: 317407.156250
Epoch [1/1], Batch [201], Loss: 328221.687500
Epoch [1/1], Batch [211], Loss: 326180.187500
Epoch [1/1], Batch [221], Loss: 339505.687500
Epoch [1/1], Batch [231], Loss: 322821.437500
Epoch [1/1], Batch [241], Loss: 335287.250000
Epoch [1/1], Batch [251], Loss: 330293.250000
Epoch [1/1], Batch [261], Loss: 322399.312500
Epoch [1/1], Batch [271], Loss: 344135.812500
Epoch [1/1], Batch [281], Loss: 327779.687500
Epoch [1/1], Batch [291], Loss: 336595.875000
Epoch [1/1], Batch [301], Loss: 321829.125000
Epoch [1/1], Batch [311], Loss: 324963.593750
Epoch [1/1], Batch [321], Loss: 330982.468750
Epoch [1/1], Batch [331], Loss: 337694.937500
Epoch [1/1], Batch [341], Loss: 331039.625000
Epoch [1/1], Batch [351], Loss: 333452.281250
Epoch [1/1], Batch [361], Loss: 333627.687500
Epoch [1/1], Batch [371], Loss: 323928.437500
Epoch [1/1], Batch [381], Loss: 332847.375000
Epoch [1/1], Batch [391], Loss: 351153.187500
Epoch [1/1], Batch [401], Loss: 326236.031250
Epoch [1/1], Batch [411], Loss: 332666.156250
Epoch [1/1], Batch [421], Loss: 341266.093750
Epoch [1/1], Batch [431], Loss: 320382.187500
Epoch [1/1], Batch [441], Loss: 339404.250000
Epoch [1/1], Batch [451], Loss: 325963.875000
Epoch [1/1], Batch [461], Loss: 334332.000000
Epoch [1/1], Batch [471], Loss: 338867.218750
Epoch [1/1], Batch [481], Loss: 328552.906250
Epoch [1/1], Batch [491], Loss: 330586.812500
Epoch [1/1], Batch [501], Loss: 317722.406250
Epoch [1/1], Batch [511], Loss: 336722.687500
Epoch [1/1], Batch [521], Loss: 329752.375000
Epoch [1/1], Batch [531], Loss: 327515.468750
Epoch [1/1], Batch [541], Loss: 325988.687500
Epoch [1/1], Batch [551], Loss: 349744.812500
Epoch [1/1], Batch [561], Loss: 330576.250000
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 330154.9023
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 321837.7619
Elapsed time: 4334.62 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 323668.3916
Elapsed time: 4349.97 seconds

Training complete!
Totoal elapsed time: 4349.97 seconds
CUDA is available!
