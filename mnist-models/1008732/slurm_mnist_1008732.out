Starting job 1008732
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = True,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 332432.437500
Epoch [1/1], Batch [11], Loss: 123353.375000
Epoch [1/1], Batch [21], Loss: 100563.562500
Epoch [1/1], Batch [31], Loss: 88373.687500
Epoch [1/1], Batch [41], Loss: 75259.000000
Epoch [1/1], Batch [51], Loss: 66548.546875
Epoch [1/1], Batch [61], Loss: 60239.027344
Epoch [1/1], Batch [71], Loss: 57627.324219
Epoch [1/1], Batch [81], Loss: 50637.554688
Epoch [1/1], Batch [91], Loss: 53534.089844
Epoch [1/1], Batch [101], Loss: 50612.625000
Epoch [1/1], Batch [111], Loss: 51017.867188
Epoch [1/1], Batch [121], Loss: 51069.242188
Epoch [1/1], Batch [131], Loss: 50334.515625
Epoch [1/1], Batch [141], Loss: 48354.273438
Epoch [1/1], Batch [151], Loss: 48575.468750
Epoch [1/1], Batch [161], Loss: 45591.281250
Epoch [1/1], Batch [171], Loss: 50417.929688
Epoch [1/1], Batch [181], Loss: 45584.824219
Epoch [1/1], Batch [191], Loss: 49925.617188
Epoch [1/1], Batch [201], Loss: 47464.507812
Epoch [1/1], Batch [211], Loss: 47575.273438
Epoch [1/1], Batch [221], Loss: 47044.718750
Epoch [1/1], Batch [231], Loss: 45233.652344
Epoch [1/1], Batch [241], Loss: 46030.886719
Epoch [1/1], Batch [251], Loss: 43667.808594
Epoch [1/1], Batch [261], Loss: 44651.097656
Epoch [1/1], Batch [271], Loss: 44815.914062
Epoch [1/1], Batch [281], Loss: 43821.859375
Epoch [1/1], Batch [291], Loss: 45392.148438
Epoch [1/1], Batch [301], Loss: 44617.687500
Epoch [1/1], Batch [311], Loss: 45288.648438
Epoch [1/1], Batch [321], Loss: 43438.007812
Epoch [1/1], Batch [331], Loss: 45946.882812
Epoch [1/1], Batch [341], Loss: 43404.519531
Epoch [1/1], Batch [351], Loss: 41988.539062
Epoch [1/1], Batch [361], Loss: 43948.761719
Epoch [1/1], Batch [371], Loss: 44234.933594
Epoch [1/1], Batch [381], Loss: 41303.492188
Epoch [1/1], Batch [391], Loss: 42347.574219
Epoch [1/1], Batch [401], Loss: 41162.132812
Epoch [1/1], Batch [411], Loss: 44320.492188
Epoch [1/1], Batch [421], Loss: 43262.800781
Epoch [1/1], Batch [431], Loss: 42717.257812
Epoch [1/1], Batch [441], Loss: 42880.921875
Epoch [1/1], Batch [451], Loss: 41802.750000
Epoch [1/1], Batch [461], Loss: 42710.937500
Epoch [1/1], Batch [471], Loss: 42262.265625
Epoch [1/1], Batch [481], Loss: 42066.472656
Epoch [1/1], Batch [491], Loss: 41501.039062
Epoch [1/1], Batch [501], Loss: 42681.820312
Epoch [1/1], Batch [511], Loss: 44533.843750
Epoch [1/1], Batch [521], Loss: 44066.921875
Epoch [1/1], Batch [531], Loss: 42662.523438
Epoch [1/1], Batch [541], Loss: 42559.695312
Epoch [1/1], Batch [551], Loss: 43085.343750
Epoch [1/1], Batch [561], Loss: 42834.109375
Epoch [1/1], Batch [571], Loss: 41605.753906
Epoch [1/1], Batch [581], Loss: 41430.250000
Epoch [1/1], Batch [591], Loss: 42685.828125
Epoch [1/1], Batch [601], Loss: 42390.335938
Epoch [1/1], Batch [611], Loss: 42082.632812
Epoch [1/1], Batch [621], Loss: 39909.132812
Epoch [1/1], Batch [631], Loss: 43269.191406
Epoch [1/1], Batch [641], Loss: 42966.492188
Epoch [1/1], Batch [651], Loss: 40727.312500
Epoch [1/1], Batch [661], Loss: 40703.906250
Epoch [1/1], Batch [671], Loss: 39299.625000
Epoch [1/1], Batch [681], Loss: 39431.328125
Epoch [1/1], Batch [691], Loss: 42603.078125
Epoch [1/1], Batch [701], Loss: 40794.535156
Epoch [1/1], Batch [711], Loss: 43582.898438
Epoch [1/1], Batch [721], Loss: 40079.761719
Epoch [1/1], Batch [731], Loss: 39320.003906
Epoch [1/1], Batch [741], Loss: 40039.125000
Epoch [1/1], Batch [751], Loss: 41134.843750
Epoch [1/1], Batch [761], Loss: 41506.070312
Epoch [1/1], Batch [771], Loss: 39462.554688
Epoch [1/1], Batch [781], Loss: 39789.472656
Epoch [1/1], Batch [791], Loss: 41347.457031
Epoch [1/1], Batch [801], Loss: 40279.820312
Epoch [1/1], Batch [811], Loss: 40760.625000
Epoch [1/1], Batch [821], Loss: 40693.996094
Epoch [1/1], Batch [831], Loss: 43046.875000
Epoch [1/1], Batch [841], Loss: 39480.386719
Epoch [1/1], Batch [851], Loss: 41704.281250
Epoch [1/1], Batch [861], Loss: 40069.746094
Epoch [1/1], Batch [871], Loss: 39349.871094
Epoch [1/1], Batch [881], Loss: 41881.578125
Epoch [1/1], Batch [891], Loss: 41172.468750
Epoch [1/1], Batch [901], Loss: 38649.628906
Epoch [1/1], Batch [911], Loss: 40236.640625
Epoch [1/1], Batch [921], Loss: 41194.851562
Epoch [1/1], Batch [931], Loss: 39984.976562
Epoch [1/1], Batch [941], Loss: 41162.718750
Epoch [1/1], Batch [951], Loss: 40896.902344
Epoch [1/1], Batch [961], Loss: 39120.820312
Epoch [1/1], Batch [971], Loss: 38192.269531
Epoch [1/1], Batch [981], Loss: 41160.000000
Epoch [1/1], Batch [991], Loss: 39187.031250
Epoch [1/1], Batch [1001], Loss: 39490.417969
Epoch [1/1], Batch [1011], Loss: 38142.195312
Epoch [1/1], Batch [1021], Loss: 36338.125000
Epoch [1/1], Batch [1031], Loss: 39206.343750
Epoch [1/1], Batch [1041], Loss: 39462.488281
Epoch [1/1], Batch [1051], Loss: 39403.847656
Epoch [1/1], Batch [1061], Loss: 39893.417969
Epoch [1/1], Batch [1071], Loss: 41658.265625
Epoch [1/1], Batch [1081], Loss: 40152.609375
Epoch [1/1], Batch [1091], Loss: 41633.460938
Epoch [1/1], Batch [1101], Loss: 39225.054688
Epoch [1/1], Batch [1111], Loss: 39028.792969
Epoch [1/1], Batch [1121], Loss: 41794.234375
Epoch [1/1], Batch [1131], Loss: 40626.539062
Epoch [1/1], Batch [1141], Loss: 38359.835938
Epoch [1/1], Batch [1151], Loss: 37441.070312
Epoch [1/1], Batch [1161], Loss: 38700.492188
Epoch [1/1], Batch [1171], Loss: 39418.437500
Epoch [1/1], Batch [1181], Loss: 39863.125000
Epoch [1/1], Batch [1191], Loss: 40249.289062
Epoch [1/1], Batch [1201], Loss: 38633.859375
Epoch [1/1], Batch [1211], Loss: 38781.781250
Epoch [1/1], Batch [1221], Loss: 38439.468750
Epoch [1/1], Batch [1231], Loss: 39098.429688
Epoch [1/1], Batch [1241], Loss: 39555.667969
Epoch [1/1], Batch [1251], Loss: 39645.894531
Epoch [1/1], Batch [1261], Loss: 39616.929688
Epoch [1/1], Batch [1271], Loss: 39787.355469
Epoch [1/1], Batch [1281], Loss: 38767.433594
Epoch [1/1], Batch [1291], Loss: 39683.710938
Epoch [1/1], Batch [1301], Loss: 39572.257812
Epoch [1/1], Batch [1311], Loss: 39914.460938
Epoch [1/1], Batch [1321], Loss: 39003.781250
Epoch [1/1], Batch [1331], Loss: 38301.640625
Epoch [1/1], Batch [1341], Loss: 36718.648438
Epoch [1/1], Batch [1351], Loss: 38107.468750
Epoch [1/1], Batch [1361], Loss: 37663.898438
Epoch [1/1], Batch [1371], Loss: 40380.722656
Epoch [1/1], Batch [1381], Loss: 40586.109375
Epoch [1/1], Batch [1391], Loss: 38743.179688
Epoch [1/1], Batch [1401], Loss: 38983.218750
Epoch [1/1], Batch [1411], Loss: 39459.265625
Epoch [1/1], Batch [1421], Loss: 39518.632812
Epoch [1/1], Batch [1431], Loss: 38695.929688
Epoch [1/1], Batch [1441], Loss: 36470.890625
Epoch [1/1], Batch [1451], Loss: 39517.390625
Epoch [1/1], Batch [1461], Loss: 38889.992188
Epoch [1/1], Batch [1471], Loss: 40008.898438
Epoch [1/1], Batch [1481], Loss: 39098.921875
Epoch [1/1], Batch [1491], Loss: 38846.234375
Epoch [1/1], Batch [1501], Loss: 39701.757812
Epoch [1/1], Batch [1511], Loss: 38268.671875
Epoch [1/1], Batch [1521], Loss: 39738.761719
Epoch [1/1], Batch [1531], Loss: 36819.445312
Epoch [1/1], Batch [1541], Loss: 38831.406250
Epoch [1/1], Batch [1551], Loss: 39216.453125
Epoch [1/1], Batch [1561], Loss: 36418.132812
Epoch [1/1], Batch [1571], Loss: 38107.007812
Epoch [1/1], Batch [1581], Loss: 38341.390625
Epoch [1/1], Batch [1591], Loss: 39149.164062
Epoch [1/1], Batch [1601], Loss: 39404.835938
Epoch [1/1], Batch [1611], Loss: 38634.253906
Epoch [1/1], Batch [1621], Loss: 39228.007812
Epoch [1/1], Batch [1631], Loss: 38081.195312
Epoch [1/1], Batch [1641], Loss: 37850.726562
Epoch [1/1], Batch [1651], Loss: 37870.898438
Epoch [1/1], Batch [1661], Loss: 36991.320312
Epoch [1/1], Batch [1671], Loss: 37323.222656
Epoch [1/1], Batch [1681], Loss: 38918.371094
Epoch [1/1], Batch [1691], Loss: 38307.074219
Epoch [1/1], Batch [1701], Loss: 36031.949219
Epoch [1/1], Batch [1711], Loss: 36695.039062
Epoch [1/1], Batch [1721], Loss: 38712.593750
Epoch [1/1], Batch [1731], Loss: 38035.660156
Epoch [1/1], Batch [1741], Loss: 36749.593750
Epoch [1/1], Batch [1751], Loss: 37627.062500
Epoch [1/1], Batch [1761], Loss: 38672.355469
Epoch [1/1], Batch [1771], Loss: 37188.117188
Epoch [1/1], Batch [1781], Loss: 37655.593750
Epoch [1/1], Batch [1791], Loss: 38754.453125
Epoch [1/1], Batch [1801], Loss: 36529.496094
Epoch [1/1], Batch [1811], Loss: 38550.363281
Epoch [1/1], Batch [1821], Loss: 39115.316406
Epoch [1/1], Batch [1831], Loss: 38218.648438
Epoch [1/1], Batch [1841], Loss: 37623.773438
Epoch [1/1], Batch [1851], Loss: 37981.933594
Epoch [1/1], Batch [1861], Loss: 39625.324219
Epoch [1/1], Batch [1871], Loss: 37808.710938
Epoch [1/1], Batch [1881], Loss: 36436.832031
Epoch [1/1], Batch [1891], Loss: 36611.898438
Epoch [1/1], Batch [1901], Loss: 39127.203125
Epoch [1/1], Batch [1911], Loss: 36324.015625
Epoch [1/1], Batch [1921], Loss: 38198.718750
Epoch [1/1], Batch [1931], Loss: 38103.796875
Epoch [1/1], Batch [1941], Loss: 38786.085938
Epoch [1/1], Batch [1951], Loss: 38638.773438
Epoch [1/1], Batch [1961], Loss: 37227.164062
Epoch [1/1], Batch [1971], Loss: 35359.507812
Epoch [1/1], Batch [1981], Loss: 37380.515625
Epoch [1/1], Batch [1991], Loss: 37726.679688
Epoch [1/1], Batch [2001], Loss: 36796.597656
Epoch [1/1], Batch [2011], Loss: 36677.335938
Epoch [1/1], Batch [2021], Loss: 39286.210938
Epoch [1/1], Batch [2031], Loss: 38077.507812
Epoch [1/1], Batch [2041], Loss: 37931.718750
Epoch [1/1], Batch [2051], Loss: 36798.910156
Epoch [1/1], Batch [2061], Loss: 36959.785156
Epoch [1/1], Batch [2071], Loss: 37514.226562
Epoch [1/1], Batch [2081], Loss: 36390.789062
Epoch [1/1], Batch [2091], Loss: 37406.289062
Epoch [1/1], Batch [2101], Loss: 37101.839844
Epoch [1/1], Batch [2111], Loss: 35600.164062
Epoch [1/1], Batch [2121], Loss: 36978.062500
Epoch [1/1], Batch [2131], Loss: 37188.820312
Epoch [1/1], Batch [2141], Loss: 37159.617188
Epoch [1/1], Batch [2151], Loss: 38077.960938
Epoch [1/1], Batch [2161], Loss: 38216.421875
Epoch [1/1], Batch [2171], Loss: 36037.726562
Epoch [1/1], Batch [2181], Loss: 37648.832031
Epoch [1/1], Batch [2191], Loss: 37347.585938
Epoch [1/1], Batch [2201], Loss: 36523.601562
Epoch [1/1], Batch [2211], Loss: 36249.472656
Epoch [1/1], Batch [2221], Loss: 38088.390625
Epoch [1/1], Batch [2231], Loss: 36916.859375
Epoch [1/1], Batch [2241], Loss: 36891.328125
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 41976.3646
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 36661.6973
Elapsed time: 489.56 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 36997.1316
Elapsed time: 510.37 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 69992.570312
Epoch [1/1], Batch [11], Loss: 75931.750000
Epoch [1/1], Batch [21], Loss: 71954.734375
Epoch [1/1], Batch [31], Loss: 69631.351562
Epoch [1/1], Batch [41], Loss: 63949.371094
Epoch [1/1], Batch [51], Loss: 64832.066406
Epoch [1/1], Batch [61], Loss: 63177.750000
Epoch [1/1], Batch [71], Loss: 62654.691406
Epoch [1/1], Batch [81], Loss: 63299.839844
Epoch [1/1], Batch [91], Loss: 63799.152344
Epoch [1/1], Batch [101], Loss: 60621.550781
Epoch [1/1], Batch [111], Loss: 66165.710938
Epoch [1/1], Batch [121], Loss: 63512.656250
Epoch [1/1], Batch [131], Loss: 59592.792969
Epoch [1/1], Batch [141], Loss: 58878.781250
Epoch [1/1], Batch [151], Loss: 59643.640625
Epoch [1/1], Batch [161], Loss: 61972.453125
Epoch [1/1], Batch [171], Loss: 60712.320312
Epoch [1/1], Batch [181], Loss: 61482.910156
Epoch [1/1], Batch [191], Loss: 60272.859375
Epoch [1/1], Batch [201], Loss: 58393.566406
Epoch [1/1], Batch [211], Loss: 55969.375000
Epoch [1/1], Batch [221], Loss: 60430.503906
Epoch [1/1], Batch [231], Loss: 62214.406250
Epoch [1/1], Batch [241], Loss: 59451.460938
Epoch [1/1], Batch [251], Loss: 57633.320312
Epoch [1/1], Batch [261], Loss: 61325.062500
Epoch [1/1], Batch [271], Loss: 58666.628906
Epoch [1/1], Batch [281], Loss: 61114.003906
Epoch [1/1], Batch [291], Loss: 59942.648438
Epoch [1/1], Batch [301], Loss: 58882.492188
Epoch [1/1], Batch [311], Loss: 62228.351562
Epoch [1/1], Batch [321], Loss: 60013.851562
Epoch [1/1], Batch [331], Loss: 56576.011719
Epoch [1/1], Batch [341], Loss: 59809.679688
Epoch [1/1], Batch [351], Loss: 57524.257812
Epoch [1/1], Batch [361], Loss: 57579.257812
Epoch [1/1], Batch [371], Loss: 58448.890625
Epoch [1/1], Batch [381], Loss: 57701.636719
Epoch [1/1], Batch [391], Loss: 61117.484375
Epoch [1/1], Batch [401], Loss: 58512.878906
Epoch [1/1], Batch [411], Loss: 57515.796875
Epoch [1/1], Batch [421], Loss: 58829.234375
Epoch [1/1], Batch [431], Loss: 57450.730469
Epoch [1/1], Batch [441], Loss: 57670.132812
Epoch [1/1], Batch [451], Loss: 55727.085938
Epoch [1/1], Batch [461], Loss: 57612.535156
Epoch [1/1], Batch [471], Loss: 60097.406250
Epoch [1/1], Batch [481], Loss: 56619.304688
Epoch [1/1], Batch [491], Loss: 57634.363281
Epoch [1/1], Batch [501], Loss: 54401.433594
Epoch [1/1], Batch [511], Loss: 60323.187500
Epoch [1/1], Batch [521], Loss: 55765.871094
Epoch [1/1], Batch [531], Loss: 57669.523438
Epoch [1/1], Batch [541], Loss: 55608.093750
Epoch [1/1], Batch [551], Loss: 60350.988281
Epoch [1/1], Batch [561], Loss: 54651.171875
Epoch [1/1], Batch [571], Loss: 58319.683594
Epoch [1/1], Batch [581], Loss: 55047.859375
Epoch [1/1], Batch [591], Loss: 58089.148438
Epoch [1/1], Batch [601], Loss: 55163.937500
Epoch [1/1], Batch [611], Loss: 55199.398438
Epoch [1/1], Batch [621], Loss: 54929.445312
Epoch [1/1], Batch [631], Loss: 53861.546875
Epoch [1/1], Batch [641], Loss: 58119.160156
Epoch [1/1], Batch [651], Loss: 54705.910156
Epoch [1/1], Batch [661], Loss: 56475.953125
Epoch [1/1], Batch [671], Loss: 57679.289062
Epoch [1/1], Batch [681], Loss: 56916.734375
Epoch [1/1], Batch [691], Loss: 58889.332031
Epoch [1/1], Batch [701], Loss: 57190.410156
Epoch [1/1], Batch [711], Loss: 58155.304688
Epoch [1/1], Batch [721], Loss: 54489.460938
Epoch [1/1], Batch [731], Loss: 55257.757812
Epoch [1/1], Batch [741], Loss: 56771.562500
Epoch [1/1], Batch [751], Loss: 57219.441406
Epoch [1/1], Batch [761], Loss: 56740.792969
Epoch [1/1], Batch [771], Loss: 56635.074219
Epoch [1/1], Batch [781], Loss: 56020.195312
Epoch [1/1], Batch [791], Loss: 56809.593750
Epoch [1/1], Batch [801], Loss: 59617.386719
Epoch [1/1], Batch [811], Loss: 57028.671875
Epoch [1/1], Batch [821], Loss: 57153.007812
Epoch [1/1], Batch [831], Loss: 57332.781250
Epoch [1/1], Batch [841], Loss: 57584.992188
Epoch [1/1], Batch [851], Loss: 56231.417969
Epoch [1/1], Batch [861], Loss: 59321.453125
Epoch [1/1], Batch [871], Loss: 56827.691406
Epoch [1/1], Batch [881], Loss: 57984.437500
Epoch [1/1], Batch [891], Loss: 57981.195312
Epoch [1/1], Batch [901], Loss: 54763.097656
Epoch [1/1], Batch [911], Loss: 56930.015625
Epoch [1/1], Batch [921], Loss: 56914.703125
Epoch [1/1], Batch [931], Loss: 55904.632812
Epoch [1/1], Batch [941], Loss: 56500.773438
Epoch [1/1], Batch [951], Loss: 55549.128906
Epoch [1/1], Batch [961], Loss: 55960.386719
Epoch [1/1], Batch [971], Loss: 59580.074219
Epoch [1/1], Batch [981], Loss: 57947.480469
Epoch [1/1], Batch [991], Loss: 57538.769531
Epoch [1/1], Batch [1001], Loss: 53339.261719
Epoch [1/1], Batch [1011], Loss: 55790.347656
Epoch [1/1], Batch [1021], Loss: 59550.593750
Epoch [1/1], Batch [1031], Loss: 54418.589844
Epoch [1/1], Batch [1041], Loss: 57776.230469
Epoch [1/1], Batch [1051], Loss: 56679.839844
Epoch [1/1], Batch [1061], Loss: 54730.410156
Epoch [1/1], Batch [1071], Loss: 56583.074219
Epoch [1/1], Batch [1081], Loss: 55469.007812
Epoch [1/1], Batch [1091], Loss: 56874.468750
Epoch [1/1], Batch [1101], Loss: 55104.769531
Epoch [1/1], Batch [1111], Loss: 55557.882812
Epoch [1/1], Batch [1121], Loss: 58191.890625
Epoch [1/1], Batch [1131], Loss: 54741.121094
Epoch [1/1], Batch [1141], Loss: 55196.253906
Epoch [1/1], Batch [1151], Loss: 60294.363281
Epoch [1/1], Batch [1161], Loss: 57804.265625
Epoch [1/1], Batch [1171], Loss: 56290.199219
Epoch [1/1], Batch [1181], Loss: 56208.250000
Epoch [1/1], Batch [1191], Loss: 55385.320312
Epoch [1/1], Batch [1201], Loss: 54359.867188
Epoch [1/1], Batch [1211], Loss: 57291.992188
Epoch [1/1], Batch [1221], Loss: 54397.125000
Epoch [1/1], Batch [1231], Loss: 57084.164062
Epoch [1/1], Batch [1241], Loss: 56668.558594
Epoch [1/1], Batch [1251], Loss: 55406.003906
Epoch [1/1], Batch [1261], Loss: 54906.843750
Epoch [1/1], Batch [1271], Loss: 54804.000000
Epoch [1/1], Batch [1281], Loss: 55217.128906
Epoch [1/1], Batch [1291], Loss: 53191.687500
Epoch [1/1], Batch [1301], Loss: 54776.234375
Epoch [1/1], Batch [1311], Loss: 53898.222656
Epoch [1/1], Batch [1321], Loss: 56608.812500
Epoch [1/1], Batch [1331], Loss: 57084.332031
Epoch [1/1], Batch [1341], Loss: 59632.312500
Epoch [1/1], Batch [1351], Loss: 59113.984375
Epoch [1/1], Batch [1361], Loss: 54420.027344
Epoch [1/1], Batch [1371], Loss: 52219.425781
Epoch [1/1], Batch [1381], Loss: 55668.570312
Epoch [1/1], Batch [1391], Loss: 55587.960938
Epoch [1/1], Batch [1401], Loss: 55870.894531
Epoch [1/1], Batch [1411], Loss: 56708.023438
Epoch [1/1], Batch [1421], Loss: 53190.015625
Epoch [1/1], Batch [1431], Loss: 56115.421875
Epoch [1/1], Batch [1441], Loss: 52641.359375
Epoch [1/1], Batch [1451], Loss: 57620.609375
Epoch [1/1], Batch [1461], Loss: 54002.792969
Epoch [1/1], Batch [1471], Loss: 54702.906250
Epoch [1/1], Batch [1481], Loss: 55247.734375
Epoch [1/1], Batch [1491], Loss: 51842.042969
Epoch [1/1], Batch [1501], Loss: 54978.175781
Epoch [1/1], Batch [1511], Loss: 53810.507812
Epoch [1/1], Batch [1521], Loss: 54407.625000
Epoch [1/1], Batch [1531], Loss: 52518.351562
Epoch [1/1], Batch [1541], Loss: 54894.378906
Epoch [1/1], Batch [1551], Loss: 58826.390625
Epoch [1/1], Batch [1561], Loss: 54500.246094
Epoch [1/1], Batch [1571], Loss: 52736.316406
Epoch [1/1], Batch [1581], Loss: 54459.445312
Epoch [1/1], Batch [1591], Loss: 53936.726562
Epoch [1/1], Batch [1601], Loss: 54673.781250
Epoch [1/1], Batch [1611], Loss: 54761.011719
Epoch [1/1], Batch [1621], Loss: 53155.308594
Epoch [1/1], Batch [1631], Loss: 54685.460938
Epoch [1/1], Batch [1641], Loss: 54197.492188
Epoch [1/1], Batch [1651], Loss: 52789.804688
Epoch [1/1], Batch [1661], Loss: 54326.523438
Epoch [1/1], Batch [1671], Loss: 54623.648438
Epoch [1/1], Batch [1681], Loss: 53513.066406
Epoch [1/1], Batch [1691], Loss: 54530.351562
Epoch [1/1], Batch [1701], Loss: 55576.605469
Epoch [1/1], Batch [1711], Loss: 54261.601562
Epoch [1/1], Batch [1721], Loss: 56246.625000
Epoch [1/1], Batch [1731], Loss: 56138.605469
Epoch [1/1], Batch [1741], Loss: 52771.945312
Epoch [1/1], Batch [1751], Loss: 54240.867188
Epoch [1/1], Batch [1761], Loss: 53084.605469
Epoch [1/1], Batch [1771], Loss: 51565.226562
Epoch [1/1], Batch [1781], Loss: 50881.203125
Epoch [1/1], Batch [1791], Loss: 55917.968750
Epoch [1/1], Batch [1801], Loss: 54636.226562
Epoch [1/1], Batch [1811], Loss: 55362.320312
Epoch [1/1], Batch [1821], Loss: 55637.515625
Epoch [1/1], Batch [1831], Loss: 54884.671875
Epoch [1/1], Batch [1841], Loss: 53324.738281
Epoch [1/1], Batch [1851], Loss: 55938.191406
Epoch [1/1], Batch [1861], Loss: 54728.148438
Epoch [1/1], Batch [1871], Loss: 53197.093750
Epoch [1/1], Batch [1881], Loss: 51609.437500
Epoch [1/1], Batch [1891], Loss: 54323.613281
Epoch [1/1], Batch [1901], Loss: 52984.453125
Epoch [1/1], Batch [1911], Loss: 55490.527344
Epoch [1/1], Batch [1921], Loss: 56508.945312
Epoch [1/1], Batch [1931], Loss: 51843.996094
Epoch [1/1], Batch [1941], Loss: 53529.062500
Epoch [1/1], Batch [1951], Loss: 53947.835938
Epoch [1/1], Batch [1961], Loss: 51264.679688
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 56898.9162
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 52681.9369
Elapsed time: 1132.65 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 54210.1820
Elapsed time: 1157.65 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 80376.117188
Epoch [1/1], Batch [11], Loss: 82614.109375
Epoch [1/1], Batch [21], Loss: 75689.328125
Epoch [1/1], Batch [31], Loss: 75699.703125
Epoch [1/1], Batch [41], Loss: 75478.085938
Epoch [1/1], Batch [51], Loss: 81867.648438
Epoch [1/1], Batch [61], Loss: 81171.703125
Epoch [1/1], Batch [71], Loss: 80471.710938
Epoch [1/1], Batch [81], Loss: 80459.609375
Epoch [1/1], Batch [91], Loss: 74448.000000
Epoch [1/1], Batch [101], Loss: 76753.460938
Epoch [1/1], Batch [111], Loss: 77768.312500
Epoch [1/1], Batch [121], Loss: 72973.203125
Epoch [1/1], Batch [131], Loss: 77817.796875
Epoch [1/1], Batch [141], Loss: 77021.890625
Epoch [1/1], Batch [151], Loss: 77424.492188
Epoch [1/1], Batch [161], Loss: 72554.234375
Epoch [1/1], Batch [171], Loss: 73981.203125
Epoch [1/1], Batch [181], Loss: 74961.835938
Epoch [1/1], Batch [191], Loss: 71646.218750
Epoch [1/1], Batch [201], Loss: 72572.523438
Epoch [1/1], Batch [211], Loss: 76689.039062
Epoch [1/1], Batch [221], Loss: 74539.609375
Epoch [1/1], Batch [231], Loss: 76110.640625
Epoch [1/1], Batch [241], Loss: 78155.359375
Epoch [1/1], Batch [251], Loss: 75149.351562
Epoch [1/1], Batch [261], Loss: 76106.265625
Epoch [1/1], Batch [271], Loss: 76780.687500
Epoch [1/1], Batch [281], Loss: 71684.718750
Epoch [1/1], Batch [291], Loss: 75761.718750
Epoch [1/1], Batch [301], Loss: 74578.828125
Epoch [1/1], Batch [311], Loss: 79798.015625
Epoch [1/1], Batch [321], Loss: 71826.000000
Epoch [1/1], Batch [331], Loss: 74758.820312
Epoch [1/1], Batch [341], Loss: 77272.281250
Epoch [1/1], Batch [351], Loss: 73515.070312
Epoch [1/1], Batch [361], Loss: 72282.335938
Epoch [1/1], Batch [371], Loss: 74177.882812
Epoch [1/1], Batch [381], Loss: 72629.109375
Epoch [1/1], Batch [391], Loss: 76952.500000
Epoch [1/1], Batch [401], Loss: 74256.218750
Epoch [1/1], Batch [411], Loss: 70633.320312
Epoch [1/1], Batch [421], Loss: 74278.531250
Epoch [1/1], Batch [431], Loss: 74156.015625
Epoch [1/1], Batch [441], Loss: 73652.718750
Epoch [1/1], Batch [451], Loss: 74370.546875
Epoch [1/1], Batch [461], Loss: 73706.460938
Epoch [1/1], Batch [471], Loss: 75899.007812
Epoch [1/1], Batch [481], Loss: 76522.445312
Epoch [1/1], Batch [491], Loss: 76589.406250
Epoch [1/1], Batch [501], Loss: 73450.031250
Epoch [1/1], Batch [511], Loss: 75358.500000
Epoch [1/1], Batch [521], Loss: 74541.976562
Epoch [1/1], Batch [531], Loss: 69142.156250
Epoch [1/1], Batch [541], Loss: 73295.351562
Epoch [1/1], Batch [551], Loss: 75134.750000
Epoch [1/1], Batch [561], Loss: 75649.718750
Epoch [1/1], Batch [571], Loss: 73824.460938
Epoch [1/1], Batch [581], Loss: 76144.718750
Epoch [1/1], Batch [591], Loss: 75243.296875
Epoch [1/1], Batch [601], Loss: 73570.453125
Epoch [1/1], Batch [611], Loss: 75404.437500
Epoch [1/1], Batch [621], Loss: 74441.156250
Epoch [1/1], Batch [631], Loss: 71187.328125
Epoch [1/1], Batch [641], Loss: 72551.328125
Epoch [1/1], Batch [651], Loss: 70309.203125
Epoch [1/1], Batch [661], Loss: 73053.453125
Epoch [1/1], Batch [671], Loss: 74197.804688
Epoch [1/1], Batch [681], Loss: 72914.843750
Epoch [1/1], Batch [691], Loss: 76404.171875
Epoch [1/1], Batch [701], Loss: 73760.468750
Epoch [1/1], Batch [711], Loss: 75104.031250
Epoch [1/1], Batch [721], Loss: 73196.156250
Epoch [1/1], Batch [731], Loss: 77228.226562
Epoch [1/1], Batch [741], Loss: 74037.640625
Epoch [1/1], Batch [751], Loss: 71725.750000
Epoch [1/1], Batch [761], Loss: 73917.492188
Epoch [1/1], Batch [771], Loss: 77758.320312
Epoch [1/1], Batch [781], Loss: 74204.117188
Epoch [1/1], Batch [791], Loss: 72234.085938
Epoch [1/1], Batch [801], Loss: 74901.531250
Epoch [1/1], Batch [811], Loss: 74235.656250
Epoch [1/1], Batch [821], Loss: 76623.546875
Epoch [1/1], Batch [831], Loss: 68744.281250
Epoch [1/1], Batch [841], Loss: 71249.718750
Epoch [1/1], Batch [851], Loss: 77367.289062
Epoch [1/1], Batch [861], Loss: 72297.187500
Epoch [1/1], Batch [871], Loss: 71196.468750
Epoch [1/1], Batch [881], Loss: 74741.765625
Epoch [1/1], Batch [891], Loss: 73348.687500
Epoch [1/1], Batch [901], Loss: 76573.031250
Epoch [1/1], Batch [911], Loss: 74035.843750
Epoch [1/1], Batch [921], Loss: 75028.687500
Epoch [1/1], Batch [931], Loss: 73280.867188
Epoch [1/1], Batch [941], Loss: 71296.359375
Epoch [1/1], Batch [951], Loss: 71085.906250
Epoch [1/1], Batch [961], Loss: 75383.156250
Epoch [1/1], Batch [971], Loss: 73781.242188
Epoch [1/1], Batch [981], Loss: 73053.171875
Epoch [1/1], Batch [991], Loss: 75346.609375
Epoch [1/1], Batch [1001], Loss: 72764.781250
Epoch [1/1], Batch [1011], Loss: 70481.828125
Epoch [1/1], Batch [1021], Loss: 71830.671875
Epoch [1/1], Batch [1031], Loss: 75003.921875
Epoch [1/1], Batch [1041], Loss: 69689.882812
Epoch [1/1], Batch [1051], Loss: 71667.187500
Epoch [1/1], Batch [1061], Loss: 72103.015625
Epoch [1/1], Batch [1071], Loss: 78583.429688
Epoch [1/1], Batch [1081], Loss: 73873.679688
Epoch [1/1], Batch [1091], Loss: 70028.390625
Epoch [1/1], Batch [1101], Loss: 71969.085938
Epoch [1/1], Batch [1111], Loss: 71302.828125
Epoch [1/1], Batch [1121], Loss: 74202.359375
Epoch [1/1], Batch [1131], Loss: 73712.664062
Epoch [1/1], Batch [1141], Loss: 70560.265625
Epoch [1/1], Batch [1151], Loss: 73288.953125
Epoch [1/1], Batch [1161], Loss: 69818.867188
Epoch [1/1], Batch [1171], Loss: 72622.046875
Epoch [1/1], Batch [1181], Loss: 71045.140625
Epoch [1/1], Batch [1191], Loss: 70200.054688
Epoch [1/1], Batch [1201], Loss: 74043.437500
Epoch [1/1], Batch [1211], Loss: 70463.140625
Epoch [1/1], Batch [1221], Loss: 74699.421875
Epoch [1/1], Batch [1231], Loss: 73265.468750
Epoch [1/1], Batch [1241], Loss: 74625.750000
Epoch [1/1], Batch [1251], Loss: 71261.859375
Epoch [1/1], Batch [1261], Loss: 69772.093750
Epoch [1/1], Batch [1271], Loss: 72343.546875
Epoch [1/1], Batch [1281], Loss: 73544.421875
Epoch [1/1], Batch [1291], Loss: 72405.000000
Epoch [1/1], Batch [1301], Loss: 72893.421875
Epoch [1/1], Batch [1311], Loss: 72788.679688
Epoch [1/1], Batch [1321], Loss: 72883.812500
Epoch [1/1], Batch [1331], Loss: 68871.328125
Epoch [1/1], Batch [1341], Loss: 72391.265625
Epoch [1/1], Batch [1351], Loss: 74219.617188
Epoch [1/1], Batch [1361], Loss: 71376.242188
Epoch [1/1], Batch [1371], Loss: 72518.335938
Epoch [1/1], Batch [1381], Loss: 72135.359375
Epoch [1/1], Batch [1391], Loss: 72407.421875
Epoch [1/1], Batch [1401], Loss: 70877.750000
Epoch [1/1], Batch [1411], Loss: 69114.765625
Epoch [1/1], Batch [1421], Loss: 72768.609375
Epoch [1/1], Batch [1431], Loss: 71522.968750
Epoch [1/1], Batch [1441], Loss: 75060.007812
Epoch [1/1], Batch [1451], Loss: 75908.109375
Epoch [1/1], Batch [1461], Loss: 67223.585938
Epoch [1/1], Batch [1471], Loss: 73262.812500
Epoch [1/1], Batch [1481], Loss: 70971.804688
Epoch [1/1], Batch [1491], Loss: 68684.648438
Epoch [1/1], Batch [1501], Loss: 69703.375000
Epoch [1/1], Batch [1511], Loss: 73556.242188
Epoch [1/1], Batch [1521], Loss: 71267.609375
Epoch [1/1], Batch [1531], Loss: 70189.953125
Epoch [1/1], Batch [1541], Loss: 69478.132812
Epoch [1/1], Batch [1551], Loss: 75230.085938
Epoch [1/1], Batch [1561], Loss: 73922.835938
Epoch [1/1], Batch [1571], Loss: 71497.046875
Epoch [1/1], Batch [1581], Loss: 70234.984375
Epoch [1/1], Batch [1591], Loss: 70441.500000
Epoch [1/1], Batch [1601], Loss: 73268.445312
Epoch [1/1], Batch [1611], Loss: 68629.328125
Epoch [1/1], Batch [1621], Loss: 72500.328125
Epoch [1/1], Batch [1631], Loss: 69706.945312
Epoch [1/1], Batch [1641], Loss: 70282.343750
Epoch [1/1], Batch [1651], Loss: 71667.468750
Epoch [1/1], Batch [1661], Loss: 72048.429688
Epoch [1/1], Batch [1671], Loss: 74511.390625
Epoch [1/1], Batch [1681], Loss: 71797.140625
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 73461.9176
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 70422.1497
Elapsed time: 1851.78 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 73002.3167
Elapsed time: 1878.98 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 90007.773438
Epoch [1/1], Batch [11], Loss: 96174.828125
Epoch [1/1], Batch [21], Loss: 97759.234375
Epoch [1/1], Batch [31], Loss: 95742.320312
Epoch [1/1], Batch [41], Loss: 89947.023438
Epoch [1/1], Batch [51], Loss: 93150.140625
Epoch [1/1], Batch [61], Loss: 97606.125000
Epoch [1/1], Batch [71], Loss: 93524.359375
Epoch [1/1], Batch [81], Loss: 92038.976562
Epoch [1/1], Batch [91], Loss: 90909.734375
Epoch [1/1], Batch [101], Loss: 96426.609375
Epoch [1/1], Batch [111], Loss: 86421.312500
Epoch [1/1], Batch [121], Loss: 92720.718750
Epoch [1/1], Batch [131], Loss: 92162.281250
Epoch [1/1], Batch [141], Loss: 94365.359375
Epoch [1/1], Batch [151], Loss: 90300.781250
Epoch [1/1], Batch [161], Loss: 93916.921875
Epoch [1/1], Batch [171], Loss: 98677.437500
Epoch [1/1], Batch [181], Loss: 96744.375000
Epoch [1/1], Batch [191], Loss: 92693.765625
Epoch [1/1], Batch [201], Loss: 92642.890625
Epoch [1/1], Batch [211], Loss: 89333.406250
Epoch [1/1], Batch [221], Loss: 91652.796875
Epoch [1/1], Batch [231], Loss: 95020.273438
Epoch [1/1], Batch [241], Loss: 93512.320312
Epoch [1/1], Batch [251], Loss: 91034.273438
Epoch [1/1], Batch [261], Loss: 93764.914062
Epoch [1/1], Batch [271], Loss: 91878.664062
Epoch [1/1], Batch [281], Loss: 92472.085938
Epoch [1/1], Batch [291], Loss: 90118.953125
Epoch [1/1], Batch [301], Loss: 94972.289062
Epoch [1/1], Batch [311], Loss: 90421.125000
Epoch [1/1], Batch [321], Loss: 91459.515625
Epoch [1/1], Batch [331], Loss: 94646.718750
Epoch [1/1], Batch [341], Loss: 92028.414062
Epoch [1/1], Batch [351], Loss: 93014.976562
Epoch [1/1], Batch [361], Loss: 94557.570312
Epoch [1/1], Batch [371], Loss: 94377.359375
Epoch [1/1], Batch [381], Loss: 89923.843750
Epoch [1/1], Batch [391], Loss: 92424.914062
Epoch [1/1], Batch [401], Loss: 91576.765625
Epoch [1/1], Batch [411], Loss: 95354.656250
Epoch [1/1], Batch [421], Loss: 88991.710938
Epoch [1/1], Batch [431], Loss: 91399.125000
Epoch [1/1], Batch [441], Loss: 92631.078125
Epoch [1/1], Batch [451], Loss: 92735.117188
Epoch [1/1], Batch [461], Loss: 96207.195312
Epoch [1/1], Batch [471], Loss: 89940.718750
Epoch [1/1], Batch [481], Loss: 92338.281250
Epoch [1/1], Batch [491], Loss: 90224.765625
Epoch [1/1], Batch [501], Loss: 90459.796875
Epoch [1/1], Batch [511], Loss: 92154.554688
Epoch [1/1], Batch [521], Loss: 89394.781250
Epoch [1/1], Batch [531], Loss: 92572.828125
Epoch [1/1], Batch [541], Loss: 90723.687500
Epoch [1/1], Batch [551], Loss: 94899.406250
Epoch [1/1], Batch [561], Loss: 86652.171875
Epoch [1/1], Batch [571], Loss: 92804.046875
Epoch [1/1], Batch [581], Loss: 86429.312500
Epoch [1/1], Batch [591], Loss: 90017.570312
Epoch [1/1], Batch [601], Loss: 91837.453125
Epoch [1/1], Batch [611], Loss: 90386.804688
Epoch [1/1], Batch [621], Loss: 92246.484375
Epoch [1/1], Batch [631], Loss: 94072.601562
Epoch [1/1], Batch [641], Loss: 92714.148438
Epoch [1/1], Batch [651], Loss: 88630.382812
Epoch [1/1], Batch [661], Loss: 88061.445312
Epoch [1/1], Batch [671], Loss: 91867.742188
Epoch [1/1], Batch [681], Loss: 92556.687500
Epoch [1/1], Batch [691], Loss: 90829.593750
Epoch [1/1], Batch [701], Loss: 91820.875000
Epoch [1/1], Batch [711], Loss: 94243.648438
Epoch [1/1], Batch [721], Loss: 93323.828125
Epoch [1/1], Batch [731], Loss: 96393.296875
Epoch [1/1], Batch [741], Loss: 93625.640625
Epoch [1/1], Batch [751], Loss: 93769.234375
Epoch [1/1], Batch [761], Loss: 89404.296875
Epoch [1/1], Batch [771], Loss: 92361.734375
Epoch [1/1], Batch [781], Loss: 96009.781250
Epoch [1/1], Batch [791], Loss: 92713.429688
Epoch [1/1], Batch [801], Loss: 91022.367188
Epoch [1/1], Batch [811], Loss: 91199.265625
Epoch [1/1], Batch [821], Loss: 89822.328125
Epoch [1/1], Batch [831], Loss: 88038.507812
Epoch [1/1], Batch [841], Loss: 89725.500000
Epoch [1/1], Batch [851], Loss: 89649.906250
Epoch [1/1], Batch [861], Loss: 88126.773438
Epoch [1/1], Batch [871], Loss: 86419.937500
Epoch [1/1], Batch [881], Loss: 90489.031250
Epoch [1/1], Batch [891], Loss: 87014.343750
Epoch [1/1], Batch [901], Loss: 89559.218750
Epoch [1/1], Batch [911], Loss: 94142.812500
Epoch [1/1], Batch [921], Loss: 90779.093750
Epoch [1/1], Batch [931], Loss: 92045.054688
Epoch [1/1], Batch [941], Loss: 89428.250000
Epoch [1/1], Batch [951], Loss: 92896.796875
Epoch [1/1], Batch [961], Loss: 88268.156250
Epoch [1/1], Batch [971], Loss: 92855.046875
Epoch [1/1], Batch [981], Loss: 90405.031250
Epoch [1/1], Batch [991], Loss: 91038.968750
Epoch [1/1], Batch [1001], Loss: 95126.328125
Epoch [1/1], Batch [1011], Loss: 89060.718750
Epoch [1/1], Batch [1021], Loss: 89631.742188
Epoch [1/1], Batch [1031], Loss: 92359.296875
Epoch [1/1], Batch [1041], Loss: 93013.609375
Epoch [1/1], Batch [1051], Loss: 87190.359375
Epoch [1/1], Batch [1061], Loss: 88647.890625
Epoch [1/1], Batch [1071], Loss: 91381.343750
Epoch [1/1], Batch [1081], Loss: 92445.453125
Epoch [1/1], Batch [1091], Loss: 87629.656250
Epoch [1/1], Batch [1101], Loss: 90060.523438
Epoch [1/1], Batch [1111], Loss: 92748.039062
Epoch [1/1], Batch [1121], Loss: 92556.937500
Epoch [1/1], Batch [1131], Loss: 92340.500000
Epoch [1/1], Batch [1141], Loss: 87959.640625
Epoch [1/1], Batch [1151], Loss: 89796.796875
Epoch [1/1], Batch [1161], Loss: 85911.367188
Epoch [1/1], Batch [1171], Loss: 90169.250000
Epoch [1/1], Batch [1181], Loss: 87020.031250
Epoch [1/1], Batch [1191], Loss: 93008.257812
Epoch [1/1], Batch [1201], Loss: 88376.125000
Epoch [1/1], Batch [1211], Loss: 89827.210938
Epoch [1/1], Batch [1221], Loss: 90094.781250
Epoch [1/1], Batch [1231], Loss: 84833.257812
Epoch [1/1], Batch [1241], Loss: 85975.039062
Epoch [1/1], Batch [1251], Loss: 90252.250000
Epoch [1/1], Batch [1261], Loss: 89613.812500
Epoch [1/1], Batch [1271], Loss: 88987.437500
Epoch [1/1], Batch [1281], Loss: 91130.835938
Epoch [1/1], Batch [1291], Loss: 90243.507812
Epoch [1/1], Batch [1301], Loss: 92612.671875
Epoch [1/1], Batch [1311], Loss: 91778.273438
Epoch [1/1], Batch [1321], Loss: 83530.273438
Epoch [1/1], Batch [1331], Loss: 90969.187500
Epoch [1/1], Batch [1341], Loss: 90458.656250
Epoch [1/1], Batch [1351], Loss: 85212.195312
Epoch [1/1], Batch [1361], Loss: 91035.585938
Epoch [1/1], Batch [1371], Loss: 93692.109375
Epoch [1/1], Batch [1381], Loss: 86917.109375
Epoch [1/1], Batch [1391], Loss: 90523.523438
Epoch [1/1], Batch [1401], Loss: 88580.914062
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 91470.4133
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 87084.4706
Elapsed time: 2594.13 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 91067.0468
Elapsed time: 2621.63 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 117181.046875
Epoch [1/1], Batch [11], Loss: 120651.554688
Epoch [1/1], Batch [21], Loss: 115122.171875
Epoch [1/1], Batch [31], Loss: 107593.054688
Epoch [1/1], Batch [41], Loss: 109381.828125
Epoch [1/1], Batch [51], Loss: 108321.195312
Epoch [1/1], Batch [61], Loss: 108196.351562
Epoch [1/1], Batch [71], Loss: 105630.859375
Epoch [1/1], Batch [81], Loss: 116840.437500
Epoch [1/1], Batch [91], Loss: 104968.812500
Epoch [1/1], Batch [101], Loss: 111969.593750
Epoch [1/1], Batch [111], Loss: 102708.390625
Epoch [1/1], Batch [121], Loss: 107780.828125
Epoch [1/1], Batch [131], Loss: 114441.203125
Epoch [1/1], Batch [141], Loss: 108378.593750
Epoch [1/1], Batch [151], Loss: 111447.203125
Epoch [1/1], Batch [161], Loss: 115378.804688
Epoch [1/1], Batch [171], Loss: 112676.632812
Epoch [1/1], Batch [181], Loss: 115523.710938
Epoch [1/1], Batch [191], Loss: 109092.390625
Epoch [1/1], Batch [201], Loss: 107710.476562
Epoch [1/1], Batch [211], Loss: 118548.640625
Epoch [1/1], Batch [221], Loss: 113215.218750
Epoch [1/1], Batch [231], Loss: 110282.601562
Epoch [1/1], Batch [241], Loss: 117849.296875
Epoch [1/1], Batch [251], Loss: 107823.992188
Epoch [1/1], Batch [261], Loss: 106851.250000
Epoch [1/1], Batch [271], Loss: 111728.023438
Epoch [1/1], Batch [281], Loss: 109647.406250
Epoch [1/1], Batch [291], Loss: 109421.125000
Epoch [1/1], Batch [301], Loss: 111283.328125
Epoch [1/1], Batch [311], Loss: 111565.507812
Epoch [1/1], Batch [321], Loss: 114379.687500
Epoch [1/1], Batch [331], Loss: 107514.156250
Epoch [1/1], Batch [341], Loss: 109098.953125
Epoch [1/1], Batch [351], Loss: 113167.234375
Epoch [1/1], Batch [361], Loss: 110865.382812
Epoch [1/1], Batch [371], Loss: 113431.953125
Epoch [1/1], Batch [381], Loss: 115328.343750
Epoch [1/1], Batch [391], Loss: 109899.148438
Epoch [1/1], Batch [401], Loss: 109867.539062
Epoch [1/1], Batch [411], Loss: 113218.835938
Epoch [1/1], Batch [421], Loss: 111148.609375
Epoch [1/1], Batch [431], Loss: 113104.960938
Epoch [1/1], Batch [441], Loss: 107592.843750
Epoch [1/1], Batch [451], Loss: 107018.156250
Epoch [1/1], Batch [461], Loss: 110556.406250
Epoch [1/1], Batch [471], Loss: 110244.742188
Epoch [1/1], Batch [481], Loss: 115606.703125
Epoch [1/1], Batch [491], Loss: 113191.093750
Epoch [1/1], Batch [501], Loss: 109646.781250
Epoch [1/1], Batch [511], Loss: 108769.500000
Epoch [1/1], Batch [521], Loss: 110298.773438
Epoch [1/1], Batch [531], Loss: 110730.890625
Epoch [1/1], Batch [541], Loss: 112174.781250
Epoch [1/1], Batch [551], Loss: 103970.921875
Epoch [1/1], Batch [561], Loss: 111051.609375
Epoch [1/1], Batch [571], Loss: 104726.882812
Epoch [1/1], Batch [581], Loss: 108189.921875
Epoch [1/1], Batch [591], Loss: 110372.593750
Epoch [1/1], Batch [601], Loss: 111539.648438
Epoch [1/1], Batch [611], Loss: 111686.500000
Epoch [1/1], Batch [621], Loss: 108327.609375
Epoch [1/1], Batch [631], Loss: 106575.914062
Epoch [1/1], Batch [641], Loss: 111958.453125
Epoch [1/1], Batch [651], Loss: 109644.375000
Epoch [1/1], Batch [661], Loss: 114110.031250
Epoch [1/1], Batch [671], Loss: 104430.000000
Epoch [1/1], Batch [681], Loss: 107078.820312
Epoch [1/1], Batch [691], Loss: 109203.875000
Epoch [1/1], Batch [701], Loss: 106751.656250
Epoch [1/1], Batch [711], Loss: 114074.015625
Epoch [1/1], Batch [721], Loss: 105414.890625
Epoch [1/1], Batch [731], Loss: 108512.437500
Epoch [1/1], Batch [741], Loss: 112117.625000
Epoch [1/1], Batch [751], Loss: 113698.968750
Epoch [1/1], Batch [761], Loss: 110993.500000
Epoch [1/1], Batch [771], Loss: 111266.453125
Epoch [1/1], Batch [781], Loss: 109057.062500
Epoch [1/1], Batch [791], Loss: 109928.453125
Epoch [1/1], Batch [801], Loss: 110557.984375
Epoch [1/1], Batch [811], Loss: 107201.015625
Epoch [1/1], Batch [821], Loss: 101880.968750
Epoch [1/1], Batch [831], Loss: 103899.507812
Epoch [1/1], Batch [841], Loss: 115033.125000
Epoch [1/1], Batch [851], Loss: 109202.375000
Epoch [1/1], Batch [861], Loss: 105066.195312
Epoch [1/1], Batch [871], Loss: 108198.523438
Epoch [1/1], Batch [881], Loss: 105459.421875
Epoch [1/1], Batch [891], Loss: 104055.796875
Epoch [1/1], Batch [901], Loss: 110163.312500
Epoch [1/1], Batch [911], Loss: 103431.609375
Epoch [1/1], Batch [921], Loss: 112905.984375
Epoch [1/1], Batch [931], Loss: 107493.757812
Epoch [1/1], Batch [941], Loss: 105666.812500
Epoch [1/1], Batch [951], Loss: 108493.742188
Epoch [1/1], Batch [961], Loss: 103914.390625
Epoch [1/1], Batch [971], Loss: 108058.812500
Epoch [1/1], Batch [981], Loss: 108450.000000
Epoch [1/1], Batch [991], Loss: 107471.328125
Epoch [1/1], Batch [1001], Loss: 111377.406250
Epoch [1/1], Batch [1011], Loss: 107583.875000
Epoch [1/1], Batch [1021], Loss: 108383.804688
Epoch [1/1], Batch [1031], Loss: 107272.789062
Epoch [1/1], Batch [1041], Loss: 111648.132812
Epoch [1/1], Batch [1051], Loss: 107427.906250
Epoch [1/1], Batch [1061], Loss: 107920.656250
Epoch [1/1], Batch [1071], Loss: 107362.234375
Epoch [1/1], Batch [1081], Loss: 112468.406250
Epoch [1/1], Batch [1091], Loss: 104190.140625
Epoch [1/1], Batch [1101], Loss: 102669.250000
Epoch [1/1], Batch [1111], Loss: 105605.687500
Epoch [1/1], Batch [1121], Loss: 110379.953125
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 109979.7783
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 105485.0644
Elapsed time: 3301.39 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 111240.2002
Elapsed time: 3327.27 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 135692.031250
Epoch [1/1], Batch [11], Loss: 131415.000000
Epoch [1/1], Batch [21], Loss: 129924.843750
Epoch [1/1], Batch [31], Loss: 132518.906250
Epoch [1/1], Batch [41], Loss: 127342.437500
Epoch [1/1], Batch [51], Loss: 122589.531250
Epoch [1/1], Batch [61], Loss: 131916.531250
Epoch [1/1], Batch [71], Loss: 133132.828125
Epoch [1/1], Batch [81], Loss: 125433.234375
Epoch [1/1], Batch [91], Loss: 130000.617188
Epoch [1/1], Batch [101], Loss: 129018.015625
Epoch [1/1], Batch [111], Loss: 132249.781250
Epoch [1/1], Batch [121], Loss: 126122.796875
Epoch [1/1], Batch [131], Loss: 136579.296875
Epoch [1/1], Batch [141], Loss: 129063.742188
Epoch [1/1], Batch [151], Loss: 133070.234375
Epoch [1/1], Batch [161], Loss: 124415.335938
Epoch [1/1], Batch [171], Loss: 128478.367188
Epoch [1/1], Batch [181], Loss: 130411.234375
Epoch [1/1], Batch [191], Loss: 130885.078125
Epoch [1/1], Batch [201], Loss: 132419.718750
Epoch [1/1], Batch [211], Loss: 132082.375000
Epoch [1/1], Batch [221], Loss: 133174.140625
Epoch [1/1], Batch [231], Loss: 124543.187500
Epoch [1/1], Batch [241], Loss: 124921.898438
Epoch [1/1], Batch [251], Loss: 120076.062500
Epoch [1/1], Batch [261], Loss: 124605.671875
Epoch [1/1], Batch [271], Loss: 128281.390625
Epoch [1/1], Batch [281], Loss: 131421.828125
Epoch [1/1], Batch [291], Loss: 129087.203125
Epoch [1/1], Batch [301], Loss: 124957.640625
Epoch [1/1], Batch [311], Loss: 132798.468750
Epoch [1/1], Batch [321], Loss: 133578.218750
Epoch [1/1], Batch [331], Loss: 128165.109375
Epoch [1/1], Batch [341], Loss: 130732.265625
Epoch [1/1], Batch [351], Loss: 126877.953125
Epoch [1/1], Batch [361], Loss: 128722.414062
Epoch [1/1], Batch [371], Loss: 130439.062500
Epoch [1/1], Batch [381], Loss: 124574.359375
Epoch [1/1], Batch [391], Loss: 133411.796875
Epoch [1/1], Batch [401], Loss: 127601.171875
Epoch [1/1], Batch [411], Loss: 131325.703125
Epoch [1/1], Batch [421], Loss: 127441.703125
Epoch [1/1], Batch [431], Loss: 134051.984375
Epoch [1/1], Batch [441], Loss: 130337.875000
Epoch [1/1], Batch [451], Loss: 121203.046875
Epoch [1/1], Batch [461], Loss: 131560.250000
Epoch [1/1], Batch [471], Loss: 129242.812500
Epoch [1/1], Batch [481], Loss: 130640.609375
Epoch [1/1], Batch [491], Loss: 130359.226562
Epoch [1/1], Batch [501], Loss: 129226.828125
Epoch [1/1], Batch [511], Loss: 128389.015625
Epoch [1/1], Batch [521], Loss: 120093.203125
Epoch [1/1], Batch [531], Loss: 126386.500000
Epoch [1/1], Batch [541], Loss: 128642.632812
Epoch [1/1], Batch [551], Loss: 128596.898438
Epoch [1/1], Batch [561], Loss: 122324.750000
Epoch [1/1], Batch [571], Loss: 130259.015625
Epoch [1/1], Batch [581], Loss: 127055.156250
Epoch [1/1], Batch [591], Loss: 125947.718750
Epoch [1/1], Batch [601], Loss: 131489.625000
Epoch [1/1], Batch [611], Loss: 134101.281250
Epoch [1/1], Batch [621], Loss: 129842.914062
Epoch [1/1], Batch [631], Loss: 128404.453125
Epoch [1/1], Batch [641], Loss: 129026.687500
Epoch [1/1], Batch [651], Loss: 126156.812500
Epoch [1/1], Batch [661], Loss: 124163.734375
Epoch [1/1], Batch [671], Loss: 129901.523438
Epoch [1/1], Batch [681], Loss: 126106.953125
Epoch [1/1], Batch [691], Loss: 127842.515625
Epoch [1/1], Batch [701], Loss: 131427.218750
Epoch [1/1], Batch [711], Loss: 124851.359375
Epoch [1/1], Batch [721], Loss: 121665.796875
Epoch [1/1], Batch [731], Loss: 120791.734375
Epoch [1/1], Batch [741], Loss: 125254.359375
Epoch [1/1], Batch [751], Loss: 126606.328125
Epoch [1/1], Batch [761], Loss: 133349.625000
Epoch [1/1], Batch [771], Loss: 121001.625000
Epoch [1/1], Batch [781], Loss: 136858.078125
Epoch [1/1], Batch [791], Loss: 120683.468750
Epoch [1/1], Batch [801], Loss: 129551.343750
Epoch [1/1], Batch [811], Loss: 124846.671875
Epoch [1/1], Batch [821], Loss: 124348.898438
Epoch [1/1], Batch [831], Loss: 127657.234375
Epoch [1/1], Batch [841], Loss: 126107.156250
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 128798.1914
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 123114.6082
Elapsed time: 3920.59 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 131296.7151
Elapsed time: 3942.91 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 146235.796875
Epoch [1/1], Batch [11], Loss: 144796.906250
Epoch [1/1], Batch [21], Loss: 146672.750000
Epoch [1/1], Batch [31], Loss: 144759.625000
Epoch [1/1], Batch [41], Loss: 149117.765625
Epoch [1/1], Batch [51], Loss: 144949.187500
Epoch [1/1], Batch [61], Loss: 149927.593750
Epoch [1/1], Batch [71], Loss: 147906.734375
Epoch [1/1], Batch [81], Loss: 153215.046875
Epoch [1/1], Batch [91], Loss: 146619.312500
Epoch [1/1], Batch [101], Loss: 154325.781250
Epoch [1/1], Batch [111], Loss: 151051.437500
Epoch [1/1], Batch [121], Loss: 150926.750000
Epoch [1/1], Batch [131], Loss: 155242.468750
Epoch [1/1], Batch [141], Loss: 147774.625000
Epoch [1/1], Batch [151], Loss: 153692.593750
Epoch [1/1], Batch [161], Loss: 143250.343750
Epoch [1/1], Batch [171], Loss: 142339.968750
Epoch [1/1], Batch [181], Loss: 150373.390625
Epoch [1/1], Batch [191], Loss: 151978.562500
Epoch [1/1], Batch [201], Loss: 141581.250000
Epoch [1/1], Batch [211], Loss: 155269.875000
Epoch [1/1], Batch [221], Loss: 147361.015625
Epoch [1/1], Batch [231], Loss: 149371.875000
Epoch [1/1], Batch [241], Loss: 148109.781250
Epoch [1/1], Batch [251], Loss: 147061.781250
Epoch [1/1], Batch [261], Loss: 149984.375000
Epoch [1/1], Batch [271], Loss: 149804.531250
Epoch [1/1], Batch [281], Loss: 149558.203125
Epoch [1/1], Batch [291], Loss: 158021.625000
Epoch [1/1], Batch [301], Loss: 146096.781250
Epoch [1/1], Batch [311], Loss: 152502.500000
Epoch [1/1], Batch [321], Loss: 151776.656250
Epoch [1/1], Batch [331], Loss: 146608.781250
Epoch [1/1], Batch [341], Loss: 155465.000000
Epoch [1/1], Batch [351], Loss: 146357.078125
Epoch [1/1], Batch [361], Loss: 148532.859375
Epoch [1/1], Batch [371], Loss: 146966.000000
Epoch [1/1], Batch [381], Loss: 148812.062500
Epoch [1/1], Batch [391], Loss: 145406.953125
Epoch [1/1], Batch [401], Loss: 151143.390625
Epoch [1/1], Batch [411], Loss: 150663.500000
Epoch [1/1], Batch [421], Loss: 150280.062500
Epoch [1/1], Batch [431], Loss: 151558.515625
Epoch [1/1], Batch [441], Loss: 153242.968750
Epoch [1/1], Batch [451], Loss: 146503.031250
Epoch [1/1], Batch [461], Loss: 149039.375000
Epoch [1/1], Batch [471], Loss: 151145.531250
Epoch [1/1], Batch [481], Loss: 145890.453125
Epoch [1/1], Batch [491], Loss: 145135.515625
Epoch [1/1], Batch [501], Loss: 144352.406250
Epoch [1/1], Batch [511], Loss: 149297.390625
Epoch [1/1], Batch [521], Loss: 145527.890625
Epoch [1/1], Batch [531], Loss: 144269.750000
Epoch [1/1], Batch [541], Loss: 154309.843750
Epoch [1/1], Batch [551], Loss: 146759.281250
Epoch [1/1], Batch [561], Loss: 142109.843750
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 148657.5073
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 140096.1367
Elapsed time: 4428.65 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 149159.0262
Elapsed time: 4445.51 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 168076.375000
Epoch [1/1], Batch [11], Loss: 167344.875000
Epoch [1/1], Batch [21], Loss: 174109.140625
Epoch [1/1], Batch [31], Loss: 170672.046875
Epoch [1/1], Batch [41], Loss: 168685.781250
Epoch [1/1], Batch [51], Loss: 168291.312500
Epoch [1/1], Batch [61], Loss: 170329.093750
Epoch [1/1], Batch [71], Loss: 169919.531250
Epoch [1/1], Batch [81], Loss: 177469.156250
Epoch [1/1], Batch [91], Loss: 173467.281250
Epoch [1/1], Batch [101], Loss: 170461.546875
Epoch [1/1], Batch [111], Loss: 170094.750000
Epoch [1/1], Batch [121], Loss: 171829.531250
Epoch [1/1], Batch [131], Loss: 173401.281250
Epoch [1/1], Batch [141], Loss: 174795.906250
Epoch [1/1], Batch [151], Loss: 167225.078125
Epoch [1/1], Batch [161], Loss: 169779.343750
Epoch [1/1], Batch [171], Loss: 169502.906250
Epoch [1/1], Batch [181], Loss: 182847.828125
Epoch [1/1], Batch [191], Loss: 172223.031250
Epoch [1/1], Batch [201], Loss: 169878.562500
Epoch [1/1], Batch [211], Loss: 170066.218750
Epoch [1/1], Batch [221], Loss: 171426.328125
Epoch [1/1], Batch [231], Loss: 174182.062500
Epoch [1/1], Batch [241], Loss: 162832.218750
Epoch [1/1], Batch [251], Loss: 163201.156250
Epoch [1/1], Batch [261], Loss: 170414.718750
Epoch [1/1], Batch [271], Loss: 163538.500000
Epoch [1/1], Batch [281], Loss: 170052.468750
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 169521.7045
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 160336.7853
Elapsed time: 4746.24 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 167582.4316
Elapsed time: 4755.66 seconds

Training complete!
Totoal elapsed time: 4755.66 seconds
CUDA is available!
