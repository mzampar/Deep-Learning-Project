Starting job 1014228
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [5, 5, 5, 5],
    leaky_slope = None,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = False,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 0.702538
Epoch [1/1], Batch [11], Loss: 0.211628
Epoch [1/1], Batch [21], Loss: 0.159242
Epoch [1/1], Batch [31], Loss: 0.134872
Epoch [1/1], Batch [41], Loss: 0.114176
Epoch [1/1], Batch [51], Loss: 0.108559
Epoch [1/1], Batch [61], Loss: 0.106134
Epoch [1/1], Batch [71], Loss: 0.104189
Epoch [1/1], Batch [81], Loss: 0.099161
Epoch [1/1], Batch [91], Loss: 0.097999
Epoch [1/1], Batch [101], Loss: 0.098945
Epoch [1/1], Batch [111], Loss: 0.097751
Epoch [1/1], Batch [121], Loss: 0.098516
Epoch [1/1], Batch [131], Loss: 0.095235
Epoch [1/1], Batch [141], Loss: 0.097354
Epoch [1/1], Batch [151], Loss: 0.090861
Epoch [1/1], Batch [161], Loss: 0.091468
Epoch [1/1], Batch [171], Loss: 0.091819
Epoch [1/1], Batch [181], Loss: 0.091854
Epoch [1/1], Batch [191], Loss: 0.088005
Epoch [1/1], Batch [201], Loss: 0.090011
Epoch [1/1], Batch [211], Loss: 0.091506
Epoch [1/1], Batch [221], Loss: 0.087000
Epoch [1/1], Batch [231], Loss: 0.086621
Epoch [1/1], Batch [241], Loss: 0.088611
Epoch [1/1], Batch [251], Loss: 0.086656
Epoch [1/1], Batch [261], Loss: 0.089692
Epoch [1/1], Batch [271], Loss: 0.086632
Epoch [1/1], Batch [281], Loss: 0.086031
Epoch [1/1], Batch [291], Loss: 0.083600
Epoch [1/1], Batch [301], Loss: 0.086322
Epoch [1/1], Batch [311], Loss: 0.085083
Epoch [1/1], Batch [321], Loss: 0.084930
Epoch [1/1], Batch [331], Loss: 0.084752
Epoch [1/1], Batch [341], Loss: 0.082088
Epoch [1/1], Batch [351], Loss: 0.077628
Epoch [1/1], Batch [361], Loss: 0.084300
Epoch [1/1], Batch [371], Loss: 0.080739
Epoch [1/1], Batch [381], Loss: 0.080589
Epoch [1/1], Batch [391], Loss: 0.083048
Epoch [1/1], Batch [401], Loss: 0.083081
Epoch [1/1], Batch [411], Loss: 0.083497
Epoch [1/1], Batch [421], Loss: 0.083648
Epoch [1/1], Batch [431], Loss: 0.083419
Epoch [1/1], Batch [441], Loss: 0.085290
Epoch [1/1], Batch [451], Loss: 0.086613
Epoch [1/1], Batch [461], Loss: 0.082030
Epoch [1/1], Batch [471], Loss: 0.080435
Epoch [1/1], Batch [481], Loss: 0.082560
Epoch [1/1], Batch [491], Loss: 0.084396
Epoch [1/1], Batch [501], Loss: 0.078783
Epoch [1/1], Batch [511], Loss: 0.079842
Epoch [1/1], Batch [521], Loss: 0.081146
Epoch [1/1], Batch [531], Loss: 0.079629
Epoch [1/1], Batch [541], Loss: 0.077729
Epoch [1/1], Batch [551], Loss: 0.082093
Epoch [1/1], Batch [561], Loss: 0.080782
Epoch [1/1], Batch [571], Loss: 0.081413
Epoch [1/1], Batch [581], Loss: 0.078884
Epoch [1/1], Batch [591], Loss: 0.083250
Epoch [1/1], Batch [601], Loss: 0.078744
Epoch [1/1], Batch [611], Loss: 0.079480
Epoch [1/1], Batch [621], Loss: 0.078398
Epoch [1/1], Batch [631], Loss: 0.078278
Epoch [1/1], Batch [641], Loss: 0.074022
Epoch [1/1], Batch [651], Loss: 0.075885
Epoch [1/1], Batch [661], Loss: 0.075379
Epoch [1/1], Batch [671], Loss: 0.077410
Epoch [1/1], Batch [681], Loss: 0.081880
Epoch [1/1], Batch [691], Loss: 0.074683
Epoch [1/1], Batch [701], Loss: 0.074981
Epoch [1/1], Batch [711], Loss: 0.077148
Epoch [1/1], Batch [721], Loss: 0.071357
Epoch [1/1], Batch [731], Loss: 0.076035
Epoch [1/1], Batch [741], Loss: 0.078772
Epoch [1/1], Batch [751], Loss: 0.073439
Epoch [1/1], Batch [761], Loss: 0.075075
Epoch [1/1], Batch [771], Loss: 0.077354
Epoch [1/1], Batch [781], Loss: 0.074786
Epoch [1/1], Batch [791], Loss: 0.076598
Epoch [1/1], Batch [801], Loss: 0.076765
Epoch [1/1], Batch [811], Loss: 0.078121
Epoch [1/1], Batch [821], Loss: 0.071601
Epoch [1/1], Batch [831], Loss: 0.074421
Epoch [1/1], Batch [841], Loss: 0.074175
Epoch [1/1], Batch [851], Loss: 0.072946
Epoch [1/1], Batch [861], Loss: 0.076010
Epoch [1/1], Batch [871], Loss: 0.070914
Epoch [1/1], Batch [881], Loss: 0.075530
Epoch [1/1], Batch [891], Loss: 0.076377
Epoch [1/1], Batch [901], Loss: 0.073115
Epoch [1/1], Batch [911], Loss: 0.076143
Epoch [1/1], Batch [921], Loss: 0.072517
Epoch [1/1], Batch [931], Loss: 0.074827
Epoch [1/1], Batch [941], Loss: 0.073765
Epoch [1/1], Batch [951], Loss: 0.076099
Epoch [1/1], Batch [961], Loss: 0.072720
Epoch [1/1], Batch [971], Loss: 0.074046
Epoch [1/1], Batch [981], Loss: 0.075479
Epoch [1/1], Batch [991], Loss: 0.075608
Epoch [1/1], Batch [1001], Loss: 0.074981
Epoch [1/1], Batch [1011], Loss: 0.074376
Epoch [1/1], Batch [1021], Loss: 0.075891
Epoch [1/1], Batch [1031], Loss: 0.073480
Epoch [1/1], Batch [1041], Loss: 0.075457
Epoch [1/1], Batch [1051], Loss: 0.076578
Epoch [1/1], Batch [1061], Loss: 0.071341
Epoch [1/1], Batch [1071], Loss: 0.073951
Epoch [1/1], Batch [1081], Loss: 0.070910
Epoch [1/1], Batch [1091], Loss: 0.071399
Epoch [1/1], Batch [1101], Loss: 0.075289
Epoch [1/1], Batch [1111], Loss: 0.073305
Epoch [1/1], Batch [1121], Loss: 0.075736
Epoch [1/1], Batch [1131], Loss: 0.071892
Epoch [1/1], Batch [1141], Loss: 0.070338
Epoch [1/1], Batch [1151], Loss: 0.072117
Epoch [1/1], Batch [1161], Loss: 0.073206
Epoch [1/1], Batch [1171], Loss: 0.069267
Epoch [1/1], Batch [1181], Loss: 0.073772
Epoch [1/1], Batch [1191], Loss: 0.071032
Epoch [1/1], Batch [1201], Loss: 0.071684
Epoch [1/1], Batch [1211], Loss: 0.073559
Epoch [1/1], Batch [1221], Loss: 0.069844
Epoch [1/1], Batch [1231], Loss: 0.069266
Epoch [1/1], Batch [1241], Loss: 0.070393
Epoch [1/1], Batch [1251], Loss: 0.071353
Epoch [1/1], Batch [1261], Loss: 0.072726
Epoch [1/1], Batch [1271], Loss: 0.072419
Epoch [1/1], Batch [1281], Loss: 0.071006
Epoch [1/1], Batch [1291], Loss: 0.070661
Epoch [1/1], Batch [1301], Loss: 0.072735
Epoch [1/1], Batch [1311], Loss: 0.071220
Epoch [1/1], Batch [1321], Loss: 0.072464
Epoch [1/1], Batch [1331], Loss: 0.071538
Epoch [1/1], Batch [1341], Loss: 0.067891
Epoch [1/1], Batch [1351], Loss: 0.072356
Epoch [1/1], Batch [1361], Loss: 0.071790
Epoch [1/1], Batch [1371], Loss: 0.070267
Epoch [1/1], Batch [1381], Loss: 0.071336
Epoch [1/1], Batch [1391], Loss: 0.071751
Epoch [1/1], Batch [1401], Loss: 0.069950
Epoch [1/1], Batch [1411], Loss: 0.068950
Epoch [1/1], Batch [1421], Loss: 0.071934
Epoch [1/1], Batch [1431], Loss: 0.064980
Epoch [1/1], Batch [1441], Loss: 0.073229
Epoch [1/1], Batch [1451], Loss: 0.070908
Epoch [1/1], Batch [1461], Loss: 0.071208
Epoch [1/1], Batch [1471], Loss: 0.073038
Epoch [1/1], Batch [1481], Loss: 0.073949
Epoch [1/1], Batch [1491], Loss: 0.073387
Epoch [1/1], Batch [1501], Loss: 0.070263
Epoch [1/1], Batch [1511], Loss: 0.070041
Epoch [1/1], Batch [1521], Loss: 0.070060
Epoch [1/1], Batch [1531], Loss: 0.073274
Epoch [1/1], Batch [1541], Loss: 0.072032
Epoch [1/1], Batch [1551], Loss: 0.068715
Epoch [1/1], Batch [1561], Loss: 0.069429
Epoch [1/1], Batch [1571], Loss: 0.069917
Epoch [1/1], Batch [1581], Loss: 0.069577
Epoch [1/1], Batch [1591], Loss: 0.069971
Epoch [1/1], Batch [1601], Loss: 0.072126
Epoch [1/1], Batch [1611], Loss: 0.070783
Epoch [1/1], Batch [1621], Loss: 0.071510
Epoch [1/1], Batch [1631], Loss: 0.074422
Epoch [1/1], Batch [1641], Loss: 0.070338
Epoch [1/1], Batch [1651], Loss: 0.070544
Epoch [1/1], Batch [1661], Loss: 0.071558
Epoch [1/1], Batch [1671], Loss: 0.072029
Epoch [1/1], Batch [1681], Loss: 0.068221
Epoch [1/1], Batch [1691], Loss: 0.069383
Epoch [1/1], Batch [1701], Loss: 0.071036
Epoch [1/1], Batch [1711], Loss: 0.070024
Epoch [1/1], Batch [1721], Loss: 0.071052
Epoch [1/1], Batch [1731], Loss: 0.066013
Epoch [1/1], Batch [1741], Loss: 0.070843
Epoch [1/1], Batch [1751], Loss: 0.068727
Epoch [1/1], Batch [1761], Loss: 0.068186
Epoch [1/1], Batch [1771], Loss: 0.070504
Epoch [1/1], Batch [1781], Loss: 0.069737
Epoch [1/1], Batch [1791], Loss: 0.069422
Epoch [1/1], Batch [1801], Loss: 0.069747
Epoch [1/1], Batch [1811], Loss: 0.067003
Epoch [1/1], Batch [1821], Loss: 0.068199
Epoch [1/1], Batch [1831], Loss: 0.071921
Epoch [1/1], Batch [1841], Loss: 0.071172
Epoch [1/1], Batch [1851], Loss: 0.067060
Epoch [1/1], Batch [1861], Loss: 0.071333
Epoch [1/1], Batch [1871], Loss: 0.069521
Epoch [1/1], Batch [1881], Loss: 0.068907
Epoch [1/1], Batch [1891], Loss: 0.068481
Epoch [1/1], Batch [1901], Loss: 0.066523
Epoch [1/1], Batch [1911], Loss: 0.071432
Epoch [1/1], Batch [1921], Loss: 0.068641
Epoch [1/1], Batch [1931], Loss: 0.070339
Epoch [1/1], Batch [1941], Loss: 0.071389
Epoch [1/1], Batch [1951], Loss: 0.067022
Epoch [1/1], Batch [1961], Loss: 0.068408
Epoch [1/1], Batch [1971], Loss: 0.072184
Epoch [1/1], Batch [1981], Loss: 0.069884
Epoch [1/1], Batch [1991], Loss: 0.068584
Epoch [1/1], Batch [2001], Loss: 0.072697
Epoch [1/1], Batch [2011], Loss: 0.068383
Epoch [1/1], Batch [2021], Loss: 0.064826
Epoch [1/1], Batch [2031], Loss: 0.068732
Epoch [1/1], Batch [2041], Loss: 0.067905
Epoch [1/1], Batch [2051], Loss: 0.066699
Epoch [1/1], Batch [2061], Loss: 0.069137
Epoch [1/1], Batch [2071], Loss: 0.066964
Epoch [1/1], Batch [2081], Loss: 0.070142
Epoch [1/1], Batch [2091], Loss: 0.067426
Epoch [1/1], Batch [2101], Loss: 0.068719
Epoch [1/1], Batch [2111], Loss: 0.069843
Epoch [1/1], Batch [2121], Loss: 0.068145
Epoch [1/1], Batch [2131], Loss: 0.066555
Epoch [1/1], Batch [2141], Loss: 0.068225
Epoch [1/1], Batch [2151], Loss: 0.067962
Epoch [1/1], Batch [2161], Loss: 0.065616
Epoch [1/1], Batch [2171], Loss: 0.065469
Epoch [1/1], Batch [2181], Loss: 0.068847
Epoch [1/1], Batch [2191], Loss: 0.069499
Epoch [1/1], Batch [2201], Loss: 0.066366
Epoch [1/1], Batch [2211], Loss: 0.067655
Epoch [1/1], Batch [2221], Loss: 0.063268
Epoch [1/1], Batch [2231], Loss: 0.068099
Epoch [1/1], Batch [2241], Loss: 0.069220
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 0.0779
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 0.0671
Elapsed time: 503.94 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 0.0681
Elapsed time: 527.03 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 0.119961
Epoch [1/1], Batch [11], Loss: 0.097977
Epoch [1/1], Batch [21], Loss: 0.090545
Epoch [1/1], Batch [31], Loss: 0.083100
Epoch [1/1], Batch [41], Loss: 0.082147
Epoch [1/1], Batch [51], Loss: 0.080124
Epoch [1/1], Batch [61], Loss: 0.075551
Epoch [1/1], Batch [71], Loss: 0.077082
Epoch [1/1], Batch [81], Loss: 0.073551
Epoch [1/1], Batch [91], Loss: 0.077416
Epoch [1/1], Batch [101], Loss: 0.078079
Epoch [1/1], Batch [111], Loss: 0.074685
Epoch [1/1], Batch [121], Loss: 0.070446
Epoch [1/1], Batch [131], Loss: 0.073035
Epoch [1/1], Batch [141], Loss: 0.073814
Epoch [1/1], Batch [151], Loss: 0.070039
Epoch [1/1], Batch [161], Loss: 0.072522
Epoch [1/1], Batch [171], Loss: 0.073994
Epoch [1/1], Batch [181], Loss: 0.073087
Epoch [1/1], Batch [191], Loss: 0.072345
Epoch [1/1], Batch [201], Loss: 0.073378
Epoch [1/1], Batch [211], Loss: 0.073470
Epoch [1/1], Batch [221], Loss: 0.075225
Epoch [1/1], Batch [231], Loss: 0.073702
Epoch [1/1], Batch [241], Loss: 0.070182
Epoch [1/1], Batch [251], Loss: 0.073699
Epoch [1/1], Batch [261], Loss: 0.070178
Epoch [1/1], Batch [271], Loss: 0.068200
Epoch [1/1], Batch [281], Loss: 0.072329
Epoch [1/1], Batch [291], Loss: 0.072703
Epoch [1/1], Batch [301], Loss: 0.074391
Epoch [1/1], Batch [311], Loss: 0.073492
Epoch [1/1], Batch [321], Loss: 0.075205
Epoch [1/1], Batch [331], Loss: 0.068457
Epoch [1/1], Batch [341], Loss: 0.066440
Epoch [1/1], Batch [351], Loss: 0.071123
Epoch [1/1], Batch [361], Loss: 0.069369
Epoch [1/1], Batch [371], Loss: 0.068275
Epoch [1/1], Batch [381], Loss: 0.071737
Epoch [1/1], Batch [391], Loss: 0.068393
Epoch [1/1], Batch [401], Loss: 0.070601
Epoch [1/1], Batch [411], Loss: 0.067041
Epoch [1/1], Batch [421], Loss: 0.069311
Epoch [1/1], Batch [431], Loss: 0.068997
Epoch [1/1], Batch [441], Loss: 0.069193
Epoch [1/1], Batch [451], Loss: 0.073194
Epoch [1/1], Batch [461], Loss: 0.070249
Epoch [1/1], Batch [471], Loss: 0.073615
Epoch [1/1], Batch [481], Loss: 0.069540
Epoch [1/1], Batch [491], Loss: 0.071290
Epoch [1/1], Batch [501], Loss: 0.069731
Epoch [1/1], Batch [511], Loss: 0.069404
Epoch [1/1], Batch [521], Loss: 0.067787
Epoch [1/1], Batch [531], Loss: 0.068660
Epoch [1/1], Batch [541], Loss: 0.070337
Epoch [1/1], Batch [551], Loss: 0.068740
Epoch [1/1], Batch [561], Loss: 0.069779
Epoch [1/1], Batch [571], Loss: 0.073106
Epoch [1/1], Batch [581], Loss: 0.071117
Epoch [1/1], Batch [591], Loss: 0.067654
Epoch [1/1], Batch [601], Loss: 0.069175
Epoch [1/1], Batch [611], Loss: 0.068433
Epoch [1/1], Batch [621], Loss: 0.067039
Epoch [1/1], Batch [631], Loss: 0.068943
Epoch [1/1], Batch [641], Loss: 0.067660
Epoch [1/1], Batch [651], Loss: 0.069013
Epoch [1/1], Batch [661], Loss: 0.067495
Epoch [1/1], Batch [671], Loss: 0.069631
Epoch [1/1], Batch [681], Loss: 0.068671
Epoch [1/1], Batch [691], Loss: 0.062150
Epoch [1/1], Batch [701], Loss: 0.065715
Epoch [1/1], Batch [711], Loss: 0.064752
Epoch [1/1], Batch [721], Loss: 0.065169
Epoch [1/1], Batch [731], Loss: 0.069752
Epoch [1/1], Batch [741], Loss: 0.070053
Epoch [1/1], Batch [751], Loss: 0.066337
Epoch [1/1], Batch [761], Loss: 0.064459
Epoch [1/1], Batch [771], Loss: 0.068264
Epoch [1/1], Batch [781], Loss: 0.070561
Epoch [1/1], Batch [791], Loss: 0.067880
Epoch [1/1], Batch [801], Loss: 0.069656
Epoch [1/1], Batch [811], Loss: 0.066819
Epoch [1/1], Batch [821], Loss: 0.067579
Epoch [1/1], Batch [831], Loss: 0.068092
Epoch [1/1], Batch [841], Loss: 0.066145
Epoch [1/1], Batch [851], Loss: 0.064206
Epoch [1/1], Batch [861], Loss: 0.066793
Epoch [1/1], Batch [871], Loss: 0.068408
Epoch [1/1], Batch [881], Loss: 0.067199
Epoch [1/1], Batch [891], Loss: 0.064700
Epoch [1/1], Batch [901], Loss: 0.067988
Epoch [1/1], Batch [911], Loss: 0.069634
Epoch [1/1], Batch [921], Loss: 0.067474
Epoch [1/1], Batch [931], Loss: 0.068440
Epoch [1/1], Batch [941], Loss: 0.064468
Epoch [1/1], Batch [951], Loss: 0.068016
Epoch [1/1], Batch [961], Loss: 0.065502
Epoch [1/1], Batch [971], Loss: 0.065846
Epoch [1/1], Batch [981], Loss: 0.069290
Epoch [1/1], Batch [991], Loss: 0.063308
Epoch [1/1], Batch [1001], Loss: 0.065224
Epoch [1/1], Batch [1011], Loss: 0.066332
Epoch [1/1], Batch [1021], Loss: 0.066673
Epoch [1/1], Batch [1031], Loss: 0.069985
Epoch [1/1], Batch [1041], Loss: 0.064741
Epoch [1/1], Batch [1051], Loss: 0.065083
Epoch [1/1], Batch [1061], Loss: 0.067461
Epoch [1/1], Batch [1071], Loss: 0.068961
Epoch [1/1], Batch [1081], Loss: 0.069360
Epoch [1/1], Batch [1091], Loss: 0.066142
Epoch [1/1], Batch [1101], Loss: 0.067276
Epoch [1/1], Batch [1111], Loss: 0.062517
Epoch [1/1], Batch [1121], Loss: 0.060923
Epoch [1/1], Batch [1131], Loss: 0.065427
Epoch [1/1], Batch [1141], Loss: 0.067860
Epoch [1/1], Batch [1151], Loss: 0.063510
Epoch [1/1], Batch [1161], Loss: 0.068714
Epoch [1/1], Batch [1171], Loss: 0.068712
Epoch [1/1], Batch [1181], Loss: 0.066762
Epoch [1/1], Batch [1191], Loss: 0.061316
Epoch [1/1], Batch [1201], Loss: 0.067809
Epoch [1/1], Batch [1211], Loss: 0.068625
Epoch [1/1], Batch [1221], Loss: 0.062121
Epoch [1/1], Batch [1231], Loss: 0.064706
Epoch [1/1], Batch [1241], Loss: 0.065627
Epoch [1/1], Batch [1251], Loss: 0.070395
Epoch [1/1], Batch [1261], Loss: 0.066283
Epoch [1/1], Batch [1271], Loss: 0.066638
Epoch [1/1], Batch [1281], Loss: 0.069457
Epoch [1/1], Batch [1291], Loss: 0.067866
Epoch [1/1], Batch [1301], Loss: 0.063941
Epoch [1/1], Batch [1311], Loss: 0.064539
Epoch [1/1], Batch [1321], Loss: 0.063096
Epoch [1/1], Batch [1331], Loss: 0.067287
Epoch [1/1], Batch [1341], Loss: 0.064694
Epoch [1/1], Batch [1351], Loss: 0.065412
Epoch [1/1], Batch [1361], Loss: 0.065074
Epoch [1/1], Batch [1371], Loss: 0.066879
Epoch [1/1], Batch [1381], Loss: 0.065145
Epoch [1/1], Batch [1391], Loss: 0.065606
Epoch [1/1], Batch [1401], Loss: 0.065738
Epoch [1/1], Batch [1411], Loss: 0.064153
Epoch [1/1], Batch [1421], Loss: 0.063041
Epoch [1/1], Batch [1431], Loss: 0.064210
Epoch [1/1], Batch [1441], Loss: 0.063702
Epoch [1/1], Batch [1451], Loss: 0.064854
Epoch [1/1], Batch [1461], Loss: 0.064394
Epoch [1/1], Batch [1471], Loss: 0.066046
Epoch [1/1], Batch [1481], Loss: 0.064466
Epoch [1/1], Batch [1491], Loss: 0.063054
Epoch [1/1], Batch [1501], Loss: 0.065486
Epoch [1/1], Batch [1511], Loss: 0.065859
Epoch [1/1], Batch [1521], Loss: 0.061579
Epoch [1/1], Batch [1531], Loss: 0.069304
Epoch [1/1], Batch [1541], Loss: 0.064794
Epoch [1/1], Batch [1551], Loss: 0.064800
Epoch [1/1], Batch [1561], Loss: 0.063557
Epoch [1/1], Batch [1571], Loss: 0.062095
Epoch [1/1], Batch [1581], Loss: 0.064868
Epoch [1/1], Batch [1591], Loss: 0.061977
Epoch [1/1], Batch [1601], Loss: 0.066525
Epoch [1/1], Batch [1611], Loss: 0.069041
Epoch [1/1], Batch [1621], Loss: 0.065389
Epoch [1/1], Batch [1631], Loss: 0.063811
Epoch [1/1], Batch [1641], Loss: 0.065661
Epoch [1/1], Batch [1651], Loss: 0.064202
Epoch [1/1], Batch [1661], Loss: 0.064684
Epoch [1/1], Batch [1671], Loss: 0.066811
Epoch [1/1], Batch [1681], Loss: 0.064125
Epoch [1/1], Batch [1691], Loss: 0.067438
Epoch [1/1], Batch [1701], Loss: 0.063839
Epoch [1/1], Batch [1711], Loss: 0.072382
Epoch [1/1], Batch [1721], Loss: 0.063039
Epoch [1/1], Batch [1731], Loss: 0.067950
Epoch [1/1], Batch [1741], Loss: 0.062592
Epoch [1/1], Batch [1751], Loss: 0.066350
Epoch [1/1], Batch [1761], Loss: 0.064915
Epoch [1/1], Batch [1771], Loss: 0.064730
Epoch [1/1], Batch [1781], Loss: 0.065364
Epoch [1/1], Batch [1791], Loss: 0.064690
Epoch [1/1], Batch [1801], Loss: 0.064121
Epoch [1/1], Batch [1811], Loss: 0.066662
Epoch [1/1], Batch [1821], Loss: 0.066933
Epoch [1/1], Batch [1831], Loss: 0.064362
Epoch [1/1], Batch [1841], Loss: 0.064199
Epoch [1/1], Batch [1851], Loss: 0.065640
Epoch [1/1], Batch [1861], Loss: 0.065792
Epoch [1/1], Batch [1871], Loss: 0.065641
Epoch [1/1], Batch [1881], Loss: 0.062623
Epoch [1/1], Batch [1891], Loss: 0.064339
Epoch [1/1], Batch [1901], Loss: 0.064420
Epoch [1/1], Batch [1911], Loss: 0.064520
Epoch [1/1], Batch [1921], Loss: 0.066986
Epoch [1/1], Batch [1931], Loss: 0.064159
Epoch [1/1], Batch [1941], Loss: 0.064017
Epoch [1/1], Batch [1951], Loss: 0.065330
Epoch [1/1], Batch [1961], Loss: 0.065644
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 0.0682
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 0.0624
Elapsed time: 1170.18 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 0.0638
Elapsed time: 1197.99 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 0.069568
Epoch [1/1], Batch [11], Loss: 0.070372
Epoch [1/1], Batch [21], Loss: 0.071305
Epoch [1/1], Batch [31], Loss: 0.061771
Epoch [1/1], Batch [41], Loss: 0.066694
Epoch [1/1], Batch [51], Loss: 0.068183
Epoch [1/1], Batch [61], Loss: 0.064101
Epoch [1/1], Batch [71], Loss: 0.069602
Epoch [1/1], Batch [81], Loss: 0.063612
Epoch [1/1], Batch [91], Loss: 0.067769
Epoch [1/1], Batch [101], Loss: 0.064531
Epoch [1/1], Batch [111], Loss: 0.068351
Epoch [1/1], Batch [121], Loss: 0.067412
Epoch [1/1], Batch [131], Loss: 0.068946
Epoch [1/1], Batch [141], Loss: 0.068612
Epoch [1/1], Batch [151], Loss: 0.067850
Epoch [1/1], Batch [161], Loss: 0.065966
Epoch [1/1], Batch [171], Loss: 0.063554
Epoch [1/1], Batch [181], Loss: 0.063781
Epoch [1/1], Batch [191], Loss: 0.066554
Epoch [1/1], Batch [201], Loss: 0.065378
Epoch [1/1], Batch [211], Loss: 0.066277
Epoch [1/1], Batch [221], Loss: 0.064275
Epoch [1/1], Batch [231], Loss: 0.066946
Epoch [1/1], Batch [241], Loss: 0.068788
Epoch [1/1], Batch [251], Loss: 0.065337
Epoch [1/1], Batch [261], Loss: 0.064745
Epoch [1/1], Batch [271], Loss: 0.067081
Epoch [1/1], Batch [281], Loss: 0.064939
Epoch [1/1], Batch [291], Loss: 0.065043
Epoch [1/1], Batch [301], Loss: 0.065941
Epoch [1/1], Batch [311], Loss: 0.061453
Epoch [1/1], Batch [321], Loss: 0.070348
Epoch [1/1], Batch [331], Loss: 0.061776
Epoch [1/1], Batch [341], Loss: 0.065210
Epoch [1/1], Batch [351], Loss: 0.062614
Epoch [1/1], Batch [361], Loss: 0.063922
Epoch [1/1], Batch [371], Loss: 0.063831
Epoch [1/1], Batch [381], Loss: 0.067557
Epoch [1/1], Batch [391], Loss: 0.065510
Epoch [1/1], Batch [401], Loss: 0.064201
Epoch [1/1], Batch [411], Loss: 0.064302
Epoch [1/1], Batch [421], Loss: 0.064542
Epoch [1/1], Batch [431], Loss: 0.066409
Epoch [1/1], Batch [441], Loss: 0.060384
Epoch [1/1], Batch [451], Loss: 0.067455
Epoch [1/1], Batch [461], Loss: 0.065255
Epoch [1/1], Batch [471], Loss: 0.064665
Epoch [1/1], Batch [481], Loss: 0.066913
Epoch [1/1], Batch [491], Loss: 0.067164
Epoch [1/1], Batch [501], Loss: 0.066052
Epoch [1/1], Batch [511], Loss: 0.066453
Epoch [1/1], Batch [521], Loss: 0.065916
Epoch [1/1], Batch [531], Loss: 0.063635
Epoch [1/1], Batch [541], Loss: 0.067539
Epoch [1/1], Batch [551], Loss: 0.063970
Epoch [1/1], Batch [561], Loss: 0.063470
Epoch [1/1], Batch [571], Loss: 0.064950
Epoch [1/1], Batch [581], Loss: 0.063119
Epoch [1/1], Batch [591], Loss: 0.066710
Epoch [1/1], Batch [601], Loss: 0.066153
Epoch [1/1], Batch [611], Loss: 0.065837
Epoch [1/1], Batch [621], Loss: 0.064485
Epoch [1/1], Batch [631], Loss: 0.063892
Epoch [1/1], Batch [641], Loss: 0.063457
Epoch [1/1], Batch [651], Loss: 0.066497
Epoch [1/1], Batch [661], Loss: 0.065423
Epoch [1/1], Batch [671], Loss: 0.064257
Epoch [1/1], Batch [681], Loss: 0.062898
Epoch [1/1], Batch [691], Loss: 0.061204
Epoch [1/1], Batch [701], Loss: 0.063957
Epoch [1/1], Batch [711], Loss: 0.060346
Epoch [1/1], Batch [721], Loss: 0.066173
Epoch [1/1], Batch [731], Loss: 0.061991
Epoch [1/1], Batch [741], Loss: 0.062202
Epoch [1/1], Batch [751], Loss: 0.063024
Epoch [1/1], Batch [761], Loss: 0.064212
Epoch [1/1], Batch [771], Loss: 0.065006
Epoch [1/1], Batch [781], Loss: 0.064277
Epoch [1/1], Batch [791], Loss: 0.065196
Epoch [1/1], Batch [801], Loss: 0.064613
Epoch [1/1], Batch [811], Loss: 0.062153
Epoch [1/1], Batch [821], Loss: 0.059208
Epoch [1/1], Batch [831], Loss: 0.066534
Epoch [1/1], Batch [841], Loss: 0.063159
Epoch [1/1], Batch [851], Loss: 0.061800
Epoch [1/1], Batch [861], Loss: 0.061147
Epoch [1/1], Batch [871], Loss: 0.064921
Epoch [1/1], Batch [881], Loss: 0.065739
Epoch [1/1], Batch [891], Loss: 0.063766
Epoch [1/1], Batch [901], Loss: 0.063268
Epoch [1/1], Batch [911], Loss: 0.062008
Epoch [1/1], Batch [921], Loss: 0.066737
Epoch [1/1], Batch [931], Loss: 0.068547
Epoch [1/1], Batch [941], Loss: 0.062668
Epoch [1/1], Batch [951], Loss: 0.062703
Epoch [1/1], Batch [961], Loss: 0.063593
Epoch [1/1], Batch [971], Loss: 0.061004
Epoch [1/1], Batch [981], Loss: 0.060322
Epoch [1/1], Batch [991], Loss: 0.063607
Epoch [1/1], Batch [1001], Loss: 0.065034
Epoch [1/1], Batch [1011], Loss: 0.062745
Epoch [1/1], Batch [1021], Loss: 0.061948
Epoch [1/1], Batch [1031], Loss: 0.064659
Epoch [1/1], Batch [1041], Loss: 0.064644
Epoch [1/1], Batch [1051], Loss: 0.065479
Epoch [1/1], Batch [1061], Loss: 0.061003
Epoch [1/1], Batch [1071], Loss: 0.061685
Epoch [1/1], Batch [1081], Loss: 0.062710
Epoch [1/1], Batch [1091], Loss: 0.064039
Epoch [1/1], Batch [1101], Loss: 0.061504
Epoch [1/1], Batch [1111], Loss: 0.062432
Epoch [1/1], Batch [1121], Loss: 0.061300
Epoch [1/1], Batch [1131], Loss: 0.062838
Epoch [1/1], Batch [1141], Loss: 0.066596
Epoch [1/1], Batch [1151], Loss: 0.062625
Epoch [1/1], Batch [1161], Loss: 0.064566
Epoch [1/1], Batch [1171], Loss: 0.062116
Epoch [1/1], Batch [1181], Loss: 0.062710
Epoch [1/1], Batch [1191], Loss: 0.064863
Epoch [1/1], Batch [1201], Loss: 0.065779
Epoch [1/1], Batch [1211], Loss: 0.060066
Epoch [1/1], Batch [1221], Loss: 0.061044
Epoch [1/1], Batch [1231], Loss: 0.063784
Epoch [1/1], Batch [1241], Loss: 0.059731
Epoch [1/1], Batch [1251], Loss: 0.064431
Epoch [1/1], Batch [1261], Loss: 0.067661
Epoch [1/1], Batch [1271], Loss: 0.060708
Epoch [1/1], Batch [1281], Loss: 0.063547
Epoch [1/1], Batch [1291], Loss: 0.062888
Epoch [1/1], Batch [1301], Loss: 0.062932
Epoch [1/1], Batch [1311], Loss: 0.064433
Epoch [1/1], Batch [1321], Loss: 0.063949
Epoch [1/1], Batch [1331], Loss: 0.065663
Epoch [1/1], Batch [1341], Loss: 0.060648
Epoch [1/1], Batch [1351], Loss: 0.061750
Epoch [1/1], Batch [1361], Loss: 0.063545
Epoch [1/1], Batch [1371], Loss: 0.063816
Epoch [1/1], Batch [1381], Loss: 0.062441
Epoch [1/1], Batch [1391], Loss: 0.062585
Epoch [1/1], Batch [1401], Loss: 0.063695
Epoch [1/1], Batch [1411], Loss: 0.063373
Epoch [1/1], Batch [1421], Loss: 0.062366
Epoch [1/1], Batch [1431], Loss: 0.060940
Epoch [1/1], Batch [1441], Loss: 0.060066
Epoch [1/1], Batch [1451], Loss: 0.061833
Epoch [1/1], Batch [1461], Loss: 0.065565
Epoch [1/1], Batch [1471], Loss: 0.060616
Epoch [1/1], Batch [1481], Loss: 0.061590
Epoch [1/1], Batch [1491], Loss: 0.065379
Epoch [1/1], Batch [1501], Loss: 0.062279
Epoch [1/1], Batch [1511], Loss: 0.063090
Epoch [1/1], Batch [1521], Loss: 0.061998
Epoch [1/1], Batch [1531], Loss: 0.064163
Epoch [1/1], Batch [1541], Loss: 0.057875
Epoch [1/1], Batch [1551], Loss: 0.064857
Epoch [1/1], Batch [1561], Loss: 0.063913
Epoch [1/1], Batch [1571], Loss: 0.063246
Epoch [1/1], Batch [1581], Loss: 0.064198
Epoch [1/1], Batch [1591], Loss: 0.062337
Epoch [1/1], Batch [1601], Loss: 0.064406
Epoch [1/1], Batch [1611], Loss: 0.064449
Epoch [1/1], Batch [1621], Loss: 0.061277
Epoch [1/1], Batch [1631], Loss: 0.058907
Epoch [1/1], Batch [1641], Loss: 0.063754
Epoch [1/1], Batch [1651], Loss: 0.064109
Epoch [1/1], Batch [1661], Loss: 0.063448
Epoch [1/1], Batch [1671], Loss: 0.062037
Epoch [1/1], Batch [1681], Loss: 0.062135
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 0.0641
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 0.0613
Elapsed time: 1920.42 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 0.0643
Elapsed time: 1950.90 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 0.069310
Epoch [1/1], Batch [11], Loss: 0.070458
Epoch [1/1], Batch [21], Loss: 0.067002
Epoch [1/1], Batch [31], Loss: 0.063245
Epoch [1/1], Batch [41], Loss: 0.065011
Epoch [1/1], Batch [51], Loss: 0.067358
Epoch [1/1], Batch [61], Loss: 0.061839
Epoch [1/1], Batch [71], Loss: 0.063045
Epoch [1/1], Batch [81], Loss: 0.065244
Epoch [1/1], Batch [91], Loss: 0.064640
Epoch [1/1], Batch [101], Loss: 0.067340
Epoch [1/1], Batch [111], Loss: 0.066330
Epoch [1/1], Batch [121], Loss: 0.063039
Epoch [1/1], Batch [131], Loss: 0.064353
Epoch [1/1], Batch [141], Loss: 0.065746
Epoch [1/1], Batch [151], Loss: 0.063189
Epoch [1/1], Batch [161], Loss: 0.062627
Epoch [1/1], Batch [171], Loss: 0.064409
Epoch [1/1], Batch [181], Loss: 0.065009
Epoch [1/1], Batch [191], Loss: 0.062331
Epoch [1/1], Batch [201], Loss: 0.063332
Epoch [1/1], Batch [211], Loss: 0.062008
Epoch [1/1], Batch [221], Loss: 0.062603
Epoch [1/1], Batch [231], Loss: 0.065445
Epoch [1/1], Batch [241], Loss: 0.060903
Epoch [1/1], Batch [251], Loss: 0.060282
Epoch [1/1], Batch [261], Loss: 0.061806
Epoch [1/1], Batch [271], Loss: 0.063262
Epoch [1/1], Batch [281], Loss: 0.063162
Epoch [1/1], Batch [291], Loss: 0.065255
Epoch [1/1], Batch [301], Loss: 0.064222
Epoch [1/1], Batch [311], Loss: 0.063511
Epoch [1/1], Batch [321], Loss: 0.060128
Epoch [1/1], Batch [331], Loss: 0.065941
Epoch [1/1], Batch [341], Loss: 0.062125
Epoch [1/1], Batch [351], Loss: 0.060126
Epoch [1/1], Batch [361], Loss: 0.062654
Epoch [1/1], Batch [371], Loss: 0.062992
Epoch [1/1], Batch [381], Loss: 0.061456
Epoch [1/1], Batch [391], Loss: 0.063489
Epoch [1/1], Batch [401], Loss: 0.065219
Epoch [1/1], Batch [411], Loss: 0.060850
Epoch [1/1], Batch [421], Loss: 0.061567
Epoch [1/1], Batch [431], Loss: 0.064331
Epoch [1/1], Batch [441], Loss: 0.062955
Epoch [1/1], Batch [451], Loss: 0.063964
Epoch [1/1], Batch [461], Loss: 0.060972
Epoch [1/1], Batch [471], Loss: 0.063006
Epoch [1/1], Batch [481], Loss: 0.064455
Epoch [1/1], Batch [491], Loss: 0.063510
Epoch [1/1], Batch [501], Loss: 0.061069
Epoch [1/1], Batch [511], Loss: 0.064191
Epoch [1/1], Batch [521], Loss: 0.065300
Epoch [1/1], Batch [531], Loss: 0.061178
Epoch [1/1], Batch [541], Loss: 0.061851
Epoch [1/1], Batch [551], Loss: 0.060940
Epoch [1/1], Batch [561], Loss: 0.061287
Epoch [1/1], Batch [571], Loss: 0.060469
Epoch [1/1], Batch [581], Loss: 0.059648
Epoch [1/1], Batch [591], Loss: 0.058915
Epoch [1/1], Batch [601], Loss: 0.065256
Epoch [1/1], Batch [611], Loss: 0.063438
Epoch [1/1], Batch [621], Loss: 0.063780
Epoch [1/1], Batch [631], Loss: 0.061340
Epoch [1/1], Batch [641], Loss: 0.063431
Epoch [1/1], Batch [651], Loss: 0.063186
Epoch [1/1], Batch [661], Loss: 0.060461
Epoch [1/1], Batch [671], Loss: 0.065001
Epoch [1/1], Batch [681], Loss: 0.060907
Epoch [1/1], Batch [691], Loss: 0.064025
Epoch [1/1], Batch [701], Loss: 0.063932
Epoch [1/1], Batch [711], Loss: 0.063454
Epoch [1/1], Batch [721], Loss: 0.063826
Epoch [1/1], Batch [731], Loss: 0.065891
Epoch [1/1], Batch [741], Loss: 0.064273
Epoch [1/1], Batch [751], Loss: 0.064279
Epoch [1/1], Batch [761], Loss: 0.065155
Epoch [1/1], Batch [771], Loss: 0.067022
Epoch [1/1], Batch [781], Loss: 0.059453
Epoch [1/1], Batch [791], Loss: 0.062645
Epoch [1/1], Batch [801], Loss: 0.063565
Epoch [1/1], Batch [811], Loss: 0.062911
Epoch [1/1], Batch [821], Loss: 0.064337
Epoch [1/1], Batch [831], Loss: 0.061936
Epoch [1/1], Batch [841], Loss: 0.061987
Epoch [1/1], Batch [851], Loss: 0.063360
Epoch [1/1], Batch [861], Loss: 0.061997
Epoch [1/1], Batch [871], Loss: 0.062787
Epoch [1/1], Batch [881], Loss: 0.058086
Epoch [1/1], Batch [891], Loss: 0.058494
Epoch [1/1], Batch [901], Loss: 0.060251
Epoch [1/1], Batch [911], Loss: 0.065743
Epoch [1/1], Batch [921], Loss: 0.061264
Epoch [1/1], Batch [931], Loss: 0.061672
Epoch [1/1], Batch [941], Loss: 0.065189
Epoch [1/1], Batch [951], Loss: 0.064458
Epoch [1/1], Batch [961], Loss: 0.065982
Epoch [1/1], Batch [971], Loss: 0.063831
Epoch [1/1], Batch [981], Loss: 0.061015
Epoch [1/1], Batch [991], Loss: 0.063021
Epoch [1/1], Batch [1001], Loss: 0.062564
Epoch [1/1], Batch [1011], Loss: 0.063117
Epoch [1/1], Batch [1021], Loss: 0.062067
Epoch [1/1], Batch [1031], Loss: 0.064381
Epoch [1/1], Batch [1041], Loss: 0.065290
Epoch [1/1], Batch [1051], Loss: 0.062404
Epoch [1/1], Batch [1061], Loss: 0.062196
Epoch [1/1], Batch [1071], Loss: 0.060667
Epoch [1/1], Batch [1081], Loss: 0.063050
Epoch [1/1], Batch [1091], Loss: 0.061463
Epoch [1/1], Batch [1101], Loss: 0.063175
Epoch [1/1], Batch [1111], Loss: 0.060852
Epoch [1/1], Batch [1121], Loss: 0.061024
Epoch [1/1], Batch [1131], Loss: 0.066195
Epoch [1/1], Batch [1141], Loss: 0.061307
Epoch [1/1], Batch [1151], Loss: 0.065141
Epoch [1/1], Batch [1161], Loss: 0.063723
Epoch [1/1], Batch [1171], Loss: 0.059866
Epoch [1/1], Batch [1181], Loss: 0.060223
Epoch [1/1], Batch [1191], Loss: 0.059605
Epoch [1/1], Batch [1201], Loss: 0.061004
Epoch [1/1], Batch [1211], Loss: 0.064508
Epoch [1/1], Batch [1221], Loss: 0.058185
Epoch [1/1], Batch [1231], Loss: 0.061535
Epoch [1/1], Batch [1241], Loss: 0.063268
Epoch [1/1], Batch [1251], Loss: 0.064792
Epoch [1/1], Batch [1261], Loss: 0.059111
Epoch [1/1], Batch [1271], Loss: 0.059760
Epoch [1/1], Batch [1281], Loss: 0.062089
Epoch [1/1], Batch [1291], Loss: 0.059896
Epoch [1/1], Batch [1301], Loss: 0.062071
Epoch [1/1], Batch [1311], Loss: 0.063150
Epoch [1/1], Batch [1321], Loss: 0.062125
Epoch [1/1], Batch [1331], Loss: 0.064801
Epoch [1/1], Batch [1341], Loss: 0.057871
Epoch [1/1], Batch [1351], Loss: 0.061073
Epoch [1/1], Batch [1361], Loss: 0.060822
Epoch [1/1], Batch [1371], Loss: 0.063736
Epoch [1/1], Batch [1381], Loss: 0.063464
Epoch [1/1], Batch [1391], Loss: 0.058448
Epoch [1/1], Batch [1401], Loss: 0.062428
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 0.0626
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 0.0596
Elapsed time: 2700.96 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 0.0634
Elapsed time: 2731.87 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 0.066525
Epoch [1/1], Batch [11], Loss: 0.064601
Epoch [1/1], Batch [21], Loss: 0.062538
Epoch [1/1], Batch [31], Loss: 0.062707
Epoch [1/1], Batch [41], Loss: 0.063589
Epoch [1/1], Batch [51], Loss: 0.064924
Epoch [1/1], Batch [61], Loss: 0.062663
Epoch [1/1], Batch [71], Loss: 0.060794
Epoch [1/1], Batch [81], Loss: 0.065095
Epoch [1/1], Batch [91], Loss: 0.066062
Epoch [1/1], Batch [101], Loss: 0.060596
Epoch [1/1], Batch [111], Loss: 0.063040
Epoch [1/1], Batch [121], Loss: 0.059564
Epoch [1/1], Batch [131], Loss: 0.062581
Epoch [1/1], Batch [141], Loss: 0.062443
Epoch [1/1], Batch [151], Loss: 0.067135
Epoch [1/1], Batch [161], Loss: 0.063437
Epoch [1/1], Batch [171], Loss: 0.064136
Epoch [1/1], Batch [181], Loss: 0.061856
Epoch [1/1], Batch [191], Loss: 0.058548
Epoch [1/1], Batch [201], Loss: 0.061553
Epoch [1/1], Batch [211], Loss: 0.060329
Epoch [1/1], Batch [221], Loss: 0.063602
Epoch [1/1], Batch [231], Loss: 0.060474
Epoch [1/1], Batch [241], Loss: 0.061091
Epoch [1/1], Batch [251], Loss: 0.060735
Epoch [1/1], Batch [261], Loss: 0.067455
Epoch [1/1], Batch [271], Loss: 0.062394
Epoch [1/1], Batch [281], Loss: 0.061062
Epoch [1/1], Batch [291], Loss: 0.062544
Epoch [1/1], Batch [301], Loss: 0.060270
Epoch [1/1], Batch [311], Loss: 0.060811
Epoch [1/1], Batch [321], Loss: 0.060190
Epoch [1/1], Batch [331], Loss: 0.059847
Epoch [1/1], Batch [341], Loss: 0.063395
Epoch [1/1], Batch [351], Loss: 0.059889
Epoch [1/1], Batch [361], Loss: 0.061934
Epoch [1/1], Batch [371], Loss: 0.062226
Epoch [1/1], Batch [381], Loss: 0.063122
Epoch [1/1], Batch [391], Loss: 0.063545
Epoch [1/1], Batch [401], Loss: 0.059259
Epoch [1/1], Batch [411], Loss: 0.059254
Epoch [1/1], Batch [421], Loss: 0.063520
Epoch [1/1], Batch [431], Loss: 0.064040
Epoch [1/1], Batch [441], Loss: 0.060424
Epoch [1/1], Batch [451], Loss: 0.063197
Epoch [1/1], Batch [461], Loss: 0.060835
Epoch [1/1], Batch [471], Loss: 0.060765
Epoch [1/1], Batch [481], Loss: 0.062701
Epoch [1/1], Batch [491], Loss: 0.058865
Epoch [1/1], Batch [501], Loss: 0.061576
Epoch [1/1], Batch [511], Loss: 0.060133
Epoch [1/1], Batch [521], Loss: 0.061331
Epoch [1/1], Batch [531], Loss: 0.060040
Epoch [1/1], Batch [541], Loss: 0.060698
Epoch [1/1], Batch [551], Loss: 0.065090
Epoch [1/1], Batch [561], Loss: 0.058176
Epoch [1/1], Batch [571], Loss: 0.059592
Epoch [1/1], Batch [581], Loss: 0.061461
Epoch [1/1], Batch [591], Loss: 0.061908
Epoch [1/1], Batch [601], Loss: 0.061862
Epoch [1/1], Batch [611], Loss: 0.060303
Epoch [1/1], Batch [621], Loss: 0.063229
Epoch [1/1], Batch [631], Loss: 0.062662
Epoch [1/1], Batch [641], Loss: 0.062812
Epoch [1/1], Batch [651], Loss: 0.061582
Epoch [1/1], Batch [661], Loss: 0.060428
Epoch [1/1], Batch [671], Loss: 0.059830
Epoch [1/1], Batch [681], Loss: 0.062083
Epoch [1/1], Batch [691], Loss: 0.059952
Epoch [1/1], Batch [701], Loss: 0.060569
Epoch [1/1], Batch [711], Loss: 0.061806
Epoch [1/1], Batch [721], Loss: 0.062240
Epoch [1/1], Batch [731], Loss: 0.061010
Epoch [1/1], Batch [741], Loss: 0.061086
Epoch [1/1], Batch [751], Loss: 0.061277
Epoch [1/1], Batch [761], Loss: 0.063233
Epoch [1/1], Batch [771], Loss: 0.060874
Epoch [1/1], Batch [781], Loss: 0.060429
Epoch [1/1], Batch [791], Loss: 0.061871
Epoch [1/1], Batch [801], Loss: 0.063174
Epoch [1/1], Batch [811], Loss: 0.063900
Epoch [1/1], Batch [821], Loss: 0.061266
Epoch [1/1], Batch [831], Loss: 0.062074
Epoch [1/1], Batch [841], Loss: 0.060890
Epoch [1/1], Batch [851], Loss: 0.060750
Epoch [1/1], Batch [861], Loss: 0.065401
Epoch [1/1], Batch [871], Loss: 0.065493
Epoch [1/1], Batch [881], Loss: 0.059998
Epoch [1/1], Batch [891], Loss: 0.057914
Epoch [1/1], Batch [901], Loss: 0.063366
Epoch [1/1], Batch [911], Loss: 0.063349
Epoch [1/1], Batch [921], Loss: 0.058211
Epoch [1/1], Batch [931], Loss: 0.061022
Epoch [1/1], Batch [941], Loss: 0.058332
Epoch [1/1], Batch [951], Loss: 0.058379
Epoch [1/1], Batch [961], Loss: 0.061195
Epoch [1/1], Batch [971], Loss: 0.064240
Epoch [1/1], Batch [981], Loss: 0.059983
Epoch [1/1], Batch [991], Loss: 0.059596
Epoch [1/1], Batch [1001], Loss: 0.060344
Epoch [1/1], Batch [1011], Loss: 0.060699
Epoch [1/1], Batch [1021], Loss: 0.060105
Epoch [1/1], Batch [1031], Loss: 0.062662
Epoch [1/1], Batch [1041], Loss: 0.056860
Epoch [1/1], Batch [1051], Loss: 0.060853
Epoch [1/1], Batch [1061], Loss: 0.061121
Epoch [1/1], Batch [1071], Loss: 0.062481
Epoch [1/1], Batch [1081], Loss: 0.061450
Epoch [1/1], Batch [1091], Loss: 0.061247
Epoch [1/1], Batch [1101], Loss: 0.059913
Epoch [1/1], Batch [1111], Loss: 0.057691
Epoch [1/1], Batch [1121], Loss: 0.059921
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 0.0615
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 0.0579
Elapsed time: 3445.20 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 0.0623
Elapsed time: 3474.31 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 0.064300
Epoch [1/1], Batch [11], Loss: 0.061186
Epoch [1/1], Batch [21], Loss: 0.062493
Epoch [1/1], Batch [31], Loss: 0.060898
Epoch [1/1], Batch [41], Loss: 0.062489
Epoch [1/1], Batch [51], Loss: 0.059834
Epoch [1/1], Batch [61], Loss: 0.056789
Epoch [1/1], Batch [71], Loss: 0.058766
Epoch [1/1], Batch [81], Loss: 0.058505
Epoch [1/1], Batch [91], Loss: 0.058801
Epoch [1/1], Batch [101], Loss: 0.062051
Epoch [1/1], Batch [111], Loss: 0.059352
Epoch [1/1], Batch [121], Loss: 0.057773
Epoch [1/1], Batch [131], Loss: 0.061790
Epoch [1/1], Batch [141], Loss: 0.060063
Epoch [1/1], Batch [151], Loss: 0.057329
Epoch [1/1], Batch [161], Loss: 0.061222
Epoch [1/1], Batch [171], Loss: 0.062308
Epoch [1/1], Batch [181], Loss: 0.058703
Epoch [1/1], Batch [191], Loss: 0.060494
Epoch [1/1], Batch [201], Loss: 0.063553
Epoch [1/1], Batch [211], Loss: 0.058906
Epoch [1/1], Batch [221], Loss: 0.058638
Epoch [1/1], Batch [231], Loss: 0.060817
Epoch [1/1], Batch [241], Loss: 0.060824
Epoch [1/1], Batch [251], Loss: 0.060877
Epoch [1/1], Batch [261], Loss: 0.060834
Epoch [1/1], Batch [271], Loss: 0.065936
Epoch [1/1], Batch [281], Loss: 0.061996
Epoch [1/1], Batch [291], Loss: 0.059068
Epoch [1/1], Batch [301], Loss: 0.060151
Epoch [1/1], Batch [311], Loss: 0.064089
Epoch [1/1], Batch [321], Loss: 0.062239
Epoch [1/1], Batch [331], Loss: 0.060529
Epoch [1/1], Batch [341], Loss: 0.058662
Epoch [1/1], Batch [351], Loss: 0.062480
Epoch [1/1], Batch [361], Loss: 0.059496
Epoch [1/1], Batch [371], Loss: 0.059050
Epoch [1/1], Batch [381], Loss: 0.058406
Epoch [1/1], Batch [391], Loss: 0.060061
Epoch [1/1], Batch [401], Loss: 0.063715
Epoch [1/1], Batch [411], Loss: 0.059693
Epoch [1/1], Batch [421], Loss: 0.060520
Epoch [1/1], Batch [431], Loss: 0.063349
Epoch [1/1], Batch [441], Loss: 0.058454
Epoch [1/1], Batch [451], Loss: 0.061319
Epoch [1/1], Batch [461], Loss: 0.059034
Epoch [1/1], Batch [471], Loss: 0.058754
Epoch [1/1], Batch [481], Loss: 0.059501
Epoch [1/1], Batch [491], Loss: 0.063057
Epoch [1/1], Batch [501], Loss: 0.060742
Epoch [1/1], Batch [511], Loss: 0.062892
Epoch [1/1], Batch [521], Loss: 0.060868
Epoch [1/1], Batch [531], Loss: 0.059288
Epoch [1/1], Batch [541], Loss: 0.063667
Epoch [1/1], Batch [551], Loss: 0.061987
Epoch [1/1], Batch [561], Loss: 0.062070
Epoch [1/1], Batch [571], Loss: 0.061315
Epoch [1/1], Batch [581], Loss: 0.058972
Epoch [1/1], Batch [591], Loss: 0.056830
Epoch [1/1], Batch [601], Loss: 0.058296
Epoch [1/1], Batch [611], Loss: 0.063786
Epoch [1/1], Batch [621], Loss: 0.058626
Epoch [1/1], Batch [631], Loss: 0.057914
Epoch [1/1], Batch [641], Loss: 0.063064
Epoch [1/1], Batch [651], Loss: 0.060356
Epoch [1/1], Batch [661], Loss: 0.061057
Epoch [1/1], Batch [671], Loss: 0.058957
Epoch [1/1], Batch [681], Loss: 0.062449
Epoch [1/1], Batch [691], Loss: 0.063467
Epoch [1/1], Batch [701], Loss: 0.061160
Epoch [1/1], Batch [711], Loss: 0.061288
Epoch [1/1], Batch [721], Loss: 0.058865
Epoch [1/1], Batch [731], Loss: 0.061109
Epoch [1/1], Batch [741], Loss: 0.058860
Epoch [1/1], Batch [751], Loss: 0.058404
Epoch [1/1], Batch [761], Loss: 0.058233
Epoch [1/1], Batch [771], Loss: 0.061217
Epoch [1/1], Batch [781], Loss: 0.060446
Epoch [1/1], Batch [791], Loss: 0.056656
Epoch [1/1], Batch [801], Loss: 0.061024
Epoch [1/1], Batch [811], Loss: 0.056627
Epoch [1/1], Batch [821], Loss: 0.058670
Epoch [1/1], Batch [831], Loss: 0.058654
Epoch [1/1], Batch [841], Loss: 0.058143
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 0.0606
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 0.0573
Elapsed time: 4096.55 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 0.0623
Elapsed time: 4121.83 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 0.062649
Epoch [1/1], Batch [11], Loss: 0.063265
Epoch [1/1], Batch [21], Loss: 0.062298
Epoch [1/1], Batch [31], Loss: 0.059204
Epoch [1/1], Batch [41], Loss: 0.062792
Epoch [1/1], Batch [51], Loss: 0.060447
Epoch [1/1], Batch [61], Loss: 0.061126
Epoch [1/1], Batch [71], Loss: 0.058591
Epoch [1/1], Batch [81], Loss: 0.059896
Epoch [1/1], Batch [91], Loss: 0.060171
Epoch [1/1], Batch [101], Loss: 0.061368
Epoch [1/1], Batch [111], Loss: 0.062451
Epoch [1/1], Batch [121], Loss: 0.059545
Epoch [1/1], Batch [131], Loss: 0.059138
Epoch [1/1], Batch [141], Loss: 0.061041
Epoch [1/1], Batch [151], Loss: 0.059277
Epoch [1/1], Batch [161], Loss: 0.061998
Epoch [1/1], Batch [171], Loss: 0.059597
Epoch [1/1], Batch [181], Loss: 0.060366
Epoch [1/1], Batch [191], Loss: 0.062331
Epoch [1/1], Batch [201], Loss: 0.062396
Epoch [1/1], Batch [211], Loss: 0.062292
Epoch [1/1], Batch [221], Loss: 0.058631
Epoch [1/1], Batch [231], Loss: 0.057744
Epoch [1/1], Batch [241], Loss: 0.060253
Epoch [1/1], Batch [251], Loss: 0.061625
Epoch [1/1], Batch [261], Loss: 0.059318
Epoch [1/1], Batch [271], Loss: 0.058664
Epoch [1/1], Batch [281], Loss: 0.061239
Epoch [1/1], Batch [291], Loss: 0.058779
Epoch [1/1], Batch [301], Loss: 0.056458
Epoch [1/1], Batch [311], Loss: 0.058572
Epoch [1/1], Batch [321], Loss: 0.061475
Epoch [1/1], Batch [331], Loss: 0.061276
Epoch [1/1], Batch [341], Loss: 0.060175
Epoch [1/1], Batch [351], Loss: 0.059608
Epoch [1/1], Batch [361], Loss: 0.060171
Epoch [1/1], Batch [371], Loss: 0.058990
Epoch [1/1], Batch [381], Loss: 0.058667
Epoch [1/1], Batch [391], Loss: 0.061215
Epoch [1/1], Batch [401], Loss: 0.059493
Epoch [1/1], Batch [411], Loss: 0.063621
Epoch [1/1], Batch [421], Loss: 0.061267
Epoch [1/1], Batch [431], Loss: 0.060240
Epoch [1/1], Batch [441], Loss: 0.061648
Epoch [1/1], Batch [451], Loss: 0.058077
Epoch [1/1], Batch [461], Loss: 0.059026
Epoch [1/1], Batch [471], Loss: 0.062787
Epoch [1/1], Batch [481], Loss: 0.060464
Epoch [1/1], Batch [491], Loss: 0.057001
Epoch [1/1], Batch [501], Loss: 0.060149
Epoch [1/1], Batch [511], Loss: 0.062234
Epoch [1/1], Batch [521], Loss: 0.057310
Epoch [1/1], Batch [531], Loss: 0.056487
Epoch [1/1], Batch [541], Loss: 0.056700
Epoch [1/1], Batch [551], Loss: 0.060143
Epoch [1/1], Batch [561], Loss: 0.060031
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 0.0606
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 0.0585
Elapsed time: 4594.90 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 0.0626
Elapsed time: 4614.03 seconds

Training complete!
Totoal elapsed time: 4614.03 seconds
Sequence Length 2: Median Loss = 0.073038
Sequence Length 3: Median Loss = 0.067287
Sequence Length 4: Median Loss = 0.063970
Sequence Length 5: Median Loss = 0.063039
Sequence Length 6: Median Loss = 0.061277
Sequence Length 7: Median Loss = 0.060494
Sequence Length 8: Median Loss = 0.060171
usage: generate_gif.py [-h] --job_id JOB_ID --num_hidden NUM_HIDDEN
                       [NUM_HIDDEN ...] --filter_size FILTER_SIZE
                       [FILTER_SIZE ...] --stride STRIDE
                       [--patch_size PATCH_SIZE] [--bias] --leaky_slope
                       LEAKY_SLOPE [--max_pooling] [--transpose]
                       [--use_lstm_output] [--layer_norm] --fig_height
                       FIG_HEIGHT --model MODEL --out_folder OUT_FOLDER
generate_gif.py: error: the following arguments are required: --leaky_slope
rm: cannot remove '*.gif': No such file or directory
