Starting job 1008760
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [5, 5, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = False,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 380079.812500
Epoch [1/1], Batch [11], Loss: 127411.000000
Epoch [1/1], Batch [21], Loss: 92879.812500
Epoch [1/1], Batch [31], Loss: 80691.781250
Epoch [1/1], Batch [41], Loss: 73695.601562
Epoch [1/1], Batch [51], Loss: 73119.304688
Epoch [1/1], Batch [61], Loss: 61789.773438
Epoch [1/1], Batch [71], Loss: 57934.796875
Epoch [1/1], Batch [81], Loss: 56649.167969
Epoch [1/1], Batch [91], Loss: 56232.210938
Epoch [1/1], Batch [101], Loss: 53161.382812
Epoch [1/1], Batch [111], Loss: 51933.359375
Epoch [1/1], Batch [121], Loss: 50587.125000
Epoch [1/1], Batch [131], Loss: 53092.531250
Epoch [1/1], Batch [141], Loss: 50299.179688
Epoch [1/1], Batch [151], Loss: 50169.597656
Epoch [1/1], Batch [161], Loss: 51987.093750
Epoch [1/1], Batch [171], Loss: 49396.394531
Epoch [1/1], Batch [181], Loss: 47984.906250
Epoch [1/1], Batch [191], Loss: 48393.687500
Epoch [1/1], Batch [201], Loss: 50321.062500
Epoch [1/1], Batch [211], Loss: 48406.476562
Epoch [1/1], Batch [221], Loss: 49796.472656
Epoch [1/1], Batch [231], Loss: 47691.148438
Epoch [1/1], Batch [241], Loss: 48563.949219
Epoch [1/1], Batch [251], Loss: 49357.507812
Epoch [1/1], Batch [261], Loss: 47244.140625
Epoch [1/1], Batch [271], Loss: 47324.421875
Epoch [1/1], Batch [281], Loss: 47710.882812
Epoch [1/1], Batch [291], Loss: 47469.335938
Epoch [1/1], Batch [301], Loss: 46820.359375
Epoch [1/1], Batch [311], Loss: 46491.363281
Epoch [1/1], Batch [321], Loss: 47561.367188
Epoch [1/1], Batch [331], Loss: 48558.191406
Epoch [1/1], Batch [341], Loss: 47464.007812
Epoch [1/1], Batch [351], Loss: 45805.582031
Epoch [1/1], Batch [361], Loss: 46201.679688
Epoch [1/1], Batch [371], Loss: 45465.242188
Epoch [1/1], Batch [381], Loss: 45166.937500
Epoch [1/1], Batch [391], Loss: 47238.050781
Epoch [1/1], Batch [401], Loss: 44963.597656
Epoch [1/1], Batch [411], Loss: 46788.726562
Epoch [1/1], Batch [421], Loss: 46986.894531
Epoch [1/1], Batch [431], Loss: 43561.007812
Epoch [1/1], Batch [441], Loss: 44414.867188
Epoch [1/1], Batch [451], Loss: 43409.859375
Epoch [1/1], Batch [461], Loss: 44386.636719
Epoch [1/1], Batch [471], Loss: 42914.886719
Epoch [1/1], Batch [481], Loss: 43583.097656
Epoch [1/1], Batch [491], Loss: 45481.117188
Epoch [1/1], Batch [501], Loss: 44425.015625
Epoch [1/1], Batch [511], Loss: 43630.796875
Epoch [1/1], Batch [521], Loss: 42131.437500
Epoch [1/1], Batch [531], Loss: 43815.648438
Epoch [1/1], Batch [541], Loss: 43463.601562
Epoch [1/1], Batch [551], Loss: 43459.250000
Epoch [1/1], Batch [561], Loss: 44701.343750
Epoch [1/1], Batch [571], Loss: 42179.664062
Epoch [1/1], Batch [581], Loss: 42690.296875
Epoch [1/1], Batch [591], Loss: 45157.750000
Epoch [1/1], Batch [601], Loss: 41469.484375
Epoch [1/1], Batch [611], Loss: 40296.035156
Epoch [1/1], Batch [621], Loss: 45576.812500
Epoch [1/1], Batch [631], Loss: 44220.554688
Epoch [1/1], Batch [641], Loss: 43769.679688
Epoch [1/1], Batch [651], Loss: 43864.468750
Epoch [1/1], Batch [661], Loss: 40629.726562
Epoch [1/1], Batch [671], Loss: 41800.203125
Epoch [1/1], Batch [681], Loss: 41718.609375
Epoch [1/1], Batch [691], Loss: 44134.496094
Epoch [1/1], Batch [701], Loss: 42840.218750
Epoch [1/1], Batch [711], Loss: 42163.031250
Epoch [1/1], Batch [721], Loss: 44387.453125
Epoch [1/1], Batch [731], Loss: 44200.535156
Epoch [1/1], Batch [741], Loss: 41642.113281
Epoch [1/1], Batch [751], Loss: 42591.382812
Epoch [1/1], Batch [761], Loss: 42032.984375
Epoch [1/1], Batch [771], Loss: 39875.949219
Epoch [1/1], Batch [781], Loss: 42566.726562
Epoch [1/1], Batch [791], Loss: 41598.917969
Epoch [1/1], Batch [801], Loss: 42778.617188
Epoch [1/1], Batch [811], Loss: 42096.656250
Epoch [1/1], Batch [821], Loss: 40663.253906
Epoch [1/1], Batch [831], Loss: 40484.226562
Epoch [1/1], Batch [841], Loss: 42257.156250
Epoch [1/1], Batch [851], Loss: 41282.343750
Epoch [1/1], Batch [861], Loss: 40262.472656
Epoch [1/1], Batch [871], Loss: 41362.632812
Epoch [1/1], Batch [881], Loss: 42091.222656
Epoch [1/1], Batch [891], Loss: 39929.058594
Epoch [1/1], Batch [901], Loss: 40448.179688
Epoch [1/1], Batch [911], Loss: 39211.722656
Epoch [1/1], Batch [921], Loss: 41744.242188
Epoch [1/1], Batch [931], Loss: 41221.125000
Epoch [1/1], Batch [941], Loss: 40224.433594
Epoch [1/1], Batch [951], Loss: 41441.722656
Epoch [1/1], Batch [961], Loss: 41357.398438
Epoch [1/1], Batch [971], Loss: 38806.187500
Epoch [1/1], Batch [981], Loss: 38456.914062
Epoch [1/1], Batch [991], Loss: 40503.804688
Epoch [1/1], Batch [1001], Loss: 40257.250000
Epoch [1/1], Batch [1011], Loss: 39954.285156
Epoch [1/1], Batch [1021], Loss: 40283.687500
Epoch [1/1], Batch [1031], Loss: 40750.250000
Epoch [1/1], Batch [1041], Loss: 40571.660156
Epoch [1/1], Batch [1051], Loss: 39378.312500
Epoch [1/1], Batch [1061], Loss: 39430.492188
Epoch [1/1], Batch [1071], Loss: 40755.453125
Epoch [1/1], Batch [1081], Loss: 38993.960938
Epoch [1/1], Batch [1091], Loss: 41694.421875
Epoch [1/1], Batch [1101], Loss: 41327.964844
Epoch [1/1], Batch [1111], Loss: 40497.945312
Epoch [1/1], Batch [1121], Loss: 39853.789062
Epoch [1/1], Batch [1131], Loss: 41038.945312
Epoch [1/1], Batch [1141], Loss: 38735.578125
Epoch [1/1], Batch [1151], Loss: 39153.617188
Epoch [1/1], Batch [1161], Loss: 40377.726562
Epoch [1/1], Batch [1171], Loss: 40449.359375
Epoch [1/1], Batch [1181], Loss: 39574.246094
Epoch [1/1], Batch [1191], Loss: 40611.367188
Epoch [1/1], Batch [1201], Loss: 41128.136719
Epoch [1/1], Batch [1211], Loss: 39510.351562
Epoch [1/1], Batch [1221], Loss: 39358.617188
Epoch [1/1], Batch [1231], Loss: 37624.105469
Epoch [1/1], Batch [1241], Loss: 39666.179688
Epoch [1/1], Batch [1251], Loss: 39217.882812
Epoch [1/1], Batch [1261], Loss: 38460.312500
Epoch [1/1], Batch [1271], Loss: 41034.550781
Epoch [1/1], Batch [1281], Loss: 39435.218750
Epoch [1/1], Batch [1291], Loss: 38373.601562
Epoch [1/1], Batch [1301], Loss: 40069.148438
Epoch [1/1], Batch [1311], Loss: 40585.597656
Epoch [1/1], Batch [1321], Loss: 37482.304688
Epoch [1/1], Batch [1331], Loss: 39116.203125
Epoch [1/1], Batch [1341], Loss: 38765.390625
Epoch [1/1], Batch [1351], Loss: 38479.406250
Epoch [1/1], Batch [1361], Loss: 39639.351562
Epoch [1/1], Batch [1371], Loss: 37652.726562
Epoch [1/1], Batch [1381], Loss: 40229.484375
Epoch [1/1], Batch [1391], Loss: 38644.050781
Epoch [1/1], Batch [1401], Loss: 40251.515625
Epoch [1/1], Batch [1411], Loss: 38865.937500
Epoch [1/1], Batch [1421], Loss: 38772.679688
Epoch [1/1], Batch [1431], Loss: 39194.519531
Epoch [1/1], Batch [1441], Loss: 36949.070312
Epoch [1/1], Batch [1451], Loss: 38540.117188
Epoch [1/1], Batch [1461], Loss: 39825.125000
Epoch [1/1], Batch [1471], Loss: 39117.609375
Epoch [1/1], Batch [1481], Loss: 39905.093750
Epoch [1/1], Batch [1491], Loss: 38816.023438
Epoch [1/1], Batch [1501], Loss: 40550.976562
Epoch [1/1], Batch [1511], Loss: 38885.945312
Epoch [1/1], Batch [1521], Loss: 37182.914062
Epoch [1/1], Batch [1531], Loss: 37926.097656
Epoch [1/1], Batch [1541], Loss: 39677.101562
Epoch [1/1], Batch [1551], Loss: 38068.875000
Epoch [1/1], Batch [1561], Loss: 40345.566406
Epoch [1/1], Batch [1571], Loss: 37994.453125
Epoch [1/1], Batch [1581], Loss: 39743.242188
Epoch [1/1], Batch [1591], Loss: 40368.710938
Epoch [1/1], Batch [1601], Loss: 37641.617188
Epoch [1/1], Batch [1611], Loss: 38341.101562
Epoch [1/1], Batch [1621], Loss: 38315.101562
Epoch [1/1], Batch [1631], Loss: 38050.054688
Epoch [1/1], Batch [1641], Loss: 38453.968750
Epoch [1/1], Batch [1651], Loss: 38813.304688
Epoch [1/1], Batch [1661], Loss: 37740.066406
Epoch [1/1], Batch [1671], Loss: 37244.972656
Epoch [1/1], Batch [1681], Loss: 38916.191406
Epoch [1/1], Batch [1691], Loss: 37802.285156
Epoch [1/1], Batch [1701], Loss: 39987.171875
Epoch [1/1], Batch [1711], Loss: 36534.289062
Epoch [1/1], Batch [1721], Loss: 39534.234375
Epoch [1/1], Batch [1731], Loss: 38685.859375
Epoch [1/1], Batch [1741], Loss: 36412.265625
Epoch [1/1], Batch [1751], Loss: 37013.390625
Epoch [1/1], Batch [1761], Loss: 38210.164062
Epoch [1/1], Batch [1771], Loss: 37767.656250
Epoch [1/1], Batch [1781], Loss: 39785.453125
Epoch [1/1], Batch [1791], Loss: 38744.773438
Epoch [1/1], Batch [1801], Loss: 37395.402344
Epoch [1/1], Batch [1811], Loss: 37917.343750
Epoch [1/1], Batch [1821], Loss: 38354.660156
Epoch [1/1], Batch [1831], Loss: 39670.886719
Epoch [1/1], Batch [1841], Loss: 40465.218750
Epoch [1/1], Batch [1851], Loss: 38529.656250
Epoch [1/1], Batch [1861], Loss: 35697.175781
Epoch [1/1], Batch [1871], Loss: 37557.289062
Epoch [1/1], Batch [1881], Loss: 36969.230469
Epoch [1/1], Batch [1891], Loss: 36698.132812
Epoch [1/1], Batch [1901], Loss: 39847.785156
Epoch [1/1], Batch [1911], Loss: 37605.402344
Epoch [1/1], Batch [1921], Loss: 41656.332031
Epoch [1/1], Batch [1931], Loss: 38904.953125
Epoch [1/1], Batch [1941], Loss: 37932.820312
Epoch [1/1], Batch [1951], Loss: 39719.101562
Epoch [1/1], Batch [1961], Loss: 38697.007812
Epoch [1/1], Batch [1971], Loss: 36960.429688
Epoch [1/1], Batch [1981], Loss: 37779.742188
Epoch [1/1], Batch [1991], Loss: 37291.035156
Epoch [1/1], Batch [2001], Loss: 36252.656250
Epoch [1/1], Batch [2011], Loss: 37828.695312
Epoch [1/1], Batch [2021], Loss: 37853.515625
Epoch [1/1], Batch [2031], Loss: 37405.890625
Epoch [1/1], Batch [2041], Loss: 39169.601562
Epoch [1/1], Batch [2051], Loss: 37215.804688
Epoch [1/1], Batch [2061], Loss: 37576.078125
Epoch [1/1], Batch [2071], Loss: 37404.410156
Epoch [1/1], Batch [2081], Loss: 35470.085938
Epoch [1/1], Batch [2091], Loss: 38486.304688
Epoch [1/1], Batch [2101], Loss: 38465.437500
Epoch [1/1], Batch [2111], Loss: 37878.316406
Epoch [1/1], Batch [2121], Loss: 37074.949219
Epoch [1/1], Batch [2131], Loss: 39100.390625
Epoch [1/1], Batch [2141], Loss: 36226.000000
Epoch [1/1], Batch [2151], Loss: 35685.000000
Epoch [1/1], Batch [2161], Loss: 38984.304688
Epoch [1/1], Batch [2171], Loss: 36476.023438
Epoch [1/1], Batch [2181], Loss: 37442.652344
Epoch [1/1], Batch [2191], Loss: 37280.914062
Epoch [1/1], Batch [2201], Loss: 38535.507812
Epoch [1/1], Batch [2211], Loss: 38125.105469
Epoch [1/1], Batch [2221], Loss: 37649.621094
Epoch [1/1], Batch [2231], Loss: 35921.429688
Epoch [1/1], Batch [2241], Loss: 37240.851562
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 43176.5522
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 37108.7351
Elapsed time: 522.27 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 37428.7275
Elapsed time: 544.53 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 76706.734375
Epoch [1/1], Batch [11], Loss: 80556.664062
Epoch [1/1], Batch [21], Loss: 77273.953125
Epoch [1/1], Batch [31], Loss: 74552.187500
Epoch [1/1], Batch [41], Loss: 73178.882812
Epoch [1/1], Batch [51], Loss: 64256.796875
Epoch [1/1], Batch [61], Loss: 67834.664062
Epoch [1/1], Batch [71], Loss: 63260.636719
Epoch [1/1], Batch [81], Loss: 65989.039062
Epoch [1/1], Batch [91], Loss: 66789.460938
Epoch [1/1], Batch [101], Loss: 64638.851562
Epoch [1/1], Batch [111], Loss: 59657.722656
Epoch [1/1], Batch [121], Loss: 63693.601562
Epoch [1/1], Batch [131], Loss: 60272.175781
Epoch [1/1], Batch [141], Loss: 61932.398438
Epoch [1/1], Batch [151], Loss: 62001.625000
Epoch [1/1], Batch [161], Loss: 60019.761719
Epoch [1/1], Batch [171], Loss: 61637.398438
Epoch [1/1], Batch [181], Loss: 59320.066406
Epoch [1/1], Batch [191], Loss: 60408.503906
Epoch [1/1], Batch [201], Loss: 60946.914062
Epoch [1/1], Batch [211], Loss: 62995.054688
Epoch [1/1], Batch [221], Loss: 62375.804688
Epoch [1/1], Batch [231], Loss: 59674.554688
Epoch [1/1], Batch [241], Loss: 60704.023438
Epoch [1/1], Batch [251], Loss: 59543.363281
Epoch [1/1], Batch [261], Loss: 58596.796875
Epoch [1/1], Batch [271], Loss: 60658.937500
Epoch [1/1], Batch [281], Loss: 58435.171875
Epoch [1/1], Batch [291], Loss: 60755.421875
Epoch [1/1], Batch [301], Loss: 59377.769531
Epoch [1/1], Batch [311], Loss: 58047.253906
Epoch [1/1], Batch [321], Loss: 59413.726562
Epoch [1/1], Batch [331], Loss: 58088.988281
Epoch [1/1], Batch [341], Loss: 60096.863281
Epoch [1/1], Batch [351], Loss: 59245.703125
Epoch [1/1], Batch [361], Loss: 56915.687500
Epoch [1/1], Batch [371], Loss: 59638.031250
Epoch [1/1], Batch [381], Loss: 58557.828125
Epoch [1/1], Batch [391], Loss: 57801.074219
Epoch [1/1], Batch [401], Loss: 59811.593750
Epoch [1/1], Batch [411], Loss: 57925.421875
Epoch [1/1], Batch [421], Loss: 59798.660156
Epoch [1/1], Batch [431], Loss: 58144.632812
Epoch [1/1], Batch [441], Loss: 58725.890625
Epoch [1/1], Batch [451], Loss: 58962.171875
Epoch [1/1], Batch [461], Loss: 55736.871094
Epoch [1/1], Batch [471], Loss: 57752.367188
Epoch [1/1], Batch [481], Loss: 57227.000000
Epoch [1/1], Batch [491], Loss: 55864.324219
Epoch [1/1], Batch [501], Loss: 57967.976562
Epoch [1/1], Batch [511], Loss: 59041.164062
Epoch [1/1], Batch [521], Loss: 55775.316406
Epoch [1/1], Batch [531], Loss: 58043.671875
Epoch [1/1], Batch [541], Loss: 56739.882812
Epoch [1/1], Batch [551], Loss: 59364.339844
Epoch [1/1], Batch [561], Loss: 57875.148438
Epoch [1/1], Batch [571], Loss: 59317.640625
Epoch [1/1], Batch [581], Loss: 57030.695312
Epoch [1/1], Batch [591], Loss: 54744.808594
Epoch [1/1], Batch [601], Loss: 55308.523438
Epoch [1/1], Batch [611], Loss: 59382.656250
Epoch [1/1], Batch [621], Loss: 61820.621094
Epoch [1/1], Batch [631], Loss: 56114.246094
Epoch [1/1], Batch [641], Loss: 57175.210938
Epoch [1/1], Batch [651], Loss: 57543.558594
Epoch [1/1], Batch [661], Loss: 58754.664062
Epoch [1/1], Batch [671], Loss: 57138.914062
Epoch [1/1], Batch [681], Loss: 59628.585938
Epoch [1/1], Batch [691], Loss: 54710.718750
Epoch [1/1], Batch [701], Loss: 56264.710938
Epoch [1/1], Batch [711], Loss: 55971.296875
Epoch [1/1], Batch [721], Loss: 54486.593750
Epoch [1/1], Batch [731], Loss: 57992.238281
Epoch [1/1], Batch [741], Loss: 54263.261719
Epoch [1/1], Batch [751], Loss: 57264.609375
Epoch [1/1], Batch [761], Loss: 57301.972656
Epoch [1/1], Batch [771], Loss: 56696.222656
Epoch [1/1], Batch [781], Loss: 57696.898438
Epoch [1/1], Batch [791], Loss: 57324.507812
Epoch [1/1], Batch [801], Loss: 57123.429688
Epoch [1/1], Batch [811], Loss: 55774.570312
Epoch [1/1], Batch [821], Loss: 55577.894531
Epoch [1/1], Batch [831], Loss: 56621.503906
Epoch [1/1], Batch [841], Loss: 58925.910156
Epoch [1/1], Batch [851], Loss: 57060.003906
Epoch [1/1], Batch [861], Loss: 54115.425781
Epoch [1/1], Batch [871], Loss: 52882.179688
Epoch [1/1], Batch [881], Loss: 56255.156250
Epoch [1/1], Batch [891], Loss: 54576.945312
Epoch [1/1], Batch [901], Loss: 56434.914062
Epoch [1/1], Batch [911], Loss: 55567.937500
Epoch [1/1], Batch [921], Loss: 55340.304688
Epoch [1/1], Batch [931], Loss: 53767.589844
Epoch [1/1], Batch [941], Loss: 54712.648438
Epoch [1/1], Batch [951], Loss: 56282.554688
Epoch [1/1], Batch [961], Loss: 57185.656250
Epoch [1/1], Batch [971], Loss: 57484.535156
Epoch [1/1], Batch [981], Loss: 54257.851562
Epoch [1/1], Batch [991], Loss: 57392.328125
Epoch [1/1], Batch [1001], Loss: 55368.437500
Epoch [1/1], Batch [1011], Loss: 53720.937500
Epoch [1/1], Batch [1021], Loss: 55206.171875
Epoch [1/1], Batch [1031], Loss: 56077.234375
Epoch [1/1], Batch [1041], Loss: 59303.957031
Epoch [1/1], Batch [1051], Loss: 55755.769531
Epoch [1/1], Batch [1061], Loss: 55087.410156
Epoch [1/1], Batch [1071], Loss: 55496.144531
Epoch [1/1], Batch [1081], Loss: 53620.203125
Epoch [1/1], Batch [1091], Loss: 53680.128906
Epoch [1/1], Batch [1101], Loss: 53518.148438
Epoch [1/1], Batch [1111], Loss: 55730.492188
Epoch [1/1], Batch [1121], Loss: 53634.718750
Epoch [1/1], Batch [1131], Loss: 51699.257812
Epoch [1/1], Batch [1141], Loss: 54108.101562
Epoch [1/1], Batch [1151], Loss: 55572.781250
Epoch [1/1], Batch [1161], Loss: 56877.652344
Epoch [1/1], Batch [1171], Loss: 53542.007812
Epoch [1/1], Batch [1181], Loss: 54827.992188
Epoch [1/1], Batch [1191], Loss: 52933.406250
Epoch [1/1], Batch [1201], Loss: 55710.367188
Epoch [1/1], Batch [1211], Loss: 56334.988281
Epoch [1/1], Batch [1221], Loss: 54325.058594
Epoch [1/1], Batch [1231], Loss: 56655.816406
Epoch [1/1], Batch [1241], Loss: 55172.507812
Epoch [1/1], Batch [1251], Loss: 56897.242188
Epoch [1/1], Batch [1261], Loss: 56335.339844
Epoch [1/1], Batch [1271], Loss: 51861.800781
Epoch [1/1], Batch [1281], Loss: 53192.324219
Epoch [1/1], Batch [1291], Loss: 58304.953125
Epoch [1/1], Batch [1301], Loss: 54909.718750
Epoch [1/1], Batch [1311], Loss: 54041.636719
Epoch [1/1], Batch [1321], Loss: 55366.812500
Epoch [1/1], Batch [1331], Loss: 55801.175781
Epoch [1/1], Batch [1341], Loss: 55833.320312
Epoch [1/1], Batch [1351], Loss: 53231.234375
Epoch [1/1], Batch [1361], Loss: 52945.687500
Epoch [1/1], Batch [1371], Loss: 57723.250000
Epoch [1/1], Batch [1381], Loss: 54847.250000
Epoch [1/1], Batch [1391], Loss: 54615.031250
Epoch [1/1], Batch [1401], Loss: 55708.523438
Epoch [1/1], Batch [1411], Loss: 54846.867188
Epoch [1/1], Batch [1421], Loss: 54810.871094
Epoch [1/1], Batch [1431], Loss: 53626.765625
Epoch [1/1], Batch [1441], Loss: 53191.335938
Epoch [1/1], Batch [1451], Loss: 53453.445312
Epoch [1/1], Batch [1461], Loss: 52790.859375
Epoch [1/1], Batch [1471], Loss: 54620.890625
Epoch [1/1], Batch [1481], Loss: 55822.812500
Epoch [1/1], Batch [1491], Loss: 55011.265625
Epoch [1/1], Batch [1501], Loss: 56397.335938
Epoch [1/1], Batch [1511], Loss: 55715.242188
Epoch [1/1], Batch [1521], Loss: 56664.621094
Epoch [1/1], Batch [1531], Loss: 56069.371094
Epoch [1/1], Batch [1541], Loss: 56569.273438
Epoch [1/1], Batch [1551], Loss: 56472.519531
Epoch [1/1], Batch [1561], Loss: 54054.421875
Epoch [1/1], Batch [1571], Loss: 55505.742188
Epoch [1/1], Batch [1581], Loss: 56501.257812
Epoch [1/1], Batch [1591], Loss: 52491.906250
Epoch [1/1], Batch [1601], Loss: 55681.531250
Epoch [1/1], Batch [1611], Loss: 55058.359375
Epoch [1/1], Batch [1621], Loss: 53259.246094
Epoch [1/1], Batch [1631], Loss: 54141.269531
Epoch [1/1], Batch [1641], Loss: 52490.453125
Epoch [1/1], Batch [1651], Loss: 53433.593750
Epoch [1/1], Batch [1661], Loss: 54306.906250
Epoch [1/1], Batch [1671], Loss: 53102.238281
Epoch [1/1], Batch [1681], Loss: 55558.933594
Epoch [1/1], Batch [1691], Loss: 54808.445312
Epoch [1/1], Batch [1701], Loss: 54246.042969
Epoch [1/1], Batch [1711], Loss: 56191.000000
Epoch [1/1], Batch [1721], Loss: 54379.503906
Epoch [1/1], Batch [1731], Loss: 52653.800781
Epoch [1/1], Batch [1741], Loss: 49732.175781
Epoch [1/1], Batch [1751], Loss: 54686.648438
Epoch [1/1], Batch [1761], Loss: 55954.550781
Epoch [1/1], Batch [1771], Loss: 52887.164062
Epoch [1/1], Batch [1781], Loss: 53176.324219
Epoch [1/1], Batch [1791], Loss: 54192.355469
Epoch [1/1], Batch [1801], Loss: 52338.609375
Epoch [1/1], Batch [1811], Loss: 55286.003906
Epoch [1/1], Batch [1821], Loss: 51909.351562
Epoch [1/1], Batch [1831], Loss: 52212.007812
Epoch [1/1], Batch [1841], Loss: 53953.339844
Epoch [1/1], Batch [1851], Loss: 56673.417969
Epoch [1/1], Batch [1861], Loss: 53623.812500
Epoch [1/1], Batch [1871], Loss: 53405.585938
Epoch [1/1], Batch [1881], Loss: 52637.625000
Epoch [1/1], Batch [1891], Loss: 53947.968750
Epoch [1/1], Batch [1901], Loss: 53084.132812
Epoch [1/1], Batch [1911], Loss: 55176.171875
Epoch [1/1], Batch [1921], Loss: 53894.578125
Epoch [1/1], Batch [1931], Loss: 53939.257812
Epoch [1/1], Batch [1941], Loss: 53956.906250
Epoch [1/1], Batch [1951], Loss: 53520.640625
Epoch [1/1], Batch [1961], Loss: 54090.148438
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 57007.2177
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 52754.8737
Elapsed time: 1180.17 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 54128.6604
Elapsed time: 1207.06 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 77911.312500
Epoch [1/1], Batch [11], Loss: 77092.578125
Epoch [1/1], Batch [21], Loss: 74517.726562
Epoch [1/1], Batch [31], Loss: 74798.554688
Epoch [1/1], Batch [41], Loss: 77786.328125
Epoch [1/1], Batch [51], Loss: 74342.093750
Epoch [1/1], Batch [61], Loss: 73152.468750
Epoch [1/1], Batch [71], Loss: 77991.609375
Epoch [1/1], Batch [81], Loss: 75043.921875
Epoch [1/1], Batch [91], Loss: 72984.703125
Epoch [1/1], Batch [101], Loss: 76018.257812
Epoch [1/1], Batch [111], Loss: 77651.640625
Epoch [1/1], Batch [121], Loss: 73957.062500
Epoch [1/1], Batch [131], Loss: 77701.726562
Epoch [1/1], Batch [141], Loss: 71624.203125
Epoch [1/1], Batch [151], Loss: 74946.500000
Epoch [1/1], Batch [161], Loss: 74425.640625
Epoch [1/1], Batch [171], Loss: 74700.593750
Epoch [1/1], Batch [181], Loss: 75737.468750
Epoch [1/1], Batch [191], Loss: 76906.781250
Epoch [1/1], Batch [201], Loss: 75125.820312
Epoch [1/1], Batch [211], Loss: 71521.937500
Epoch [1/1], Batch [221], Loss: 74407.835938
Epoch [1/1], Batch [231], Loss: 74224.351562
Epoch [1/1], Batch [241], Loss: 73669.156250
Epoch [1/1], Batch [251], Loss: 76737.328125
Epoch [1/1], Batch [261], Loss: 73351.492188
Epoch [1/1], Batch [271], Loss: 78226.453125
Epoch [1/1], Batch [281], Loss: 76489.937500
Epoch [1/1], Batch [291], Loss: 74418.195312
Epoch [1/1], Batch [301], Loss: 70421.953125
Epoch [1/1], Batch [311], Loss: 73904.562500
Epoch [1/1], Batch [321], Loss: 73518.515625
Epoch [1/1], Batch [331], Loss: 73602.507812
Epoch [1/1], Batch [341], Loss: 75288.671875
Epoch [1/1], Batch [351], Loss: 74864.656250
Epoch [1/1], Batch [361], Loss: 69159.929688
Epoch [1/1], Batch [371], Loss: 73415.156250
Epoch [1/1], Batch [381], Loss: 72025.906250
Epoch [1/1], Batch [391], Loss: 73547.539062
Epoch [1/1], Batch [401], Loss: 74874.257812
Epoch [1/1], Batch [411], Loss: 71762.921875
Epoch [1/1], Batch [421], Loss: 74098.476562
Epoch [1/1], Batch [431], Loss: 75123.039062
Epoch [1/1], Batch [441], Loss: 74539.210938
Epoch [1/1], Batch [451], Loss: 72361.859375
Epoch [1/1], Batch [461], Loss: 71985.960938
Epoch [1/1], Batch [471], Loss: 71590.937500
Epoch [1/1], Batch [481], Loss: 77081.054688
Epoch [1/1], Batch [491], Loss: 68276.015625
Epoch [1/1], Batch [501], Loss: 70646.070312
Epoch [1/1], Batch [511], Loss: 74125.453125
Epoch [1/1], Batch [521], Loss: 72930.218750
Epoch [1/1], Batch [531], Loss: 74771.601562
Epoch [1/1], Batch [541], Loss: 72017.296875
Epoch [1/1], Batch [551], Loss: 75038.812500
Epoch [1/1], Batch [561], Loss: 74677.992188
Epoch [1/1], Batch [571], Loss: 70496.468750
Epoch [1/1], Batch [581], Loss: 70729.343750
Epoch [1/1], Batch [591], Loss: 70730.523438
Epoch [1/1], Batch [601], Loss: 71658.281250
Epoch [1/1], Batch [611], Loss: 69616.164062
Epoch [1/1], Batch [621], Loss: 76440.656250
Epoch [1/1], Batch [631], Loss: 71929.367188
Epoch [1/1], Batch [641], Loss: 71135.859375
Epoch [1/1], Batch [651], Loss: 76355.421875
Epoch [1/1], Batch [661], Loss: 73607.406250
Epoch [1/1], Batch [671], Loss: 75135.546875
Epoch [1/1], Batch [681], Loss: 69621.695312
Epoch [1/1], Batch [691], Loss: 69848.328125
Epoch [1/1], Batch [701], Loss: 69167.539062
Epoch [1/1], Batch [711], Loss: 71216.921875
Epoch [1/1], Batch [721], Loss: 70606.968750
Epoch [1/1], Batch [731], Loss: 75530.125000
Epoch [1/1], Batch [741], Loss: 74842.562500
Epoch [1/1], Batch [751], Loss: 70135.687500
Epoch [1/1], Batch [761], Loss: 76169.281250
Epoch [1/1], Batch [771], Loss: 73855.015625
Epoch [1/1], Batch [781], Loss: 72992.718750
Epoch [1/1], Batch [791], Loss: 70389.468750
Epoch [1/1], Batch [801], Loss: 72801.445312
Epoch [1/1], Batch [811], Loss: 70165.765625
Epoch [1/1], Batch [821], Loss: 68301.171875
Epoch [1/1], Batch [831], Loss: 70196.796875
Epoch [1/1], Batch [841], Loss: 76086.796875
Epoch [1/1], Batch [851], Loss: 72794.843750
Epoch [1/1], Batch [861], Loss: 74484.796875
Epoch [1/1], Batch [871], Loss: 71485.773438
Epoch [1/1], Batch [881], Loss: 72325.726562
Epoch [1/1], Batch [891], Loss: 70365.250000
Epoch [1/1], Batch [901], Loss: 70867.507812
Epoch [1/1], Batch [911], Loss: 70343.171875
Epoch [1/1], Batch [921], Loss: 73843.804688
Epoch [1/1], Batch [931], Loss: 74406.859375
Epoch [1/1], Batch [941], Loss: 74940.914062
Epoch [1/1], Batch [951], Loss: 71766.375000
Epoch [1/1], Batch [961], Loss: 69656.781250
Epoch [1/1], Batch [971], Loss: 74167.000000
Epoch [1/1], Batch [981], Loss: 73961.296875
Epoch [1/1], Batch [991], Loss: 71273.453125
Epoch [1/1], Batch [1001], Loss: 74080.046875
Epoch [1/1], Batch [1011], Loss: 70217.125000
Epoch [1/1], Batch [1021], Loss: 68358.492188
Epoch [1/1], Batch [1031], Loss: 71222.734375
Epoch [1/1], Batch [1041], Loss: 71306.382812
Epoch [1/1], Batch [1051], Loss: 70951.625000
Epoch [1/1], Batch [1061], Loss: 72745.078125
Epoch [1/1], Batch [1071], Loss: 70000.781250
Epoch [1/1], Batch [1081], Loss: 70246.093750
Epoch [1/1], Batch [1091], Loss: 71268.601562
Epoch [1/1], Batch [1101], Loss: 72061.726562
Epoch [1/1], Batch [1111], Loss: 69119.703125
Epoch [1/1], Batch [1121], Loss: 66829.296875
Epoch [1/1], Batch [1131], Loss: 71688.210938
Epoch [1/1], Batch [1141], Loss: 72141.687500
Epoch [1/1], Batch [1151], Loss: 70477.062500
Epoch [1/1], Batch [1161], Loss: 72463.281250
Epoch [1/1], Batch [1171], Loss: 72271.289062
Epoch [1/1], Batch [1181], Loss: 73986.109375
Epoch [1/1], Batch [1191], Loss: 71145.867188
Epoch [1/1], Batch [1201], Loss: 70740.625000
Epoch [1/1], Batch [1211], Loss: 72366.789062
Epoch [1/1], Batch [1221], Loss: 71081.390625
Epoch [1/1], Batch [1231], Loss: 73187.734375
Epoch [1/1], Batch [1241], Loss: 69519.468750
Epoch [1/1], Batch [1251], Loss: 70024.703125
Epoch [1/1], Batch [1261], Loss: 72754.648438
Epoch [1/1], Batch [1271], Loss: 66158.296875
Epoch [1/1], Batch [1281], Loss: 67957.210938
Epoch [1/1], Batch [1291], Loss: 71680.351562
Epoch [1/1], Batch [1301], Loss: 69988.773438
Epoch [1/1], Batch [1311], Loss: 73803.062500
Epoch [1/1], Batch [1321], Loss: 71170.000000
Epoch [1/1], Batch [1331], Loss: 66992.406250
Epoch [1/1], Batch [1341], Loss: 68435.296875
Epoch [1/1], Batch [1351], Loss: 71052.265625
Epoch [1/1], Batch [1361], Loss: 72027.132812
Epoch [1/1], Batch [1371], Loss: 68451.500000
Epoch [1/1], Batch [1381], Loss: 72017.593750
Epoch [1/1], Batch [1391], Loss: 72997.171875
Epoch [1/1], Batch [1401], Loss: 69904.015625
Epoch [1/1], Batch [1411], Loss: 67585.664062
Epoch [1/1], Batch [1421], Loss: 71664.671875
Epoch [1/1], Batch [1431], Loss: 70250.343750
Epoch [1/1], Batch [1441], Loss: 70700.718750
Epoch [1/1], Batch [1451], Loss: 75202.593750
Epoch [1/1], Batch [1461], Loss: 69258.304688
Epoch [1/1], Batch [1471], Loss: 69583.062500
Epoch [1/1], Batch [1481], Loss: 72211.789062
Epoch [1/1], Batch [1491], Loss: 70383.578125
Epoch [1/1], Batch [1501], Loss: 70665.304688
Epoch [1/1], Batch [1511], Loss: 69169.820312
Epoch [1/1], Batch [1521], Loss: 69348.750000
Epoch [1/1], Batch [1531], Loss: 66645.734375
Epoch [1/1], Batch [1541], Loss: 71543.851562
Epoch [1/1], Batch [1551], Loss: 72657.656250
Epoch [1/1], Batch [1561], Loss: 72713.585938
Epoch [1/1], Batch [1571], Loss: 68981.156250
Epoch [1/1], Batch [1581], Loss: 69952.343750
Epoch [1/1], Batch [1591], Loss: 71103.125000
Epoch [1/1], Batch [1601], Loss: 70595.812500
Epoch [1/1], Batch [1611], Loss: 69732.984375
Epoch [1/1], Batch [1621], Loss: 67382.921875
Epoch [1/1], Batch [1631], Loss: 67493.093750
Epoch [1/1], Batch [1641], Loss: 68404.046875
Epoch [1/1], Batch [1651], Loss: 69572.945312
Epoch [1/1], Batch [1661], Loss: 72309.531250
Epoch [1/1], Batch [1671], Loss: 69120.039062
Epoch [1/1], Batch [1681], Loss: 69014.765625
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 71988.8874
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 68588.8931
Elapsed time: 1919.38 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 71577.0959
Elapsed time: 1948.71 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 89966.953125
Epoch [1/1], Batch [11], Loss: 93847.359375
Epoch [1/1], Batch [21], Loss: 88366.609375
Epoch [1/1], Batch [31], Loss: 90412.671875
Epoch [1/1], Batch [41], Loss: 89055.593750
Epoch [1/1], Batch [51], Loss: 89233.828125
Epoch [1/1], Batch [61], Loss: 90598.890625
Epoch [1/1], Batch [71], Loss: 89484.140625
Epoch [1/1], Batch [81], Loss: 93128.937500
Epoch [1/1], Batch [91], Loss: 91882.398438
Epoch [1/1], Batch [101], Loss: 85328.281250
Epoch [1/1], Batch [111], Loss: 86061.343750
Epoch [1/1], Batch [121], Loss: 87442.500000
Epoch [1/1], Batch [131], Loss: 90384.757812
Epoch [1/1], Batch [141], Loss: 89917.531250
Epoch [1/1], Batch [151], Loss: 93536.609375
Epoch [1/1], Batch [161], Loss: 89734.593750
Epoch [1/1], Batch [171], Loss: 91215.906250
Epoch [1/1], Batch [181], Loss: 92593.140625
Epoch [1/1], Batch [191], Loss: 90609.617188
Epoch [1/1], Batch [201], Loss: 89651.687500
Epoch [1/1], Batch [211], Loss: 89622.250000
Epoch [1/1], Batch [221], Loss: 95633.296875
Epoch [1/1], Batch [231], Loss: 90190.890625
Epoch [1/1], Batch [241], Loss: 91318.437500
Epoch [1/1], Batch [251], Loss: 88826.070312
Epoch [1/1], Batch [261], Loss: 91792.437500
Epoch [1/1], Batch [271], Loss: 88331.335938
Epoch [1/1], Batch [281], Loss: 88788.328125
Epoch [1/1], Batch [291], Loss: 92220.265625
Epoch [1/1], Batch [301], Loss: 93897.273438
Epoch [1/1], Batch [311], Loss: 95048.875000
Epoch [1/1], Batch [321], Loss: 96575.859375
Epoch [1/1], Batch [331], Loss: 95168.500000
Epoch [1/1], Batch [341], Loss: 90191.984375
Epoch [1/1], Batch [351], Loss: 87709.906250
Epoch [1/1], Batch [361], Loss: 84351.468750
Epoch [1/1], Batch [371], Loss: 87471.890625
Epoch [1/1], Batch [381], Loss: 90902.570312
Epoch [1/1], Batch [391], Loss: 91496.335938
Epoch [1/1], Batch [401], Loss: 94742.796875
Epoch [1/1], Batch [411], Loss: 93279.937500
Epoch [1/1], Batch [421], Loss: 91761.718750
Epoch [1/1], Batch [431], Loss: 89921.625000
Epoch [1/1], Batch [441], Loss: 90193.328125
Epoch [1/1], Batch [451], Loss: 89830.679688
Epoch [1/1], Batch [461], Loss: 88777.023438
Epoch [1/1], Batch [471], Loss: 90564.937500
Epoch [1/1], Batch [481], Loss: 88765.437500
Epoch [1/1], Batch [491], Loss: 91769.531250
Epoch [1/1], Batch [501], Loss: 86661.375000
Epoch [1/1], Batch [511], Loss: 86698.335938
Epoch [1/1], Batch [521], Loss: 89105.656250
Epoch [1/1], Batch [531], Loss: 87062.078125
Epoch [1/1], Batch [541], Loss: 91385.492188
Epoch [1/1], Batch [551], Loss: 88677.937500
Epoch [1/1], Batch [561], Loss: 85681.539062
Epoch [1/1], Batch [571], Loss: 89015.937500
Epoch [1/1], Batch [581], Loss: 88286.476562
Epoch [1/1], Batch [591], Loss: 88383.367188
Epoch [1/1], Batch [601], Loss: 82744.953125
Epoch [1/1], Batch [611], Loss: 85896.937500
Epoch [1/1], Batch [621], Loss: 89981.343750
Epoch [1/1], Batch [631], Loss: 86500.281250
Epoch [1/1], Batch [641], Loss: 92190.421875
Epoch [1/1], Batch [651], Loss: 91153.695312
Epoch [1/1], Batch [661], Loss: 91534.515625
Epoch [1/1], Batch [671], Loss: 90127.718750
Epoch [1/1], Batch [681], Loss: 88091.976562
Epoch [1/1], Batch [691], Loss: 92598.750000
Epoch [1/1], Batch [701], Loss: 91424.242188
Epoch [1/1], Batch [711], Loss: 84909.906250
Epoch [1/1], Batch [721], Loss: 88458.039062
Epoch [1/1], Batch [731], Loss: 85261.578125
Epoch [1/1], Batch [741], Loss: 85847.062500
Epoch [1/1], Batch [751], Loss: 85776.281250
Epoch [1/1], Batch [761], Loss: 88280.078125
Epoch [1/1], Batch [771], Loss: 85835.031250
Epoch [1/1], Batch [781], Loss: 89780.710938
Epoch [1/1], Batch [791], Loss: 90391.218750
Epoch [1/1], Batch [801], Loss: 90981.601562
Epoch [1/1], Batch [811], Loss: 87044.359375
Epoch [1/1], Batch [821], Loss: 91638.695312
Epoch [1/1], Batch [831], Loss: 85171.562500
Epoch [1/1], Batch [841], Loss: 86793.765625
Epoch [1/1], Batch [851], Loss: 86955.195312
Epoch [1/1], Batch [861], Loss: 87825.593750
Epoch [1/1], Batch [871], Loss: 86131.546875
Epoch [1/1], Batch [881], Loss: 83587.875000
Epoch [1/1], Batch [891], Loss: 91204.296875
Epoch [1/1], Batch [901], Loss: 86993.312500
Epoch [1/1], Batch [911], Loss: 85588.062500
Epoch [1/1], Batch [921], Loss: 84155.093750
Epoch [1/1], Batch [931], Loss: 86256.390625
Epoch [1/1], Batch [941], Loss: 86910.984375
Epoch [1/1], Batch [951], Loss: 85563.156250
Epoch [1/1], Batch [961], Loss: 87156.281250
Epoch [1/1], Batch [971], Loss: 89154.617188
Epoch [1/1], Batch [981], Loss: 92018.171875
Epoch [1/1], Batch [991], Loss: 85032.828125
Epoch [1/1], Batch [1001], Loss: 89793.406250
Epoch [1/1], Batch [1011], Loss: 86326.687500
Epoch [1/1], Batch [1021], Loss: 86690.796875
Epoch [1/1], Batch [1031], Loss: 90397.750000
Epoch [1/1], Batch [1041], Loss: 82847.664062
Epoch [1/1], Batch [1051], Loss: 86228.890625
Epoch [1/1], Batch [1061], Loss: 87198.023438
Epoch [1/1], Batch [1071], Loss: 83308.914062
Epoch [1/1], Batch [1081], Loss: 87996.359375
Epoch [1/1], Batch [1091], Loss: 86535.976562
Epoch [1/1], Batch [1101], Loss: 89879.421875
Epoch [1/1], Batch [1111], Loss: 86678.335938
Epoch [1/1], Batch [1121], Loss: 86903.890625
Epoch [1/1], Batch [1131], Loss: 87901.632812
Epoch [1/1], Batch [1141], Loss: 87195.078125
Epoch [1/1], Batch [1151], Loss: 87236.382812
Epoch [1/1], Batch [1161], Loss: 86859.070312
Epoch [1/1], Batch [1171], Loss: 85961.914062
Epoch [1/1], Batch [1181], Loss: 86672.062500
Epoch [1/1], Batch [1191], Loss: 84170.960938
Epoch [1/1], Batch [1201], Loss: 86189.390625
Epoch [1/1], Batch [1211], Loss: 86640.078125
Epoch [1/1], Batch [1221], Loss: 88397.437500
Epoch [1/1], Batch [1231], Loss: 86005.203125
Epoch [1/1], Batch [1241], Loss: 82778.156250
Epoch [1/1], Batch [1251], Loss: 84780.843750
Epoch [1/1], Batch [1261], Loss: 87630.984375
Epoch [1/1], Batch [1271], Loss: 83534.570312
Epoch [1/1], Batch [1281], Loss: 86848.578125
Epoch [1/1], Batch [1291], Loss: 88237.226562
Epoch [1/1], Batch [1301], Loss: 90092.054688
Epoch [1/1], Batch [1311], Loss: 88246.859375
Epoch [1/1], Batch [1321], Loss: 88128.898438
Epoch [1/1], Batch [1331], Loss: 85681.890625
Epoch [1/1], Batch [1341], Loss: 84418.421875
Epoch [1/1], Batch [1351], Loss: 84366.343750
Epoch [1/1], Batch [1361], Loss: 84715.937500
Epoch [1/1], Batch [1371], Loss: 89713.875000
Epoch [1/1], Batch [1381], Loss: 85385.156250
Epoch [1/1], Batch [1391], Loss: 86948.421875
Epoch [1/1], Batch [1401], Loss: 88334.546875
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 88437.5597
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 82922.4102
Elapsed time: 2683.70 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 88164.8588
Elapsed time: 2713.42 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 110014.906250
Epoch [1/1], Batch [11], Loss: 109649.375000
Epoch [1/1], Batch [21], Loss: 108184.437500
Epoch [1/1], Batch [31], Loss: 108391.125000
Epoch [1/1], Batch [41], Loss: 110253.492188
Epoch [1/1], Batch [51], Loss: 113887.062500
Epoch [1/1], Batch [61], Loss: 111645.765625
Epoch [1/1], Batch [71], Loss: 110190.531250
Epoch [1/1], Batch [81], Loss: 109965.640625
Epoch [1/1], Batch [91], Loss: 110293.265625
Epoch [1/1], Batch [101], Loss: 109253.117188
Epoch [1/1], Batch [111], Loss: 102500.109375
Epoch [1/1], Batch [121], Loss: 107329.828125
Epoch [1/1], Batch [131], Loss: 107022.765625
Epoch [1/1], Batch [141], Loss: 110403.406250
Epoch [1/1], Batch [151], Loss: 104808.656250
Epoch [1/1], Batch [161], Loss: 105841.578125
Epoch [1/1], Batch [171], Loss: 102234.695312
Epoch [1/1], Batch [181], Loss: 105887.210938
Epoch [1/1], Batch [191], Loss: 105244.664062
Epoch [1/1], Batch [201], Loss: 111391.492188
Epoch [1/1], Batch [211], Loss: 102868.281250
Epoch [1/1], Batch [221], Loss: 104350.226562
Epoch [1/1], Batch [231], Loss: 112145.468750
Epoch [1/1], Batch [241], Loss: 103050.062500
Epoch [1/1], Batch [251], Loss: 109354.070312
Epoch [1/1], Batch [261], Loss: 104769.796875
Epoch [1/1], Batch [271], Loss: 106504.531250
Epoch [1/1], Batch [281], Loss: 107472.601562
Epoch [1/1], Batch [291], Loss: 105701.632812
Epoch [1/1], Batch [301], Loss: 105115.789062
Epoch [1/1], Batch [311], Loss: 104502.648438
Epoch [1/1], Batch [321], Loss: 107350.750000
Epoch [1/1], Batch [331], Loss: 104596.859375
Epoch [1/1], Batch [341], Loss: 109688.781250
Epoch [1/1], Batch [351], Loss: 101778.734375
Epoch [1/1], Batch [361], Loss: 111509.570312
Epoch [1/1], Batch [371], Loss: 106286.257812
Epoch [1/1], Batch [381], Loss: 111913.281250
Epoch [1/1], Batch [391], Loss: 106382.906250
Epoch [1/1], Batch [401], Loss: 102116.250000
Epoch [1/1], Batch [411], Loss: 108718.062500
Epoch [1/1], Batch [421], Loss: 105396.703125
Epoch [1/1], Batch [431], Loss: 107855.328125
Epoch [1/1], Batch [441], Loss: 105906.562500
Epoch [1/1], Batch [451], Loss: 104940.828125
Epoch [1/1], Batch [461], Loss: 102973.609375
Epoch [1/1], Batch [471], Loss: 101247.570312
Epoch [1/1], Batch [481], Loss: 106257.289062
Epoch [1/1], Batch [491], Loss: 106167.937500
Epoch [1/1], Batch [501], Loss: 106277.593750
Epoch [1/1], Batch [511], Loss: 100792.546875
Epoch [1/1], Batch [521], Loss: 105080.015625
Epoch [1/1], Batch [531], Loss: 106977.765625
Epoch [1/1], Batch [541], Loss: 104249.101562
Epoch [1/1], Batch [551], Loss: 110405.218750
Epoch [1/1], Batch [561], Loss: 102837.953125
Epoch [1/1], Batch [571], Loss: 108967.750000
Epoch [1/1], Batch [581], Loss: 100010.859375
Epoch [1/1], Batch [591], Loss: 105043.718750
Epoch [1/1], Batch [601], Loss: 109017.421875
Epoch [1/1], Batch [611], Loss: 109462.851562
Epoch [1/1], Batch [621], Loss: 104857.406250
Epoch [1/1], Batch [631], Loss: 100246.000000
Epoch [1/1], Batch [641], Loss: 105714.656250
Epoch [1/1], Batch [651], Loss: 103402.914062
Epoch [1/1], Batch [661], Loss: 104265.859375
Epoch [1/1], Batch [671], Loss: 104834.625000
Epoch [1/1], Batch [681], Loss: 100816.546875
Epoch [1/1], Batch [691], Loss: 104467.703125
Epoch [1/1], Batch [701], Loss: 102384.992188
Epoch [1/1], Batch [711], Loss: 105643.562500
Epoch [1/1], Batch [721], Loss: 104707.148438
Epoch [1/1], Batch [731], Loss: 104957.617188
Epoch [1/1], Batch [741], Loss: 108331.062500
Epoch [1/1], Batch [751], Loss: 107191.781250
Epoch [1/1], Batch [761], Loss: 99212.718750
Epoch [1/1], Batch [771], Loss: 103236.500000
Epoch [1/1], Batch [781], Loss: 107381.093750
Epoch [1/1], Batch [791], Loss: 106440.921875
Epoch [1/1], Batch [801], Loss: 102609.656250
Epoch [1/1], Batch [811], Loss: 103374.734375
Epoch [1/1], Batch [821], Loss: 101289.023438
Epoch [1/1], Batch [831], Loss: 104326.312500
Epoch [1/1], Batch [841], Loss: 100171.250000
Epoch [1/1], Batch [851], Loss: 102525.062500
Epoch [1/1], Batch [861], Loss: 101531.718750
Epoch [1/1], Batch [871], Loss: 105408.835938
Epoch [1/1], Batch [881], Loss: 99247.453125
Epoch [1/1], Batch [891], Loss: 104646.992188
Epoch [1/1], Batch [901], Loss: 101253.570312
Epoch [1/1], Batch [911], Loss: 107780.984375
Epoch [1/1], Batch [921], Loss: 106398.843750
Epoch [1/1], Batch [931], Loss: 103519.484375
Epoch [1/1], Batch [941], Loss: 106774.578125
Epoch [1/1], Batch [951], Loss: 102481.914062
Epoch [1/1], Batch [961], Loss: 98850.085938
Epoch [1/1], Batch [971], Loss: 105548.460938
Epoch [1/1], Batch [981], Loss: 103085.523438
Epoch [1/1], Batch [991], Loss: 104211.289062
Epoch [1/1], Batch [1001], Loss: 103812.250000
Epoch [1/1], Batch [1011], Loss: 104736.250000
Epoch [1/1], Batch [1021], Loss: 109231.757812
Epoch [1/1], Batch [1031], Loss: 102235.515625
Epoch [1/1], Batch [1041], Loss: 103842.015625
Epoch [1/1], Batch [1051], Loss: 101475.296875
Epoch [1/1], Batch [1061], Loss: 104593.078125
Epoch [1/1], Batch [1071], Loss: 103231.203125
Epoch [1/1], Batch [1081], Loss: 100024.187500
Epoch [1/1], Batch [1091], Loss: 101723.218750
Epoch [1/1], Batch [1101], Loss: 98266.132812
Epoch [1/1], Batch [1111], Loss: 105294.242188
Epoch [1/1], Batch [1121], Loss: 107718.476562
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 105277.6199
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 100611.7946
Elapsed time: 3414.30 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 107381.7002
Elapsed time: 3442.30 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 127544.843750
Epoch [1/1], Batch [11], Loss: 120940.156250
Epoch [1/1], Batch [21], Loss: 119219.640625
Epoch [1/1], Batch [31], Loss: 123369.882812
Epoch [1/1], Batch [41], Loss: 123461.750000
Epoch [1/1], Batch [51], Loss: 124622.750000
Epoch [1/1], Batch [61], Loss: 124834.421875
Epoch [1/1], Batch [71], Loss: 123928.187500
Epoch [1/1], Batch [81], Loss: 122187.015625
Epoch [1/1], Batch [91], Loss: 126752.304688
Epoch [1/1], Batch [101], Loss: 124788.312500
Epoch [1/1], Batch [111], Loss: 128259.851562
Epoch [1/1], Batch [121], Loss: 126562.359375
Epoch [1/1], Batch [131], Loss: 121146.421875
Epoch [1/1], Batch [141], Loss: 124217.445312
Epoch [1/1], Batch [151], Loss: 123559.828125
Epoch [1/1], Batch [161], Loss: 131801.125000
Epoch [1/1], Batch [171], Loss: 121329.585938
Epoch [1/1], Batch [181], Loss: 125039.718750
Epoch [1/1], Batch [191], Loss: 121442.015625
Epoch [1/1], Batch [201], Loss: 120519.718750
Epoch [1/1], Batch [211], Loss: 117855.640625
Epoch [1/1], Batch [221], Loss: 127799.328125
Epoch [1/1], Batch [231], Loss: 123157.820312
Epoch [1/1], Batch [241], Loss: 119635.031250
Epoch [1/1], Batch [251], Loss: 128100.281250
Epoch [1/1], Batch [261], Loss: 124856.640625
Epoch [1/1], Batch [271], Loss: 118617.109375
Epoch [1/1], Batch [281], Loss: 129267.078125
Epoch [1/1], Batch [291], Loss: 120722.609375
Epoch [1/1], Batch [301], Loss: 120312.359375
Epoch [1/1], Batch [311], Loss: 115954.015625
Epoch [1/1], Batch [321], Loss: 122419.632812
Epoch [1/1], Batch [331], Loss: 124515.085938
Epoch [1/1], Batch [341], Loss: 127686.796875
Epoch [1/1], Batch [351], Loss: 121332.617188
Epoch [1/1], Batch [361], Loss: 123920.828125
Epoch [1/1], Batch [371], Loss: 125305.265625
Epoch [1/1], Batch [381], Loss: 120661.867188
Epoch [1/1], Batch [391], Loss: 123414.156250
Epoch [1/1], Batch [401], Loss: 119053.445312
Epoch [1/1], Batch [411], Loss: 119852.687500
Epoch [1/1], Batch [421], Loss: 120480.187500
Epoch [1/1], Batch [431], Loss: 123834.218750
Epoch [1/1], Batch [441], Loss: 123711.781250
Epoch [1/1], Batch [451], Loss: 127955.812500
Epoch [1/1], Batch [461], Loss: 123094.250000
Epoch [1/1], Batch [471], Loss: 120682.585938
Epoch [1/1], Batch [481], Loss: 125752.843750
Epoch [1/1], Batch [491], Loss: 119653.562500
Epoch [1/1], Batch [501], Loss: 122581.437500
Epoch [1/1], Batch [511], Loss: 124711.171875
Epoch [1/1], Batch [521], Loss: 123176.421875
Epoch [1/1], Batch [531], Loss: 121762.585938
Epoch [1/1], Batch [541], Loss: 121831.914062
Epoch [1/1], Batch [551], Loss: 127492.851562
Epoch [1/1], Batch [561], Loss: 119947.890625
Epoch [1/1], Batch [571], Loss: 125330.703125
Epoch [1/1], Batch [581], Loss: 119507.429688
Epoch [1/1], Batch [591], Loss: 129416.375000
Epoch [1/1], Batch [601], Loss: 126538.734375
Epoch [1/1], Batch [611], Loss: 118576.773438
Epoch [1/1], Batch [621], Loss: 118853.101562
Epoch [1/1], Batch [631], Loss: 122923.085938
Epoch [1/1], Batch [641], Loss: 126389.875000
Epoch [1/1], Batch [651], Loss: 126690.968750
Epoch [1/1], Batch [661], Loss: 118780.593750
Epoch [1/1], Batch [671], Loss: 117051.796875
Epoch [1/1], Batch [681], Loss: 121818.914062
Epoch [1/1], Batch [691], Loss: 120261.953125
Epoch [1/1], Batch [701], Loss: 118831.437500
Epoch [1/1], Batch [711], Loss: 122330.703125
Epoch [1/1], Batch [721], Loss: 120082.718750
Epoch [1/1], Batch [731], Loss: 120560.453125
Epoch [1/1], Batch [741], Loss: 117819.171875
Epoch [1/1], Batch [751], Loss: 122120.437500
Epoch [1/1], Batch [761], Loss: 119231.382812
Epoch [1/1], Batch [771], Loss: 119250.312500
Epoch [1/1], Batch [781], Loss: 113456.562500
Epoch [1/1], Batch [791], Loss: 118862.609375
Epoch [1/1], Batch [801], Loss: 120019.328125
Epoch [1/1], Batch [811], Loss: 119669.750000
Epoch [1/1], Batch [821], Loss: 117730.570312
Epoch [1/1], Batch [831], Loss: 116416.257812
Epoch [1/1], Batch [841], Loss: 116174.101562
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 122242.3264
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 117434.1177
Elapsed time: 4052.37 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 126962.4574
Elapsed time: 4076.58 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 137935.625000
Epoch [1/1], Batch [11], Loss: 140395.500000
Epoch [1/1], Batch [21], Loss: 135584.062500
Epoch [1/1], Batch [31], Loss: 137367.062500
Epoch [1/1], Batch [41], Loss: 140863.953125
Epoch [1/1], Batch [51], Loss: 145999.031250
Epoch [1/1], Batch [61], Loss: 143980.171875
Epoch [1/1], Batch [71], Loss: 131834.812500
Epoch [1/1], Batch [81], Loss: 138811.406250
Epoch [1/1], Batch [91], Loss: 144390.765625
Epoch [1/1], Batch [101], Loss: 144557.875000
Epoch [1/1], Batch [111], Loss: 144307.000000
Epoch [1/1], Batch [121], Loss: 142493.156250
Epoch [1/1], Batch [131], Loss: 142334.687500
Epoch [1/1], Batch [141], Loss: 141513.734375
Epoch [1/1], Batch [151], Loss: 140096.171875
Epoch [1/1], Batch [161], Loss: 137752.359375
Epoch [1/1], Batch [171], Loss: 144359.656250
Epoch [1/1], Batch [181], Loss: 144753.218750
Epoch [1/1], Batch [191], Loss: 142023.015625
Epoch [1/1], Batch [201], Loss: 144744.859375
Epoch [1/1], Batch [211], Loss: 144598.406250
Epoch [1/1], Batch [221], Loss: 133864.890625
Epoch [1/1], Batch [231], Loss: 139991.250000
Epoch [1/1], Batch [241], Loss: 141806.625000
Epoch [1/1], Batch [251], Loss: 141234.343750
Epoch [1/1], Batch [261], Loss: 144349.750000
Epoch [1/1], Batch [271], Loss: 143173.281250
Epoch [1/1], Batch [281], Loss: 142879.968750
Epoch [1/1], Batch [291], Loss: 141488.859375
Epoch [1/1], Batch [301], Loss: 137414.937500
Epoch [1/1], Batch [311], Loss: 141330.796875
Epoch [1/1], Batch [321], Loss: 146457.203125
Epoch [1/1], Batch [331], Loss: 133668.562500
Epoch [1/1], Batch [341], Loss: 141736.453125
Epoch [1/1], Batch [351], Loss: 143041.187500
Epoch [1/1], Batch [361], Loss: 145874.187500
Epoch [1/1], Batch [371], Loss: 139700.718750
Epoch [1/1], Batch [381], Loss: 134858.812500
Epoch [1/1], Batch [391], Loss: 139188.718750
Epoch [1/1], Batch [401], Loss: 137505.468750
Epoch [1/1], Batch [411], Loss: 134777.906250
Epoch [1/1], Batch [421], Loss: 141037.156250
Epoch [1/1], Batch [431], Loss: 138343.562500
Epoch [1/1], Batch [441], Loss: 134661.812500
Epoch [1/1], Batch [451], Loss: 134334.687500
Epoch [1/1], Batch [461], Loss: 138712.562500
Epoch [1/1], Batch [471], Loss: 143986.203125
Epoch [1/1], Batch [481], Loss: 140976.390625
Epoch [1/1], Batch [491], Loss: 131664.281250
Epoch [1/1], Batch [501], Loss: 137142.859375
Epoch [1/1], Batch [511], Loss: 141693.625000
Epoch [1/1], Batch [521], Loss: 134605.109375
Epoch [1/1], Batch [531], Loss: 142782.468750
Epoch [1/1], Batch [541], Loss: 136734.218750
Epoch [1/1], Batch [551], Loss: 144051.781250
Epoch [1/1], Batch [561], Loss: 138043.531250
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 140014.7924
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 132815.5787
Elapsed time: 4565.29 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 140904.4320
Elapsed time: 4583.59 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 156614.765625
Epoch [1/1], Batch [11], Loss: 155942.156250
Epoch [1/1], Batch [21], Loss: 157318.218750
Epoch [1/1], Batch [31], Loss: 161246.343750
Epoch [1/1], Batch [41], Loss: 169901.609375
Epoch [1/1], Batch [51], Loss: 155350.281250
Epoch [1/1], Batch [61], Loss: 159821.500000
Epoch [1/1], Batch [71], Loss: 156118.921875
Epoch [1/1], Batch [81], Loss: 162456.750000
Epoch [1/1], Batch [91], Loss: 155401.312500
Epoch [1/1], Batch [101], Loss: 159137.906250
Epoch [1/1], Batch [111], Loss: 167291.296875
Epoch [1/1], Batch [121], Loss: 165486.687500
Epoch [1/1], Batch [131], Loss: 155435.859375
Epoch [1/1], Batch [141], Loss: 155273.437500
Epoch [1/1], Batch [151], Loss: 156793.125000
Epoch [1/1], Batch [161], Loss: 153200.656250
Epoch [1/1], Batch [171], Loss: 158837.343750
Epoch [1/1], Batch [181], Loss: 160640.359375
Epoch [1/1], Batch [191], Loss: 156008.812500
Epoch [1/1], Batch [201], Loss: 158979.593750
Epoch [1/1], Batch [211], Loss: 152346.734375
Epoch [1/1], Batch [221], Loss: 166128.968750
Epoch [1/1], Batch [231], Loss: 157242.218750
Epoch [1/1], Batch [241], Loss: 157250.656250
Epoch [1/1], Batch [251], Loss: 160563.312500
Epoch [1/1], Batch [261], Loss: 163992.281250
Epoch [1/1], Batch [271], Loss: 153126.750000
Epoch [1/1], Batch [281], Loss: 164978.500000
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 158803.9411
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 156621.2528
Elapsed time: 4885.34 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 163775.5403
Elapsed time: 4895.55 seconds

Training complete!
Totoal elapsed time: 4895.55 seconds
CUDA is available!
