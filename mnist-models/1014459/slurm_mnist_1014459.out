Starting job 1014459
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [5, 5, 5, 5],
    leaky_slope = None,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 2,
    scheduled_sampling = False,
    bias = False,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/2], Batch [1], Loss: 0.690683
Epoch [1/2], Batch [11], Loss: 0.216289
Epoch [1/2], Batch [21], Loss: 0.152630
Epoch [1/2], Batch [31], Loss: 0.126829
Epoch [1/2], Batch [41], Loss: 0.106790
Epoch [1/2], Batch [51], Loss: 0.105948
Epoch [1/2], Batch [61], Loss: 0.103959
Epoch [1/2], Batch [71], Loss: 0.101428
Epoch [1/2], Batch [81], Loss: 0.100646
Epoch [1/2], Batch [91], Loss: 0.098935
Epoch [1/2], Batch [101], Loss: 0.096939
Epoch [1/2], Batch [111], Loss: 0.094627
Epoch [1/2], Batch [121], Loss: 0.091272
Epoch [1/2], Batch [131], Loss: 0.092129
Epoch [1/2], Batch [141], Loss: 0.094379
Epoch [1/2], Batch [151], Loss: 0.088505
Epoch [1/2], Batch [161], Loss: 0.099780
Epoch [1/2], Batch [171], Loss: 0.091163
Epoch [1/2], Batch [181], Loss: 0.090325
Epoch [1/2], Batch [191], Loss: 0.091078
Epoch [1/2], Batch [201], Loss: 0.084425
Epoch [1/2], Batch [211], Loss: 0.087915
Epoch [1/2], Batch [221], Loss: 0.086351
Epoch [1/2], Batch [231], Loss: 0.088332
Epoch [1/2], Batch [241], Loss: 0.085442
Epoch [1/2], Batch [251], Loss: 0.085262
Epoch [1/2], Batch [261], Loss: 0.085869
Epoch [1/2], Batch [271], Loss: 0.086793
Epoch [1/2], Batch [281], Loss: 0.086112
Epoch [1/2], Batch [291], Loss: 0.083674
Epoch [1/2], Batch [301], Loss: 0.085550
Epoch [1/2], Batch [311], Loss: 0.082530
Epoch [1/2], Batch [321], Loss: 0.083576
Epoch [1/2], Batch [331], Loss: 0.083743
Epoch [1/2], Batch [341], Loss: 0.084903
Epoch [1/2], Batch [351], Loss: 0.082331
Epoch [1/2], Batch [361], Loss: 0.079333
Epoch [1/2], Batch [371], Loss: 0.080640
Epoch [1/2], Batch [381], Loss: 0.079233
Epoch [1/2], Batch [391], Loss: 0.080450
Epoch [1/2], Batch [401], Loss: 0.077094
Epoch [1/2], Batch [411], Loss: 0.080650
Epoch [1/2], Batch [421], Loss: 0.081695
Epoch [1/2], Batch [431], Loss: 0.079145
Epoch [1/2], Batch [441], Loss: 0.082482
Epoch [1/2], Batch [451], Loss: 0.076567
Epoch [1/2], Batch [461], Loss: 0.077069
Epoch [1/2], Batch [471], Loss: 0.077165
Epoch [1/2], Batch [481], Loss: 0.078857
Epoch [1/2], Batch [491], Loss: 0.081394
Epoch [1/2], Batch [501], Loss: 0.082492
Epoch [1/2], Batch [511], Loss: 0.076718
Epoch [1/2], Batch [521], Loss: 0.079675
Epoch [1/2], Batch [531], Loss: 0.078595
Epoch [1/2], Batch [541], Loss: 0.075197
Epoch [1/2], Batch [551], Loss: 0.076099
Epoch [1/2], Batch [561], Loss: 0.072920
Epoch [1/2], Batch [571], Loss: 0.077081
Epoch [1/2], Batch [581], Loss: 0.075985
Epoch [1/2], Batch [591], Loss: 0.076358
Epoch [1/2], Batch [601], Loss: 0.077741
Epoch [1/2], Batch [611], Loss: 0.075232
Epoch [1/2], Batch [621], Loss: 0.077712
Epoch [1/2], Batch [631], Loss: 0.073475
Epoch [1/2], Batch [641], Loss: 0.076593
Epoch [1/2], Batch [651], Loss: 0.075812
Epoch [1/2], Batch [661], Loss: 0.073075
Epoch [1/2], Batch [671], Loss: 0.073399
Epoch [1/2], Batch [681], Loss: 0.074472
Epoch [1/2], Batch [691], Loss: 0.074002
Epoch [1/2], Batch [701], Loss: 0.072881
Epoch [1/2], Batch [711], Loss: 0.076320
Epoch [1/2], Batch [721], Loss: 0.072941
Epoch [1/2], Batch [731], Loss: 0.075240
Epoch [1/2], Batch [741], Loss: 0.076874
Epoch [1/2], Batch [751], Loss: 0.073383
Epoch [1/2], Batch [761], Loss: 0.076611
Epoch [1/2], Batch [771], Loss: 0.075790
Epoch [1/2], Batch [781], Loss: 0.077364
Epoch [1/2], Batch [791], Loss: 0.073705
Epoch [1/2], Batch [801], Loss: 0.071662
Epoch [1/2], Batch [811], Loss: 0.074797
Epoch [1/2], Batch [821], Loss: 0.075825
Epoch [1/2], Batch [831], Loss: 0.072588
Epoch [1/2], Batch [841], Loss: 0.072496
Epoch [1/2], Batch [851], Loss: 0.075763
Epoch [1/2], Batch [861], Loss: 0.075634
Epoch [1/2], Batch [871], Loss: 0.073084
Epoch [1/2], Batch [881], Loss: 0.072825
Epoch [1/2], Batch [891], Loss: 0.071539
Epoch [1/2], Batch [901], Loss: 0.071443
Epoch [1/2], Batch [911], Loss: 0.076576
Epoch [1/2], Batch [921], Loss: 0.073795
Epoch [1/2], Batch [931], Loss: 0.075758
Epoch [1/2], Batch [941], Loss: 0.073689
Epoch [1/2], Batch [951], Loss: 0.074585
Epoch [1/2], Batch [961], Loss: 0.070925
Epoch [1/2], Batch [971], Loss: 0.075675
Epoch [1/2], Batch [981], Loss: 0.071085
Epoch [1/2], Batch [991], Loss: 0.072321
Epoch [1/2], Batch [1001], Loss: 0.072034
Epoch [1/2], Batch [1011], Loss: 0.075841
Epoch [1/2], Batch [1021], Loss: 0.072835
Epoch [1/2], Batch [1031], Loss: 0.069612
Epoch [1/2], Batch [1041], Loss: 0.071461
Epoch [1/2], Batch [1051], Loss: 0.072883
Epoch [1/2], Batch [1061], Loss: 0.071893
Epoch [1/2], Batch [1071], Loss: 0.079066
Epoch [1/2], Batch [1081], Loss: 0.070063
Epoch [1/2], Batch [1091], Loss: 0.072880
Epoch [1/2], Batch [1101], Loss: 0.071511
Epoch [1/2], Batch [1111], Loss: 0.071086
Epoch [1/2], Batch [1121], Loss: 0.071611
Epoch [1/2], Batch [1131], Loss: 0.071970
Epoch [1/2], Batch [1141], Loss: 0.069086
Epoch [1/2], Batch [1151], Loss: 0.068958
Epoch [1/2], Batch [1161], Loss: 0.073079
Epoch [1/2], Batch [1171], Loss: 0.071838
Epoch [1/2], Batch [1181], Loss: 0.070877
Epoch [1/2], Batch [1191], Loss: 0.067798
Epoch [1/2], Batch [1201], Loss: 0.072806
Epoch [1/2], Batch [1211], Loss: 0.070889
Epoch [1/2], Batch [1221], Loss: 0.072661
Epoch [1/2], Batch [1231], Loss: 0.069940
Epoch [1/2], Batch [1241], Loss: 0.069715
Epoch [1/2], Batch [1251], Loss: 0.071975
Epoch [1/2], Batch [1261], Loss: 0.073312
Epoch [1/2], Batch [1271], Loss: 0.068097
Epoch [1/2], Batch [1281], Loss: 0.069446
Epoch [1/2], Batch [1291], Loss: 0.073421
Epoch [1/2], Batch [1301], Loss: 0.070982
Epoch [1/2], Batch [1311], Loss: 0.072018
Epoch [1/2], Batch [1321], Loss: 0.071950
Epoch [1/2], Batch [1331], Loss: 0.071508
Epoch [1/2], Batch [1341], Loss: 0.070675
Epoch [1/2], Batch [1351], Loss: 0.069162
Epoch [1/2], Batch [1361], Loss: 0.068330
Epoch [1/2], Batch [1371], Loss: 0.072394
Epoch [1/2], Batch [1381], Loss: 0.068775
Epoch [1/2], Batch [1391], Loss: 0.072442
Epoch [1/2], Batch [1401], Loss: 0.071896
Epoch [1/2], Batch [1411], Loss: 0.069153
Epoch [1/2], Batch [1421], Loss: 0.069228
Epoch [1/2], Batch [1431], Loss: 0.071502
Epoch [1/2], Batch [1441], Loss: 0.070760
Epoch [1/2], Batch [1451], Loss: 0.073481
Epoch [1/2], Batch [1461], Loss: 0.067778
Epoch [1/2], Batch [1471], Loss: 0.067284
Epoch [1/2], Batch [1481], Loss: 0.069057
Epoch [1/2], Batch [1491], Loss: 0.066513
Epoch [1/2], Batch [1501], Loss: 0.069534
Epoch [1/2], Batch [1511], Loss: 0.068583
Epoch [1/2], Batch [1521], Loss: 0.071027
Epoch [1/2], Batch [1531], Loss: 0.069059
Epoch [1/2], Batch [1541], Loss: 0.070093
Epoch [1/2], Batch [1551], Loss: 0.070257
Epoch [1/2], Batch [1561], Loss: 0.066908
Epoch [1/2], Batch [1571], Loss: 0.069279
Epoch [1/2], Batch [1581], Loss: 0.068186
Epoch [1/2], Batch [1591], Loss: 0.070125
Epoch [1/2], Batch [1601], Loss: 0.068485
Epoch [1/2], Batch [1611], Loss: 0.071310
Epoch [1/2], Batch [1621], Loss: 0.069686
Epoch [1/2], Batch [1631], Loss: 0.066328
Epoch [1/2], Batch [1641], Loss: 0.066392
Epoch [1/2], Batch [1651], Loss: 0.065438
Epoch [1/2], Batch [1661], Loss: 0.071493
Epoch [1/2], Batch [1671], Loss: 0.067412
Epoch [1/2], Batch [1681], Loss: 0.069048
Epoch [1/2], Batch [1691], Loss: 0.068626
Epoch [1/2], Batch [1701], Loss: 0.068568
Epoch [1/2], Batch [1711], Loss: 0.065239
Epoch [1/2], Batch [1721], Loss: 0.069936
Epoch [1/2], Batch [1731], Loss: 0.069674
Epoch [1/2], Batch [1741], Loss: 0.072075
Epoch [1/2], Batch [1751], Loss: 0.068767
Epoch [1/2], Batch [1761], Loss: 0.070497
Epoch [1/2], Batch [1771], Loss: 0.070374
Epoch [1/2], Batch [1781], Loss: 0.069916
Epoch [1/2], Batch [1791], Loss: 0.067375
Epoch [1/2], Batch [1801], Loss: 0.071583
Epoch [1/2], Batch [1811], Loss: 0.069381
Epoch [1/2], Batch [1821], Loss: 0.066898
Epoch [1/2], Batch [1831], Loss: 0.068185
Epoch [1/2], Batch [1841], Loss: 0.066827
Epoch [1/2], Batch [1851], Loss: 0.067705
Epoch [1/2], Batch [1861], Loss: 0.067861
Epoch [1/2], Batch [1871], Loss: 0.069976
Epoch [1/2], Batch [1881], Loss: 0.068576
Epoch [1/2], Batch [1891], Loss: 0.069416
Epoch [1/2], Batch [1901], Loss: 0.065991
Epoch [1/2], Batch [1911], Loss: 0.067370
Epoch [1/2], Batch [1921], Loss: 0.066273
Epoch [1/2], Batch [1931], Loss: 0.068425
Epoch [1/2], Batch [1941], Loss: 0.070419
Epoch [1/2], Batch [1951], Loss: 0.067708
Epoch [1/2], Batch [1961], Loss: 0.065452
Epoch [1/2], Batch [1971], Loss: 0.066708
Epoch [1/2], Batch [1981], Loss: 0.066436
Epoch [1/2], Batch [1991], Loss: 0.068603
Epoch [1/2], Batch [2001], Loss: 0.068735
Epoch [1/2], Batch [2011], Loss: 0.069165
Epoch [1/2], Batch [2021], Loss: 0.070113
Epoch [1/2], Batch [2031], Loss: 0.066637
Epoch [1/2], Batch [2041], Loss: 0.070321
Epoch [1/2], Batch [2051], Loss: 0.069182
Epoch [1/2], Batch [2061], Loss: 0.064942
Epoch [1/2], Batch [2071], Loss: 0.063277
Epoch [1/2], Batch [2081], Loss: 0.065028
Epoch [1/2], Batch [2091], Loss: 0.065832
Epoch [1/2], Batch [2101], Loss: 0.068301
Epoch [1/2], Batch [2111], Loss: 0.067290
Epoch [1/2], Batch [2121], Loss: 0.068715
Epoch [1/2], Batch [2131], Loss: 0.068353
Epoch [1/2], Batch [2141], Loss: 0.066216
Epoch [1/2], Batch [2151], Loss: 0.069161
Epoch [1/2], Batch [2161], Loss: 0.068075
Epoch [1/2], Batch [2171], Loss: 0.066562
Epoch [1/2], Batch [2181], Loss: 0.064503
Epoch [1/2], Batch [2191], Loss: 0.065203
Epoch [1/2], Batch [2201], Loss: 0.065817
Epoch [1/2], Batch [2211], Loss: 0.068198
Epoch [1/2], Batch [2221], Loss: 0.069320
Epoch [1/2], Batch [2231], Loss: 0.071937
Epoch [1/2], Batch [2241], Loss: 0.067606
Seq_Len: 2, Epoch [1/2] - Average Train Loss: 0.0766
Seq_Len: 2, Epoch [1/2] - Average Test Loss: 0.0669
Elapsed time: 512.05 seconds
Seq_Len: 2, Epoch [1/2] - Average Validation Loss: 0.0685
Elapsed time: 535.34 seconds
Epoch [2/2], Batch [1], Loss: 0.064735
Epoch [2/2], Batch [11], Loss: 0.066942
Epoch [2/2], Batch [21], Loss: 0.068033
Epoch [2/2], Batch [31], Loss: 0.064822
Epoch [2/2], Batch [41], Loss: 0.070025
Epoch [2/2], Batch [51], Loss: 0.065079
Epoch [2/2], Batch [61], Loss: 0.068653
Epoch [2/2], Batch [71], Loss: 0.064203
Epoch [2/2], Batch [81], Loss: 0.066065
Epoch [2/2], Batch [91], Loss: 0.064085
Epoch [2/2], Batch [101], Loss: 0.067355
Epoch [2/2], Batch [111], Loss: 0.067494
Epoch [2/2], Batch [121], Loss: 0.069485
Epoch [2/2], Batch [131], Loss: 0.067501
Epoch [2/2], Batch [141], Loss: 0.063305
Epoch [2/2], Batch [151], Loss: 0.069089
Epoch [2/2], Batch [161], Loss: 0.061512
Epoch [2/2], Batch [171], Loss: 0.065783
Epoch [2/2], Batch [181], Loss: 0.066489
Epoch [2/2], Batch [191], Loss: 0.062148
Epoch [2/2], Batch [201], Loss: 0.065173
Epoch [2/2], Batch [211], Loss: 0.064735
Epoch [2/2], Batch [221], Loss: 0.063991
Epoch [2/2], Batch [231], Loss: 0.065182
Epoch [2/2], Batch [241], Loss: 0.068005
Epoch [2/2], Batch [251], Loss: 0.064753
Epoch [2/2], Batch [261], Loss: 0.066574
Epoch [2/2], Batch [271], Loss: 0.066494
Epoch [2/2], Batch [281], Loss: 0.066287
Epoch [2/2], Batch [291], Loss: 0.065410
Epoch [2/2], Batch [301], Loss: 0.063992
Epoch [2/2], Batch [311], Loss: 0.066669
Epoch [2/2], Batch [321], Loss: 0.066413
Epoch [2/2], Batch [331], Loss: 0.062582
Epoch [2/2], Batch [341], Loss: 0.069484
Epoch [2/2], Batch [351], Loss: 0.064325
Epoch [2/2], Batch [361], Loss: 0.064334
Epoch [2/2], Batch [371], Loss: 0.066627
Epoch [2/2], Batch [381], Loss: 0.067551
Epoch [2/2], Batch [391], Loss: 0.065047
Epoch [2/2], Batch [401], Loss: 0.063760
Epoch [2/2], Batch [411], Loss: 0.065621
Epoch [2/2], Batch [421], Loss: 0.065092
Epoch [2/2], Batch [431], Loss: 0.064028
Epoch [2/2], Batch [441], Loss: 0.067492
Epoch [2/2], Batch [451], Loss: 0.067778
Epoch [2/2], Batch [461], Loss: 0.065216
Epoch [2/2], Batch [471], Loss: 0.064607
Epoch [2/2], Batch [481], Loss: 0.065225
Epoch [2/2], Batch [491], Loss: 0.067236
Epoch [2/2], Batch [501], Loss: 0.065920
Epoch [2/2], Batch [511], Loss: 0.063940
Epoch [2/2], Batch [521], Loss: 0.068840
Epoch [2/2], Batch [531], Loss: 0.062986
Epoch [2/2], Batch [541], Loss: 0.064645
Epoch [2/2], Batch [551], Loss: 0.065492
Epoch [2/2], Batch [561], Loss: 0.067434
Epoch [2/2], Batch [571], Loss: 0.064040
Epoch [2/2], Batch [581], Loss: 0.067936
Epoch [2/2], Batch [591], Loss: 0.063692
Epoch [2/2], Batch [601], Loss: 0.066937
Epoch [2/2], Batch [611], Loss: 0.062635
Epoch [2/2], Batch [621], Loss: 0.071429
Epoch [2/2], Batch [631], Loss: 0.062020
Epoch [2/2], Batch [641], Loss: 0.066630
Epoch [2/2], Batch [651], Loss: 0.065548
Epoch [2/2], Batch [661], Loss: 0.062653
Epoch [2/2], Batch [671], Loss: 0.065657
Epoch [2/2], Batch [681], Loss: 0.068154
Epoch [2/2], Batch [691], Loss: 0.064358
Epoch [2/2], Batch [701], Loss: 0.066523
Epoch [2/2], Batch [711], Loss: 0.067840
Epoch [2/2], Batch [721], Loss: 0.068328
Epoch [2/2], Batch [731], Loss: 0.064437
Epoch [2/2], Batch [741], Loss: 0.063751
Epoch [2/2], Batch [751], Loss: 0.064539
Epoch [2/2], Batch [761], Loss: 0.062562
Epoch [2/2], Batch [771], Loss: 0.065291
Epoch [2/2], Batch [781], Loss: 0.066994
Epoch [2/2], Batch [791], Loss: 0.066496
Epoch [2/2], Batch [801], Loss: 0.065820
Epoch [2/2], Batch [811], Loss: 0.068101
Epoch [2/2], Batch [821], Loss: 0.066001
Epoch [2/2], Batch [831], Loss: 0.065371
Epoch [2/2], Batch [841], Loss: 0.064681
Epoch [2/2], Batch [851], Loss: 0.066363
Epoch [2/2], Batch [861], Loss: 0.066478
Epoch [2/2], Batch [871], Loss: 0.064603
Epoch [2/2], Batch [881], Loss: 0.065646
Epoch [2/2], Batch [891], Loss: 0.068009
Epoch [2/2], Batch [901], Loss: 0.065232
Epoch [2/2], Batch [911], Loss: 0.066307
Epoch [2/2], Batch [921], Loss: 0.064556
Epoch [2/2], Batch [931], Loss: 0.064199
Epoch [2/2], Batch [941], Loss: 0.063563
Epoch [2/2], Batch [951], Loss: 0.064770
Epoch [2/2], Batch [961], Loss: 0.065891
Epoch [2/2], Batch [971], Loss: 0.064438
Epoch [2/2], Batch [981], Loss: 0.067819
Epoch [2/2], Batch [991], Loss: 0.063789
Epoch [2/2], Batch [1001], Loss: 0.063656
Epoch [2/2], Batch [1011], Loss: 0.065652
Epoch [2/2], Batch [1021], Loss: 0.066283
Epoch [2/2], Batch [1031], Loss: 0.065701
Epoch [2/2], Batch [1041], Loss: 0.065887
Epoch [2/2], Batch [1051], Loss: 0.062449
Epoch [2/2], Batch [1061], Loss: 0.065046
Epoch [2/2], Batch [1071], Loss: 0.066205
Epoch [2/2], Batch [1081], Loss: 0.068426
Epoch [2/2], Batch [1091], Loss: 0.067975
Epoch [2/2], Batch [1101], Loss: 0.064154
Epoch [2/2], Batch [1111], Loss: 0.060436
Epoch [2/2], Batch [1121], Loss: 0.061069
Epoch [2/2], Batch [1131], Loss: 0.065969
Epoch [2/2], Batch [1141], Loss: 0.062410
Epoch [2/2], Batch [1151], Loss: 0.063052
Epoch [2/2], Batch [1161], Loss: 0.063993
Epoch [2/2], Batch [1171], Loss: 0.064318
Epoch [2/2], Batch [1181], Loss: 0.063722
Epoch [2/2], Batch [1191], Loss: 0.063811
Epoch [2/2], Batch [1201], Loss: 0.067202
Epoch [2/2], Batch [1211], Loss: 0.064609
Epoch [2/2], Batch [1221], Loss: 0.066078
Epoch [2/2], Batch [1231], Loss: 0.063868
Epoch [2/2], Batch [1241], Loss: 0.065139
Epoch [2/2], Batch [1251], Loss: 0.059921
Epoch [2/2], Batch [1261], Loss: 0.064953
Epoch [2/2], Batch [1271], Loss: 0.067610
Epoch [2/2], Batch [1281], Loss: 0.068022
Epoch [2/2], Batch [1291], Loss: 0.062090
Epoch [2/2], Batch [1301], Loss: 0.063467
Epoch [2/2], Batch [1311], Loss: 0.064381
Epoch [2/2], Batch [1321], Loss: 0.066626
Epoch [2/2], Batch [1331], Loss: 0.065954
Epoch [2/2], Batch [1341], Loss: 0.064260
Epoch [2/2], Batch [1351], Loss: 0.064660
Epoch [2/2], Batch [1361], Loss: 0.062861
Epoch [2/2], Batch [1371], Loss: 0.064023
Epoch [2/2], Batch [1381], Loss: 0.065801
Epoch [2/2], Batch [1391], Loss: 0.061027
Epoch [2/2], Batch [1401], Loss: 0.065609
Epoch [2/2], Batch [1411], Loss: 0.065952
Epoch [2/2], Batch [1421], Loss: 0.064797
Epoch [2/2], Batch [1431], Loss: 0.062950
Epoch [2/2], Batch [1441], Loss: 0.063127
Epoch [2/2], Batch [1451], Loss: 0.063103
Epoch [2/2], Batch [1461], Loss: 0.066954
Epoch [2/2], Batch [1471], Loss: 0.066338
Epoch [2/2], Batch [1481], Loss: 0.062858
Epoch [2/2], Batch [1491], Loss: 0.061530
Epoch [2/2], Batch [1501], Loss: 0.065473
Epoch [2/2], Batch [1511], Loss: 0.064479
Epoch [2/2], Batch [1521], Loss: 0.066498
Epoch [2/2], Batch [1531], Loss: 0.063170
Epoch [2/2], Batch [1541], Loss: 0.066259
Epoch [2/2], Batch [1551], Loss: 0.064298
Epoch [2/2], Batch [1561], Loss: 0.065006
Epoch [2/2], Batch [1571], Loss: 0.066202
Epoch [2/2], Batch [1581], Loss: 0.065060
Epoch [2/2], Batch [1591], Loss: 0.061790
Epoch [2/2], Batch [1601], Loss: 0.062309
Epoch [2/2], Batch [1611], Loss: 0.065190
Epoch [2/2], Batch [1621], Loss: 0.064582
Epoch [2/2], Batch [1631], Loss: 0.064015
Epoch [2/2], Batch [1641], Loss: 0.064758
Epoch [2/2], Batch [1651], Loss: 0.064465
Epoch [2/2], Batch [1661], Loss: 0.065734
Epoch [2/2], Batch [1671], Loss: 0.069842
Epoch [2/2], Batch [1681], Loss: 0.069111
Epoch [2/2], Batch [1691], Loss: 0.065975
Epoch [2/2], Batch [1701], Loss: 0.064445
Epoch [2/2], Batch [1711], Loss: 0.066154
Epoch [2/2], Batch [1721], Loss: 0.065920
Epoch [2/2], Batch [1731], Loss: 0.067764
Epoch [2/2], Batch [1741], Loss: 0.062661
Epoch [2/2], Batch [1751], Loss: 0.064233
Epoch [2/2], Batch [1761], Loss: 0.064411
Epoch [2/2], Batch [1771], Loss: 0.063046
Epoch [2/2], Batch [1781], Loss: 0.065731
Epoch [2/2], Batch [1791], Loss: 0.061883
Epoch [2/2], Batch [1801], Loss: 0.061194
Epoch [2/2], Batch [1811], Loss: 0.064208
Epoch [2/2], Batch [1821], Loss: 0.062815
Epoch [2/2], Batch [1831], Loss: 0.065089
Epoch [2/2], Batch [1841], Loss: 0.066583
Epoch [2/2], Batch [1851], Loss: 0.063552
Epoch [2/2], Batch [1861], Loss: 0.064130
Epoch [2/2], Batch [1871], Loss: 0.062156
Epoch [2/2], Batch [1881], Loss: 0.063957
Epoch [2/2], Batch [1891], Loss: 0.064851
Epoch [2/2], Batch [1901], Loss: 0.062247
Epoch [2/2], Batch [1911], Loss: 0.065341
Epoch [2/2], Batch [1921], Loss: 0.064682
Epoch [2/2], Batch [1931], Loss: 0.066886
Epoch [2/2], Batch [1941], Loss: 0.065105
Epoch [2/2], Batch [1951], Loss: 0.061293
Epoch [2/2], Batch [1961], Loss: 0.068399
Epoch [2/2], Batch [1971], Loss: 0.059420
Epoch [2/2], Batch [1981], Loss: 0.062909
Epoch [2/2], Batch [1991], Loss: 0.063917
Epoch [2/2], Batch [2001], Loss: 0.063735
Epoch [2/2], Batch [2011], Loss: 0.065103
Epoch [2/2], Batch [2021], Loss: 0.064273
Epoch [2/2], Batch [2031], Loss: 0.063320
Epoch [2/2], Batch [2041], Loss: 0.063325
Epoch [2/2], Batch [2051], Loss: 0.065017
Epoch [2/2], Batch [2061], Loss: 0.066475
Epoch [2/2], Batch [2071], Loss: 0.063832
Epoch [2/2], Batch [2081], Loss: 0.062218
Epoch [2/2], Batch [2091], Loss: 0.066811
Epoch [2/2], Batch [2101], Loss: 0.061357
Epoch [2/2], Batch [2111], Loss: 0.063610
Epoch [2/2], Batch [2121], Loss: 0.062169
Epoch [2/2], Batch [2131], Loss: 0.064704
Epoch [2/2], Batch [2141], Loss: 0.065130
Epoch [2/2], Batch [2151], Loss: 0.061603
Epoch [2/2], Batch [2161], Loss: 0.064222
Epoch [2/2], Batch [2171], Loss: 0.061780
Epoch [2/2], Batch [2181], Loss: 0.065221
Epoch [2/2], Batch [2191], Loss: 0.064676
Epoch [2/2], Batch [2201], Loss: 0.066463
Epoch [2/2], Batch [2211], Loss: 0.064685
Epoch [2/2], Batch [2221], Loss: 0.061213
Epoch [2/2], Batch [2231], Loss: 0.066327
Epoch [2/2], Batch [2241], Loss: 0.064577
Seq_Len: 2, Epoch [2/2] - Average Train Loss: 0.0648
Seq_Len: 2, Epoch [2/2] - Average Test Loss: 0.0623
Elapsed time: 1036.40 seconds
Seq_Len: 2, Epoch [2/2] - Average Validation Loss: 0.0644
Elapsed time: 1059.50 seconds

Training with sequence length 3.
Epoch [1/2], Batch [1], Loss: 0.103164
Epoch [1/2], Batch [11], Loss: 0.087453
Epoch [1/2], Batch [21], Loss: 0.078211
Epoch [1/2], Batch [31], Loss: 0.070509
Epoch [1/2], Batch [41], Loss: 0.068320
Epoch [1/2], Batch [51], Loss: 0.067605
Epoch [1/2], Batch [61], Loss: 0.073329
Epoch [1/2], Batch [71], Loss: 0.063578
Epoch [1/2], Batch [81], Loss: 0.068614
Epoch [1/2], Batch [91], Loss: 0.067309
Epoch [1/2], Batch [101], Loss: 0.069901
Epoch [1/2], Batch [111], Loss: 0.068317
Epoch [1/2], Batch [121], Loss: 0.068342
Epoch [1/2], Batch [131], Loss: 0.069754
Epoch [1/2], Batch [141], Loss: 0.063131
Epoch [1/2], Batch [151], Loss: 0.069954
Epoch [1/2], Batch [161], Loss: 0.068821
Epoch [1/2], Batch [171], Loss: 0.065736
Epoch [1/2], Batch [181], Loss: 0.068892
Epoch [1/2], Batch [191], Loss: 0.068140
Epoch [1/2], Batch [201], Loss: 0.068811
Epoch [1/2], Batch [211], Loss: 0.067912
Epoch [1/2], Batch [221], Loss: 0.067877
Epoch [1/2], Batch [231], Loss: 0.069426
Epoch [1/2], Batch [241], Loss: 0.067299
Epoch [1/2], Batch [251], Loss: 0.069996
Epoch [1/2], Batch [261], Loss: 0.068727
Epoch [1/2], Batch [271], Loss: 0.067726
Epoch [1/2], Batch [281], Loss: 0.068826
Epoch [1/2], Batch [291], Loss: 0.068669
Epoch [1/2], Batch [301], Loss: 0.069103
Epoch [1/2], Batch [311], Loss: 0.068040
Epoch [1/2], Batch [321], Loss: 0.064642
Epoch [1/2], Batch [331], Loss: 0.067140
Epoch [1/2], Batch [341], Loss: 0.067289
Epoch [1/2], Batch [351], Loss: 0.065045
Epoch [1/2], Batch [361], Loss: 0.062759
Epoch [1/2], Batch [371], Loss: 0.068125
Epoch [1/2], Batch [381], Loss: 0.066638
Epoch [1/2], Batch [391], Loss: 0.065476
Epoch [1/2], Batch [401], Loss: 0.065134
Epoch [1/2], Batch [411], Loss: 0.063415
Epoch [1/2], Batch [421], Loss: 0.067105
Epoch [1/2], Batch [431], Loss: 0.066097
Epoch [1/2], Batch [441], Loss: 0.065478
Epoch [1/2], Batch [451], Loss: 0.062868
Epoch [1/2], Batch [461], Loss: 0.063033
Epoch [1/2], Batch [471], Loss: 0.063872
Epoch [1/2], Batch [481], Loss: 0.064118
Epoch [1/2], Batch [491], Loss: 0.064642
Epoch [1/2], Batch [501], Loss: 0.065579
Epoch [1/2], Batch [511], Loss: 0.067031
Epoch [1/2], Batch [521], Loss: 0.063419
Epoch [1/2], Batch [531], Loss: 0.065021
Epoch [1/2], Batch [541], Loss: 0.063584
Epoch [1/2], Batch [551], Loss: 0.066216
Epoch [1/2], Batch [561], Loss: 0.065643
Epoch [1/2], Batch [571], Loss: 0.066316
Epoch [1/2], Batch [581], Loss: 0.068835
Epoch [1/2], Batch [591], Loss: 0.065280
Epoch [1/2], Batch [601], Loss: 0.063465
Epoch [1/2], Batch [611], Loss: 0.066573
Epoch [1/2], Batch [621], Loss: 0.062491
Epoch [1/2], Batch [631], Loss: 0.064966
Epoch [1/2], Batch [641], Loss: 0.064182
Epoch [1/2], Batch [651], Loss: 0.067262
Epoch [1/2], Batch [661], Loss: 0.064758
Epoch [1/2], Batch [671], Loss: 0.062374
Epoch [1/2], Batch [681], Loss: 0.062698
Epoch [1/2], Batch [691], Loss: 0.063329
Epoch [1/2], Batch [701], Loss: 0.061058
Epoch [1/2], Batch [711], Loss: 0.068152
Epoch [1/2], Batch [721], Loss: 0.062815
Epoch [1/2], Batch [731], Loss: 0.066725
Epoch [1/2], Batch [741], Loss: 0.063950
Epoch [1/2], Batch [751], Loss: 0.066613
Epoch [1/2], Batch [761], Loss: 0.064191
Epoch [1/2], Batch [771], Loss: 0.065102
Epoch [1/2], Batch [781], Loss: 0.065028
Epoch [1/2], Batch [791], Loss: 0.062862
Epoch [1/2], Batch [801], Loss: 0.061605
Epoch [1/2], Batch [811], Loss: 0.061968
Epoch [1/2], Batch [821], Loss: 0.065096
Epoch [1/2], Batch [831], Loss: 0.065891
Epoch [1/2], Batch [841], Loss: 0.064619
Epoch [1/2], Batch [851], Loss: 0.063910
Epoch [1/2], Batch [861], Loss: 0.062961
Epoch [1/2], Batch [871], Loss: 0.062188
Epoch [1/2], Batch [881], Loss: 0.065150
Epoch [1/2], Batch [891], Loss: 0.065208
Epoch [1/2], Batch [901], Loss: 0.060527
Epoch [1/2], Batch [911], Loss: 0.066006
Epoch [1/2], Batch [921], Loss: 0.063052
Epoch [1/2], Batch [931], Loss: 0.066448
Epoch [1/2], Batch [941], Loss: 0.063047
Epoch [1/2], Batch [951], Loss: 0.066262
Epoch [1/2], Batch [961], Loss: 0.063696
Epoch [1/2], Batch [971], Loss: 0.064343
Epoch [1/2], Batch [981], Loss: 0.063043
Epoch [1/2], Batch [991], Loss: 0.069023
Epoch [1/2], Batch [1001], Loss: 0.063910
Epoch [1/2], Batch [1011], Loss: 0.060835
Epoch [1/2], Batch [1021], Loss: 0.064059
Epoch [1/2], Batch [1031], Loss: 0.064231
Epoch [1/2], Batch [1041], Loss: 0.059521
Epoch [1/2], Batch [1051], Loss: 0.058119
Epoch [1/2], Batch [1061], Loss: 0.067466
Epoch [1/2], Batch [1071], Loss: 0.067396
Epoch [1/2], Batch [1081], Loss: 0.063927
Epoch [1/2], Batch [1091], Loss: 0.064122
Epoch [1/2], Batch [1101], Loss: 0.063961
Epoch [1/2], Batch [1111], Loss: 0.060347
Epoch [1/2], Batch [1121], Loss: 0.061615
Epoch [1/2], Batch [1131], Loss: 0.064846
Epoch [1/2], Batch [1141], Loss: 0.062198
Epoch [1/2], Batch [1151], Loss: 0.063343
Epoch [1/2], Batch [1161], Loss: 0.060456
Epoch [1/2], Batch [1171], Loss: 0.063938
Epoch [1/2], Batch [1181], Loss: 0.064200
Epoch [1/2], Batch [1191], Loss: 0.064355
Epoch [1/2], Batch [1201], Loss: 0.061092
Epoch [1/2], Batch [1211], Loss: 0.066983
Epoch [1/2], Batch [1221], Loss: 0.064464
Epoch [1/2], Batch [1231], Loss: 0.065105
Epoch [1/2], Batch [1241], Loss: 0.061871
Epoch [1/2], Batch [1251], Loss: 0.065073
Epoch [1/2], Batch [1261], Loss: 0.063523
Epoch [1/2], Batch [1271], Loss: 0.060005
Epoch [1/2], Batch [1281], Loss: 0.064891
Epoch [1/2], Batch [1291], Loss: 0.062442
Epoch [1/2], Batch [1301], Loss: 0.062413
Epoch [1/2], Batch [1311], Loss: 0.059438
Epoch [1/2], Batch [1321], Loss: 0.062962
Epoch [1/2], Batch [1331], Loss: 0.064561
Epoch [1/2], Batch [1341], Loss: 0.060178
Epoch [1/2], Batch [1351], Loss: 0.064427
Epoch [1/2], Batch [1361], Loss: 0.060825
Epoch [1/2], Batch [1371], Loss: 0.062950
Epoch [1/2], Batch [1381], Loss: 0.062341
Epoch [1/2], Batch [1391], Loss: 0.066500
Epoch [1/2], Batch [1401], Loss: 0.063475
Epoch [1/2], Batch [1411], Loss: 0.061356
Epoch [1/2], Batch [1421], Loss: 0.062434
Epoch [1/2], Batch [1431], Loss: 0.066879
Epoch [1/2], Batch [1441], Loss: 0.058910
Epoch [1/2], Batch [1451], Loss: 0.060554
Epoch [1/2], Batch [1461], Loss: 0.062619
Epoch [1/2], Batch [1471], Loss: 0.065114
Epoch [1/2], Batch [1481], Loss: 0.062930
Epoch [1/2], Batch [1491], Loss: 0.061130
Epoch [1/2], Batch [1501], Loss: 0.062740
Epoch [1/2], Batch [1511], Loss: 0.061875
Epoch [1/2], Batch [1521], Loss: 0.060767
Epoch [1/2], Batch [1531], Loss: 0.067505
Epoch [1/2], Batch [1541], Loss: 0.061009
Epoch [1/2], Batch [1551], Loss: 0.060773
Epoch [1/2], Batch [1561], Loss: 0.063282
Epoch [1/2], Batch [1571], Loss: 0.062757
Epoch [1/2], Batch [1581], Loss: 0.066492
Epoch [1/2], Batch [1591], Loss: 0.061812
Epoch [1/2], Batch [1601], Loss: 0.061423
Epoch [1/2], Batch [1611], Loss: 0.063902
Epoch [1/2], Batch [1621], Loss: 0.062959
Epoch [1/2], Batch [1631], Loss: 0.064076
Epoch [1/2], Batch [1641], Loss: 0.062592
Epoch [1/2], Batch [1651], Loss: 0.061591
Epoch [1/2], Batch [1661], Loss: 0.063103
Epoch [1/2], Batch [1671], Loss: 0.059438
Epoch [1/2], Batch [1681], Loss: 0.059066
Epoch [1/2], Batch [1691], Loss: 0.061739
Epoch [1/2], Batch [1701], Loss: 0.060615
Epoch [1/2], Batch [1711], Loss: 0.061399
Epoch [1/2], Batch [1721], Loss: 0.063809
Epoch [1/2], Batch [1731], Loss: 0.060478
Epoch [1/2], Batch [1741], Loss: 0.062316
Epoch [1/2], Batch [1751], Loss: 0.063113
Epoch [1/2], Batch [1761], Loss: 0.059461
Epoch [1/2], Batch [1771], Loss: 0.061616
Epoch [1/2], Batch [1781], Loss: 0.063342
Epoch [1/2], Batch [1791], Loss: 0.063495
Epoch [1/2], Batch [1801], Loss: 0.060237
Epoch [1/2], Batch [1811], Loss: 0.061529
Epoch [1/2], Batch [1821], Loss: 0.061159
Epoch [1/2], Batch [1831], Loss: 0.059972
Epoch [1/2], Batch [1841], Loss: 0.061439
Epoch [1/2], Batch [1851], Loss: 0.063856
Epoch [1/2], Batch [1861], Loss: 0.061890
Epoch [1/2], Batch [1871], Loss: 0.062543
Epoch [1/2], Batch [1881], Loss: 0.065257
Epoch [1/2], Batch [1891], Loss: 0.060724
Epoch [1/2], Batch [1901], Loss: 0.062707
Epoch [1/2], Batch [1911], Loss: 0.062460
Epoch [1/2], Batch [1921], Loss: 0.065257
Epoch [1/2], Batch [1931], Loss: 0.059297
Epoch [1/2], Batch [1941], Loss: 0.060220
Epoch [1/2], Batch [1951], Loss: 0.059503
Epoch [1/2], Batch [1961], Loss: 0.059892
Seq_Len: 3, Epoch [1/2] - Average Train Loss: 0.0644
Seq_Len: 3, Epoch [1/2] - Average Test Loss: 0.0597
Elapsed time: 1710.18 seconds
Seq_Len: 3, Epoch [1/2] - Average Validation Loss: 0.0615
Elapsed time: 1738.10 seconds
Epoch [2/2], Batch [1], Loss: 0.059264
Epoch [2/2], Batch [11], Loss: 0.057493
Epoch [2/2], Batch [21], Loss: 0.060273
Epoch [2/2], Batch [31], Loss: 0.060290
Epoch [2/2], Batch [41], Loss: 0.060750
Epoch [2/2], Batch [51], Loss: 0.061501
Epoch [2/2], Batch [61], Loss: 0.060837
Epoch [2/2], Batch [71], Loss: 0.057711
Epoch [2/2], Batch [81], Loss: 0.062091
Epoch [2/2], Batch [91], Loss: 0.060706
Epoch [2/2], Batch [101], Loss: 0.057944
Epoch [2/2], Batch [111], Loss: 0.058546
Epoch [2/2], Batch [121], Loss: 0.058670
Epoch [2/2], Batch [131], Loss: 0.063331
Epoch [2/2], Batch [141], Loss: 0.058161
Epoch [2/2], Batch [151], Loss: 0.060337
Epoch [2/2], Batch [161], Loss: 0.062841
Epoch [2/2], Batch [171], Loss: 0.064436
Epoch [2/2], Batch [181], Loss: 0.058428
Epoch [2/2], Batch [191], Loss: 0.057674
Epoch [2/2], Batch [201], Loss: 0.059242
Epoch [2/2], Batch [211], Loss: 0.058818
Epoch [2/2], Batch [221], Loss: 0.060764
Epoch [2/2], Batch [231], Loss: 0.060273
Epoch [2/2], Batch [241], Loss: 0.061013
Epoch [2/2], Batch [251], Loss: 0.061354
Epoch [2/2], Batch [261], Loss: 0.060624
Epoch [2/2], Batch [271], Loss: 0.060502
Epoch [2/2], Batch [281], Loss: 0.057897
Epoch [2/2], Batch [291], Loss: 0.058293
Epoch [2/2], Batch [301], Loss: 0.059112
Epoch [2/2], Batch [311], Loss: 0.060493
Epoch [2/2], Batch [321], Loss: 0.063002
Epoch [2/2], Batch [331], Loss: 0.061841
Epoch [2/2], Batch [341], Loss: 0.060467
Epoch [2/2], Batch [351], Loss: 0.057920
Epoch [2/2], Batch [361], Loss: 0.060372
Epoch [2/2], Batch [371], Loss: 0.060707
Epoch [2/2], Batch [381], Loss: 0.060591
Epoch [2/2], Batch [391], Loss: 0.060354
Epoch [2/2], Batch [401], Loss: 0.060120
Epoch [2/2], Batch [411], Loss: 0.060082
Epoch [2/2], Batch [421], Loss: 0.058663
Epoch [2/2], Batch [431], Loss: 0.061148
Epoch [2/2], Batch [441], Loss: 0.062559
Epoch [2/2], Batch [451], Loss: 0.058107
Epoch [2/2], Batch [461], Loss: 0.062551
Epoch [2/2], Batch [471], Loss: 0.058944
Epoch [2/2], Batch [481], Loss: 0.059639
Epoch [2/2], Batch [491], Loss: 0.058507
Epoch [2/2], Batch [501], Loss: 0.056990
Epoch [2/2], Batch [511], Loss: 0.057484
Epoch [2/2], Batch [521], Loss: 0.056165
Epoch [2/2], Batch [531], Loss: 0.059559
Epoch [2/2], Batch [541], Loss: 0.062998
Epoch [2/2], Batch [551], Loss: 0.065783
Epoch [2/2], Batch [561], Loss: 0.059416
Epoch [2/2], Batch [571], Loss: 0.058030
Epoch [2/2], Batch [581], Loss: 0.061265
Epoch [2/2], Batch [591], Loss: 0.059745
Epoch [2/2], Batch [601], Loss: 0.058514
Epoch [2/2], Batch [611], Loss: 0.063213
Epoch [2/2], Batch [621], Loss: 0.060765
Epoch [2/2], Batch [631], Loss: 0.061440
Epoch [2/2], Batch [641], Loss: 0.056059
Epoch [2/2], Batch [651], Loss: 0.057859
Epoch [2/2], Batch [661], Loss: 0.056100
Epoch [2/2], Batch [671], Loss: 0.057610
Epoch [2/2], Batch [681], Loss: 0.060755
Epoch [2/2], Batch [691], Loss: 0.060614
Epoch [2/2], Batch [701], Loss: 0.060244
Epoch [2/2], Batch [711], Loss: 0.060514
Epoch [2/2], Batch [721], Loss: 0.061161
Epoch [2/2], Batch [731], Loss: 0.058614
Epoch [2/2], Batch [741], Loss: 0.058482
Epoch [2/2], Batch [751], Loss: 0.060643
Epoch [2/2], Batch [761], Loss: 0.062835
Epoch [2/2], Batch [771], Loss: 0.056992
Epoch [2/2], Batch [781], Loss: 0.061242
Epoch [2/2], Batch [791], Loss: 0.060750
Epoch [2/2], Batch [801], Loss: 0.061207
Epoch [2/2], Batch [811], Loss: 0.061018
Epoch [2/2], Batch [821], Loss: 0.061396
Epoch [2/2], Batch [831], Loss: 0.060865
Epoch [2/2], Batch [841], Loss: 0.060771
Epoch [2/2], Batch [851], Loss: 0.061712
Epoch [2/2], Batch [861], Loss: 0.059808
Epoch [2/2], Batch [871], Loss: 0.060705
Epoch [2/2], Batch [881], Loss: 0.059977
Epoch [2/2], Batch [891], Loss: 0.059915
Epoch [2/2], Batch [901], Loss: 0.062437
Epoch [2/2], Batch [911], Loss: 0.059160
Epoch [2/2], Batch [921], Loss: 0.058004
Epoch [2/2], Batch [931], Loss: 0.058710
Epoch [2/2], Batch [941], Loss: 0.060492
Epoch [2/2], Batch [951], Loss: 0.062723
Epoch [2/2], Batch [961], Loss: 0.057601
Epoch [2/2], Batch [971], Loss: 0.063302
Epoch [2/2], Batch [981], Loss: 0.060368
Epoch [2/2], Batch [991], Loss: 0.059309
Epoch [2/2], Batch [1001], Loss: 0.060586
Epoch [2/2], Batch [1011], Loss: 0.060904
Epoch [2/2], Batch [1021], Loss: 0.060520
Epoch [2/2], Batch [1031], Loss: 0.061115
Epoch [2/2], Batch [1041], Loss: 0.058801
Epoch [2/2], Batch [1051], Loss: 0.058833
Epoch [2/2], Batch [1061], Loss: 0.059235
Epoch [2/2], Batch [1071], Loss: 0.059066
Epoch [2/2], Batch [1081], Loss: 0.061212
Epoch [2/2], Batch [1091], Loss: 0.060876
Epoch [2/2], Batch [1101], Loss: 0.060922
Epoch [2/2], Batch [1111], Loss: 0.063344
Epoch [2/2], Batch [1121], Loss: 0.062716
Epoch [2/2], Batch [1131], Loss: 0.060381
Epoch [2/2], Batch [1141], Loss: 0.060209
Epoch [2/2], Batch [1151], Loss: 0.058483
Epoch [2/2], Batch [1161], Loss: 0.059791
Epoch [2/2], Batch [1171], Loss: 0.057781
Epoch [2/2], Batch [1181], Loss: 0.060328
Epoch [2/2], Batch [1191], Loss: 0.058732
Epoch [2/2], Batch [1201], Loss: 0.062766
Epoch [2/2], Batch [1211], Loss: 0.058829
Epoch [2/2], Batch [1221], Loss: 0.063202
Epoch [2/2], Batch [1231], Loss: 0.060197
Epoch [2/2], Batch [1241], Loss: 0.058231
Epoch [2/2], Batch [1251], Loss: 0.064843
Epoch [2/2], Batch [1261], Loss: 0.059521
Epoch [2/2], Batch [1271], Loss: 0.058111
Epoch [2/2], Batch [1281], Loss: 0.056460
Epoch [2/2], Batch [1291], Loss: 0.061132
Epoch [2/2], Batch [1301], Loss: 0.058860
Epoch [2/2], Batch [1311], Loss: 0.060800
Epoch [2/2], Batch [1321], Loss: 0.059237
Epoch [2/2], Batch [1331], Loss: 0.061610
Epoch [2/2], Batch [1341], Loss: 0.063218
Epoch [2/2], Batch [1351], Loss: 0.060727
Epoch [2/2], Batch [1361], Loss: 0.061826
Epoch [2/2], Batch [1371], Loss: 0.058615
Epoch [2/2], Batch [1381], Loss: 0.060587
Epoch [2/2], Batch [1391], Loss: 0.055428
Epoch [2/2], Batch [1401], Loss: 0.060401
Epoch [2/2], Batch [1411], Loss: 0.059483
Epoch [2/2], Batch [1421], Loss: 0.061067
Epoch [2/2], Batch [1431], Loss: 0.060004
Epoch [2/2], Batch [1441], Loss: 0.060967
Epoch [2/2], Batch [1451], Loss: 0.060523
Epoch [2/2], Batch [1461], Loss: 0.060193
Epoch [2/2], Batch [1471], Loss: 0.057742
Epoch [2/2], Batch [1481], Loss: 0.056573
Epoch [2/2], Batch [1491], Loss: 0.060255
Epoch [2/2], Batch [1501], Loss: 0.061277
Epoch [2/2], Batch [1511], Loss: 0.058342
Epoch [2/2], Batch [1521], Loss: 0.062117
Epoch [2/2], Batch [1531], Loss: 0.061302
Epoch [2/2], Batch [1541], Loss: 0.062018
Epoch [2/2], Batch [1551], Loss: 0.059712
Epoch [2/2], Batch [1561], Loss: 0.060109
Epoch [2/2], Batch [1571], Loss: 0.061284
Epoch [2/2], Batch [1581], Loss: 0.059921
Epoch [2/2], Batch [1591], Loss: 0.057612
Epoch [2/2], Batch [1601], Loss: 0.057608
Epoch [2/2], Batch [1611], Loss: 0.058084
Epoch [2/2], Batch [1621], Loss: 0.057855
Epoch [2/2], Batch [1631], Loss: 0.061134
Epoch [2/2], Batch [1641], Loss: 0.059576
Epoch [2/2], Batch [1651], Loss: 0.057677
Epoch [2/2], Batch [1661], Loss: 0.061884
Epoch [2/2], Batch [1671], Loss: 0.058109
Epoch [2/2], Batch [1681], Loss: 0.057988
Epoch [2/2], Batch [1691], Loss: 0.061134
Epoch [2/2], Batch [1701], Loss: 0.061234
Epoch [2/2], Batch [1711], Loss: 0.061868
Epoch [2/2], Batch [1721], Loss: 0.057625
Epoch [2/2], Batch [1731], Loss: 0.060422
Epoch [2/2], Batch [1741], Loss: 0.055250
Epoch [2/2], Batch [1751], Loss: 0.059208
Epoch [2/2], Batch [1761], Loss: 0.057605
Epoch [2/2], Batch [1771], Loss: 0.062207
Epoch [2/2], Batch [1781], Loss: 0.058903
Epoch [2/2], Batch [1791], Loss: 0.059283
Epoch [2/2], Batch [1801], Loss: 0.057769
Epoch [2/2], Batch [1811], Loss: 0.060516
Epoch [2/2], Batch [1821], Loss: 0.063055
Epoch [2/2], Batch [1831], Loss: 0.061039
Epoch [2/2], Batch [1841], Loss: 0.058494
Epoch [2/2], Batch [1851], Loss: 0.058447
Epoch [2/2], Batch [1861], Loss: 0.057184
Epoch [2/2], Batch [1871], Loss: 0.063324
Epoch [2/2], Batch [1881], Loss: 0.061519
Epoch [2/2], Batch [1891], Loss: 0.059909
Epoch [2/2], Batch [1901], Loss: 0.060115
Epoch [2/2], Batch [1911], Loss: 0.061603
Epoch [2/2], Batch [1921], Loss: 0.058778
Epoch [2/2], Batch [1931], Loss: 0.057736
Epoch [2/2], Batch [1941], Loss: 0.061448
Epoch [2/2], Batch [1951], Loss: 0.060580
Epoch [2/2], Batch [1961], Loss: 0.058830
Seq_Len: 3, Epoch [2/2] - Average Train Loss: 0.0601
Seq_Len: 3, Epoch [2/2] - Average Test Loss: 0.0579
Elapsed time: 2380.22 seconds
Seq_Len: 3, Epoch [2/2] - Average Validation Loss: 0.0605
Elapsed time: 2408.65 seconds

Training with sequence length 4.
Epoch [1/2], Batch [1], Loss: 0.068737
Epoch [1/2], Batch [11], Loss: 0.065744
Epoch [1/2], Batch [21], Loss: 0.063927
Epoch [1/2], Batch [31], Loss: 0.062130
Epoch [1/2], Batch [41], Loss: 0.061551
Epoch [1/2], Batch [51], Loss: 0.062629
Epoch [1/2], Batch [61], Loss: 0.063457
Epoch [1/2], Batch [71], Loss: 0.064533
Epoch [1/2], Batch [81], Loss: 0.061079
Epoch [1/2], Batch [91], Loss: 0.062419
Epoch [1/2], Batch [101], Loss: 0.062101
Epoch [1/2], Batch [111], Loss: 0.061541
Epoch [1/2], Batch [121], Loss: 0.061379
Epoch [1/2], Batch [131], Loss: 0.062006
Epoch [1/2], Batch [141], Loss: 0.060535
Epoch [1/2], Batch [151], Loss: 0.060653
Epoch [1/2], Batch [161], Loss: 0.059449
Epoch [1/2], Batch [171], Loss: 0.060510
Epoch [1/2], Batch [181], Loss: 0.061476
Epoch [1/2], Batch [191], Loss: 0.062287
Epoch [1/2], Batch [201], Loss: 0.060882
Epoch [1/2], Batch [211], Loss: 0.061959
Epoch [1/2], Batch [221], Loss: 0.063258
Epoch [1/2], Batch [231], Loss: 0.061832
Epoch [1/2], Batch [241], Loss: 0.058747
Epoch [1/2], Batch [251], Loss: 0.061251
Epoch [1/2], Batch [261], Loss: 0.059484
Epoch [1/2], Batch [271], Loss: 0.063433
Epoch [1/2], Batch [281], Loss: 0.061577
Epoch [1/2], Batch [291], Loss: 0.060334
Epoch [1/2], Batch [301], Loss: 0.061790
Epoch [1/2], Batch [311], Loss: 0.060205
Epoch [1/2], Batch [321], Loss: 0.058610
Epoch [1/2], Batch [331], Loss: 0.059187
Epoch [1/2], Batch [341], Loss: 0.060406
Epoch [1/2], Batch [351], Loss: 0.059446
Epoch [1/2], Batch [361], Loss: 0.060337
Epoch [1/2], Batch [371], Loss: 0.062969
Epoch [1/2], Batch [381], Loss: 0.060969
Epoch [1/2], Batch [391], Loss: 0.060814
Epoch [1/2], Batch [401], Loss: 0.060828
Epoch [1/2], Batch [411], Loss: 0.061814
Epoch [1/2], Batch [421], Loss: 0.062671
Epoch [1/2], Batch [431], Loss: 0.065629
Epoch [1/2], Batch [441], Loss: 0.061618
Epoch [1/2], Batch [451], Loss: 0.062799
Epoch [1/2], Batch [461], Loss: 0.058988
Epoch [1/2], Batch [471], Loss: 0.058756
Epoch [1/2], Batch [481], Loss: 0.059333
Epoch [1/2], Batch [491], Loss: 0.062638
Epoch [1/2], Batch [501], Loss: 0.062798
Epoch [1/2], Batch [511], Loss: 0.063424
Epoch [1/2], Batch [521], Loss: 0.057759
Epoch [1/2], Batch [531], Loss: 0.061758
Epoch [1/2], Batch [541], Loss: 0.060451
Epoch [1/2], Batch [551], Loss: 0.061524
Epoch [1/2], Batch [561], Loss: 0.061533
Epoch [1/2], Batch [571], Loss: 0.060512
Epoch [1/2], Batch [581], Loss: 0.062481
Epoch [1/2], Batch [591], Loss: 0.058659
Epoch [1/2], Batch [601], Loss: 0.061948
Epoch [1/2], Batch [611], Loss: 0.057342
Epoch [1/2], Batch [621], Loss: 0.060252
Epoch [1/2], Batch [631], Loss: 0.060536
Epoch [1/2], Batch [641], Loss: 0.062036
Epoch [1/2], Batch [651], Loss: 0.063488
Epoch [1/2], Batch [661], Loss: 0.061569
Epoch [1/2], Batch [671], Loss: 0.061463
Epoch [1/2], Batch [681], Loss: 0.060009
Epoch [1/2], Batch [691], Loss: 0.063118
Epoch [1/2], Batch [701], Loss: 0.059603
Epoch [1/2], Batch [711], Loss: 0.061681
Epoch [1/2], Batch [721], Loss: 0.057109
Epoch [1/2], Batch [731], Loss: 0.060336
Epoch [1/2], Batch [741], Loss: 0.063885
Epoch [1/2], Batch [751], Loss: 0.056390
Epoch [1/2], Batch [761], Loss: 0.062327
Epoch [1/2], Batch [771], Loss: 0.062239
Epoch [1/2], Batch [781], Loss: 0.059758
Epoch [1/2], Batch [791], Loss: 0.059633
Epoch [1/2], Batch [801], Loss: 0.058552
Epoch [1/2], Batch [811], Loss: 0.057599
Epoch [1/2], Batch [821], Loss: 0.059112
Epoch [1/2], Batch [831], Loss: 0.060587
Epoch [1/2], Batch [841], Loss: 0.058801
Epoch [1/2], Batch [851], Loss: 0.061791
Epoch [1/2], Batch [861], Loss: 0.059676
Epoch [1/2], Batch [871], Loss: 0.061011
Epoch [1/2], Batch [881], Loss: 0.060850
Epoch [1/2], Batch [891], Loss: 0.057338
Epoch [1/2], Batch [901], Loss: 0.062163
Epoch [1/2], Batch [911], Loss: 0.062720
Epoch [1/2], Batch [921], Loss: 0.059976
Epoch [1/2], Batch [931], Loss: 0.059776
Epoch [1/2], Batch [941], Loss: 0.058953
Epoch [1/2], Batch [951], Loss: 0.063270
Epoch [1/2], Batch [961], Loss: 0.058837
Epoch [1/2], Batch [971], Loss: 0.059088
Epoch [1/2], Batch [981], Loss: 0.059255
Epoch [1/2], Batch [991], Loss: 0.062361
Epoch [1/2], Batch [1001], Loss: 0.063367
Epoch [1/2], Batch [1011], Loss: 0.060223
Epoch [1/2], Batch [1021], Loss: 0.057110
Epoch [1/2], Batch [1031], Loss: 0.061841
Epoch [1/2], Batch [1041], Loss: 0.059733
Epoch [1/2], Batch [1051], Loss: 0.060022
Epoch [1/2], Batch [1061], Loss: 0.056011
Epoch [1/2], Batch [1071], Loss: 0.062235
Epoch [1/2], Batch [1081], Loss: 0.057015
Epoch [1/2], Batch [1091], Loss: 0.059914
Epoch [1/2], Batch [1101], Loss: 0.060185
Epoch [1/2], Batch [1111], Loss: 0.059216
Epoch [1/2], Batch [1121], Loss: 0.057964
Epoch [1/2], Batch [1131], Loss: 0.058797
Epoch [1/2], Batch [1141], Loss: 0.062283
Epoch [1/2], Batch [1151], Loss: 0.058449
Epoch [1/2], Batch [1161], Loss: 0.059506
Epoch [1/2], Batch [1171], Loss: 0.057265
Epoch [1/2], Batch [1181], Loss: 0.056599
Epoch [1/2], Batch [1191], Loss: 0.060863
Epoch [1/2], Batch [1201], Loss: 0.059856
Epoch [1/2], Batch [1211], Loss: 0.060612
Epoch [1/2], Batch [1221], Loss: 0.060622
Epoch [1/2], Batch [1231], Loss: 0.058536
Epoch [1/2], Batch [1241], Loss: 0.057017
Epoch [1/2], Batch [1251], Loss: 0.059883
Epoch [1/2], Batch [1261], Loss: 0.056535
Epoch [1/2], Batch [1271], Loss: 0.058382
Epoch [1/2], Batch [1281], Loss: 0.057436
Epoch [1/2], Batch [1291], Loss: 0.058628
Epoch [1/2], Batch [1301], Loss: 0.059554
Epoch [1/2], Batch [1311], Loss: 0.058331
Epoch [1/2], Batch [1321], Loss: 0.059479
Epoch [1/2], Batch [1331], Loss: 0.057146
Epoch [1/2], Batch [1341], Loss: 0.061755
Epoch [1/2], Batch [1351], Loss: 0.061173
Epoch [1/2], Batch [1361], Loss: 0.059721
Epoch [1/2], Batch [1371], Loss: 0.062727
Epoch [1/2], Batch [1381], Loss: 0.060645
Epoch [1/2], Batch [1391], Loss: 0.061071
Epoch [1/2], Batch [1401], Loss: 0.061893
Epoch [1/2], Batch [1411], Loss: 0.055972
Epoch [1/2], Batch [1421], Loss: 0.057553
Epoch [1/2], Batch [1431], Loss: 0.059847
Epoch [1/2], Batch [1441], Loss: 0.059095
Epoch [1/2], Batch [1451], Loss: 0.059326
Epoch [1/2], Batch [1461], Loss: 0.058585
Epoch [1/2], Batch [1471], Loss: 0.057867
Epoch [1/2], Batch [1481], Loss: 0.060380
Epoch [1/2], Batch [1491], Loss: 0.059340
Epoch [1/2], Batch [1501], Loss: 0.062296
Epoch [1/2], Batch [1511], Loss: 0.060799
Epoch [1/2], Batch [1521], Loss: 0.063050
Epoch [1/2], Batch [1531], Loss: 0.059335
Epoch [1/2], Batch [1541], Loss: 0.061197
Epoch [1/2], Batch [1551], Loss: 0.059209
Epoch [1/2], Batch [1561], Loss: 0.058714
Epoch [1/2], Batch [1571], Loss: 0.059211
Epoch [1/2], Batch [1581], Loss: 0.058968
Epoch [1/2], Batch [1591], Loss: 0.058619
Epoch [1/2], Batch [1601], Loss: 0.061699
Epoch [1/2], Batch [1611], Loss: 0.056949
Epoch [1/2], Batch [1621], Loss: 0.061059
Epoch [1/2], Batch [1631], Loss: 0.058480
Epoch [1/2], Batch [1641], Loss: 0.061015
Epoch [1/2], Batch [1651], Loss: 0.057870
Epoch [1/2], Batch [1661], Loss: 0.059633
Epoch [1/2], Batch [1671], Loss: 0.061552
Epoch [1/2], Batch [1681], Loss: 0.060211
Seq_Len: 4, Epoch [1/2] - Average Train Loss: 0.0603
Seq_Len: 4, Epoch [1/2] - Average Test Loss: 0.0573
Elapsed time: 3142.30 seconds
Seq_Len: 4, Epoch [1/2] - Average Validation Loss: 0.0610
Elapsed time: 3172.80 seconds
Epoch [2/2], Batch [1], Loss: 0.056596
Epoch [2/2], Batch [11], Loss: 0.055447
Epoch [2/2], Batch [21], Loss: 0.058710
Epoch [2/2], Batch [31], Loss: 0.053540
Epoch [2/2], Batch [41], Loss: 0.056064
Epoch [2/2], Batch [51], Loss: 0.056329
Epoch [2/2], Batch [61], Loss: 0.055852
Epoch [2/2], Batch [71], Loss: 0.058749
Epoch [2/2], Batch [81], Loss: 0.057263
Epoch [2/2], Batch [91], Loss: 0.055988
Epoch [2/2], Batch [101], Loss: 0.055104
Epoch [2/2], Batch [111], Loss: 0.060115
Epoch [2/2], Batch [121], Loss: 0.054956
Epoch [2/2], Batch [131], Loss: 0.057259
Epoch [2/2], Batch [141], Loss: 0.056887
Epoch [2/2], Batch [151], Loss: 0.057525
Epoch [2/2], Batch [161], Loss: 0.056491
Epoch [2/2], Batch [171], Loss: 0.057553
Epoch [2/2], Batch [181], Loss: 0.058767
Epoch [2/2], Batch [191], Loss: 0.059662
Epoch [2/2], Batch [201], Loss: 0.056688
Epoch [2/2], Batch [211], Loss: 0.057580
Epoch [2/2], Batch [221], Loss: 0.058223
Epoch [2/2], Batch [231], Loss: 0.057693
Epoch [2/2], Batch [241], Loss: 0.056518
Epoch [2/2], Batch [251], Loss: 0.057864
Epoch [2/2], Batch [261], Loss: 0.060161
Epoch [2/2], Batch [271], Loss: 0.059689
Epoch [2/2], Batch [281], Loss: 0.056674
Epoch [2/2], Batch [291], Loss: 0.058825
Epoch [2/2], Batch [301], Loss: 0.059158
Epoch [2/2], Batch [311], Loss: 0.057384
Epoch [2/2], Batch [321], Loss: 0.059836
Epoch [2/2], Batch [331], Loss: 0.059529
Epoch [2/2], Batch [341], Loss: 0.056371
Epoch [2/2], Batch [351], Loss: 0.056496
Epoch [2/2], Batch [361], Loss: 0.058515
Epoch [2/2], Batch [371], Loss: 0.058330
Epoch [2/2], Batch [381], Loss: 0.057600
Epoch [2/2], Batch [391], Loss: 0.057046
Epoch [2/2], Batch [401], Loss: 0.056544
Epoch [2/2], Batch [411], Loss: 0.056703
Epoch [2/2], Batch [421], Loss: 0.061155
Epoch [2/2], Batch [431], Loss: 0.056418
Epoch [2/2], Batch [441], Loss: 0.057680
Epoch [2/2], Batch [451], Loss: 0.058241
Epoch [2/2], Batch [461], Loss: 0.058858
Epoch [2/2], Batch [471], Loss: 0.058275
Epoch [2/2], Batch [481], Loss: 0.056653
Epoch [2/2], Batch [491], Loss: 0.054372
Epoch [2/2], Batch [501], Loss: 0.055235
Epoch [2/2], Batch [511], Loss: 0.055389
Epoch [2/2], Batch [521], Loss: 0.056529
Epoch [2/2], Batch [531], Loss: 0.055893
Epoch [2/2], Batch [541], Loss: 0.054810
Epoch [2/2], Batch [551], Loss: 0.056788
Epoch [2/2], Batch [561], Loss: 0.058979
Epoch [2/2], Batch [571], Loss: 0.055101
Epoch [2/2], Batch [581], Loss: 0.054961
Epoch [2/2], Batch [591], Loss: 0.057284
Epoch [2/2], Batch [601], Loss: 0.059893
Epoch [2/2], Batch [611], Loss: 0.056353
Epoch [2/2], Batch [621], Loss: 0.057679
Epoch [2/2], Batch [631], Loss: 0.058372
Epoch [2/2], Batch [641], Loss: 0.059082
Epoch [2/2], Batch [651], Loss: 0.056416
Epoch [2/2], Batch [661], Loss: 0.057865
Epoch [2/2], Batch [671], Loss: 0.057212
Epoch [2/2], Batch [681], Loss: 0.056524
Epoch [2/2], Batch [691], Loss: 0.056170
Epoch [2/2], Batch [701], Loss: 0.059223
Epoch [2/2], Batch [711], Loss: 0.058472
Epoch [2/2], Batch [721], Loss: 0.058918
Epoch [2/2], Batch [731], Loss: 0.056184
Epoch [2/2], Batch [741], Loss: 0.061556
Epoch [2/2], Batch [751], Loss: 0.058200
Epoch [2/2], Batch [761], Loss: 0.054567
Epoch [2/2], Batch [771], Loss: 0.058814
Epoch [2/2], Batch [781], Loss: 0.058788
Epoch [2/2], Batch [791], Loss: 0.057565
Epoch [2/2], Batch [801], Loss: 0.058320
Epoch [2/2], Batch [811], Loss: 0.057136
Epoch [2/2], Batch [821], Loss: 0.058191
Epoch [2/2], Batch [831], Loss: 0.058967
Epoch [2/2], Batch [841], Loss: 0.057833
Epoch [2/2], Batch [851], Loss: 0.058516
Epoch [2/2], Batch [861], Loss: 0.056588
Epoch [2/2], Batch [871], Loss: 0.052356
Epoch [2/2], Batch [881], Loss: 0.055246
Epoch [2/2], Batch [891], Loss: 0.054807
Epoch [2/2], Batch [901], Loss: 0.057447
Epoch [2/2], Batch [911], Loss: 0.059017
Epoch [2/2], Batch [921], Loss: 0.055597
Epoch [2/2], Batch [931], Loss: 0.057571
Epoch [2/2], Batch [941], Loss: 0.055267
Epoch [2/2], Batch [951], Loss: 0.058356
Epoch [2/2], Batch [961], Loss: 0.058199
Epoch [2/2], Batch [971], Loss: 0.057657
Epoch [2/2], Batch [981], Loss: 0.057114
Epoch [2/2], Batch [991], Loss: 0.058079
Epoch [2/2], Batch [1001], Loss: 0.055006
Epoch [2/2], Batch [1011], Loss: 0.056017
Epoch [2/2], Batch [1021], Loss: 0.058866
Epoch [2/2], Batch [1031], Loss: 0.058324
Epoch [2/2], Batch [1041], Loss: 0.056913
Epoch [2/2], Batch [1051], Loss: 0.057670
Epoch [2/2], Batch [1061], Loss: 0.057278
Epoch [2/2], Batch [1071], Loss: 0.058393
Epoch [2/2], Batch [1081], Loss: 0.054888
Epoch [2/2], Batch [1091], Loss: 0.057364
Epoch [2/2], Batch [1101], Loss: 0.053731
Epoch [2/2], Batch [1111], Loss: 0.056253
Epoch [2/2], Batch [1121], Loss: 0.056555
Epoch [2/2], Batch [1131], Loss: 0.057479
Epoch [2/2], Batch [1141], Loss: 0.055856
Epoch [2/2], Batch [1151], Loss: 0.058688
Epoch [2/2], Batch [1161], Loss: 0.055495
Epoch [2/2], Batch [1171], Loss: 0.059959
Epoch [2/2], Batch [1181], Loss: 0.056958
Epoch [2/2], Batch [1191], Loss: 0.059634
Epoch [2/2], Batch [1201], Loss: 0.057022
Epoch [2/2], Batch [1211], Loss: 0.055588
Epoch [2/2], Batch [1221], Loss: 0.057775
Epoch [2/2], Batch [1231], Loss: 0.059951
Epoch [2/2], Batch [1241], Loss: 0.058050
Epoch [2/2], Batch [1251], Loss: 0.058765
Epoch [2/2], Batch [1261], Loss: 0.056998
Epoch [2/2], Batch [1271], Loss: 0.055951
Epoch [2/2], Batch [1281], Loss: 0.059952
Epoch [2/2], Batch [1291], Loss: 0.056901
Epoch [2/2], Batch [1301], Loss: 0.054266
Epoch [2/2], Batch [1311], Loss: 0.054385
Epoch [2/2], Batch [1321], Loss: 0.057468
Epoch [2/2], Batch [1331], Loss: 0.056279
Epoch [2/2], Batch [1341], Loss: 0.060548
Epoch [2/2], Batch [1351], Loss: 0.054765
Epoch [2/2], Batch [1361], Loss: 0.056552
Epoch [2/2], Batch [1371], Loss: 0.058240
Epoch [2/2], Batch [1381], Loss: 0.055969
Epoch [2/2], Batch [1391], Loss: 0.057413
Epoch [2/2], Batch [1401], Loss: 0.056136
Epoch [2/2], Batch [1411], Loss: 0.056092
Epoch [2/2], Batch [1421], Loss: 0.058791
Epoch [2/2], Batch [1431], Loss: 0.060520
Epoch [2/2], Batch [1441], Loss: 0.056404
Epoch [2/2], Batch [1451], Loss: 0.055869
Epoch [2/2], Batch [1461], Loss: 0.057724
Epoch [2/2], Batch [1471], Loss: 0.055732
Epoch [2/2], Batch [1481], Loss: 0.056510
Epoch [2/2], Batch [1491], Loss: 0.056563
Epoch [2/2], Batch [1501], Loss: 0.058849
Epoch [2/2], Batch [1511], Loss: 0.057075
Epoch [2/2], Batch [1521], Loss: 0.057590
Epoch [2/2], Batch [1531], Loss: 0.057078
Epoch [2/2], Batch [1541], Loss: 0.055971
Epoch [2/2], Batch [1551], Loss: 0.057581
Epoch [2/2], Batch [1561], Loss: 0.058652
Epoch [2/2], Batch [1571], Loss: 0.057118
Epoch [2/2], Batch [1581], Loss: 0.061732
Epoch [2/2], Batch [1591], Loss: 0.058466
Epoch [2/2], Batch [1601], Loss: 0.061021
Epoch [2/2], Batch [1611], Loss: 0.054687
Epoch [2/2], Batch [1621], Loss: 0.055872
Epoch [2/2], Batch [1631], Loss: 0.057981
Epoch [2/2], Batch [1641], Loss: 0.059541
Epoch [2/2], Batch [1651], Loss: 0.055523
Epoch [2/2], Batch [1661], Loss: 0.057269
Epoch [2/2], Batch [1671], Loss: 0.056403
Epoch [2/2], Batch [1681], Loss: 0.057638
Seq_Len: 4, Epoch [2/2] - Average Train Loss: 0.0573
Seq_Len: 4, Epoch [2/2] - Average Test Loss: 0.0545
Elapsed time: 3889.28 seconds
Seq_Len: 4, Epoch [2/2] - Average Validation Loss: 0.0599
Elapsed time: 3919.75 seconds

Training with sequence length 5.
Epoch [1/2], Batch [1], Loss: 0.060999
Epoch [1/2], Batch [11], Loss: 0.058143
Epoch [1/2], Batch [21], Loss: 0.058314
Epoch [1/2], Batch [31], Loss: 0.055229
Epoch [1/2], Batch [41], Loss: 0.057969
Epoch [1/2], Batch [51], Loss: 0.058879
Epoch [1/2], Batch [61], Loss: 0.056912
Epoch [1/2], Batch [71], Loss: 0.056454
Epoch [1/2], Batch [81], Loss: 0.058643
Epoch [1/2], Batch [91], Loss: 0.055370
Epoch [1/2], Batch [101], Loss: 0.058119
Epoch [1/2], Batch [111], Loss: 0.057826
Epoch [1/2], Batch [121], Loss: 0.059535
Epoch [1/2], Batch [131], Loss: 0.058316
Epoch [1/2], Batch [141], Loss: 0.059334
Epoch [1/2], Batch [151], Loss: 0.058353
Epoch [1/2], Batch [161], Loss: 0.057412
Epoch [1/2], Batch [171], Loss: 0.057689
Epoch [1/2], Batch [181], Loss: 0.058232
Epoch [1/2], Batch [191], Loss: 0.059786
Epoch [1/2], Batch [201], Loss: 0.056585
Epoch [1/2], Batch [211], Loss: 0.057245
Epoch [1/2], Batch [221], Loss: 0.056368
Epoch [1/2], Batch [231], Loss: 0.059824
Epoch [1/2], Batch [241], Loss: 0.057191
Epoch [1/2], Batch [251], Loss: 0.057441
Epoch [1/2], Batch [261], Loss: 0.057791
Epoch [1/2], Batch [271], Loss: 0.057012
Epoch [1/2], Batch [281], Loss: 0.057378
Epoch [1/2], Batch [291], Loss: 0.058191
Epoch [1/2], Batch [301], Loss: 0.059942
Epoch [1/2], Batch [311], Loss: 0.056358
Epoch [1/2], Batch [321], Loss: 0.058613
Epoch [1/2], Batch [331], Loss: 0.058250
Epoch [1/2], Batch [341], Loss: 0.060490
Epoch [1/2], Batch [351], Loss: 0.056688
Epoch [1/2], Batch [361], Loss: 0.060723
Epoch [1/2], Batch [371], Loss: 0.059859
Epoch [1/2], Batch [381], Loss: 0.060293
Epoch [1/2], Batch [391], Loss: 0.058739
Epoch [1/2], Batch [401], Loss: 0.059561
Epoch [1/2], Batch [411], Loss: 0.055191
Epoch [1/2], Batch [421], Loss: 0.055586
Epoch [1/2], Batch [431], Loss: 0.057216
Epoch [1/2], Batch [441], Loss: 0.057158
Epoch [1/2], Batch [451], Loss: 0.058673
Epoch [1/2], Batch [461], Loss: 0.056065
Epoch [1/2], Batch [471], Loss: 0.059168
Epoch [1/2], Batch [481], Loss: 0.057774
Epoch [1/2], Batch [491], Loss: 0.057866
Epoch [1/2], Batch [501], Loss: 0.059218
Epoch [1/2], Batch [511], Loss: 0.056194
Epoch [1/2], Batch [521], Loss: 0.058984
Epoch [1/2], Batch [531], Loss: 0.056775
Epoch [1/2], Batch [541], Loss: 0.061400
Epoch [1/2], Batch [551], Loss: 0.056439
Epoch [1/2], Batch [561], Loss: 0.059030
Epoch [1/2], Batch [571], Loss: 0.055995
Epoch [1/2], Batch [581], Loss: 0.058580
Epoch [1/2], Batch [591], Loss: 0.054844
Epoch [1/2], Batch [601], Loss: 0.058875
Epoch [1/2], Batch [611], Loss: 0.059357
Epoch [1/2], Batch [621], Loss: 0.057899
Epoch [1/2], Batch [631], Loss: 0.061170
Epoch [1/2], Batch [641], Loss: 0.053678
Epoch [1/2], Batch [651], Loss: 0.055412
Epoch [1/2], Batch [661], Loss: 0.058549
Epoch [1/2], Batch [671], Loss: 0.055885
Epoch [1/2], Batch [681], Loss: 0.057687
Epoch [1/2], Batch [691], Loss: 0.060208
Epoch [1/2], Batch [701], Loss: 0.058902
Epoch [1/2], Batch [711], Loss: 0.057019
Epoch [1/2], Batch [721], Loss: 0.055750
Epoch [1/2], Batch [731], Loss: 0.056696
Epoch [1/2], Batch [741], Loss: 0.060722
Epoch [1/2], Batch [751], Loss: 0.058835
Epoch [1/2], Batch [761], Loss: 0.060505
Epoch [1/2], Batch [771], Loss: 0.056685
Epoch [1/2], Batch [781], Loss: 0.056255
Epoch [1/2], Batch [791], Loss: 0.056301
Epoch [1/2], Batch [801], Loss: 0.054523
Epoch [1/2], Batch [811], Loss: 0.055753
Epoch [1/2], Batch [821], Loss: 0.056680
Epoch [1/2], Batch [831], Loss: 0.058107
Epoch [1/2], Batch [841], Loss: 0.054205
Epoch [1/2], Batch [851], Loss: 0.058847
Epoch [1/2], Batch [861], Loss: 0.059596
Epoch [1/2], Batch [871], Loss: 0.061028
Epoch [1/2], Batch [881], Loss: 0.057414
Epoch [1/2], Batch [891], Loss: 0.058140
Epoch [1/2], Batch [901], Loss: 0.057802
Epoch [1/2], Batch [911], Loss: 0.056989
Epoch [1/2], Batch [921], Loss: 0.060027
Epoch [1/2], Batch [931], Loss: 0.057356
Epoch [1/2], Batch [941], Loss: 0.056902
Epoch [1/2], Batch [951], Loss: 0.055463
Epoch [1/2], Batch [961], Loss: 0.054341
Epoch [1/2], Batch [971], Loss: 0.057411
Epoch [1/2], Batch [981], Loss: 0.055792
Epoch [1/2], Batch [991], Loss: 0.057338
Epoch [1/2], Batch [1001], Loss: 0.055777
Epoch [1/2], Batch [1011], Loss: 0.059833
Epoch [1/2], Batch [1021], Loss: 0.058644
Epoch [1/2], Batch [1031], Loss: 0.059139
Epoch [1/2], Batch [1041], Loss: 0.057607
Epoch [1/2], Batch [1051], Loss: 0.056521
Epoch [1/2], Batch [1061], Loss: 0.057980
Epoch [1/2], Batch [1071], Loss: 0.059913
Epoch [1/2], Batch [1081], Loss: 0.056955
Epoch [1/2], Batch [1091], Loss: 0.058471
Epoch [1/2], Batch [1101], Loss: 0.059283
Epoch [1/2], Batch [1111], Loss: 0.058977
Epoch [1/2], Batch [1121], Loss: 0.059303
Epoch [1/2], Batch [1131], Loss: 0.056506
Epoch [1/2], Batch [1141], Loss: 0.060297
Epoch [1/2], Batch [1151], Loss: 0.057623
Epoch [1/2], Batch [1161], Loss: 0.057797
Epoch [1/2], Batch [1171], Loss: 0.059111
Epoch [1/2], Batch [1181], Loss: 0.057944
Epoch [1/2], Batch [1191], Loss: 0.059948
Epoch [1/2], Batch [1201], Loss: 0.055508
Epoch [1/2], Batch [1211], Loss: 0.059962
Epoch [1/2], Batch [1221], Loss: 0.058933
Epoch [1/2], Batch [1231], Loss: 0.058217
Epoch [1/2], Batch [1241], Loss: 0.058671
Epoch [1/2], Batch [1251], Loss: 0.060149
Epoch [1/2], Batch [1261], Loss: 0.059330
Epoch [1/2], Batch [1271], Loss: 0.059347
Epoch [1/2], Batch [1281], Loss: 0.055099
Epoch [1/2], Batch [1291], Loss: 0.056927
Epoch [1/2], Batch [1301], Loss: 0.054936
Epoch [1/2], Batch [1311], Loss: 0.055224
Epoch [1/2], Batch [1321], Loss: 0.056409
Epoch [1/2], Batch [1331], Loss: 0.056993
Epoch [1/2], Batch [1341], Loss: 0.057891
Epoch [1/2], Batch [1351], Loss: 0.056963
Epoch [1/2], Batch [1361], Loss: 0.056422
Epoch [1/2], Batch [1371], Loss: 0.057141
Epoch [1/2], Batch [1381], Loss: 0.058594
Epoch [1/2], Batch [1391], Loss: 0.056641
Epoch [1/2], Batch [1401], Loss: 0.059322
Seq_Len: 5, Epoch [1/2] - Average Train Loss: 0.0580
Seq_Len: 5, Epoch [1/2] - Average Test Loss: 0.0562
Elapsed time: 4675.39 seconds
Seq_Len: 5, Epoch [1/2] - Average Validation Loss: 0.0604
Elapsed time: 4706.22 seconds
Epoch [2/2], Batch [1], Loss: 0.057210
Epoch [2/2], Batch [11], Loss: 0.057162
Epoch [2/2], Batch [21], Loss: 0.057548
Epoch [2/2], Batch [31], Loss: 0.054185
Epoch [2/2], Batch [41], Loss: 0.054630
Epoch [2/2], Batch [51], Loss: 0.057714
Epoch [2/2], Batch [61], Loss: 0.054777
Epoch [2/2], Batch [71], Loss: 0.053519
Epoch [2/2], Batch [81], Loss: 0.051073
Epoch [2/2], Batch [91], Loss: 0.057253
Epoch [2/2], Batch [101], Loss: 0.055484
Epoch [2/2], Batch [111], Loss: 0.054520
Epoch [2/2], Batch [121], Loss: 0.056353
Epoch [2/2], Batch [131], Loss: 0.056108
Epoch [2/2], Batch [141], Loss: 0.055218
Epoch [2/2], Batch [151], Loss: 0.056039
Epoch [2/2], Batch [161], Loss: 0.055650
Epoch [2/2], Batch [171], Loss: 0.055457
Epoch [2/2], Batch [181], Loss: 0.056095
Epoch [2/2], Batch [191], Loss: 0.056364
Epoch [2/2], Batch [201], Loss: 0.054215
Epoch [2/2], Batch [211], Loss: 0.054008
Epoch [2/2], Batch [221], Loss: 0.055973
Epoch [2/2], Batch [231], Loss: 0.054658
Epoch [2/2], Batch [241], Loss: 0.054720
Epoch [2/2], Batch [251], Loss: 0.052805
Epoch [2/2], Batch [261], Loss: 0.055464
Epoch [2/2], Batch [271], Loss: 0.054427
Epoch [2/2], Batch [281], Loss: 0.055386
Epoch [2/2], Batch [291], Loss: 0.056555
Epoch [2/2], Batch [301], Loss: 0.053840
Epoch [2/2], Batch [311], Loss: 0.058307
Epoch [2/2], Batch [321], Loss: 0.056755
Epoch [2/2], Batch [331], Loss: 0.055665
Epoch [2/2], Batch [341], Loss: 0.055553
Epoch [2/2], Batch [351], Loss: 0.056429
Epoch [2/2], Batch [361], Loss: 0.055630
Epoch [2/2], Batch [371], Loss: 0.054778
Epoch [2/2], Batch [381], Loss: 0.052838
Epoch [2/2], Batch [391], Loss: 0.054500
Epoch [2/2], Batch [401], Loss: 0.054775
Epoch [2/2], Batch [411], Loss: 0.057020
Epoch [2/2], Batch [421], Loss: 0.055082
Epoch [2/2], Batch [431], Loss: 0.054926
Epoch [2/2], Batch [441], Loss: 0.056223
Epoch [2/2], Batch [451], Loss: 0.054422
Epoch [2/2], Batch [461], Loss: 0.054254
Epoch [2/2], Batch [471], Loss: 0.056782
Epoch [2/2], Batch [481], Loss: 0.055600
Epoch [2/2], Batch [491], Loss: 0.055191
Epoch [2/2], Batch [501], Loss: 0.054893
Epoch [2/2], Batch [511], Loss: 0.057276
Epoch [2/2], Batch [521], Loss: 0.055099
Epoch [2/2], Batch [531], Loss: 0.055687
Epoch [2/2], Batch [541], Loss: 0.053429
Epoch [2/2], Batch [551], Loss: 0.054793
Epoch [2/2], Batch [561], Loss: 0.056113
Epoch [2/2], Batch [571], Loss: 0.054201
Epoch [2/2], Batch [581], Loss: 0.054303
Epoch [2/2], Batch [591], Loss: 0.054426
Epoch [2/2], Batch [601], Loss: 0.056298
Epoch [2/2], Batch [611], Loss: 0.055264
Epoch [2/2], Batch [621], Loss: 0.055556
Epoch [2/2], Batch [631], Loss: 0.054071
Epoch [2/2], Batch [641], Loss: 0.052489
Epoch [2/2], Batch [651], Loss: 0.054497
Epoch [2/2], Batch [661], Loss: 0.052168
Epoch [2/2], Batch [671], Loss: 0.053824
Epoch [2/2], Batch [681], Loss: 0.053824
Epoch [2/2], Batch [691], Loss: 0.054497
Epoch [2/2], Batch [701], Loss: 0.055099
Epoch [2/2], Batch [711], Loss: 0.051968
Epoch [2/2], Batch [721], Loss: 0.055392
Epoch [2/2], Batch [731], Loss: 0.055303
Epoch [2/2], Batch [741], Loss: 0.057704
Epoch [2/2], Batch [751], Loss: 0.054922
Epoch [2/2], Batch [761], Loss: 0.054300
Epoch [2/2], Batch [771], Loss: 0.051599
Epoch [2/2], Batch [781], Loss: 0.055437
Epoch [2/2], Batch [791], Loss: 0.055801
Epoch [2/2], Batch [801], Loss: 0.053452
Epoch [2/2], Batch [811], Loss: 0.056759
Epoch [2/2], Batch [821], Loss: 0.056009
Epoch [2/2], Batch [831], Loss: 0.059576
Epoch [2/2], Batch [841], Loss: 0.055631
Epoch [2/2], Batch [851], Loss: 0.054202
Epoch [2/2], Batch [861], Loss: 0.054573
Epoch [2/2], Batch [871], Loss: 0.056837
Epoch [2/2], Batch [881], Loss: 0.058440
Epoch [2/2], Batch [891], Loss: 0.054402
Epoch [2/2], Batch [901], Loss: 0.052588
Epoch [2/2], Batch [911], Loss: 0.055318
Epoch [2/2], Batch [921], Loss: 0.054098
Epoch [2/2], Batch [931], Loss: 0.056349
Epoch [2/2], Batch [941], Loss: 0.054018
Epoch [2/2], Batch [951], Loss: 0.057389
Epoch [2/2], Batch [961], Loss: 0.054802
Epoch [2/2], Batch [971], Loss: 0.054237
Epoch [2/2], Batch [981], Loss: 0.052601
Epoch [2/2], Batch [991], Loss: 0.056876
Epoch [2/2], Batch [1001], Loss: 0.053191
Epoch [2/2], Batch [1011], Loss: 0.057036
Epoch [2/2], Batch [1021], Loss: 0.056554
Epoch [2/2], Batch [1031], Loss: 0.055878
Epoch [2/2], Batch [1041], Loss: 0.054293
Epoch [2/2], Batch [1051], Loss: 0.054451
Epoch [2/2], Batch [1061], Loss: 0.055687
Epoch [2/2], Batch [1071], Loss: 0.055540
Epoch [2/2], Batch [1081], Loss: 0.058011
Epoch [2/2], Batch [1091], Loss: 0.054543
Epoch [2/2], Batch [1101], Loss: 0.056381
Epoch [2/2], Batch [1111], Loss: 0.055442
Epoch [2/2], Batch [1121], Loss: 0.054637
Epoch [2/2], Batch [1131], Loss: 0.055368
Epoch [2/2], Batch [1141], Loss: 0.054868
Epoch [2/2], Batch [1151], Loss: 0.052685
Epoch [2/2], Batch [1161], Loss: 0.053972
Epoch [2/2], Batch [1171], Loss: 0.054898
Epoch [2/2], Batch [1181], Loss: 0.058319
Epoch [2/2], Batch [1191], Loss: 0.055735
Epoch [2/2], Batch [1201], Loss: 0.057374
Epoch [2/2], Batch [1211], Loss: 0.056385
Epoch [2/2], Batch [1221], Loss: 0.054415
Epoch [2/2], Batch [1231], Loss: 0.054929
Epoch [2/2], Batch [1241], Loss: 0.056705
Epoch [2/2], Batch [1251], Loss: 0.054976
Epoch [2/2], Batch [1261], Loss: 0.053966
Epoch [2/2], Batch [1271], Loss: 0.054792
Epoch [2/2], Batch [1281], Loss: 0.056100
Epoch [2/2], Batch [1291], Loss: 0.055128
Epoch [2/2], Batch [1301], Loss: 0.057728
Epoch [2/2], Batch [1311], Loss: 0.054124
Epoch [2/2], Batch [1321], Loss: 0.055873
Epoch [2/2], Batch [1331], Loss: 0.055424
Epoch [2/2], Batch [1341], Loss: 0.055767
Epoch [2/2], Batch [1351], Loss: 0.054057
Epoch [2/2], Batch [1361], Loss: 0.054668
Epoch [2/2], Batch [1371], Loss: 0.054248
Epoch [2/2], Batch [1381], Loss: 0.056270
Epoch [2/2], Batch [1391], Loss: 0.055967
Epoch [2/2], Batch [1401], Loss: 0.054651
Seq_Len: 5, Epoch [2/2] - Average Train Loss: 0.0552
Seq_Len: 5, Epoch [2/2] - Average Test Loss: 0.0535
Elapsed time: 5447.52 seconds
Seq_Len: 5, Epoch [2/2] - Average Validation Loss: 0.0603
Elapsed time: 5478.35 seconds

Training with sequence length 6.
Epoch [1/2], Batch [1], Loss: 0.057799
Epoch [1/2], Batch [11], Loss: 0.055861
Epoch [1/2], Batch [21], Loss: 0.058495
Epoch [1/2], Batch [31], Loss: 0.057675
Epoch [1/2], Batch [41], Loss: 0.054254
Epoch [1/2], Batch [51], Loss: 0.058132
Epoch [1/2], Batch [61], Loss: 0.056336
Epoch [1/2], Batch [71], Loss: 0.058608
Epoch [1/2], Batch [81], Loss: 0.055750
Epoch [1/2], Batch [91], Loss: 0.057066
Epoch [1/2], Batch [101], Loss: 0.059508
Epoch [1/2], Batch [111], Loss: 0.056575
Epoch [1/2], Batch [121], Loss: 0.059777
Epoch [1/2], Batch [131], Loss: 0.057364
Epoch [1/2], Batch [141], Loss: 0.056049
Epoch [1/2], Batch [151], Loss: 0.062230
Epoch [1/2], Batch [161], Loss: 0.056771
Epoch [1/2], Batch [171], Loss: 0.058484
Epoch [1/2], Batch [181], Loss: 0.055101
Epoch [1/2], Batch [191], Loss: 0.053330
Epoch [1/2], Batch [201], Loss: 0.056821
Epoch [1/2], Batch [211], Loss: 0.053182
Epoch [1/2], Batch [221], Loss: 0.056563
Epoch [1/2], Batch [231], Loss: 0.054919
Epoch [1/2], Batch [241], Loss: 0.058147
Epoch [1/2], Batch [251], Loss: 0.056004
Epoch [1/2], Batch [261], Loss: 0.056662
Epoch [1/2], Batch [271], Loss: 0.057246
Epoch [1/2], Batch [281], Loss: 0.057834
Epoch [1/2], Batch [291], Loss: 0.056338
Epoch [1/2], Batch [301], Loss: 0.059790
Epoch [1/2], Batch [311], Loss: 0.056282
Epoch [1/2], Batch [321], Loss: 0.057136
Epoch [1/2], Batch [331], Loss: 0.055254
Epoch [1/2], Batch [341], Loss: 0.058268
Epoch [1/2], Batch [351], Loss: 0.057727
Epoch [1/2], Batch [361], Loss: 0.058054
Epoch [1/2], Batch [371], Loss: 0.055613
Epoch [1/2], Batch [381], Loss: 0.056348
Epoch [1/2], Batch [391], Loss: 0.058135
Epoch [1/2], Batch [401], Loss: 0.057891
Epoch [1/2], Batch [411], Loss: 0.056878
Epoch [1/2], Batch [421], Loss: 0.057766
Epoch [1/2], Batch [431], Loss: 0.059300
Epoch [1/2], Batch [441], Loss: 0.056004
Epoch [1/2], Batch [451], Loss: 0.057481
Epoch [1/2], Batch [461], Loss: 0.057179
Epoch [1/2], Batch [471], Loss: 0.056952
Epoch [1/2], Batch [481], Loss: 0.054869
Epoch [1/2], Batch [491], Loss: 0.054909
Epoch [1/2], Batch [501], Loss: 0.054895
Epoch [1/2], Batch [511], Loss: 0.055728
Epoch [1/2], Batch [521], Loss: 0.058785
Epoch [1/2], Batch [531], Loss: 0.057102
Epoch [1/2], Batch [541], Loss: 0.056392
Epoch [1/2], Batch [551], Loss: 0.056068
Epoch [1/2], Batch [561], Loss: 0.058464
Epoch [1/2], Batch [571], Loss: 0.056450
Epoch [1/2], Batch [581], Loss: 0.057019
Epoch [1/2], Batch [591], Loss: 0.057172
Epoch [1/2], Batch [601], Loss: 0.057729
Epoch [1/2], Batch [611], Loss: 0.057972
Epoch [1/2], Batch [621], Loss: 0.057201
Epoch [1/2], Batch [631], Loss: 0.053709
Epoch [1/2], Batch [641], Loss: 0.054460
Epoch [1/2], Batch [651], Loss: 0.055317
Epoch [1/2], Batch [661], Loss: 0.054958
Epoch [1/2], Batch [671], Loss: 0.054780
Epoch [1/2], Batch [681], Loss: 0.053245
Epoch [1/2], Batch [691], Loss: 0.055582
Epoch [1/2], Batch [701], Loss: 0.058552
Epoch [1/2], Batch [711], Loss: 0.059427
Epoch [1/2], Batch [721], Loss: 0.057439
Epoch [1/2], Batch [731], Loss: 0.053349
Epoch [1/2], Batch [741], Loss: 0.056995
Epoch [1/2], Batch [751], Loss: 0.053954
Epoch [1/2], Batch [761], Loss: 0.055560
Epoch [1/2], Batch [771], Loss: 0.055389
Epoch [1/2], Batch [781], Loss: 0.057190
Epoch [1/2], Batch [791], Loss: 0.052826
Epoch [1/2], Batch [801], Loss: 0.052851
Epoch [1/2], Batch [811], Loss: 0.055173
Epoch [1/2], Batch [821], Loss: 0.060920
Epoch [1/2], Batch [831], Loss: 0.054813
Epoch [1/2], Batch [841], Loss: 0.056629
Epoch [1/2], Batch [851], Loss: 0.057840
Epoch [1/2], Batch [861], Loss: 0.058591
Epoch [1/2], Batch [871], Loss: 0.053509
Epoch [1/2], Batch [881], Loss: 0.057870
Epoch [1/2], Batch [891], Loss: 0.054553
Epoch [1/2], Batch [901], Loss: 0.055209
Epoch [1/2], Batch [911], Loss: 0.056006
Epoch [1/2], Batch [921], Loss: 0.054393
Epoch [1/2], Batch [931], Loss: 0.058181
Epoch [1/2], Batch [941], Loss: 0.057980
Epoch [1/2], Batch [951], Loss: 0.057354
Epoch [1/2], Batch [961], Loss: 0.053163
Epoch [1/2], Batch [971], Loss: 0.055807
Epoch [1/2], Batch [981], Loss: 0.057466
Epoch [1/2], Batch [991], Loss: 0.055531
Epoch [1/2], Batch [1001], Loss: 0.054744
Epoch [1/2], Batch [1011], Loss: 0.056676
Epoch [1/2], Batch [1021], Loss: 0.055595
Epoch [1/2], Batch [1031], Loss: 0.053741
Epoch [1/2], Batch [1041], Loss: 0.054588
Epoch [1/2], Batch [1051], Loss: 0.055607
Epoch [1/2], Batch [1061], Loss: 0.054690
Epoch [1/2], Batch [1071], Loss: 0.056508
Epoch [1/2], Batch [1081], Loss: 0.055013
Epoch [1/2], Batch [1091], Loss: 0.052384
Epoch [1/2], Batch [1101], Loss: 0.056649
Epoch [1/2], Batch [1111], Loss: 0.054703
Epoch [1/2], Batch [1121], Loss: 0.055126
Seq_Len: 6, Epoch [1/2] - Average Train Loss: 0.0564
Seq_Len: 6, Epoch [1/2] - Average Test Loss: 0.0527
Elapsed time: 6197.56 seconds
Seq_Len: 6, Epoch [1/2] - Average Validation Loss: 0.0571
Elapsed time: 6226.69 seconds
Epoch [2/2], Batch [1], Loss: 0.051565
Epoch [2/2], Batch [11], Loss: 0.050357
Epoch [2/2], Batch [21], Loss: 0.052842
Epoch [2/2], Batch [31], Loss: 0.051588
Epoch [2/2], Batch [41], Loss: 0.055017
Epoch [2/2], Batch [51], Loss: 0.052842
Epoch [2/2], Batch [61], Loss: 0.054236
Epoch [2/2], Batch [71], Loss: 0.054913
Epoch [2/2], Batch [81], Loss: 0.054766
Epoch [2/2], Batch [91], Loss: 0.053406
Epoch [2/2], Batch [101], Loss: 0.048626
Epoch [2/2], Batch [111], Loss: 0.051735
Epoch [2/2], Batch [121], Loss: 0.050800
Epoch [2/2], Batch [131], Loss: 0.056174
Epoch [2/2], Batch [141], Loss: 0.052527
Epoch [2/2], Batch [151], Loss: 0.053994
Epoch [2/2], Batch [161], Loss: 0.052476
Epoch [2/2], Batch [171], Loss: 0.053081
Epoch [2/2], Batch [181], Loss: 0.051713
Epoch [2/2], Batch [191], Loss: 0.054377
Epoch [2/2], Batch [201], Loss: 0.052601
Epoch [2/2], Batch [211], Loss: 0.053775
Epoch [2/2], Batch [221], Loss: 0.055437
Epoch [2/2], Batch [231], Loss: 0.053268
Epoch [2/2], Batch [241], Loss: 0.053677
Epoch [2/2], Batch [251], Loss: 0.052850
Epoch [2/2], Batch [261], Loss: 0.054358
Epoch [2/2], Batch [271], Loss: 0.054953
Epoch [2/2], Batch [281], Loss: 0.054782
Epoch [2/2], Batch [291], Loss: 0.053493
Epoch [2/2], Batch [301], Loss: 0.050002
Epoch [2/2], Batch [311], Loss: 0.052521
Epoch [2/2], Batch [321], Loss: 0.050706
Epoch [2/2], Batch [331], Loss: 0.053005
Epoch [2/2], Batch [341], Loss: 0.053215
Epoch [2/2], Batch [351], Loss: 0.053966
Epoch [2/2], Batch [361], Loss: 0.055411
Epoch [2/2], Batch [371], Loss: 0.054163
Epoch [2/2], Batch [381], Loss: 0.053707
Epoch [2/2], Batch [391], Loss: 0.053711
Epoch [2/2], Batch [401], Loss: 0.055317
Epoch [2/2], Batch [411], Loss: 0.053142
Epoch [2/2], Batch [421], Loss: 0.056017
Epoch [2/2], Batch [431], Loss: 0.054156
Epoch [2/2], Batch [441], Loss: 0.056347
Epoch [2/2], Batch [451], Loss: 0.055993
Epoch [2/2], Batch [461], Loss: 0.054927
Epoch [2/2], Batch [471], Loss: 0.052962
Epoch [2/2], Batch [481], Loss: 0.052679
Epoch [2/2], Batch [491], Loss: 0.052835
Epoch [2/2], Batch [501], Loss: 0.055290
Epoch [2/2], Batch [511], Loss: 0.052737
Epoch [2/2], Batch [521], Loss: 0.054573
Epoch [2/2], Batch [531], Loss: 0.052990
Epoch [2/2], Batch [541], Loss: 0.054596
Epoch [2/2], Batch [551], Loss: 0.056545
Epoch [2/2], Batch [561], Loss: 0.054373
Epoch [2/2], Batch [571], Loss: 0.050781
Epoch [2/2], Batch [581], Loss: 0.051910
Epoch [2/2], Batch [591], Loss: 0.052623
Epoch [2/2], Batch [601], Loss: 0.052989
Epoch [2/2], Batch [611], Loss: 0.050834
Epoch [2/2], Batch [621], Loss: 0.054328
Epoch [2/2], Batch [631], Loss: 0.051747
Epoch [2/2], Batch [641], Loss: 0.052608
Epoch [2/2], Batch [651], Loss: 0.052880
Epoch [2/2], Batch [661], Loss: 0.054875
Epoch [2/2], Batch [671], Loss: 0.053055
Epoch [2/2], Batch [681], Loss: 0.054003
Epoch [2/2], Batch [691], Loss: 0.055119
Epoch [2/2], Batch [701], Loss: 0.053337
Epoch [2/2], Batch [711], Loss: 0.054441
Epoch [2/2], Batch [721], Loss: 0.056246
Epoch [2/2], Batch [731], Loss: 0.053930
Epoch [2/2], Batch [741], Loss: 0.054094
Epoch [2/2], Batch [751], Loss: 0.055404
Epoch [2/2], Batch [761], Loss: 0.052958
Epoch [2/2], Batch [771], Loss: 0.054368
Epoch [2/2], Batch [781], Loss: 0.055787
Epoch [2/2], Batch [791], Loss: 0.053645
Epoch [2/2], Batch [801], Loss: 0.052140
Epoch [2/2], Batch [811], Loss: 0.056762
Epoch [2/2], Batch [821], Loss: 0.053593
Epoch [2/2], Batch [831], Loss: 0.053240
Epoch [2/2], Batch [841], Loss: 0.057120
Epoch [2/2], Batch [851], Loss: 0.055741
Epoch [2/2], Batch [861], Loss: 0.052291
Epoch [2/2], Batch [871], Loss: 0.054730
Epoch [2/2], Batch [881], Loss: 0.055626
Epoch [2/2], Batch [891], Loss: 0.056409
Epoch [2/2], Batch [901], Loss: 0.054448
Epoch [2/2], Batch [911], Loss: 0.052728
Epoch [2/2], Batch [921], Loss: 0.053820
Epoch [2/2], Batch [931], Loss: 0.054363
Epoch [2/2], Batch [941], Loss: 0.053926
Epoch [2/2], Batch [951], Loss: 0.052397
Epoch [2/2], Batch [961], Loss: 0.054388
Epoch [2/2], Batch [971], Loss: 0.053775
Epoch [2/2], Batch [981], Loss: 0.054718
Epoch [2/2], Batch [991], Loss: 0.055316
Epoch [2/2], Batch [1001], Loss: 0.054848
Epoch [2/2], Batch [1011], Loss: 0.054328
Epoch [2/2], Batch [1021], Loss: 0.055415
Epoch [2/2], Batch [1031], Loss: 0.054575
Epoch [2/2], Batch [1041], Loss: 0.056233
Epoch [2/2], Batch [1051], Loss: 0.055529
Epoch [2/2], Batch [1061], Loss: 0.054698
Epoch [2/2], Batch [1071], Loss: 0.054119
Epoch [2/2], Batch [1081], Loss: 0.055876
Epoch [2/2], Batch [1091], Loss: 0.056123
Epoch [2/2], Batch [1101], Loss: 0.055278
Epoch [2/2], Batch [1111], Loss: 0.050329
Epoch [2/2], Batch [1121], Loss: 0.054130
Seq_Len: 6, Epoch [2/2] - Average Train Loss: 0.0539
Seq_Len: 6, Epoch [2/2] - Average Test Loss: 0.0514
Elapsed time: 6933.27 seconds
Seq_Len: 6, Epoch [2/2] - Average Validation Loss: 0.0579
Elapsed time: 6962.40 seconds

Training complete!
Totoal elapsed time: 6962.40 seconds
Sequence Length 2: Median Loss = 0.067493
Sequence Length 3: Median Loss = 0.061444
Sequence Length 4: Median Loss = 0.058752
Sequence Length 5: Median Loss = 0.056383
Sequence Length 6: Median Loss = 0.054956
CUDA is available!
Traceback (most recent call last):
  File "/u/dssc/mzampar/Deep-Learning-Project/display/generate_gif.py", line 224, in <module>
    generate_gif(input_frames=input_frames, output_frames=output_frames, model=model, filename="train_pred.gif", predict=True)
  File "/u/dssc/mzampar/Deep-Learning-Project/display/generate_gif.py", line 66, in generate_gif
    predicted_frames = model(input_frames, mask_true, schedule_sampling=False)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/u/dssc/mzampar/Deep-Learning-Project/train/ConvLSTM_model.py", line 127, in forward
    hidden, context, output = self.cell_list[0](net, h_t_prev[0], c_t_prev[0])
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/u/dssc/mzampar/Deep-Learning-Project/train/ConvLSTM_module.py", line 95, in forward
    x_concat = self.conv_x(x_t_new)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
  File "/u/dssc/mzampar/.local/lib/python3.9/site-packages/torch/nn/functional.py", line 2900, in layer_norm
    return torch.layer_norm(
RuntimeError: Given normalized_shape=[256, 32, 32], expected input with shape [*, 256, 32, 32], but got input of size[1, 256, 64, 64]
rm: cannot remove '*.gif': No such file or directory
