Starting job 1013557
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [5, 5, 5, 5],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = True,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.001,
    gamma = 0.95.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 0.726072
Epoch [1/1], Batch [11], Loss: 0.321027
Epoch [1/1], Batch [21], Loss: 0.293211
Epoch [1/1], Batch [31], Loss: 0.270801
Epoch [1/1], Batch [41], Loss: 0.253561
Epoch [1/1], Batch [51], Loss: 0.227622
Epoch [1/1], Batch [61], Loss: 0.213535
Epoch [1/1], Batch [71], Loss: 0.197343
Epoch [1/1], Batch [81], Loss: 0.187657
Epoch [1/1], Batch [91], Loss: 0.175745
Epoch [1/1], Batch [101], Loss: 0.173204
Epoch [1/1], Batch [111], Loss: 0.163533
Epoch [1/1], Batch [121], Loss: 0.156400
Epoch [1/1], Batch [131], Loss: 0.148396
Epoch [1/1], Batch [141], Loss: 0.142438
Epoch [1/1], Batch [151], Loss: 0.141663
Epoch [1/1], Batch [161], Loss: 0.137910
Epoch [1/1], Batch [171], Loss: 0.133075
Epoch [1/1], Batch [181], Loss: 0.127716
Epoch [1/1], Batch [191], Loss: 0.125326
Epoch [1/1], Batch [201], Loss: 0.119481
Epoch [1/1], Batch [211], Loss: 0.119736
Epoch [1/1], Batch [221], Loss: 0.117810
Epoch [1/1], Batch [231], Loss: 0.113010
Epoch [1/1], Batch [241], Loss: 0.111229
Epoch [1/1], Batch [251], Loss: 0.112928
Epoch [1/1], Batch [261], Loss: 0.106496
Epoch [1/1], Batch [271], Loss: 0.108382
Epoch [1/1], Batch [281], Loss: 0.106039
Epoch [1/1], Batch [291], Loss: 0.104216
Epoch [1/1], Batch [301], Loss: 0.102583
Epoch [1/1], Batch [311], Loss: 0.101787
Epoch [1/1], Batch [321], Loss: 0.099901
Epoch [1/1], Batch [331], Loss: 0.100639
Epoch [1/1], Batch [341], Loss: 0.102547
Epoch [1/1], Batch [351], Loss: 0.096920
Epoch [1/1], Batch [361], Loss: 0.098891
Epoch [1/1], Batch [371], Loss: 0.092915
Epoch [1/1], Batch [381], Loss: 0.093523
Epoch [1/1], Batch [391], Loss: 0.094415
Epoch [1/1], Batch [401], Loss: 0.091791
Epoch [1/1], Batch [411], Loss: 0.096992
Epoch [1/1], Batch [421], Loss: 0.096453
Epoch [1/1], Batch [431], Loss: 0.093875
Epoch [1/1], Batch [441], Loss: 0.095831
Epoch [1/1], Batch [451], Loss: 0.092995
Epoch [1/1], Batch [461], Loss: 0.086864
Epoch [1/1], Batch [471], Loss: 0.091395
Epoch [1/1], Batch [481], Loss: 0.091945
Epoch [1/1], Batch [491], Loss: 0.083845
Epoch [1/1], Batch [501], Loss: 0.089629
Epoch [1/1], Batch [511], Loss: 0.092432
Epoch [1/1], Batch [521], Loss: 0.089200
Epoch [1/1], Batch [531], Loss: 0.087024
Epoch [1/1], Batch [541], Loss: 0.085741
Epoch [1/1], Batch [551], Loss: 0.082480
Epoch [1/1], Batch [561], Loss: 0.083297
Epoch [1/1], Batch [571], Loss: 0.085822
Epoch [1/1], Batch [581], Loss: 0.085311
Epoch [1/1], Batch [591], Loss: 0.082351
Epoch [1/1], Batch [601], Loss: 0.080733
Epoch [1/1], Batch [611], Loss: 0.083510
Epoch [1/1], Batch [621], Loss: 0.081433
Epoch [1/1], Batch [631], Loss: 0.085360
Epoch [1/1], Batch [641], Loss: 0.081761
Epoch [1/1], Batch [651], Loss: 0.081915
Epoch [1/1], Batch [661], Loss: 0.084942
Epoch [1/1], Batch [671], Loss: 0.081650
Epoch [1/1], Batch [681], Loss: 0.081966
Epoch [1/1], Batch [691], Loss: 0.079204
Epoch [1/1], Batch [701], Loss: 0.083905
Epoch [1/1], Batch [711], Loss: 0.079498
Epoch [1/1], Batch [721], Loss: 0.082342
Epoch [1/1], Batch [731], Loss: 0.081038
Epoch [1/1], Batch [741], Loss: 0.080888
Epoch [1/1], Batch [751], Loss: 0.078406
Epoch [1/1], Batch [761], Loss: 0.081977
Epoch [1/1], Batch [771], Loss: 0.078425
Epoch [1/1], Batch [781], Loss: 0.077078
Epoch [1/1], Batch [791], Loss: 0.079897
Epoch [1/1], Batch [801], Loss: 0.081097
Epoch [1/1], Batch [811], Loss: 0.078881
Epoch [1/1], Batch [821], Loss: 0.078274
Epoch [1/1], Batch [831], Loss: 0.077931
Epoch [1/1], Batch [841], Loss: 0.080916
Epoch [1/1], Batch [851], Loss: 0.081512
Epoch [1/1], Batch [861], Loss: 0.074365
Epoch [1/1], Batch [871], Loss: 0.081597
Epoch [1/1], Batch [881], Loss: 0.077756
Epoch [1/1], Batch [891], Loss: 0.079753
Epoch [1/1], Batch [901], Loss: 0.080306
Epoch [1/1], Batch [911], Loss: 0.073639
Epoch [1/1], Batch [921], Loss: 0.080313
Epoch [1/1], Batch [931], Loss: 0.079903
Epoch [1/1], Batch [941], Loss: 0.074135
Epoch [1/1], Batch [951], Loss: 0.078581
Epoch [1/1], Batch [961], Loss: 0.076406
Epoch [1/1], Batch [971], Loss: 0.078053
Epoch [1/1], Batch [981], Loss: 0.077155
Epoch [1/1], Batch [991], Loss: 0.075663
Epoch [1/1], Batch [1001], Loss: 0.077795
Epoch [1/1], Batch [1011], Loss: 0.079257
Epoch [1/1], Batch [1021], Loss: 0.078292
Epoch [1/1], Batch [1031], Loss: 0.079318
Epoch [1/1], Batch [1041], Loss: 0.074512
Epoch [1/1], Batch [1051], Loss: 0.077715
Epoch [1/1], Batch [1061], Loss: 0.073998
Epoch [1/1], Batch [1071], Loss: 0.071406
Epoch [1/1], Batch [1081], Loss: 0.075575
Epoch [1/1], Batch [1091], Loss: 0.075220
Epoch [1/1], Batch [1101], Loss: 0.077571
Epoch [1/1], Batch [1111], Loss: 0.072656
Epoch [1/1], Batch [1121], Loss: 0.076960
Epoch [1/1], Batch [1131], Loss: 0.075026
Epoch [1/1], Batch [1141], Loss: 0.072243
Epoch [1/1], Batch [1151], Loss: 0.073635
Epoch [1/1], Batch [1161], Loss: 0.073188
Epoch [1/1], Batch [1171], Loss: 0.075528
Epoch [1/1], Batch [1181], Loss: 0.072983
Epoch [1/1], Batch [1191], Loss: 0.074379
Epoch [1/1], Batch [1201], Loss: 0.074179
Epoch [1/1], Batch [1211], Loss: 0.077430
Epoch [1/1], Batch [1221], Loss: 0.076676
Epoch [1/1], Batch [1231], Loss: 0.073773
Epoch [1/1], Batch [1241], Loss: 0.075617
Epoch [1/1], Batch [1251], Loss: 0.076000
Epoch [1/1], Batch [1261], Loss: 0.072864
Epoch [1/1], Batch [1271], Loss: 0.074106
Epoch [1/1], Batch [1281], Loss: 0.074762
Epoch [1/1], Batch [1291], Loss: 0.074345
Epoch [1/1], Batch [1301], Loss: 0.074005
Epoch [1/1], Batch [1311], Loss: 0.072318
Epoch [1/1], Batch [1321], Loss: 0.071654
Epoch [1/1], Batch [1331], Loss: 0.071586
Epoch [1/1], Batch [1341], Loss: 0.072842
Epoch [1/1], Batch [1351], Loss: 0.072605
Epoch [1/1], Batch [1361], Loss: 0.071015
Epoch [1/1], Batch [1371], Loss: 0.075452
Epoch [1/1], Batch [1381], Loss: 0.073545
Epoch [1/1], Batch [1391], Loss: 0.070929
Epoch [1/1], Batch [1401], Loss: 0.076966
Epoch [1/1], Batch [1411], Loss: 0.075380
Epoch [1/1], Batch [1421], Loss: 0.071629
Epoch [1/1], Batch [1431], Loss: 0.074024
Epoch [1/1], Batch [1441], Loss: 0.073065
Epoch [1/1], Batch [1451], Loss: 0.072931
Epoch [1/1], Batch [1461], Loss: 0.068042
Epoch [1/1], Batch [1471], Loss: 0.072271
Epoch [1/1], Batch [1481], Loss: 0.073922
Epoch [1/1], Batch [1491], Loss: 0.069293
Epoch [1/1], Batch [1501], Loss: 0.071414
Epoch [1/1], Batch [1511], Loss: 0.071601
Epoch [1/1], Batch [1521], Loss: 0.070693
Epoch [1/1], Batch [1531], Loss: 0.068185
Epoch [1/1], Batch [1541], Loss: 0.071550
Epoch [1/1], Batch [1551], Loss: 0.070342
Epoch [1/1], Batch [1561], Loss: 0.070959
Epoch [1/1], Batch [1571], Loss: 0.070190
Epoch [1/1], Batch [1581], Loss: 0.073121
Epoch [1/1], Batch [1591], Loss: 0.070163
Epoch [1/1], Batch [1601], Loss: 0.071783
Epoch [1/1], Batch [1611], Loss: 0.073069
Epoch [1/1], Batch [1621], Loss: 0.071472
Epoch [1/1], Batch [1631], Loss: 0.070771
Epoch [1/1], Batch [1641], Loss: 0.068599
Epoch [1/1], Batch [1651], Loss: 0.070309
Epoch [1/1], Batch [1661], Loss: 0.068903
Epoch [1/1], Batch [1671], Loss: 0.066278
Epoch [1/1], Batch [1681], Loss: 0.070759
Epoch [1/1], Batch [1691], Loss: 0.071069
Epoch [1/1], Batch [1701], Loss: 0.067244
Epoch [1/1], Batch [1711], Loss: 0.069926
Epoch [1/1], Batch [1721], Loss: 0.069351
Epoch [1/1], Batch [1731], Loss: 0.070329
Epoch [1/1], Batch [1741], Loss: 0.072614
Epoch [1/1], Batch [1751], Loss: 0.068453
Epoch [1/1], Batch [1761], Loss: 0.071363
Epoch [1/1], Batch [1771], Loss: 0.073278
Epoch [1/1], Batch [1781], Loss: 0.069072
Epoch [1/1], Batch [1791], Loss: 0.070049
Epoch [1/1], Batch [1801], Loss: 0.070568
Epoch [1/1], Batch [1811], Loss: 0.071867
Epoch [1/1], Batch [1821], Loss: 0.067025
Epoch [1/1], Batch [1831], Loss: 0.067206
Epoch [1/1], Batch [1841], Loss: 0.067961
Epoch [1/1], Batch [1851], Loss: 0.068220
Epoch [1/1], Batch [1861], Loss: 0.071975
Epoch [1/1], Batch [1871], Loss: 0.068935
Epoch [1/1], Batch [1881], Loss: 0.068506
Epoch [1/1], Batch [1891], Loss: 0.073719
Epoch [1/1], Batch [1901], Loss: 0.069045
Epoch [1/1], Batch [1911], Loss: 0.069475
Epoch [1/1], Batch [1921], Loss: 0.071465
Epoch [1/1], Batch [1931], Loss: 0.070676
Epoch [1/1], Batch [1941], Loss: 0.067793
Epoch [1/1], Batch [1951], Loss: 0.071322
Epoch [1/1], Batch [1961], Loss: 0.065681
Epoch [1/1], Batch [1971], Loss: 0.072298
Epoch [1/1], Batch [1981], Loss: 0.072859
Epoch [1/1], Batch [1991], Loss: 0.067214
Epoch [1/1], Batch [2001], Loss: 0.068723
Epoch [1/1], Batch [2011], Loss: 0.067030
Epoch [1/1], Batch [2021], Loss: 0.070732
Epoch [1/1], Batch [2031], Loss: 0.069818
Epoch [1/1], Batch [2041], Loss: 0.067896
Epoch [1/1], Batch [2051], Loss: 0.066109
Epoch [1/1], Batch [2061], Loss: 0.067542
Epoch [1/1], Batch [2071], Loss: 0.066393
Epoch [1/1], Batch [2081], Loss: 0.066856
Epoch [1/1], Batch [2091], Loss: 0.067380
Epoch [1/1], Batch [2101], Loss: 0.070069
Epoch [1/1], Batch [2111], Loss: 0.070934
Epoch [1/1], Batch [2121], Loss: 0.067986
Epoch [1/1], Batch [2131], Loss: 0.069384
Epoch [1/1], Batch [2141], Loss: 0.067758
Epoch [1/1], Batch [2151], Loss: 0.067467
Epoch [1/1], Batch [2161], Loss: 0.068186
Epoch [1/1], Batch [2171], Loss: 0.069904
Epoch [1/1], Batch [2181], Loss: 0.070756
Epoch [1/1], Batch [2191], Loss: 0.069555
Epoch [1/1], Batch [2201], Loss: 0.071464
Epoch [1/1], Batch [2211], Loss: 0.069127
Epoch [1/1], Batch [2221], Loss: 0.069014
Epoch [1/1], Batch [2231], Loss: 0.066542
Epoch [1/1], Batch [2241], Loss: 0.065117
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 0.0884
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 0.0677
Elapsed time: 556.37 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 0.0680
Elapsed time: 581.75 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 0.115669
Epoch [1/1], Batch [11], Loss: 0.103325
Epoch [1/1], Batch [21], Loss: 0.096771
Epoch [1/1], Batch [31], Loss: 0.088211
Epoch [1/1], Batch [41], Loss: 0.083886
Epoch [1/1], Batch [51], Loss: 0.082604
Epoch [1/1], Batch [61], Loss: 0.080624
Epoch [1/1], Batch [71], Loss: 0.073695
Epoch [1/1], Batch [81], Loss: 0.077363
Epoch [1/1], Batch [91], Loss: 0.080223
Epoch [1/1], Batch [101], Loss: 0.079479
Epoch [1/1], Batch [111], Loss: 0.075979
Epoch [1/1], Batch [121], Loss: 0.076620
Epoch [1/1], Batch [131], Loss: 0.074816
Epoch [1/1], Batch [141], Loss: 0.076557
Epoch [1/1], Batch [151], Loss: 0.076001
Epoch [1/1], Batch [161], Loss: 0.076193
Epoch [1/1], Batch [171], Loss: 0.072440
Epoch [1/1], Batch [181], Loss: 0.076690
Epoch [1/1], Batch [191], Loss: 0.070817
Epoch [1/1], Batch [201], Loss: 0.075351
Epoch [1/1], Batch [211], Loss: 0.074913
Epoch [1/1], Batch [221], Loss: 0.072682
Epoch [1/1], Batch [231], Loss: 0.072791
Epoch [1/1], Batch [241], Loss: 0.076251
Epoch [1/1], Batch [251], Loss: 0.070073
Epoch [1/1], Batch [261], Loss: 0.072436
Epoch [1/1], Batch [271], Loss: 0.069816
Epoch [1/1], Batch [281], Loss: 0.074331
Epoch [1/1], Batch [291], Loss: 0.071964
Epoch [1/1], Batch [301], Loss: 0.073777
Epoch [1/1], Batch [311], Loss: 0.071913
Epoch [1/1], Batch [321], Loss: 0.071427
Epoch [1/1], Batch [331], Loss: 0.075201
Epoch [1/1], Batch [341], Loss: 0.069774
Epoch [1/1], Batch [351], Loss: 0.070597
Epoch [1/1], Batch [361], Loss: 0.069814
Epoch [1/1], Batch [371], Loss: 0.070471
Epoch [1/1], Batch [381], Loss: 0.072174
Epoch [1/1], Batch [391], Loss: 0.072376
Epoch [1/1], Batch [401], Loss: 0.068383
Epoch [1/1], Batch [411], Loss: 0.071148
Epoch [1/1], Batch [421], Loss: 0.071910
Epoch [1/1], Batch [431], Loss: 0.072828
Epoch [1/1], Batch [441], Loss: 0.071073
Epoch [1/1], Batch [451], Loss: 0.070158
Epoch [1/1], Batch [461], Loss: 0.069401
Epoch [1/1], Batch [471], Loss: 0.072539
Epoch [1/1], Batch [481], Loss: 0.075227
Epoch [1/1], Batch [491], Loss: 0.071057
Epoch [1/1], Batch [501], Loss: 0.069562
Epoch [1/1], Batch [511], Loss: 0.069562
Epoch [1/1], Batch [521], Loss: 0.072350
Epoch [1/1], Batch [531], Loss: 0.072587
Epoch [1/1], Batch [541], Loss: 0.070614
Epoch [1/1], Batch [551], Loss: 0.073168
Epoch [1/1], Batch [561], Loss: 0.070611
Epoch [1/1], Batch [571], Loss: 0.069104
Epoch [1/1], Batch [581], Loss: 0.070601
Epoch [1/1], Batch [591], Loss: 0.067052
Epoch [1/1], Batch [601], Loss: 0.070715
Epoch [1/1], Batch [611], Loss: 0.071318
Epoch [1/1], Batch [621], Loss: 0.070190
Epoch [1/1], Batch [631], Loss: 0.070472
Epoch [1/1], Batch [641], Loss: 0.069946
Epoch [1/1], Batch [651], Loss: 0.069134
Epoch [1/1], Batch [661], Loss: 0.069664
Epoch [1/1], Batch [671], Loss: 0.069509
Epoch [1/1], Batch [681], Loss: 0.069810
Epoch [1/1], Batch [691], Loss: 0.068914
Epoch [1/1], Batch [701], Loss: 0.069778
Epoch [1/1], Batch [711], Loss: 0.068367
Epoch [1/1], Batch [721], Loss: 0.070514
Epoch [1/1], Batch [731], Loss: 0.070599
Epoch [1/1], Batch [741], Loss: 0.067343
Epoch [1/1], Batch [751], Loss: 0.070518
Epoch [1/1], Batch [761], Loss: 0.066601
Epoch [1/1], Batch [771], Loss: 0.072590
Epoch [1/1], Batch [781], Loss: 0.066561
Epoch [1/1], Batch [791], Loss: 0.070307
Epoch [1/1], Batch [801], Loss: 0.069581
Epoch [1/1], Batch [811], Loss: 0.066146
Epoch [1/1], Batch [821], Loss: 0.068219
Epoch [1/1], Batch [831], Loss: 0.067763
Epoch [1/1], Batch [841], Loss: 0.071884
Epoch [1/1], Batch [851], Loss: 0.068231
Epoch [1/1], Batch [861], Loss: 0.069392
Epoch [1/1], Batch [871], Loss: 0.064882
Epoch [1/1], Batch [881], Loss: 0.069587
Epoch [1/1], Batch [891], Loss: 0.070815
Epoch [1/1], Batch [901], Loss: 0.067346
Epoch [1/1], Batch [911], Loss: 0.065670
Epoch [1/1], Batch [921], Loss: 0.067029
Epoch [1/1], Batch [931], Loss: 0.070746
Epoch [1/1], Batch [941], Loss: 0.068243
Epoch [1/1], Batch [951], Loss: 0.069606
Epoch [1/1], Batch [961], Loss: 0.068844
Epoch [1/1], Batch [971], Loss: 0.066247
Epoch [1/1], Batch [981], Loss: 0.068002
Epoch [1/1], Batch [991], Loss: 0.066480
Epoch [1/1], Batch [1001], Loss: 0.066115
Epoch [1/1], Batch [1011], Loss: 0.067040
Epoch [1/1], Batch [1021], Loss: 0.066085
Epoch [1/1], Batch [1031], Loss: 0.067360
Epoch [1/1], Batch [1041], Loss: 0.066138
Epoch [1/1], Batch [1051], Loss: 0.065444
Epoch [1/1], Batch [1061], Loss: 0.067823
Epoch [1/1], Batch [1071], Loss: 0.065046
Epoch [1/1], Batch [1081], Loss: 0.067210
Epoch [1/1], Batch [1091], Loss: 0.069368
Epoch [1/1], Batch [1101], Loss: 0.067126
Epoch [1/1], Batch [1111], Loss: 0.065593
Epoch [1/1], Batch [1121], Loss: 0.068482
Epoch [1/1], Batch [1131], Loss: 0.066552
Epoch [1/1], Batch [1141], Loss: 0.066541
Epoch [1/1], Batch [1151], Loss: 0.068754
Epoch [1/1], Batch [1161], Loss: 0.066619
Epoch [1/1], Batch [1171], Loss: 0.068648
Epoch [1/1], Batch [1181], Loss: 0.067661
Epoch [1/1], Batch [1191], Loss: 0.068708
Epoch [1/1], Batch [1201], Loss: 0.067976
Epoch [1/1], Batch [1211], Loss: 0.069312
Epoch [1/1], Batch [1221], Loss: 0.066995
Epoch [1/1], Batch [1231], Loss: 0.068724
Epoch [1/1], Batch [1241], Loss: 0.067226
Epoch [1/1], Batch [1251], Loss: 0.067331
Epoch [1/1], Batch [1261], Loss: 0.064603
Epoch [1/1], Batch [1271], Loss: 0.066151
Epoch [1/1], Batch [1281], Loss: 0.065060
Epoch [1/1], Batch [1291], Loss: 0.066163
Epoch [1/1], Batch [1301], Loss: 0.065950
Epoch [1/1], Batch [1311], Loss: 0.068415
Epoch [1/1], Batch [1321], Loss: 0.064956
Epoch [1/1], Batch [1331], Loss: 0.063989
Epoch [1/1], Batch [1341], Loss: 0.068685
Epoch [1/1], Batch [1351], Loss: 0.064562
Epoch [1/1], Batch [1361], Loss: 0.066437
Epoch [1/1], Batch [1371], Loss: 0.064270
Epoch [1/1], Batch [1381], Loss: 0.067173
Epoch [1/1], Batch [1391], Loss: 0.066872
Epoch [1/1], Batch [1401], Loss: 0.067361
Epoch [1/1], Batch [1411], Loss: 0.068245
Epoch [1/1], Batch [1421], Loss: 0.068976
Epoch [1/1], Batch [1431], Loss: 0.069113
Epoch [1/1], Batch [1441], Loss: 0.068671
Epoch [1/1], Batch [1451], Loss: 0.064432
Epoch [1/1], Batch [1461], Loss: 0.067408
Epoch [1/1], Batch [1471], Loss: 0.065470
Epoch [1/1], Batch [1481], Loss: 0.065301
Epoch [1/1], Batch [1491], Loss: 0.060644
Epoch [1/1], Batch [1501], Loss: 0.065521
Epoch [1/1], Batch [1511], Loss: 0.065920
Epoch [1/1], Batch [1521], Loss: 0.062985
Epoch [1/1], Batch [1531], Loss: 0.066617
Epoch [1/1], Batch [1541], Loss: 0.064559
Epoch [1/1], Batch [1551], Loss: 0.062897
Epoch [1/1], Batch [1561], Loss: 0.063527
Epoch [1/1], Batch [1571], Loss: 0.065998
Epoch [1/1], Batch [1581], Loss: 0.065724
Epoch [1/1], Batch [1591], Loss: 0.067395
Epoch [1/1], Batch [1601], Loss: 0.066566
Epoch [1/1], Batch [1611], Loss: 0.066047
Epoch [1/1], Batch [1621], Loss: 0.066871
Epoch [1/1], Batch [1631], Loss: 0.066453
Epoch [1/1], Batch [1641], Loss: 0.067578
Epoch [1/1], Batch [1651], Loss: 0.065607
Epoch [1/1], Batch [1661], Loss: 0.066573
Epoch [1/1], Batch [1671], Loss: 0.067244
Epoch [1/1], Batch [1681], Loss: 0.067534
Epoch [1/1], Batch [1691], Loss: 0.063870
Epoch [1/1], Batch [1701], Loss: 0.066499
Epoch [1/1], Batch [1711], Loss: 0.065522
Epoch [1/1], Batch [1721], Loss: 0.064802
Epoch [1/1], Batch [1731], Loss: 0.064880
Epoch [1/1], Batch [1741], Loss: 0.070485
Epoch [1/1], Batch [1751], Loss: 0.064165
Epoch [1/1], Batch [1761], Loss: 0.064163
Epoch [1/1], Batch [1771], Loss: 0.063328
Epoch [1/1], Batch [1781], Loss: 0.067921
Epoch [1/1], Batch [1791], Loss: 0.064412
Epoch [1/1], Batch [1801], Loss: 0.065916
Epoch [1/1], Batch [1811], Loss: 0.063401
Epoch [1/1], Batch [1821], Loss: 0.066347
Epoch [1/1], Batch [1831], Loss: 0.064924
Epoch [1/1], Batch [1841], Loss: 0.066344
Epoch [1/1], Batch [1851], Loss: 0.062239
Epoch [1/1], Batch [1861], Loss: 0.063089
Epoch [1/1], Batch [1871], Loss: 0.066030
Epoch [1/1], Batch [1881], Loss: 0.064296
Epoch [1/1], Batch [1891], Loss: 0.065391
Epoch [1/1], Batch [1901], Loss: 0.063627
Epoch [1/1], Batch [1911], Loss: 0.060125
Epoch [1/1], Batch [1921], Loss: 0.062981
Epoch [1/1], Batch [1931], Loss: 0.063487
Epoch [1/1], Batch [1941], Loss: 0.064070
Epoch [1/1], Batch [1951], Loss: 0.064088
Epoch [1/1], Batch [1961], Loss: 0.066395
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 0.0690
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 0.0639
Elapsed time: 1297.06 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 0.0648
Elapsed time: 1328.07 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 0.076085
Epoch [1/1], Batch [11], Loss: 0.071223
Epoch [1/1], Batch [21], Loss: 0.067460
Epoch [1/1], Batch [31], Loss: 0.068978
Epoch [1/1], Batch [41], Loss: 0.068581
Epoch [1/1], Batch [51], Loss: 0.069669
Epoch [1/1], Batch [61], Loss: 0.069806
Epoch [1/1], Batch [71], Loss: 0.066881
Epoch [1/1], Batch [81], Loss: 0.066589
Epoch [1/1], Batch [91], Loss: 0.068431
Epoch [1/1], Batch [101], Loss: 0.069886
Epoch [1/1], Batch [111], Loss: 0.067493
Epoch [1/1], Batch [121], Loss: 0.068089
Epoch [1/1], Batch [131], Loss: 0.068205
Epoch [1/1], Batch [141], Loss: 0.069301
Epoch [1/1], Batch [151], Loss: 0.063740
Epoch [1/1], Batch [161], Loss: 0.066569
Epoch [1/1], Batch [171], Loss: 0.069590
Epoch [1/1], Batch [181], Loss: 0.065238
Epoch [1/1], Batch [191], Loss: 0.067725
Epoch [1/1], Batch [201], Loss: 0.069688
Epoch [1/1], Batch [211], Loss: 0.067444
Epoch [1/1], Batch [221], Loss: 0.070879
Epoch [1/1], Batch [231], Loss: 0.064297
Epoch [1/1], Batch [241], Loss: 0.067231
Epoch [1/1], Batch [251], Loss: 0.068914
Epoch [1/1], Batch [261], Loss: 0.066845
Epoch [1/1], Batch [271], Loss: 0.068719
Epoch [1/1], Batch [281], Loss: 0.064211
Epoch [1/1], Batch [291], Loss: 0.066797
Epoch [1/1], Batch [301], Loss: 0.067171
Epoch [1/1], Batch [311], Loss: 0.068110
Epoch [1/1], Batch [321], Loss: 0.064735
Epoch [1/1], Batch [331], Loss: 0.065202
Epoch [1/1], Batch [341], Loss: 0.066072
Epoch [1/1], Batch [351], Loss: 0.068318
Epoch [1/1], Batch [361], Loss: 0.062740
Epoch [1/1], Batch [371], Loss: 0.066849
Epoch [1/1], Batch [381], Loss: 0.069413
Epoch [1/1], Batch [391], Loss: 0.064866
Epoch [1/1], Batch [401], Loss: 0.066545
Epoch [1/1], Batch [411], Loss: 0.068509
Epoch [1/1], Batch [421], Loss: 0.064266
Epoch [1/1], Batch [431], Loss: 0.062302
Epoch [1/1], Batch [441], Loss: 0.065106
Epoch [1/1], Batch [451], Loss: 0.061204
Epoch [1/1], Batch [461], Loss: 0.069554
Epoch [1/1], Batch [471], Loss: 0.065240
Epoch [1/1], Batch [481], Loss: 0.066548
Epoch [1/1], Batch [491], Loss: 0.066308
Epoch [1/1], Batch [501], Loss: 0.068851
Epoch [1/1], Batch [511], Loss: 0.065024
Epoch [1/1], Batch [521], Loss: 0.066526
Epoch [1/1], Batch [531], Loss: 0.068508
Epoch [1/1], Batch [541], Loss: 0.066935
Epoch [1/1], Batch [551], Loss: 0.066542
Epoch [1/1], Batch [561], Loss: 0.065765
Epoch [1/1], Batch [571], Loss: 0.065644
Epoch [1/1], Batch [581], Loss: 0.064775
Epoch [1/1], Batch [591], Loss: 0.061365
Epoch [1/1], Batch [601], Loss: 0.068902
Epoch [1/1], Batch [611], Loss: 0.063039
Epoch [1/1], Batch [621], Loss: 0.064987
Epoch [1/1], Batch [631], Loss: 0.067936
Epoch [1/1], Batch [641], Loss: 0.064833
Epoch [1/1], Batch [651], Loss: 0.069771
Epoch [1/1], Batch [661], Loss: 0.064964
Epoch [1/1], Batch [671], Loss: 0.065774
Epoch [1/1], Batch [681], Loss: 0.066058
Epoch [1/1], Batch [691], Loss: 0.062426
Epoch [1/1], Batch [701], Loss: 0.065568
Epoch [1/1], Batch [711], Loss: 0.063905
Epoch [1/1], Batch [721], Loss: 0.066145
Epoch [1/1], Batch [731], Loss: 0.065270
Epoch [1/1], Batch [741], Loss: 0.063561
Epoch [1/1], Batch [751], Loss: 0.066525
Epoch [1/1], Batch [761], Loss: 0.064546
Epoch [1/1], Batch [771], Loss: 0.063613
Epoch [1/1], Batch [781], Loss: 0.064766
Epoch [1/1], Batch [791], Loss: 0.067862
Epoch [1/1], Batch [801], Loss: 0.065288
Epoch [1/1], Batch [811], Loss: 0.065576
Epoch [1/1], Batch [821], Loss: 0.066885
Epoch [1/1], Batch [831], Loss: 0.066827
Epoch [1/1], Batch [841], Loss: 0.061158
Epoch [1/1], Batch [851], Loss: 0.066548
Epoch [1/1], Batch [861], Loss: 0.065540
Epoch [1/1], Batch [871], Loss: 0.064252
Epoch [1/1], Batch [881], Loss: 0.065257
Epoch [1/1], Batch [891], Loss: 0.065328
Epoch [1/1], Batch [901], Loss: 0.063779
Epoch [1/1], Batch [911], Loss: 0.065812
Epoch [1/1], Batch [921], Loss: 0.065023
Epoch [1/1], Batch [931], Loss: 0.063008
Epoch [1/1], Batch [941], Loss: 0.066399
Epoch [1/1], Batch [951], Loss: 0.063257
Epoch [1/1], Batch [961], Loss: 0.066570
Epoch [1/1], Batch [971], Loss: 0.066262
Epoch [1/1], Batch [981], Loss: 0.064109
Epoch [1/1], Batch [991], Loss: 0.059236
Epoch [1/1], Batch [1001], Loss: 0.065339
Epoch [1/1], Batch [1011], Loss: 0.060749
Epoch [1/1], Batch [1021], Loss: 0.067016
Epoch [1/1], Batch [1031], Loss: 0.067561
Epoch [1/1], Batch [1041], Loss: 0.064384
Epoch [1/1], Batch [1051], Loss: 0.066977
Epoch [1/1], Batch [1061], Loss: 0.066815
Epoch [1/1], Batch [1071], Loss: 0.063230
Epoch [1/1], Batch [1081], Loss: 0.063258
Epoch [1/1], Batch [1091], Loss: 0.064334
Epoch [1/1], Batch [1101], Loss: 0.062985
Epoch [1/1], Batch [1111], Loss: 0.065629
Epoch [1/1], Batch [1121], Loss: 0.062769
Epoch [1/1], Batch [1131], Loss: 0.062446
Epoch [1/1], Batch [1141], Loss: 0.064733
Epoch [1/1], Batch [1151], Loss: 0.065716
Epoch [1/1], Batch [1161], Loss: 0.065975
Epoch [1/1], Batch [1171], Loss: 0.063355
Epoch [1/1], Batch [1181], Loss: 0.066315
Epoch [1/1], Batch [1191], Loss: 0.065161
Epoch [1/1], Batch [1201], Loss: 0.063105
Epoch [1/1], Batch [1211], Loss: 0.065706
Epoch [1/1], Batch [1221], Loss: 0.065074
Epoch [1/1], Batch [1231], Loss: 0.062134
Epoch [1/1], Batch [1241], Loss: 0.065939
Epoch [1/1], Batch [1251], Loss: 0.059963
Epoch [1/1], Batch [1261], Loss: 0.063825
Epoch [1/1], Batch [1271], Loss: 0.065534
Epoch [1/1], Batch [1281], Loss: 0.064320
Epoch [1/1], Batch [1291], Loss: 0.063201
Epoch [1/1], Batch [1301], Loss: 0.065761
Epoch [1/1], Batch [1311], Loss: 0.060530
Epoch [1/1], Batch [1321], Loss: 0.062557
Epoch [1/1], Batch [1331], Loss: 0.063391
Epoch [1/1], Batch [1341], Loss: 0.062093
Epoch [1/1], Batch [1351], Loss: 0.060901
Epoch [1/1], Batch [1361], Loss: 0.062529
Epoch [1/1], Batch [1371], Loss: 0.063412
Epoch [1/1], Batch [1381], Loss: 0.064100
Epoch [1/1], Batch [1391], Loss: 0.063628
Epoch [1/1], Batch [1401], Loss: 0.061993
Epoch [1/1], Batch [1411], Loss: 0.061667
Epoch [1/1], Batch [1421], Loss: 0.066402
Epoch [1/1], Batch [1431], Loss: 0.063129
Epoch [1/1], Batch [1441], Loss: 0.063541
Epoch [1/1], Batch [1451], Loss: 0.064966
Epoch [1/1], Batch [1461], Loss: 0.062907
Epoch [1/1], Batch [1471], Loss: 0.064417
Epoch [1/1], Batch [1481], Loss: 0.064596
Epoch [1/1], Batch [1491], Loss: 0.063373
Epoch [1/1], Batch [1501], Loss: 0.062608
Epoch [1/1], Batch [1511], Loss: 0.064218
Epoch [1/1], Batch [1521], Loss: 0.062222
Epoch [1/1], Batch [1531], Loss: 0.059175
Epoch [1/1], Batch [1541], Loss: 0.067079
Epoch [1/1], Batch [1551], Loss: 0.060402
Epoch [1/1], Batch [1561], Loss: 0.061016
Epoch [1/1], Batch [1571], Loss: 0.062356
Epoch [1/1], Batch [1581], Loss: 0.066440
Epoch [1/1], Batch [1591], Loss: 0.060076
Epoch [1/1], Batch [1601], Loss: 0.061878
Epoch [1/1], Batch [1611], Loss: 0.058988
Epoch [1/1], Batch [1621], Loss: 0.061006
Epoch [1/1], Batch [1631], Loss: 0.065306
Epoch [1/1], Batch [1641], Loss: 0.058439
Epoch [1/1], Batch [1651], Loss: 0.062734
Epoch [1/1], Batch [1661], Loss: 0.066683
Epoch [1/1], Batch [1671], Loss: 0.066590
Epoch [1/1], Batch [1681], Loss: 0.062076
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 0.0654
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 0.0626
Elapsed time: 2133.56 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 0.0641
Elapsed time: 2167.70 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 0.075441
Epoch [1/1], Batch [11], Loss: 0.068012
Epoch [1/1], Batch [21], Loss: 0.070745
Epoch [1/1], Batch [31], Loss: 0.064149
Epoch [1/1], Batch [41], Loss: 0.062798
Epoch [1/1], Batch [51], Loss: 0.069116
Epoch [1/1], Batch [61], Loss: 0.067611
Epoch [1/1], Batch [71], Loss: 0.066907
Epoch [1/1], Batch [81], Loss: 0.062564
Epoch [1/1], Batch [91], Loss: 0.067816
Epoch [1/1], Batch [101], Loss: 0.064239
Epoch [1/1], Batch [111], Loss: 0.064441
Epoch [1/1], Batch [121], Loss: 0.063667
Epoch [1/1], Batch [131], Loss: 0.066108
Epoch [1/1], Batch [141], Loss: 0.066538
Epoch [1/1], Batch [151], Loss: 0.066546
Epoch [1/1], Batch [161], Loss: 0.065802
Epoch [1/1], Batch [171], Loss: 0.064494
Epoch [1/1], Batch [181], Loss: 0.067753
Epoch [1/1], Batch [191], Loss: 0.066118
Epoch [1/1], Batch [201], Loss: 0.063692
Epoch [1/1], Batch [211], Loss: 0.065678
Epoch [1/1], Batch [221], Loss: 0.063164
Epoch [1/1], Batch [231], Loss: 0.066479
Epoch [1/1], Batch [241], Loss: 0.063477
Epoch [1/1], Batch [251], Loss: 0.066425
Epoch [1/1], Batch [261], Loss: 0.065437
Epoch [1/1], Batch [271], Loss: 0.064024
Epoch [1/1], Batch [281], Loss: 0.065004
Epoch [1/1], Batch [291], Loss: 0.068544
Epoch [1/1], Batch [301], Loss: 0.062968
Epoch [1/1], Batch [311], Loss: 0.062247
Epoch [1/1], Batch [321], Loss: 0.062259
Epoch [1/1], Batch [331], Loss: 0.063168
Epoch [1/1], Batch [341], Loss: 0.066004
Epoch [1/1], Batch [351], Loss: 0.066772
Epoch [1/1], Batch [361], Loss: 0.064449
Epoch [1/1], Batch [371], Loss: 0.064340
Epoch [1/1], Batch [381], Loss: 0.063254
Epoch [1/1], Batch [391], Loss: 0.065411
Epoch [1/1], Batch [401], Loss: 0.064147
Epoch [1/1], Batch [411], Loss: 0.065884
Epoch [1/1], Batch [421], Loss: 0.065610
Epoch [1/1], Batch [431], Loss: 0.063644
Epoch [1/1], Batch [441], Loss: 0.064588
Epoch [1/1], Batch [451], Loss: 0.064849
Epoch [1/1], Batch [461], Loss: 0.065026
Epoch [1/1], Batch [471], Loss: 0.064421
Epoch [1/1], Batch [481], Loss: 0.067227
Epoch [1/1], Batch [491], Loss: 0.063311
Epoch [1/1], Batch [501], Loss: 0.063104
Epoch [1/1], Batch [511], Loss: 0.060339
Epoch [1/1], Batch [521], Loss: 0.061585
Epoch [1/1], Batch [531], Loss: 0.066133
Epoch [1/1], Batch [541], Loss: 0.063928
Epoch [1/1], Batch [551], Loss: 0.066217
Epoch [1/1], Batch [561], Loss: 0.064761
Epoch [1/1], Batch [571], Loss: 0.063550
Epoch [1/1], Batch [581], Loss: 0.062517
Epoch [1/1], Batch [591], Loss: 0.063134
Epoch [1/1], Batch [601], Loss: 0.063048
Epoch [1/1], Batch [611], Loss: 0.063451
Epoch [1/1], Batch [621], Loss: 0.063554
Epoch [1/1], Batch [631], Loss: 0.064165
Epoch [1/1], Batch [641], Loss: 0.065637
Epoch [1/1], Batch [651], Loss: 0.066145
Epoch [1/1], Batch [661], Loss: 0.064439
Epoch [1/1], Batch [671], Loss: 0.062439
Epoch [1/1], Batch [681], Loss: 0.065361
Epoch [1/1], Batch [691], Loss: 0.067272
Epoch [1/1], Batch [701], Loss: 0.063109
Epoch [1/1], Batch [711], Loss: 0.066119
Epoch [1/1], Batch [721], Loss: 0.064743
Epoch [1/1], Batch [731], Loss: 0.067135
Epoch [1/1], Batch [741], Loss: 0.064798
Epoch [1/1], Batch [751], Loss: 0.064593
Epoch [1/1], Batch [761], Loss: 0.063900
Epoch [1/1], Batch [771], Loss: 0.061678
Epoch [1/1], Batch [781], Loss: 0.066555
Epoch [1/1], Batch [791], Loss: 0.065998
Epoch [1/1], Batch [801], Loss: 0.062695
Epoch [1/1], Batch [811], Loss: 0.059723
Epoch [1/1], Batch [821], Loss: 0.066490
Epoch [1/1], Batch [831], Loss: 0.066445
Epoch [1/1], Batch [841], Loss: 0.063875
Epoch [1/1], Batch [851], Loss: 0.065604
Epoch [1/1], Batch [861], Loss: 0.068313
Epoch [1/1], Batch [871], Loss: 0.061065
Epoch [1/1], Batch [881], Loss: 0.067407
Epoch [1/1], Batch [891], Loss: 0.064527
Epoch [1/1], Batch [901], Loss: 0.064200
Epoch [1/1], Batch [911], Loss: 0.063688
Epoch [1/1], Batch [921], Loss: 0.062903
Epoch [1/1], Batch [931], Loss: 0.065940
Epoch [1/1], Batch [941], Loss: 0.059928
Epoch [1/1], Batch [951], Loss: 0.065346
Epoch [1/1], Batch [961], Loss: 0.061485
Epoch [1/1], Batch [971], Loss: 0.067067
Epoch [1/1], Batch [981], Loss: 0.064681
Epoch [1/1], Batch [991], Loss: 0.061738
Epoch [1/1], Batch [1001], Loss: 0.064726
Epoch [1/1], Batch [1011], Loss: 0.063382
Epoch [1/1], Batch [1021], Loss: 0.065564
Epoch [1/1], Batch [1031], Loss: 0.067202
Epoch [1/1], Batch [1041], Loss: 0.063270
Epoch [1/1], Batch [1051], Loss: 0.061274
Epoch [1/1], Batch [1061], Loss: 0.061421
Epoch [1/1], Batch [1071], Loss: 0.063144
Epoch [1/1], Batch [1081], Loss: 0.062665
Epoch [1/1], Batch [1091], Loss: 0.062286
Epoch [1/1], Batch [1101], Loss: 0.062452
Epoch [1/1], Batch [1111], Loss: 0.062067
Epoch [1/1], Batch [1121], Loss: 0.061423
Epoch [1/1], Batch [1131], Loss: 0.064807
Epoch [1/1], Batch [1141], Loss: 0.062343
Epoch [1/1], Batch [1151], Loss: 0.062143
Epoch [1/1], Batch [1161], Loss: 0.064484
Epoch [1/1], Batch [1171], Loss: 0.061007
Epoch [1/1], Batch [1181], Loss: 0.060248
Epoch [1/1], Batch [1191], Loss: 0.062803
Epoch [1/1], Batch [1201], Loss: 0.063948
Epoch [1/1], Batch [1211], Loss: 0.062604
Epoch [1/1], Batch [1221], Loss: 0.061553
Epoch [1/1], Batch [1231], Loss: 0.062992
Epoch [1/1], Batch [1241], Loss: 0.060536
Epoch [1/1], Batch [1251], Loss: 0.065420
Epoch [1/1], Batch [1261], Loss: 0.060920
Epoch [1/1], Batch [1271], Loss: 0.061075
Epoch [1/1], Batch [1281], Loss: 0.062997
Epoch [1/1], Batch [1291], Loss: 0.064303
Epoch [1/1], Batch [1301], Loss: 0.066093
Epoch [1/1], Batch [1311], Loss: 0.065331
Epoch [1/1], Batch [1321], Loss: 0.064299
Epoch [1/1], Batch [1331], Loss: 0.062787
Epoch [1/1], Batch [1341], Loss: 0.064421
Epoch [1/1], Batch [1351], Loss: 0.063333
Epoch [1/1], Batch [1361], Loss: 0.063672
Epoch [1/1], Batch [1371], Loss: 0.060310
Epoch [1/1], Batch [1381], Loss: 0.060896
Epoch [1/1], Batch [1391], Loss: 0.061326
Epoch [1/1], Batch [1401], Loss: 0.063396
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 0.0643
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 0.0627
Elapsed time: 3005.47 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 0.0646
Elapsed time: 3040.21 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 0.068313
Epoch [1/1], Batch [11], Loss: 0.063538
Epoch [1/1], Batch [21], Loss: 0.067602
Epoch [1/1], Batch [31], Loss: 0.067338
Epoch [1/1], Batch [41], Loss: 0.067224
Epoch [1/1], Batch [51], Loss: 0.066671
Epoch [1/1], Batch [61], Loss: 0.063377
Epoch [1/1], Batch [71], Loss: 0.067683
Epoch [1/1], Batch [81], Loss: 0.064693
Epoch [1/1], Batch [91], Loss: 0.063067
Epoch [1/1], Batch [101], Loss: 0.066037
Epoch [1/1], Batch [111], Loss: 0.062368
Epoch [1/1], Batch [121], Loss: 0.063684
Epoch [1/1], Batch [131], Loss: 0.065558
Epoch [1/1], Batch [141], Loss: 0.063180
Epoch [1/1], Batch [151], Loss: 0.063490
Epoch [1/1], Batch [161], Loss: 0.064437
Epoch [1/1], Batch [171], Loss: 0.064746
Epoch [1/1], Batch [181], Loss: 0.064706
Epoch [1/1], Batch [191], Loss: 0.061315
Epoch [1/1], Batch [201], Loss: 0.063945
Epoch [1/1], Batch [211], Loss: 0.065680
Epoch [1/1], Batch [221], Loss: 0.066759
Epoch [1/1], Batch [231], Loss: 0.063343
Epoch [1/1], Batch [241], Loss: 0.063097
Epoch [1/1], Batch [251], Loss: 0.064433
Epoch [1/1], Batch [261], Loss: 0.064318
Epoch [1/1], Batch [271], Loss: 0.063751
Epoch [1/1], Batch [281], Loss: 0.062218
Epoch [1/1], Batch [291], Loss: 0.065378
Epoch [1/1], Batch [301], Loss: 0.062691
Epoch [1/1], Batch [311], Loss: 0.063844
Epoch [1/1], Batch [321], Loss: 0.064647
Epoch [1/1], Batch [331], Loss: 0.061244
Epoch [1/1], Batch [341], Loss: 0.065190
Epoch [1/1], Batch [351], Loss: 0.063046
Epoch [1/1], Batch [361], Loss: 0.060861
Epoch [1/1], Batch [371], Loss: 0.062096
Epoch [1/1], Batch [381], Loss: 0.063732
Epoch [1/1], Batch [391], Loss: 0.066309
Epoch [1/1], Batch [401], Loss: 0.065877
Epoch [1/1], Batch [411], Loss: 0.060945
Epoch [1/1], Batch [421], Loss: 0.064765
Epoch [1/1], Batch [431], Loss: 0.064120
Epoch [1/1], Batch [441], Loss: 0.063501
Epoch [1/1], Batch [451], Loss: 0.060846
Epoch [1/1], Batch [461], Loss: 0.060364
Epoch [1/1], Batch [471], Loss: 0.063264
Epoch [1/1], Batch [481], Loss: 0.063200
Epoch [1/1], Batch [491], Loss: 0.064199
Epoch [1/1], Batch [501], Loss: 0.065266
Epoch [1/1], Batch [511], Loss: 0.063079
Epoch [1/1], Batch [521], Loss: 0.063504
Epoch [1/1], Batch [531], Loss: 0.064779
Epoch [1/1], Batch [541], Loss: 0.062342
Epoch [1/1], Batch [551], Loss: 0.063716
Epoch [1/1], Batch [561], Loss: 0.061161
Epoch [1/1], Batch [571], Loss: 0.063727
Epoch [1/1], Batch [581], Loss: 0.062320
Epoch [1/1], Batch [591], Loss: 0.062529
Epoch [1/1], Batch [601], Loss: 0.064165
Epoch [1/1], Batch [611], Loss: 0.062737
Epoch [1/1], Batch [621], Loss: 0.064276
Epoch [1/1], Batch [631], Loss: 0.064778
Epoch [1/1], Batch [641], Loss: 0.063576
Epoch [1/1], Batch [651], Loss: 0.063622
Epoch [1/1], Batch [661], Loss: 0.068135
Epoch [1/1], Batch [671], Loss: 0.064445
Epoch [1/1], Batch [681], Loss: 0.063437
Epoch [1/1], Batch [691], Loss: 0.063514
Epoch [1/1], Batch [701], Loss: 0.067078
Epoch [1/1], Batch [711], Loss: 0.062371
Epoch [1/1], Batch [721], Loss: 0.065267
Epoch [1/1], Batch [731], Loss: 0.064187
Epoch [1/1], Batch [741], Loss: 0.062146
Epoch [1/1], Batch [751], Loss: 0.063153
Epoch [1/1], Batch [761], Loss: 0.064649
Epoch [1/1], Batch [771], Loss: 0.062816
Epoch [1/1], Batch [781], Loss: 0.067076
Epoch [1/1], Batch [791], Loss: 0.062390
Epoch [1/1], Batch [801], Loss: 0.061712
Epoch [1/1], Batch [811], Loss: 0.062907
Epoch [1/1], Batch [821], Loss: 0.062838
Epoch [1/1], Batch [831], Loss: 0.064022
Epoch [1/1], Batch [841], Loss: 0.062858
Epoch [1/1], Batch [851], Loss: 0.063290
Epoch [1/1], Batch [861], Loss: 0.064270
Epoch [1/1], Batch [871], Loss: 0.065388
Epoch [1/1], Batch [881], Loss: 0.063885
Epoch [1/1], Batch [891], Loss: 0.062314
Epoch [1/1], Batch [901], Loss: 0.063740
Epoch [1/1], Batch [911], Loss: 0.065493
Epoch [1/1], Batch [921], Loss: 0.062074
Epoch [1/1], Batch [931], Loss: 0.060060
Epoch [1/1], Batch [941], Loss: 0.063855
Epoch [1/1], Batch [951], Loss: 0.063481
Epoch [1/1], Batch [961], Loss: 0.059551
Epoch [1/1], Batch [971], Loss: 0.062547
Epoch [1/1], Batch [981], Loss: 0.063309
Epoch [1/1], Batch [991], Loss: 0.061674
Epoch [1/1], Batch [1001], Loss: 0.059336
Epoch [1/1], Batch [1011], Loss: 0.062463
Epoch [1/1], Batch [1021], Loss: 0.063162
Epoch [1/1], Batch [1031], Loss: 0.062155
Epoch [1/1], Batch [1041], Loss: 0.062517
Epoch [1/1], Batch [1051], Loss: 0.065396
Epoch [1/1], Batch [1061], Loss: 0.063239
Epoch [1/1], Batch [1071], Loss: 0.058065
Epoch [1/1], Batch [1081], Loss: 0.062379
Epoch [1/1], Batch [1091], Loss: 0.063331
Epoch [1/1], Batch [1101], Loss: 0.062536
Epoch [1/1], Batch [1111], Loss: 0.061777
Epoch [1/1], Batch [1121], Loss: 0.063094
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 0.0636
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 0.0610
Elapsed time: 3838.20 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 0.0641
Elapsed time: 3871.02 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 0.066386
Epoch [1/1], Batch [11], Loss: 0.061734
Epoch [1/1], Batch [21], Loss: 0.062813
Epoch [1/1], Batch [31], Loss: 0.065760
Epoch [1/1], Batch [41], Loss: 0.064561
Epoch [1/1], Batch [51], Loss: 0.068983
Epoch [1/1], Batch [61], Loss: 0.063169
Epoch [1/1], Batch [71], Loss: 0.063976
Epoch [1/1], Batch [81], Loss: 0.064434
Epoch [1/1], Batch [91], Loss: 0.067795
Epoch [1/1], Batch [101], Loss: 0.063992
Epoch [1/1], Batch [111], Loss: 0.064514
Epoch [1/1], Batch [121], Loss: 0.063511
Epoch [1/1], Batch [131], Loss: 0.061700
Epoch [1/1], Batch [141], Loss: 0.061171
Epoch [1/1], Batch [151], Loss: 0.061971
Epoch [1/1], Batch [161], Loss: 0.060832
Epoch [1/1], Batch [171], Loss: 0.061463
Epoch [1/1], Batch [181], Loss: 0.062942
Epoch [1/1], Batch [191], Loss: 0.065603
Epoch [1/1], Batch [201], Loss: 0.066980
Epoch [1/1], Batch [211], Loss: 0.059905
Epoch [1/1], Batch [221], Loss: 0.064242
Epoch [1/1], Batch [231], Loss: 0.064914
Epoch [1/1], Batch [241], Loss: 0.063499
Epoch [1/1], Batch [251], Loss: 0.062559
Epoch [1/1], Batch [261], Loss: 0.064278
Epoch [1/1], Batch [271], Loss: 0.063925
Epoch [1/1], Batch [281], Loss: 0.061649
Epoch [1/1], Batch [291], Loss: 0.062295
Epoch [1/1], Batch [301], Loss: 0.062607
Epoch [1/1], Batch [311], Loss: 0.062232
Epoch [1/1], Batch [321], Loss: 0.061521
Epoch [1/1], Batch [331], Loss: 0.064162
Epoch [1/1], Batch [341], Loss: 0.064647
Epoch [1/1], Batch [351], Loss: 0.063528
Epoch [1/1], Batch [361], Loss: 0.065367
Epoch [1/1], Batch [371], Loss: 0.064305
Epoch [1/1], Batch [381], Loss: 0.062225
Epoch [1/1], Batch [391], Loss: 0.062821
Epoch [1/1], Batch [401], Loss: 0.062284
Epoch [1/1], Batch [411], Loss: 0.065234
Epoch [1/1], Batch [421], Loss: 0.065692
Epoch [1/1], Batch [431], Loss: 0.064272
Epoch [1/1], Batch [441], Loss: 0.063371
Epoch [1/1], Batch [451], Loss: 0.061292
Epoch [1/1], Batch [461], Loss: 0.065226
Epoch [1/1], Batch [471], Loss: 0.060976
Epoch [1/1], Batch [481], Loss: 0.062304
Epoch [1/1], Batch [491], Loss: 0.062325
Epoch [1/1], Batch [501], Loss: 0.061008
Epoch [1/1], Batch [511], Loss: 0.063005
Epoch [1/1], Batch [521], Loss: 0.063744
Epoch [1/1], Batch [531], Loss: 0.062723
Epoch [1/1], Batch [541], Loss: 0.062782
Epoch [1/1], Batch [551], Loss: 0.061794
Epoch [1/1], Batch [561], Loss: 0.064891
Epoch [1/1], Batch [571], Loss: 0.065066
Epoch [1/1], Batch [581], Loss: 0.063150
Epoch [1/1], Batch [591], Loss: 0.065555
Epoch [1/1], Batch [601], Loss: 0.063648
Epoch [1/1], Batch [611], Loss: 0.062848
Epoch [1/1], Batch [621], Loss: 0.062925
Epoch [1/1], Batch [631], Loss: 0.060319
Epoch [1/1], Batch [641], Loss: 0.061872
Epoch [1/1], Batch [651], Loss: 0.063716
Epoch [1/1], Batch [661], Loss: 0.062150
Epoch [1/1], Batch [671], Loss: 0.066659
Epoch [1/1], Batch [681], Loss: 0.064372
Epoch [1/1], Batch [691], Loss: 0.059207
Epoch [1/1], Batch [701], Loss: 0.063619
Epoch [1/1], Batch [711], Loss: 0.063165
Epoch [1/1], Batch [721], Loss: 0.065672
Epoch [1/1], Batch [731], Loss: 0.063545
Epoch [1/1], Batch [741], Loss: 0.066681
Epoch [1/1], Batch [751], Loss: 0.063152
Epoch [1/1], Batch [761], Loss: 0.061766
Epoch [1/1], Batch [771], Loss: 0.064483
Epoch [1/1], Batch [781], Loss: 0.059078
Epoch [1/1], Batch [791], Loss: 0.063330
Epoch [1/1], Batch [801], Loss: 0.061530
Epoch [1/1], Batch [811], Loss: 0.061163
Epoch [1/1], Batch [821], Loss: 0.057481
Epoch [1/1], Batch [831], Loss: 0.060677
Epoch [1/1], Batch [841], Loss: 0.060089
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 0.0632
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 0.0600
Elapsed time: 4569.12 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 0.0627
Elapsed time: 4597.57 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 0.065809
Epoch [1/1], Batch [11], Loss: 0.064831
Epoch [1/1], Batch [21], Loss: 0.063254
Epoch [1/1], Batch [31], Loss: 0.064074
Epoch [1/1], Batch [41], Loss: 0.063983
Epoch [1/1], Batch [51], Loss: 0.061220
Epoch [1/1], Batch [61], Loss: 0.065000
Epoch [1/1], Batch [71], Loss: 0.063681
Epoch [1/1], Batch [81], Loss: 0.064080
Epoch [1/1], Batch [91], Loss: 0.064254
Epoch [1/1], Batch [101], Loss: 0.063807
Epoch [1/1], Batch [111], Loss: 0.065702
Epoch [1/1], Batch [121], Loss: 0.060273
Epoch [1/1], Batch [131], Loss: 0.064357
Epoch [1/1], Batch [141], Loss: 0.067457
Epoch [1/1], Batch [151], Loss: 0.061889
Epoch [1/1], Batch [161], Loss: 0.062138
Epoch [1/1], Batch [171], Loss: 0.065377
Epoch [1/1], Batch [181], Loss: 0.064267
Epoch [1/1], Batch [191], Loss: 0.060450
Epoch [1/1], Batch [201], Loss: 0.061534
Epoch [1/1], Batch [211], Loss: 0.059049
Epoch [1/1], Batch [221], Loss: 0.064328
Epoch [1/1], Batch [231], Loss: 0.062792
Epoch [1/1], Batch [241], Loss: 0.062788
Epoch [1/1], Batch [251], Loss: 0.061985
Epoch [1/1], Batch [261], Loss: 0.060850
Epoch [1/1], Batch [271], Loss: 0.063368
Epoch [1/1], Batch [281], Loss: 0.062904
Epoch [1/1], Batch [291], Loss: 0.063895
Epoch [1/1], Batch [301], Loss: 0.060169
Epoch [1/1], Batch [311], Loss: 0.059399
Epoch [1/1], Batch [321], Loss: 0.062374
Epoch [1/1], Batch [331], Loss: 0.063688
Epoch [1/1], Batch [341], Loss: 0.062256
Epoch [1/1], Batch [351], Loss: 0.060739
Epoch [1/1], Batch [361], Loss: 0.061124
Epoch [1/1], Batch [371], Loss: 0.063034
Epoch [1/1], Batch [381], Loss: 0.066479
Epoch [1/1], Batch [391], Loss: 0.061140
Epoch [1/1], Batch [401], Loss: 0.061790
Epoch [1/1], Batch [411], Loss: 0.062174
Epoch [1/1], Batch [421], Loss: 0.062650
Epoch [1/1], Batch [431], Loss: 0.062908
Epoch [1/1], Batch [441], Loss: 0.061799
Epoch [1/1], Batch [451], Loss: 0.060107
Epoch [1/1], Batch [461], Loss: 0.061489
Epoch [1/1], Batch [471], Loss: 0.063803
Epoch [1/1], Batch [481], Loss: 0.067301
Epoch [1/1], Batch [491], Loss: 0.065730
Epoch [1/1], Batch [501], Loss: 0.058065
Epoch [1/1], Batch [511], Loss: 0.059991
Epoch [1/1], Batch [521], Loss: 0.062864
Epoch [1/1], Batch [531], Loss: 0.065074
Epoch [1/1], Batch [541], Loss: 0.059791
Epoch [1/1], Batch [551], Loss: 0.062914
Epoch [1/1], Batch [561], Loss: 0.063069
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 0.0629
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 0.0615
Elapsed time: 5137.82 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 0.0668
Elapsed time: 5159.35 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 0.066844
Epoch [1/1], Batch [11], Loss: 0.066234
Epoch [1/1], Batch [21], Loss: 0.064244
Epoch [1/1], Batch [31], Loss: 0.064335
Epoch [1/1], Batch [41], Loss: 0.070043
Epoch [1/1], Batch [51], Loss: 0.066398
Epoch [1/1], Batch [61], Loss: 0.060731
Epoch [1/1], Batch [71], Loss: 0.062649
Epoch [1/1], Batch [81], Loss: 0.063011
Epoch [1/1], Batch [91], Loss: 0.060891
Epoch [1/1], Batch [101], Loss: 0.064166
Epoch [1/1], Batch [111], Loss: 0.060805
Epoch [1/1], Batch [121], Loss: 0.063909
Epoch [1/1], Batch [131], Loss: 0.067821
Epoch [1/1], Batch [141], Loss: 0.064026
Epoch [1/1], Batch [151], Loss: 0.062287
Epoch [1/1], Batch [161], Loss: 0.062917
Epoch [1/1], Batch [171], Loss: 0.061454
Epoch [1/1], Batch [181], Loss: 0.064508
Epoch [1/1], Batch [191], Loss: 0.061788
Epoch [1/1], Batch [201], Loss: 0.063201
Epoch [1/1], Batch [211], Loss: 0.061546
Epoch [1/1], Batch [221], Loss: 0.062644
Epoch [1/1], Batch [231], Loss: 0.060697
Epoch [1/1], Batch [241], Loss: 0.063623
Epoch [1/1], Batch [251], Loss: 0.061635
Epoch [1/1], Batch [261], Loss: 0.062608
Epoch [1/1], Batch [271], Loss: 0.062916
Epoch [1/1], Batch [281], Loss: 0.061477
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 0.0633
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 0.0619
Elapsed time: 5493.76 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 0.0650
Elapsed time: 5505.80 seconds

Training complete!
Totoal elapsed time: 5505.80 seconds
CUDA is available!
