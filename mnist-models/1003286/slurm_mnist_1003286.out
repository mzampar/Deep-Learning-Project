Starting job 1003286
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = True,
    scheduler = False,
    bias = False,
    transpose = True,
    use_lstm_output = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 385880.218750
Epoch [1/1], Batch [11], Loss: 142441.359375
Epoch [1/1], Batch [21], Loss: 100649.679688
Epoch [1/1], Batch [31], Loss: 84415.640625
Epoch [1/1], Batch [41], Loss: 75705.781250
Epoch [1/1], Batch [51], Loss: 75515.578125
Epoch [1/1], Batch [61], Loss: 73147.382812
Epoch [1/1], Batch [71], Loss: 71744.257812
Epoch [1/1], Batch [81], Loss: 71551.562500
Epoch [1/1], Batch [91], Loss: 73663.789062
Epoch [1/1], Batch [101], Loss: 71266.562500
Epoch [1/1], Batch [111], Loss: 72183.460938
Epoch [1/1], Batch [121], Loss: 73200.062500
Epoch [1/1], Batch [131], Loss: 73293.718750
Epoch [1/1], Batch [141], Loss: 71158.335938
Epoch [1/1], Batch [151], Loss: 71537.656250
Epoch [1/1], Batch [161], Loss: 71086.140625
Epoch [1/1], Batch [171], Loss: 74038.507812
Epoch [1/1], Batch [181], Loss: 69190.500000
Epoch [1/1], Batch [191], Loss: 71005.171875
Epoch [1/1], Batch [201], Loss: 69773.109375
Epoch [1/1], Batch [211], Loss: 74557.109375
Epoch [1/1], Batch [221], Loss: 68784.546875
Epoch [1/1], Batch [231], Loss: 67687.203125
Epoch [1/1], Batch [241], Loss: 69351.078125
Epoch [1/1], Batch [251], Loss: 67892.562500
Epoch [1/1], Batch [261], Loss: 68953.617188
Epoch [1/1], Batch [271], Loss: 69462.265625
Epoch [1/1], Batch [281], Loss: 69060.109375
Epoch [1/1], Batch [291], Loss: 69002.000000
Epoch [1/1], Batch [301], Loss: 69853.570312
Epoch [1/1], Batch [311], Loss: 69368.859375
Epoch [1/1], Batch [321], Loss: 68225.359375
Epoch [1/1], Batch [331], Loss: 66454.953125
Epoch [1/1], Batch [341], Loss: 66398.679688
Epoch [1/1], Batch [351], Loss: 67194.515625
Epoch [1/1], Batch [361], Loss: 68047.664062
Epoch [1/1], Batch [371], Loss: 67464.703125
Epoch [1/1], Batch [381], Loss: 68651.734375
Epoch [1/1], Batch [391], Loss: 70282.484375
Epoch [1/1], Batch [401], Loss: 67397.046875
Epoch [1/1], Batch [411], Loss: 66853.343750
Epoch [1/1], Batch [421], Loss: 67106.320312
Epoch [1/1], Batch [431], Loss: 68510.632812
Epoch [1/1], Batch [441], Loss: 70830.640625
Epoch [1/1], Batch [451], Loss: 68575.031250
Epoch [1/1], Batch [461], Loss: 67119.320312
Epoch [1/1], Batch [471], Loss: 70623.429688
Epoch [1/1], Batch [481], Loss: 70285.195312
Epoch [1/1], Batch [491], Loss: 67979.906250
Epoch [1/1], Batch [501], Loss: 66104.031250
Epoch [1/1], Batch [511], Loss: 70283.414062
Epoch [1/1], Batch [521], Loss: 72296.695312
Epoch [1/1], Batch [531], Loss: 70113.367188
Epoch [1/1], Batch [541], Loss: 62901.820312
Epoch [1/1], Batch [551], Loss: 67136.218750
Epoch [1/1], Batch [561], Loss: 66091.781250
Epoch [1/1], Batch [571], Loss: 69420.335938
Epoch [1/1], Batch [581], Loss: 69462.703125
Epoch [1/1], Batch [591], Loss: 67101.343750
Epoch [1/1], Batch [601], Loss: 65942.273438
Epoch [1/1], Batch [611], Loss: 69530.062500
Epoch [1/1], Batch [621], Loss: 68910.484375
Epoch [1/1], Batch [631], Loss: 69465.148438
Epoch [1/1], Batch [641], Loss: 69080.929688
Epoch [1/1], Batch [651], Loss: 68364.804688
Epoch [1/1], Batch [661], Loss: 69214.539062
Epoch [1/1], Batch [671], Loss: 68904.132812
Epoch [1/1], Batch [681], Loss: 68262.843750
Epoch [1/1], Batch [691], Loss: 65844.109375
Epoch [1/1], Batch [701], Loss: 68001.531250
Epoch [1/1], Batch [711], Loss: 65340.234375
Epoch [1/1], Batch [721], Loss: 70656.242188
Epoch [1/1], Batch [731], Loss: 68700.343750
Epoch [1/1], Batch [741], Loss: 68218.953125
Epoch [1/1], Batch [751], Loss: 66570.453125
Epoch [1/1], Batch [761], Loss: 66907.281250
Epoch [1/1], Batch [771], Loss: 66413.093750
Epoch [1/1], Batch [781], Loss: 66607.015625
Epoch [1/1], Batch [791], Loss: 69390.757812
Epoch [1/1], Batch [801], Loss: 66838.421875
Epoch [1/1], Batch [811], Loss: 67904.328125
Epoch [1/1], Batch [821], Loss: 66840.007812
Epoch [1/1], Batch [831], Loss: 65493.437500
Epoch [1/1], Batch [841], Loss: 67793.218750
Epoch [1/1], Batch [851], Loss: 67522.765625
Epoch [1/1], Batch [861], Loss: 67866.890625
Epoch [1/1], Batch [871], Loss: 69296.273438
Epoch [1/1], Batch [881], Loss: 65622.804688
Epoch [1/1], Batch [891], Loss: 71139.523438
Epoch [1/1], Batch [901], Loss: 67599.289062
Epoch [1/1], Batch [911], Loss: 69201.171875
Epoch [1/1], Batch [921], Loss: 70180.781250
Epoch [1/1], Batch [931], Loss: 67301.328125
Epoch [1/1], Batch [941], Loss: 66125.351562
Epoch [1/1], Batch [951], Loss: 64730.449219
Epoch [1/1], Batch [961], Loss: 68324.265625
Epoch [1/1], Batch [971], Loss: 71150.398438
Epoch [1/1], Batch [981], Loss: 66735.585938
Epoch [1/1], Batch [991], Loss: 66817.812500
Epoch [1/1], Batch [1001], Loss: 70916.359375
Epoch [1/1], Batch [1011], Loss: 69095.859375
Epoch [1/1], Batch [1021], Loss: 65774.187500
Epoch [1/1], Batch [1031], Loss: 64188.917969
Epoch [1/1], Batch [1041], Loss: 68917.640625
Epoch [1/1], Batch [1051], Loss: 65097.921875
Epoch [1/1], Batch [1061], Loss: 68446.078125
Epoch [1/1], Batch [1071], Loss: 67965.640625
Epoch [1/1], Batch [1081], Loss: 68313.976562
Epoch [1/1], Batch [1091], Loss: 68793.171875
Epoch [1/1], Batch [1101], Loss: 65284.062500
Epoch [1/1], Batch [1111], Loss: 68087.804688
Epoch [1/1], Batch [1121], Loss: 65143.234375
Epoch [1/1], Batch [1131], Loss: 68439.484375
Epoch [1/1], Batch [1141], Loss: 69904.289062
Epoch [1/1], Batch [1151], Loss: 65499.164062
Epoch [1/1], Batch [1161], Loss: 67637.132812
Epoch [1/1], Batch [1171], Loss: 69726.812500
Epoch [1/1], Batch [1181], Loss: 68354.625000
Epoch [1/1], Batch [1191], Loss: 68887.304688
Epoch [1/1], Batch [1201], Loss: 67666.929688
Epoch [1/1], Batch [1211], Loss: 66928.187500
Epoch [1/1], Batch [1221], Loss: 70260.156250
Epoch [1/1], Batch [1231], Loss: 68932.156250
Epoch [1/1], Batch [1241], Loss: 66812.585938
Epoch [1/1], Batch [1251], Loss: 64929.710938
Epoch [1/1], Batch [1261], Loss: 68764.921875
Epoch [1/1], Batch [1271], Loss: 67349.210938
Epoch [1/1], Batch [1281], Loss: 71013.351562
Epoch [1/1], Batch [1291], Loss: 66859.875000
Epoch [1/1], Batch [1301], Loss: 66402.062500
Epoch [1/1], Batch [1311], Loss: 67092.937500
Epoch [1/1], Batch [1321], Loss: 66856.312500
Epoch [1/1], Batch [1331], Loss: 66347.507812
Epoch [1/1], Batch [1341], Loss: 65721.593750
Epoch [1/1], Batch [1351], Loss: 67934.859375
Epoch [1/1], Batch [1361], Loss: 65907.703125
Epoch [1/1], Batch [1371], Loss: 66524.242188
Epoch [1/1], Batch [1381], Loss: 66271.031250
Epoch [1/1], Batch [1391], Loss: 66673.617188
Epoch [1/1], Batch [1401], Loss: 65829.210938
Epoch [1/1], Batch [1411], Loss: 67119.390625
Epoch [1/1], Batch [1421], Loss: 69787.710938
Epoch [1/1], Batch [1431], Loss: 69168.242188
Epoch [1/1], Batch [1441], Loss: 67192.468750
Epoch [1/1], Batch [1451], Loss: 66770.929688
Epoch [1/1], Batch [1461], Loss: 66256.218750
Epoch [1/1], Batch [1471], Loss: 65870.992188
Epoch [1/1], Batch [1481], Loss: 65108.445312
Epoch [1/1], Batch [1491], Loss: 68827.953125
Epoch [1/1], Batch [1501], Loss: 67902.218750
Epoch [1/1], Batch [1511], Loss: 66196.937500
Epoch [1/1], Batch [1521], Loss: 67014.671875
Epoch [1/1], Batch [1531], Loss: 67656.304688
Epoch [1/1], Batch [1541], Loss: 68374.617188
Epoch [1/1], Batch [1551], Loss: 65958.343750
Epoch [1/1], Batch [1561], Loss: 66268.562500
Epoch [1/1], Batch [1571], Loss: 68773.406250
Epoch [1/1], Batch [1581], Loss: 67189.789062
Epoch [1/1], Batch [1591], Loss: 66905.757812
Epoch [1/1], Batch [1601], Loss: 66238.812500
Epoch [1/1], Batch [1611], Loss: 65518.511719
Epoch [1/1], Batch [1621], Loss: 69318.625000
Epoch [1/1], Batch [1631], Loss: 66678.265625
Epoch [1/1], Batch [1641], Loss: 64901.281250
Epoch [1/1], Batch [1651], Loss: 68347.656250
Epoch [1/1], Batch [1661], Loss: 66460.484375
Epoch [1/1], Batch [1671], Loss: 69255.484375
Epoch [1/1], Batch [1681], Loss: 67419.718750
Epoch [1/1], Batch [1691], Loss: 66478.406250
Epoch [1/1], Batch [1701], Loss: 67400.242188
Epoch [1/1], Batch [1711], Loss: 65326.621094
Epoch [1/1], Batch [1721], Loss: 67700.875000
Epoch [1/1], Batch [1731], Loss: 68777.437500
Epoch [1/1], Batch [1741], Loss: 66631.148438
Epoch [1/1], Batch [1751], Loss: 68610.867188
Epoch [1/1], Batch [1761], Loss: 67300.835938
Epoch [1/1], Batch [1771], Loss: 67383.140625
Epoch [1/1], Batch [1781], Loss: 66269.710938
Epoch [1/1], Batch [1791], Loss: 68224.960938
Epoch [1/1], Batch [1801], Loss: 64961.894531
Epoch [1/1], Batch [1811], Loss: 66606.914062
Epoch [1/1], Batch [1821], Loss: 69074.500000
Epoch [1/1], Batch [1831], Loss: 66863.296875
Epoch [1/1], Batch [1841], Loss: 64662.574219
Epoch [1/1], Batch [1851], Loss: 67501.976562
Epoch [1/1], Batch [1861], Loss: 70636.039062
Epoch [1/1], Batch [1871], Loss: 66232.546875
Epoch [1/1], Batch [1881], Loss: 69288.460938
Epoch [1/1], Batch [1891], Loss: 66113.929688
Epoch [1/1], Batch [1901], Loss: 65364.695312
Epoch [1/1], Batch [1911], Loss: 67226.234375
Epoch [1/1], Batch [1921], Loss: 69895.375000
Epoch [1/1], Batch [1931], Loss: 69391.390625
Epoch [1/1], Batch [1941], Loss: 68209.562500
Epoch [1/1], Batch [1951], Loss: 69660.750000
Epoch [1/1], Batch [1961], Loss: 69414.281250
Epoch [1/1], Batch [1971], Loss: 67561.031250
Epoch [1/1], Batch [1981], Loss: 69209.304688
Epoch [1/1], Batch [1991], Loss: 63961.566406
Epoch [1/1], Batch [2001], Loss: 66722.609375
Epoch [1/1], Batch [2011], Loss: 67344.000000
Epoch [1/1], Batch [2021], Loss: 67837.554688
Epoch [1/1], Batch [2031], Loss: 66841.078125
Epoch [1/1], Batch [2041], Loss: 65977.671875
Epoch [1/1], Batch [2051], Loss: 65337.156250
Epoch [1/1], Batch [2061], Loss: 66358.437500
Epoch [1/1], Batch [2071], Loss: 65498.531250
Epoch [1/1], Batch [2081], Loss: 66990.960938
Epoch [1/1], Batch [2091], Loss: 67711.945312
Epoch [1/1], Batch [2101], Loss: 66358.859375
Epoch [1/1], Batch [2111], Loss: 66965.734375
Epoch [1/1], Batch [2121], Loss: 66482.656250
Epoch [1/1], Batch [2131], Loss: 67944.515625
Epoch [1/1], Batch [2141], Loss: 66784.562500
Epoch [1/1], Batch [2151], Loss: 66205.171875
Epoch [1/1], Batch [2161], Loss: 65666.492188
Epoch [1/1], Batch [2171], Loss: 68291.859375
Epoch [1/1], Batch [2181], Loss: 69213.585938
Epoch [1/1], Batch [2191], Loss: 68618.093750
Epoch [1/1], Batch [2201], Loss: 67404.281250
Epoch [1/1], Batch [2211], Loss: 71894.882812
Epoch [1/1], Batch [2221], Loss: 67599.757812
Epoch [1/1], Batch [2231], Loss: 65823.320312
Epoch [1/1], Batch [2241], Loss: 68142.046875
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 69314.0163
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 66915.8399
Elapsed time: 483.14 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 66890.1032
Elapsed time: 502.31 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 120940.109375
Epoch [1/1], Batch [11], Loss: 114224.054688
Epoch [1/1], Batch [21], Loss: 115441.562500
Epoch [1/1], Batch [31], Loss: 110174.906250
Epoch [1/1], Batch [41], Loss: 116937.500000
Epoch [1/1], Batch [51], Loss: 115727.867188
Epoch [1/1], Batch [61], Loss: 116824.140625
Epoch [1/1], Batch [71], Loss: 115274.671875
Epoch [1/1], Batch [81], Loss: 116203.914062
Epoch [1/1], Batch [91], Loss: 115937.921875
Epoch [1/1], Batch [101], Loss: 111794.562500
Epoch [1/1], Batch [111], Loss: 109703.101562
Epoch [1/1], Batch [121], Loss: 115823.078125
Epoch [1/1], Batch [131], Loss: 114756.718750
Epoch [1/1], Batch [141], Loss: 112843.117188
Epoch [1/1], Batch [151], Loss: 115318.359375
Epoch [1/1], Batch [161], Loss: 116264.687500
Epoch [1/1], Batch [171], Loss: 112476.656250
Epoch [1/1], Batch [181], Loss: 116657.578125
Epoch [1/1], Batch [191], Loss: 115360.632812
Epoch [1/1], Batch [201], Loss: 111193.773438
Epoch [1/1], Batch [211], Loss: 113130.312500
Epoch [1/1], Batch [221], Loss: 111519.968750
Epoch [1/1], Batch [231], Loss: 110198.875000
Epoch [1/1], Batch [241], Loss: 117722.546875
Epoch [1/1], Batch [251], Loss: 115949.585938
Epoch [1/1], Batch [261], Loss: 110065.453125
Epoch [1/1], Batch [271], Loss: 112390.179688
Epoch [1/1], Batch [281], Loss: 112281.984375
Epoch [1/1], Batch [291], Loss: 111673.234375
Epoch [1/1], Batch [301], Loss: 113643.281250
Epoch [1/1], Batch [311], Loss: 118030.765625
Epoch [1/1], Batch [321], Loss: 119732.507812
Epoch [1/1], Batch [331], Loss: 114672.265625
Epoch [1/1], Batch [341], Loss: 109282.312500
Epoch [1/1], Batch [351], Loss: 112783.617188
Epoch [1/1], Batch [361], Loss: 110808.093750
Epoch [1/1], Batch [371], Loss: 115769.007812
Epoch [1/1], Batch [381], Loss: 108195.601562
Epoch [1/1], Batch [391], Loss: 118704.640625
Epoch [1/1], Batch [401], Loss: 113200.320312
Epoch [1/1], Batch [411], Loss: 112877.437500
Epoch [1/1], Batch [421], Loss: 113026.875000
Epoch [1/1], Batch [431], Loss: 116979.179688
Epoch [1/1], Batch [441], Loss: 117229.609375
Epoch [1/1], Batch [451], Loss: 110192.453125
Epoch [1/1], Batch [461], Loss: 112737.882812
Epoch [1/1], Batch [471], Loss: 116396.843750
Epoch [1/1], Batch [481], Loss: 116448.078125
Epoch [1/1], Batch [491], Loss: 114120.281250
Epoch [1/1], Batch [501], Loss: 111327.656250
Epoch [1/1], Batch [511], Loss: 113014.328125
Epoch [1/1], Batch [521], Loss: 111946.031250
Epoch [1/1], Batch [531], Loss: 109263.562500
Epoch [1/1], Batch [541], Loss: 115956.210938
Epoch [1/1], Batch [551], Loss: 114028.867188
Epoch [1/1], Batch [561], Loss: 111111.687500
Epoch [1/1], Batch [571], Loss: 114146.562500
Epoch [1/1], Batch [581], Loss: 114978.656250
Epoch [1/1], Batch [591], Loss: 113802.828125
Epoch [1/1], Batch [601], Loss: 116704.078125
Epoch [1/1], Batch [611], Loss: 113159.226562
Epoch [1/1], Batch [621], Loss: 111407.250000
Epoch [1/1], Batch [631], Loss: 113680.617188
Epoch [1/1], Batch [641], Loss: 109301.710938
Epoch [1/1], Batch [651], Loss: 112394.375000
Epoch [1/1], Batch [661], Loss: 111473.562500
Epoch [1/1], Batch [671], Loss: 109393.804688
Epoch [1/1], Batch [681], Loss: 110120.718750
Epoch [1/1], Batch [691], Loss: 115203.234375
Epoch [1/1], Batch [701], Loss: 111158.031250
Epoch [1/1], Batch [711], Loss: 119350.804688
Epoch [1/1], Batch [721], Loss: 119493.062500
Epoch [1/1], Batch [731], Loss: 111198.820312
Epoch [1/1], Batch [741], Loss: 111978.437500
Epoch [1/1], Batch [751], Loss: 112698.937500
Epoch [1/1], Batch [761], Loss: 117933.304688
Epoch [1/1], Batch [771], Loss: 117828.132812
Epoch [1/1], Batch [781], Loss: 109769.109375
Epoch [1/1], Batch [791], Loss: 118602.921875
Epoch [1/1], Batch [801], Loss: 113035.507812
Epoch [1/1], Batch [811], Loss: 110339.875000
Epoch [1/1], Batch [821], Loss: 111344.812500
Epoch [1/1], Batch [831], Loss: 114885.109375
Epoch [1/1], Batch [841], Loss: 112817.390625
Epoch [1/1], Batch [851], Loss: 114009.765625
Epoch [1/1], Batch [861], Loss: 111971.156250
Epoch [1/1], Batch [871], Loss: 112834.812500
Epoch [1/1], Batch [881], Loss: 107466.000000
Epoch [1/1], Batch [891], Loss: 117674.289062
Epoch [1/1], Batch [901], Loss: 110370.187500
Epoch [1/1], Batch [911], Loss: 115536.117188
Epoch [1/1], Batch [921], Loss: 104532.734375
Epoch [1/1], Batch [931], Loss: 113212.304688
Epoch [1/1], Batch [941], Loss: 111396.882812
Epoch [1/1], Batch [951], Loss: 113327.906250
Epoch [1/1], Batch [961], Loss: 110227.218750
Epoch [1/1], Batch [971], Loss: 109066.671875
Epoch [1/1], Batch [981], Loss: 111846.781250
Epoch [1/1], Batch [991], Loss: 115986.343750
Epoch [1/1], Batch [1001], Loss: 106732.164062
Epoch [1/1], Batch [1011], Loss: 111955.710938
Epoch [1/1], Batch [1021], Loss: 115120.703125
Epoch [1/1], Batch [1031], Loss: 114693.507812
Epoch [1/1], Batch [1041], Loss: 115869.843750
Epoch [1/1], Batch [1051], Loss: 113382.820312
Epoch [1/1], Batch [1061], Loss: 112688.250000
Epoch [1/1], Batch [1071], Loss: 119154.960938
Epoch [1/1], Batch [1081], Loss: 110853.000000
Epoch [1/1], Batch [1091], Loss: 113152.179688
Epoch [1/1], Batch [1101], Loss: 108416.250000
Epoch [1/1], Batch [1111], Loss: 114036.820312
Epoch [1/1], Batch [1121], Loss: 110068.351562
Epoch [1/1], Batch [1131], Loss: 112834.335938
Epoch [1/1], Batch [1141], Loss: 104348.335938
Epoch [1/1], Batch [1151], Loss: 110725.851562
Epoch [1/1], Batch [1161], Loss: 113577.570312
Epoch [1/1], Batch [1171], Loss: 105037.562500
Epoch [1/1], Batch [1181], Loss: 114010.054688
Epoch [1/1], Batch [1191], Loss: 112850.835938
Epoch [1/1], Batch [1201], Loss: 115488.187500
Epoch [1/1], Batch [1211], Loss: 113990.562500
Epoch [1/1], Batch [1221], Loss: 118338.796875
Epoch [1/1], Batch [1231], Loss: 107781.289062
Epoch [1/1], Batch [1241], Loss: 118958.140625
Epoch [1/1], Batch [1251], Loss: 115553.570312
Epoch [1/1], Batch [1261], Loss: 111326.617188
Epoch [1/1], Batch [1271], Loss: 111079.523438
Epoch [1/1], Batch [1281], Loss: 108076.328125
Epoch [1/1], Batch [1291], Loss: 110796.203125
Epoch [1/1], Batch [1301], Loss: 106932.875000
Epoch [1/1], Batch [1311], Loss: 109027.515625
Epoch [1/1], Batch [1321], Loss: 113661.031250
Epoch [1/1], Batch [1331], Loss: 109901.742188
Epoch [1/1], Batch [1341], Loss: 112389.406250
Epoch [1/1], Batch [1351], Loss: 111664.742188
Epoch [1/1], Batch [1361], Loss: 115077.281250
Epoch [1/1], Batch [1371], Loss: 112325.109375
Epoch [1/1], Batch [1381], Loss: 115078.851562
Epoch [1/1], Batch [1391], Loss: 112525.437500
Epoch [1/1], Batch [1401], Loss: 113803.234375
Epoch [1/1], Batch [1411], Loss: 114153.421875
Epoch [1/1], Batch [1421], Loss: 111929.101562
Epoch [1/1], Batch [1431], Loss: 117719.734375
Epoch [1/1], Batch [1441], Loss: 115216.312500
Epoch [1/1], Batch [1451], Loss: 116074.531250
Epoch [1/1], Batch [1461], Loss: 118320.203125
Epoch [1/1], Batch [1471], Loss: 108421.437500
Epoch [1/1], Batch [1481], Loss: 115133.718750
Epoch [1/1], Batch [1491], Loss: 112172.796875
Epoch [1/1], Batch [1501], Loss: 120355.484375
Epoch [1/1], Batch [1511], Loss: 117337.578125
Epoch [1/1], Batch [1521], Loss: 113390.265625
Epoch [1/1], Batch [1531], Loss: 108605.390625
Epoch [1/1], Batch [1541], Loss: 113657.265625
Epoch [1/1], Batch [1551], Loss: 120803.054688
Epoch [1/1], Batch [1561], Loss: 110181.000000
Epoch [1/1], Batch [1571], Loss: 115704.406250
Epoch [1/1], Batch [1581], Loss: 110434.843750
Epoch [1/1], Batch [1591], Loss: 115847.812500
Epoch [1/1], Batch [1601], Loss: 106271.000000
Epoch [1/1], Batch [1611], Loss: 115237.062500
Epoch [1/1], Batch [1621], Loss: 111463.046875
Epoch [1/1], Batch [1631], Loss: 107343.632812
Epoch [1/1], Batch [1641], Loss: 113802.093750
Epoch [1/1], Batch [1651], Loss: 112690.421875
Epoch [1/1], Batch [1661], Loss: 113426.765625
Epoch [1/1], Batch [1671], Loss: 107698.265625
Epoch [1/1], Batch [1681], Loss: 111723.734375
Epoch [1/1], Batch [1691], Loss: 109602.812500
Epoch [1/1], Batch [1701], Loss: 113595.851562
Epoch [1/1], Batch [1711], Loss: 113770.515625
Epoch [1/1], Batch [1721], Loss: 119802.273438
Epoch [1/1], Batch [1731], Loss: 111048.296875
Epoch [1/1], Batch [1741], Loss: 115961.531250
Epoch [1/1], Batch [1751], Loss: 108802.765625
Epoch [1/1], Batch [1761], Loss: 112319.101562
Epoch [1/1], Batch [1771], Loss: 110966.890625
Epoch [1/1], Batch [1781], Loss: 114035.015625
Epoch [1/1], Batch [1791], Loss: 115876.203125
Epoch [1/1], Batch [1801], Loss: 114802.093750
Epoch [1/1], Batch [1811], Loss: 111682.210938
Epoch [1/1], Batch [1821], Loss: 113905.968750
Epoch [1/1], Batch [1831], Loss: 111591.828125
Epoch [1/1], Batch [1841], Loss: 112409.656250
Epoch [1/1], Batch [1851], Loss: 108230.515625
Epoch [1/1], Batch [1861], Loss: 119114.218750
Epoch [1/1], Batch [1871], Loss: 114824.390625
Epoch [1/1], Batch [1881], Loss: 113053.000000
Epoch [1/1], Batch [1891], Loss: 114335.921875
Epoch [1/1], Batch [1901], Loss: 109471.531250
Epoch [1/1], Batch [1911], Loss: 116155.304688
Epoch [1/1], Batch [1921], Loss: 109405.289062
Epoch [1/1], Batch [1931], Loss: 112102.296875
Epoch [1/1], Batch [1941], Loss: 116849.210938
Epoch [1/1], Batch [1951], Loss: 109937.156250
Epoch [1/1], Batch [1961], Loss: 109350.406250
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 113023.3149
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 111427.4115
Elapsed time: 1115.32 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 111803.9239
Elapsed time: 1138.26 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 159358.125000
Epoch [1/1], Batch [11], Loss: 165876.390625
Epoch [1/1], Batch [21], Loss: 158512.546875
Epoch [1/1], Batch [31], Loss: 159167.828125
Epoch [1/1], Batch [41], Loss: 160144.250000
Epoch [1/1], Batch [51], Loss: 156479.984375
Epoch [1/1], Batch [61], Loss: 154097.625000
Epoch [1/1], Batch [71], Loss: 156802.718750
Epoch [1/1], Batch [81], Loss: 161634.796875
Epoch [1/1], Batch [91], Loss: 160273.062500
Epoch [1/1], Batch [101], Loss: 161145.640625
Epoch [1/1], Batch [111], Loss: 159086.296875
Epoch [1/1], Batch [121], Loss: 163078.765625
Epoch [1/1], Batch [131], Loss: 159910.140625
Epoch [1/1], Batch [141], Loss: 153929.531250
Epoch [1/1], Batch [151], Loss: 160275.468750
Epoch [1/1], Batch [161], Loss: 157903.281250
Epoch [1/1], Batch [171], Loss: 151891.562500
Epoch [1/1], Batch [181], Loss: 165730.218750
Epoch [1/1], Batch [191], Loss: 155450.281250
Epoch [1/1], Batch [201], Loss: 161689.625000
Epoch [1/1], Batch [211], Loss: 154368.906250
Epoch [1/1], Batch [221], Loss: 159070.796875
Epoch [1/1], Batch [231], Loss: 160972.281250
Epoch [1/1], Batch [241], Loss: 159043.171875
Epoch [1/1], Batch [251], Loss: 154476.750000
Epoch [1/1], Batch [261], Loss: 158966.312500
Epoch [1/1], Batch [271], Loss: 154964.875000
Epoch [1/1], Batch [281], Loss: 150439.515625
Epoch [1/1], Batch [291], Loss: 159895.531250
Epoch [1/1], Batch [301], Loss: 155761.328125
Epoch [1/1], Batch [311], Loss: 159701.234375
Epoch [1/1], Batch [321], Loss: 156974.531250
Epoch [1/1], Batch [331], Loss: 158457.687500
Epoch [1/1], Batch [341], Loss: 158846.843750
Epoch [1/1], Batch [351], Loss: 156705.406250
Epoch [1/1], Batch [361], Loss: 166565.281250
Epoch [1/1], Batch [371], Loss: 158893.750000
Epoch [1/1], Batch [381], Loss: 162278.375000
Epoch [1/1], Batch [391], Loss: 159851.609375
Epoch [1/1], Batch [401], Loss: 158787.031250
Epoch [1/1], Batch [411], Loss: 160403.968750
Epoch [1/1], Batch [421], Loss: 162440.781250
Epoch [1/1], Batch [431], Loss: 152870.187500
Epoch [1/1], Batch [441], Loss: 162714.671875
Epoch [1/1], Batch [451], Loss: 164399.109375
Epoch [1/1], Batch [461], Loss: 156974.468750
Epoch [1/1], Batch [471], Loss: 159809.562500
Epoch [1/1], Batch [481], Loss: 157825.968750
Epoch [1/1], Batch [491], Loss: 159816.968750
Epoch [1/1], Batch [501], Loss: 158514.328125
Epoch [1/1], Batch [511], Loss: 159788.500000
Epoch [1/1], Batch [521], Loss: 157882.281250
Epoch [1/1], Batch [531], Loss: 155480.625000
Epoch [1/1], Batch [541], Loss: 158337.156250
Epoch [1/1], Batch [551], Loss: 162031.000000
Epoch [1/1], Batch [561], Loss: 157763.000000
Epoch [1/1], Batch [571], Loss: 157900.437500
Epoch [1/1], Batch [581], Loss: 159375.156250
Epoch [1/1], Batch [591], Loss: 160890.796875
Epoch [1/1], Batch [601], Loss: 162262.312500
Epoch [1/1], Batch [611], Loss: 158485.406250
Epoch [1/1], Batch [621], Loss: 163041.062500
Epoch [1/1], Batch [631], Loss: 161834.343750
Epoch [1/1], Batch [641], Loss: 156943.718750
Epoch [1/1], Batch [651], Loss: 161013.390625
Epoch [1/1], Batch [661], Loss: 154312.015625
Epoch [1/1], Batch [671], Loss: 154209.109375
Epoch [1/1], Batch [681], Loss: 159171.296875
Epoch [1/1], Batch [691], Loss: 160157.234375
Epoch [1/1], Batch [701], Loss: 163814.500000
Epoch [1/1], Batch [711], Loss: 158100.000000
Epoch [1/1], Batch [721], Loss: 154457.093750
Epoch [1/1], Batch [731], Loss: 161566.812500
Epoch [1/1], Batch [741], Loss: 158443.296875
Epoch [1/1], Batch [751], Loss: 155681.484375
Epoch [1/1], Batch [761], Loss: 155652.281250
Epoch [1/1], Batch [771], Loss: 156238.453125
Epoch [1/1], Batch [781], Loss: 162058.312500
Epoch [1/1], Batch [791], Loss: 153747.218750
Epoch [1/1], Batch [801], Loss: 156094.562500
Epoch [1/1], Batch [811], Loss: 162333.468750
Epoch [1/1], Batch [821], Loss: 160511.593750
Epoch [1/1], Batch [831], Loss: 162703.562500
Epoch [1/1], Batch [841], Loss: 159412.890625
Epoch [1/1], Batch [851], Loss: 161519.359375
Epoch [1/1], Batch [861], Loss: 158390.875000
Epoch [1/1], Batch [871], Loss: 151995.468750
Epoch [1/1], Batch [881], Loss: 156128.640625
Epoch [1/1], Batch [891], Loss: 153904.703125
Epoch [1/1], Batch [901], Loss: 160319.750000
Epoch [1/1], Batch [911], Loss: 155544.843750
Epoch [1/1], Batch [921], Loss: 154147.656250
Epoch [1/1], Batch [931], Loss: 158161.406250
Epoch [1/1], Batch [941], Loss: 156742.171875
Epoch [1/1], Batch [951], Loss: 151702.281250
Epoch [1/1], Batch [961], Loss: 154588.843750
Epoch [1/1], Batch [971], Loss: 165424.250000
Epoch [1/1], Batch [981], Loss: 165535.515625
Epoch [1/1], Batch [991], Loss: 163699.875000
Epoch [1/1], Batch [1001], Loss: 156656.937500
Epoch [1/1], Batch [1011], Loss: 155628.406250
Epoch [1/1], Batch [1021], Loss: 161250.593750
Epoch [1/1], Batch [1031], Loss: 161120.234375
Epoch [1/1], Batch [1041], Loss: 159763.500000
Epoch [1/1], Batch [1051], Loss: 158673.187500
Epoch [1/1], Batch [1061], Loss: 162495.343750
Epoch [1/1], Batch [1071], Loss: 156979.531250
Epoch [1/1], Batch [1081], Loss: 156249.421875
Epoch [1/1], Batch [1091], Loss: 158612.609375
Epoch [1/1], Batch [1101], Loss: 158687.750000
Epoch [1/1], Batch [1111], Loss: 157125.109375
Epoch [1/1], Batch [1121], Loss: 161221.406250
Epoch [1/1], Batch [1131], Loss: 156212.437500
Epoch [1/1], Batch [1141], Loss: 156264.093750
Epoch [1/1], Batch [1151], Loss: 152739.156250
Epoch [1/1], Batch [1161], Loss: 155833.312500
Epoch [1/1], Batch [1171], Loss: 160309.093750
Epoch [1/1], Batch [1181], Loss: 155863.375000
Epoch [1/1], Batch [1191], Loss: 160952.437500
Epoch [1/1], Batch [1201], Loss: 158844.484375
Epoch [1/1], Batch [1211], Loss: 154457.625000
Epoch [1/1], Batch [1221], Loss: 159308.625000
Epoch [1/1], Batch [1231], Loss: 157037.812500
Epoch [1/1], Batch [1241], Loss: 152178.718750
Epoch [1/1], Batch [1251], Loss: 151312.125000
Epoch [1/1], Batch [1261], Loss: 161008.312500
Epoch [1/1], Batch [1271], Loss: 153159.390625
Epoch [1/1], Batch [1281], Loss: 159326.171875
Epoch [1/1], Batch [1291], Loss: 156145.203125
Epoch [1/1], Batch [1301], Loss: 156644.062500
Epoch [1/1], Batch [1311], Loss: 162322.125000
Epoch [1/1], Batch [1321], Loss: 158607.687500
Epoch [1/1], Batch [1331], Loss: 156817.187500
Epoch [1/1], Batch [1341], Loss: 160544.375000
Epoch [1/1], Batch [1351], Loss: 156205.859375
Epoch [1/1], Batch [1361], Loss: 157194.125000
Epoch [1/1], Batch [1371], Loss: 152964.921875
Epoch [1/1], Batch [1381], Loss: 152137.031250
Epoch [1/1], Batch [1391], Loss: 165335.812500
Epoch [1/1], Batch [1401], Loss: 159588.703125
Epoch [1/1], Batch [1411], Loss: 159278.093750
Epoch [1/1], Batch [1421], Loss: 151535.187500
Epoch [1/1], Batch [1431], Loss: 159511.484375
Epoch [1/1], Batch [1441], Loss: 161554.031250
Epoch [1/1], Batch [1451], Loss: 156405.343750
Epoch [1/1], Batch [1461], Loss: 161399.687500
Epoch [1/1], Batch [1471], Loss: 159249.375000
Epoch [1/1], Batch [1481], Loss: 159532.000000
Epoch [1/1], Batch [1491], Loss: 159417.468750
Epoch [1/1], Batch [1501], Loss: 165321.406250
Epoch [1/1], Batch [1511], Loss: 158885.593750
Epoch [1/1], Batch [1521], Loss: 154923.562500
Epoch [1/1], Batch [1531], Loss: 152568.921875
Epoch [1/1], Batch [1541], Loss: 159966.250000
Epoch [1/1], Batch [1551], Loss: 166359.406250
Epoch [1/1], Batch [1561], Loss: 155847.500000
Epoch [1/1], Batch [1571], Loss: 152087.750000
Epoch [1/1], Batch [1581], Loss: 157036.171875
Epoch [1/1], Batch [1591], Loss: 161238.781250
Epoch [1/1], Batch [1601], Loss: 158434.578125
Epoch [1/1], Batch [1611], Loss: 169934.125000
Epoch [1/1], Batch [1621], Loss: 162059.093750
Epoch [1/1], Batch [1631], Loss: 161768.390625
Epoch [1/1], Batch [1641], Loss: 159104.187500
Epoch [1/1], Batch [1651], Loss: 161644.218750
Epoch [1/1], Batch [1661], Loss: 153207.312500
Epoch [1/1], Batch [1671], Loss: 155282.046875
Epoch [1/1], Batch [1681], Loss: 155094.500000
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 158735.5251
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 156394.0247
Elapsed time: 1824.46 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 157370.9719
Elapsed time: 1849.45 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 203127.562500
Epoch [1/1], Batch [11], Loss: 211501.250000
Epoch [1/1], Batch [21], Loss: 207529.468750
Epoch [1/1], Batch [31], Loss: 203106.968750
Epoch [1/1], Batch [41], Loss: 202135.375000
Epoch [1/1], Batch [51], Loss: 199175.437500
Epoch [1/1], Batch [61], Loss: 204872.281250
Epoch [1/1], Batch [71], Loss: 195783.109375
Epoch [1/1], Batch [81], Loss: 196161.812500
Epoch [1/1], Batch [91], Loss: 205299.046875
Epoch [1/1], Batch [101], Loss: 205515.640625
Epoch [1/1], Batch [111], Loss: 203925.531250
Epoch [1/1], Batch [121], Loss: 203998.000000
Epoch [1/1], Batch [131], Loss: 196422.078125
Epoch [1/1], Batch [141], Loss: 207920.781250
Epoch [1/1], Batch [151], Loss: 203149.453125
Epoch [1/1], Batch [161], Loss: 200594.250000
Epoch [1/1], Batch [171], Loss: 205640.218750
Epoch [1/1], Batch [181], Loss: 200029.500000
Epoch [1/1], Batch [191], Loss: 204731.921875
Epoch [1/1], Batch [201], Loss: 206195.890625
Epoch [1/1], Batch [211], Loss: 205597.093750
Epoch [1/1], Batch [221], Loss: 207271.500000
Epoch [1/1], Batch [231], Loss: 210427.437500
Epoch [1/1], Batch [241], Loss: 207831.093750
Epoch [1/1], Batch [251], Loss: 194987.093750
Epoch [1/1], Batch [261], Loss: 211093.703125
Epoch [1/1], Batch [271], Loss: 195110.406250
Epoch [1/1], Batch [281], Loss: 204965.250000
Epoch [1/1], Batch [291], Loss: 205371.656250
Epoch [1/1], Batch [301], Loss: 206965.562500
Epoch [1/1], Batch [311], Loss: 210186.859375
Epoch [1/1], Batch [321], Loss: 207317.078125
Epoch [1/1], Batch [331], Loss: 201902.421875
Epoch [1/1], Batch [341], Loss: 197320.968750
Epoch [1/1], Batch [351], Loss: 205264.531250
Epoch [1/1], Batch [361], Loss: 198668.765625
Epoch [1/1], Batch [371], Loss: 208093.500000
Epoch [1/1], Batch [381], Loss: 209344.531250
Epoch [1/1], Batch [391], Loss: 208683.859375
Epoch [1/1], Batch [401], Loss: 201577.984375
Epoch [1/1], Batch [411], Loss: 199408.062500
Epoch [1/1], Batch [421], Loss: 201061.796875
Epoch [1/1], Batch [431], Loss: 202185.265625
Epoch [1/1], Batch [441], Loss: 201872.625000
Epoch [1/1], Batch [451], Loss: 206123.156250
Epoch [1/1], Batch [461], Loss: 198593.906250
Epoch [1/1], Batch [471], Loss: 207671.578125
Epoch [1/1], Batch [481], Loss: 199963.718750
Epoch [1/1], Batch [491], Loss: 205407.734375
Epoch [1/1], Batch [501], Loss: 208208.781250
Epoch [1/1], Batch [511], Loss: 198741.125000
Epoch [1/1], Batch [521], Loss: 207866.156250
Epoch [1/1], Batch [531], Loss: 204645.203125
Epoch [1/1], Batch [541], Loss: 209483.515625
Epoch [1/1], Batch [551], Loss: 194692.718750
Epoch [1/1], Batch [561], Loss: 204036.125000
Epoch [1/1], Batch [571], Loss: 203687.843750
Epoch [1/1], Batch [581], Loss: 203180.625000
Epoch [1/1], Batch [591], Loss: 199657.062500
Epoch [1/1], Batch [601], Loss: 209132.906250
Epoch [1/1], Batch [611], Loss: 205991.000000
Epoch [1/1], Batch [621], Loss: 204748.015625
Epoch [1/1], Batch [631], Loss: 193666.625000
Epoch [1/1], Batch [641], Loss: 210630.312500
Epoch [1/1], Batch [651], Loss: 209323.906250
Epoch [1/1], Batch [661], Loss: 203851.765625
Epoch [1/1], Batch [671], Loss: 205360.687500
Epoch [1/1], Batch [681], Loss: 206204.781250
Epoch [1/1], Batch [691], Loss: 207125.390625
Epoch [1/1], Batch [701], Loss: 199161.843750
Epoch [1/1], Batch [711], Loss: 208511.421875
Epoch [1/1], Batch [721], Loss: 209721.937500
Epoch [1/1], Batch [731], Loss: 201837.671875
Epoch [1/1], Batch [741], Loss: 203917.078125
Epoch [1/1], Batch [751], Loss: 199231.046875
Epoch [1/1], Batch [761], Loss: 208561.375000
Epoch [1/1], Batch [771], Loss: 200380.390625
Epoch [1/1], Batch [781], Loss: 203744.281250
Epoch [1/1], Batch [791], Loss: 201973.468750
Epoch [1/1], Batch [801], Loss: 207259.390625
Epoch [1/1], Batch [811], Loss: 195408.484375
Epoch [1/1], Batch [821], Loss: 206184.390625
Epoch [1/1], Batch [831], Loss: 205970.140625
Epoch [1/1], Batch [841], Loss: 207133.234375
Epoch [1/1], Batch [851], Loss: 209727.062500
Epoch [1/1], Batch [861], Loss: 204039.687500
Epoch [1/1], Batch [871], Loss: 204613.671875
Epoch [1/1], Batch [881], Loss: 207780.093750
Epoch [1/1], Batch [891], Loss: 205029.937500
Epoch [1/1], Batch [901], Loss: 200832.343750
Epoch [1/1], Batch [911], Loss: 198517.296875
Epoch [1/1], Batch [921], Loss: 205069.656250
Epoch [1/1], Batch [931], Loss: 211808.281250
Epoch [1/1], Batch [941], Loss: 205942.562500
Epoch [1/1], Batch [951], Loss: 202491.031250
Epoch [1/1], Batch [961], Loss: 206611.531250
Epoch [1/1], Batch [971], Loss: 209513.656250
Epoch [1/1], Batch [981], Loss: 205068.875000
Epoch [1/1], Batch [991], Loss: 204742.343750
Epoch [1/1], Batch [1001], Loss: 196467.281250
Epoch [1/1], Batch [1011], Loss: 207638.812500
Epoch [1/1], Batch [1021], Loss: 209773.031250
Epoch [1/1], Batch [1031], Loss: 206555.187500
Epoch [1/1], Batch [1041], Loss: 199494.000000
Epoch [1/1], Batch [1051], Loss: 206686.281250
Epoch [1/1], Batch [1061], Loss: 198096.437500
Epoch [1/1], Batch [1071], Loss: 198166.734375
Epoch [1/1], Batch [1081], Loss: 202548.468750
Epoch [1/1], Batch [1091], Loss: 200538.468750
Epoch [1/1], Batch [1101], Loss: 205157.343750
Epoch [1/1], Batch [1111], Loss: 208688.437500
Epoch [1/1], Batch [1121], Loss: 199480.328125
Epoch [1/1], Batch [1131], Loss: 203740.859375
Epoch [1/1], Batch [1141], Loss: 204958.203125
Epoch [1/1], Batch [1151], Loss: 202190.562500
Epoch [1/1], Batch [1161], Loss: 205767.093750
Epoch [1/1], Batch [1171], Loss: 203350.843750
Epoch [1/1], Batch [1181], Loss: 205436.156250
Epoch [1/1], Batch [1191], Loss: 202374.906250
Epoch [1/1], Batch [1201], Loss: 205194.406250
Epoch [1/1], Batch [1211], Loss: 207952.250000
Epoch [1/1], Batch [1221], Loss: 196658.500000
Epoch [1/1], Batch [1231], Loss: 203222.593750
Epoch [1/1], Batch [1241], Loss: 204071.093750
Epoch [1/1], Batch [1251], Loss: 207758.312500
Epoch [1/1], Batch [1261], Loss: 204425.531250
Epoch [1/1], Batch [1271], Loss: 200097.718750
Epoch [1/1], Batch [1281], Loss: 212296.921875
Epoch [1/1], Batch [1291], Loss: 197288.359375
Epoch [1/1], Batch [1301], Loss: 206207.843750
Epoch [1/1], Batch [1311], Loss: 199101.421875
Epoch [1/1], Batch [1321], Loss: 204135.843750
Epoch [1/1], Batch [1331], Loss: 202405.578125
Epoch [1/1], Batch [1341], Loss: 196729.578125
Epoch [1/1], Batch [1351], Loss: 195038.343750
Epoch [1/1], Batch [1361], Loss: 201516.468750
Epoch [1/1], Batch [1371], Loss: 202255.562500
Epoch [1/1], Batch [1381], Loss: 201367.781250
Epoch [1/1], Batch [1391], Loss: 206833.062500
Epoch [1/1], Batch [1401], Loss: 194738.406250
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 204131.3978
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 200604.6085
Elapsed time: 2559.90 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 200922.7063
Elapsed time: 2585.02 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 234879.843750
Epoch [1/1], Batch [11], Loss: 253378.781250
Epoch [1/1], Batch [21], Loss: 244840.328125
Epoch [1/1], Batch [31], Loss: 243606.515625
Epoch [1/1], Batch [41], Loss: 250251.562500
Epoch [1/1], Batch [51], Loss: 241757.531250
Epoch [1/1], Batch [61], Loss: 245518.875000
Epoch [1/1], Batch [71], Loss: 252655.718750
Epoch [1/1], Batch [81], Loss: 251540.171875
Epoch [1/1], Batch [91], Loss: 239231.875000
Epoch [1/1], Batch [101], Loss: 239149.687500
Epoch [1/1], Batch [111], Loss: 242937.312500
Epoch [1/1], Batch [121], Loss: 241360.796875
Epoch [1/1], Batch [131], Loss: 248395.406250
Epoch [1/1], Batch [141], Loss: 243460.437500
Epoch [1/1], Batch [151], Loss: 243439.468750
Epoch [1/1], Batch [161], Loss: 235190.484375
Epoch [1/1], Batch [171], Loss: 242975.937500
Epoch [1/1], Batch [181], Loss: 249564.906250
Epoch [1/1], Batch [191], Loss: 243875.968750
Epoch [1/1], Batch [201], Loss: 243018.093750
Epoch [1/1], Batch [211], Loss: 254614.781250
Epoch [1/1], Batch [221], Loss: 248345.453125
Epoch [1/1], Batch [231], Loss: 242403.375000
Epoch [1/1], Batch [241], Loss: 244812.968750
Epoch [1/1], Batch [251], Loss: 247925.640625
Epoch [1/1], Batch [261], Loss: 248750.906250
Epoch [1/1], Batch [271], Loss: 250057.281250
Epoch [1/1], Batch [281], Loss: 241756.281250
Epoch [1/1], Batch [291], Loss: 255363.140625
Epoch [1/1], Batch [301], Loss: 236089.968750
Epoch [1/1], Batch [311], Loss: 242812.890625
Epoch [1/1], Batch [321], Loss: 247485.046875
Epoch [1/1], Batch [331], Loss: 248328.843750
Epoch [1/1], Batch [341], Loss: 252294.765625
Epoch [1/1], Batch [351], Loss: 247499.843750
Epoch [1/1], Batch [361], Loss: 249980.718750
Epoch [1/1], Batch [371], Loss: 241467.500000
Epoch [1/1], Batch [381], Loss: 254498.218750
Epoch [1/1], Batch [391], Loss: 240764.093750
Epoch [1/1], Batch [401], Loss: 246012.875000
Epoch [1/1], Batch [411], Loss: 237845.687500
Epoch [1/1], Batch [421], Loss: 244390.218750
Epoch [1/1], Batch [431], Loss: 244529.093750
Epoch [1/1], Batch [441], Loss: 255707.125000
Epoch [1/1], Batch [451], Loss: 251287.875000
Epoch [1/1], Batch [461], Loss: 245500.015625
Epoch [1/1], Batch [471], Loss: 247003.843750
Epoch [1/1], Batch [481], Loss: 244442.656250
Epoch [1/1], Batch [491], Loss: 249778.093750
Epoch [1/1], Batch [501], Loss: 256048.625000
Epoch [1/1], Batch [511], Loss: 246922.281250
Epoch [1/1], Batch [521], Loss: 253156.218750
Epoch [1/1], Batch [531], Loss: 252528.578125
Epoch [1/1], Batch [541], Loss: 248216.125000
Epoch [1/1], Batch [551], Loss: 249193.828125
Epoch [1/1], Batch [561], Loss: 238377.343750
Epoch [1/1], Batch [571], Loss: 243520.656250
Epoch [1/1], Batch [581], Loss: 247686.281250
Epoch [1/1], Batch [591], Loss: 238896.906250
Epoch [1/1], Batch [601], Loss: 246821.375000
Epoch [1/1], Batch [611], Loss: 248250.375000
Epoch [1/1], Batch [621], Loss: 258301.218750
Epoch [1/1], Batch [631], Loss: 238679.468750
Epoch [1/1], Batch [641], Loss: 244392.281250
Epoch [1/1], Batch [651], Loss: 244714.578125
Epoch [1/1], Batch [661], Loss: 249608.406250
Epoch [1/1], Batch [671], Loss: 254255.500000
Epoch [1/1], Batch [681], Loss: 247335.906250
Epoch [1/1], Batch [691], Loss: 246028.718750
Epoch [1/1], Batch [701], Loss: 247496.906250
Epoch [1/1], Batch [711], Loss: 243224.437500
Epoch [1/1], Batch [721], Loss: 236939.375000
Epoch [1/1], Batch [731], Loss: 240177.640625
Epoch [1/1], Batch [741], Loss: 243065.984375
Epoch [1/1], Batch [751], Loss: 256490.281250
Epoch [1/1], Batch [761], Loss: 238700.718750
Epoch [1/1], Batch [771], Loss: 243796.890625
Epoch [1/1], Batch [781], Loss: 243074.921875
Epoch [1/1], Batch [791], Loss: 244596.453125
Epoch [1/1], Batch [801], Loss: 237100.656250
Epoch [1/1], Batch [811], Loss: 233703.312500
Epoch [1/1], Batch [821], Loss: 251192.718750
Epoch [1/1], Batch [831], Loss: 242169.968750
Epoch [1/1], Batch [841], Loss: 244158.921875
Epoch [1/1], Batch [851], Loss: 241376.328125
Epoch [1/1], Batch [861], Loss: 242898.156250
Epoch [1/1], Batch [871], Loss: 249370.859375
Epoch [1/1], Batch [881], Loss: 244333.671875
Epoch [1/1], Batch [891], Loss: 249059.343750
Epoch [1/1], Batch [901], Loss: 240120.500000
Epoch [1/1], Batch [911], Loss: 249717.781250
Epoch [1/1], Batch [921], Loss: 246752.937500
Epoch [1/1], Batch [931], Loss: 250137.875000
Epoch [1/1], Batch [941], Loss: 239857.843750
Epoch [1/1], Batch [951], Loss: 258568.187500
Epoch [1/1], Batch [961], Loss: 250701.406250
Epoch [1/1], Batch [971], Loss: 245948.562500
Epoch [1/1], Batch [981], Loss: 256126.968750
Epoch [1/1], Batch [991], Loss: 246191.250000
Epoch [1/1], Batch [1001], Loss: 251014.796875
Epoch [1/1], Batch [1011], Loss: 244974.875000
Epoch [1/1], Batch [1021], Loss: 254440.906250
Epoch [1/1], Batch [1031], Loss: 256580.546875
Epoch [1/1], Batch [1041], Loss: 233560.046875
Epoch [1/1], Batch [1051], Loss: 248039.187500
Epoch [1/1], Batch [1061], Loss: 241314.656250
Epoch [1/1], Batch [1071], Loss: 250095.937500
Epoch [1/1], Batch [1081], Loss: 244389.906250
Epoch [1/1], Batch [1091], Loss: 259333.375000
Epoch [1/1], Batch [1101], Loss: 235294.328125
Epoch [1/1], Batch [1111], Loss: 257392.562500
Epoch [1/1], Batch [1121], Loss: 248391.515625
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 246645.5510
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 243071.8551
Elapsed time: 3261.08 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 244820.5273
Elapsed time: 3284.70 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 295182.312500
Epoch [1/1], Batch [11], Loss: 290164.062500
Epoch [1/1], Batch [21], Loss: 281312.812500
Epoch [1/1], Batch [31], Loss: 291090.625000
Epoch [1/1], Batch [41], Loss: 280327.906250
Epoch [1/1], Batch [51], Loss: 296936.187500
Epoch [1/1], Batch [61], Loss: 290916.437500
Epoch [1/1], Batch [71], Loss: 284303.625000
Epoch [1/1], Batch [81], Loss: 287999.375000
Epoch [1/1], Batch [91], Loss: 292016.093750
Epoch [1/1], Batch [101], Loss: 288491.156250
Epoch [1/1], Batch [111], Loss: 293174.093750
Epoch [1/1], Batch [121], Loss: 284216.656250
Epoch [1/1], Batch [131], Loss: 282718.656250
Epoch [1/1], Batch [141], Loss: 296998.437500
Epoch [1/1], Batch [151], Loss: 276714.718750
Epoch [1/1], Batch [161], Loss: 283623.250000
Epoch [1/1], Batch [171], Loss: 298116.750000
Epoch [1/1], Batch [181], Loss: 281334.375000
Epoch [1/1], Batch [191], Loss: 285783.500000
Epoch [1/1], Batch [201], Loss: 291609.781250
Epoch [1/1], Batch [211], Loss: 287847.000000
Epoch [1/1], Batch [221], Loss: 290654.500000
Epoch [1/1], Batch [231], Loss: 283453.031250
Epoch [1/1], Batch [241], Loss: 296782.437500
Epoch [1/1], Batch [251], Loss: 283950.812500
Epoch [1/1], Batch [261], Loss: 275445.468750
Epoch [1/1], Batch [271], Loss: 292691.593750
Epoch [1/1], Batch [281], Loss: 287938.062500
Epoch [1/1], Batch [291], Loss: 294521.375000
Epoch [1/1], Batch [301], Loss: 303682.093750
Epoch [1/1], Batch [311], Loss: 288358.218750
Epoch [1/1], Batch [321], Loss: 298164.718750
Epoch [1/1], Batch [331], Loss: 287439.000000
Epoch [1/1], Batch [341], Loss: 289281.468750
Epoch [1/1], Batch [351], Loss: 289420.593750
Epoch [1/1], Batch [361], Loss: 280819.687500
Epoch [1/1], Batch [371], Loss: 303116.718750
Epoch [1/1], Batch [381], Loss: 288419.062500
Epoch [1/1], Batch [391], Loss: 291447.906250
Epoch [1/1], Batch [401], Loss: 293792.812500
Epoch [1/1], Batch [411], Loss: 292009.000000
Epoch [1/1], Batch [421], Loss: 285671.187500
Epoch [1/1], Batch [431], Loss: 279772.875000
Epoch [1/1], Batch [441], Loss: 278888.125000
Epoch [1/1], Batch [451], Loss: 287191.625000
Epoch [1/1], Batch [461], Loss: 282430.625000
Epoch [1/1], Batch [471], Loss: 290729.906250
Epoch [1/1], Batch [481], Loss: 292730.718750
Epoch [1/1], Batch [491], Loss: 293051.000000
Epoch [1/1], Batch [501], Loss: 283135.375000
Epoch [1/1], Batch [511], Loss: 284653.281250
Epoch [1/1], Batch [521], Loss: 289318.875000
Epoch [1/1], Batch [531], Loss: 302173.437500
Epoch [1/1], Batch [541], Loss: 291574.437500
Epoch [1/1], Batch [551], Loss: 293134.750000
Epoch [1/1], Batch [561], Loss: 285695.062500
Epoch [1/1], Batch [571], Loss: 281785.937500
Epoch [1/1], Batch [581], Loss: 295205.312500
Epoch [1/1], Batch [591], Loss: 281762.843750
Epoch [1/1], Batch [601], Loss: 286720.187500
Epoch [1/1], Batch [611], Loss: 294389.562500
Epoch [1/1], Batch [621], Loss: 293227.906250
Epoch [1/1], Batch [631], Loss: 286416.750000
Epoch [1/1], Batch [641], Loss: 281342.593750
Epoch [1/1], Batch [651], Loss: 287386.625000
Epoch [1/1], Batch [661], Loss: 289069.031250
Epoch [1/1], Batch [671], Loss: 304780.562500
Epoch [1/1], Batch [681], Loss: 287529.625000
Epoch [1/1], Batch [691], Loss: 288614.312500
Epoch [1/1], Batch [701], Loss: 287437.593750
Epoch [1/1], Batch [711], Loss: 290879.968750
Epoch [1/1], Batch [721], Loss: 285788.312500
Epoch [1/1], Batch [731], Loss: 278508.250000
Epoch [1/1], Batch [741], Loss: 297063.250000
Epoch [1/1], Batch [751], Loss: 289675.000000
Epoch [1/1], Batch [761], Loss: 289952.937500
Epoch [1/1], Batch [771], Loss: 291335.437500
Epoch [1/1], Batch [781], Loss: 282127.843750
Epoch [1/1], Batch [791], Loss: 292256.000000
Epoch [1/1], Batch [801], Loss: 279833.125000
Epoch [1/1], Batch [811], Loss: 289725.062500
Epoch [1/1], Batch [821], Loss: 293350.937500
Epoch [1/1], Batch [831], Loss: 288872.093750
Epoch [1/1], Batch [841], Loss: 287368.062500
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 288105.1572
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 282968.2141
Elapsed time: 3874.61 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 284332.4443
Elapsed time: 3894.95 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 330251.437500
Epoch [1/1], Batch [11], Loss: 327866.250000
Epoch [1/1], Batch [21], Loss: 332738.062500
Epoch [1/1], Batch [31], Loss: 319149.062500
Epoch [1/1], Batch [41], Loss: 332162.250000
Epoch [1/1], Batch [51], Loss: 345531.250000
Epoch [1/1], Batch [61], Loss: 326967.125000
Epoch [1/1], Batch [71], Loss: 330436.625000
Epoch [1/1], Batch [81], Loss: 339363.937500
Epoch [1/1], Batch [91], Loss: 331204.875000
Epoch [1/1], Batch [101], Loss: 336823.656250
Epoch [1/1], Batch [111], Loss: 330423.468750
Epoch [1/1], Batch [121], Loss: 332168.437500
Epoch [1/1], Batch [131], Loss: 341444.500000
Epoch [1/1], Batch [141], Loss: 326893.000000
Epoch [1/1], Batch [151], Loss: 334297.437500
Epoch [1/1], Batch [161], Loss: 333039.875000
Epoch [1/1], Batch [171], Loss: 313595.312500
Epoch [1/1], Batch [181], Loss: 323663.875000
Epoch [1/1], Batch [191], Loss: 328279.187500
Epoch [1/1], Batch [201], Loss: 329045.750000
Epoch [1/1], Batch [211], Loss: 330540.593750
Epoch [1/1], Batch [221], Loss: 321186.000000
Epoch [1/1], Batch [231], Loss: 334095.468750
Epoch [1/1], Batch [241], Loss: 321444.906250
Epoch [1/1], Batch [251], Loss: 325488.687500
Epoch [1/1], Batch [261], Loss: 342879.750000
Epoch [1/1], Batch [271], Loss: 305394.468750
Epoch [1/1], Batch [281], Loss: 330978.718750
Epoch [1/1], Batch [291], Loss: 323895.031250
Epoch [1/1], Batch [301], Loss: 317318.593750
Epoch [1/1], Batch [311], Loss: 334908.687500
Epoch [1/1], Batch [321], Loss: 323634.687500
Epoch [1/1], Batch [331], Loss: 321790.000000
Epoch [1/1], Batch [341], Loss: 333340.375000
Epoch [1/1], Batch [351], Loss: 347023.937500
Epoch [1/1], Batch [361], Loss: 328584.781250
Epoch [1/1], Batch [371], Loss: 322950.187500
Epoch [1/1], Batch [381], Loss: 334133.218750
Epoch [1/1], Batch [391], Loss: 319505.000000
Epoch [1/1], Batch [401], Loss: 344477.656250
Epoch [1/1], Batch [411], Loss: 330983.968750
Epoch [1/1], Batch [421], Loss: 316242.750000
Epoch [1/1], Batch [431], Loss: 326323.468750
Epoch [1/1], Batch [441], Loss: 327512.593750
Epoch [1/1], Batch [451], Loss: 323277.750000
Epoch [1/1], Batch [461], Loss: 336166.687500
Epoch [1/1], Batch [471], Loss: 335097.093750
Epoch [1/1], Batch [481], Loss: 321519.343750
Epoch [1/1], Batch [491], Loss: 338625.375000
Epoch [1/1], Batch [501], Loss: 316361.437500
Epoch [1/1], Batch [511], Loss: 334597.687500
Epoch [1/1], Batch [521], Loss: 329655.906250
Epoch [1/1], Batch [531], Loss: 328370.625000
Epoch [1/1], Batch [541], Loss: 326554.281250
Epoch [1/1], Batch [551], Loss: 323499.187500
Epoch [1/1], Batch [561], Loss: 325367.875000
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 329558.7389
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 322371.7091
Elapsed time: 4387.76 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 325345.9702
Elapsed time: 4403.07 seconds

Training complete!
Totoal elapsed time: 4403.07 seconds
CUDA is available!
