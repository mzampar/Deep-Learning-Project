Starting job 1013361
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = True,
    transpose = True,
    use_lstm_output = False,
    scheduler = True,
    initial_lr = 0.01,
    gamma = 0.95.

CUDA is available!
Using learning rate scheduler with initial_lr = 0.01 and gamma = 0.95.
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 0.748262
Epoch [1/1], Batch [11], Loss: 0.245762
Epoch [1/1], Batch [21], Loss: 0.167997
Epoch [1/1], Batch [31], Loss: 0.137837
Epoch [1/1], Batch [41], Loss: 0.123719
Epoch [1/1], Batch [51], Loss: 0.113449
Epoch [1/1], Batch [61], Loss: 0.109278
Epoch [1/1], Batch [71], Loss: 0.100895
Epoch [1/1], Batch [81], Loss: 0.102405
Epoch [1/1], Batch [91], Loss: 0.096713
Epoch [1/1], Batch [101], Loss: 0.095927
Epoch [1/1], Batch [111], Loss: 0.094978
Epoch [1/1], Batch [121], Loss: 0.094526
Epoch [1/1], Batch [131], Loss: 0.098759
Epoch [1/1], Batch [141], Loss: 0.091384
Epoch [1/1], Batch [151], Loss: 0.088890
Epoch [1/1], Batch [161], Loss: 0.093003
Epoch [1/1], Batch [171], Loss: 0.092314
Epoch [1/1], Batch [181], Loss: 0.090846
Epoch [1/1], Batch [191], Loss: 0.088225
Epoch [1/1], Batch [201], Loss: 0.088750
Epoch [1/1], Batch [211], Loss: 0.085037
Epoch [1/1], Batch [221], Loss: 0.093369
Epoch [1/1], Batch [231], Loss: 0.091631
Epoch [1/1], Batch [241], Loss: 0.083328
Epoch [1/1], Batch [251], Loss: 0.090299
Epoch [1/1], Batch [261], Loss: 0.087517
Epoch [1/1], Batch [271], Loss: 0.081754
Epoch [1/1], Batch [281], Loss: 0.087107
Epoch [1/1], Batch [291], Loss: 0.090940
Epoch [1/1], Batch [301], Loss: 0.084345
Epoch [1/1], Batch [311], Loss: 0.087282
Epoch [1/1], Batch [321], Loss: 0.084508
Epoch [1/1], Batch [331], Loss: 0.082352
Epoch [1/1], Batch [341], Loss: 0.085918
Epoch [1/1], Batch [351], Loss: 0.086437
Epoch [1/1], Batch [361], Loss: 0.087935
Epoch [1/1], Batch [371], Loss: 0.082524
Epoch [1/1], Batch [381], Loss: 0.087889
Epoch [1/1], Batch [391], Loss: 0.085563
Epoch [1/1], Batch [401], Loss: 0.086717
Epoch [1/1], Batch [411], Loss: 0.082925
Epoch [1/1], Batch [421], Loss: 0.087921
Epoch [1/1], Batch [431], Loss: 0.083835
Epoch [1/1], Batch [441], Loss: 0.080071
Epoch [1/1], Batch [451], Loss: 0.082595
Epoch [1/1], Batch [461], Loss: 0.080847
Epoch [1/1], Batch [471], Loss: 0.081270
Epoch [1/1], Batch [481], Loss: 0.082838
Epoch [1/1], Batch [491], Loss: 0.082024
Epoch [1/1], Batch [501], Loss: 0.080803
Epoch [1/1], Batch [511], Loss: 0.080365
Epoch [1/1], Batch [521], Loss: 0.084986
Epoch [1/1], Batch [531], Loss: 0.082352
Epoch [1/1], Batch [541], Loss: 0.081425
Epoch [1/1], Batch [551], Loss: 0.080212
Epoch [1/1], Batch [561], Loss: 0.080508
Epoch [1/1], Batch [571], Loss: 0.079374
Epoch [1/1], Batch [581], Loss: 0.081026
Epoch [1/1], Batch [591], Loss: 0.080632
Epoch [1/1], Batch [601], Loss: 0.079766
Epoch [1/1], Batch [611], Loss: 0.083575
Epoch [1/1], Batch [621], Loss: 0.081402
Epoch [1/1], Batch [631], Loss: 0.081110
Epoch [1/1], Batch [641], Loss: 0.076007
Epoch [1/1], Batch [651], Loss: 0.078543
Epoch [1/1], Batch [661], Loss: 0.078672
Epoch [1/1], Batch [671], Loss: 0.080186
Epoch [1/1], Batch [681], Loss: 0.080414
Epoch [1/1], Batch [691], Loss: 0.075506
Epoch [1/1], Batch [701], Loss: 0.080057
Epoch [1/1], Batch [711], Loss: 0.077310
Epoch [1/1], Batch [721], Loss: 0.078538
Epoch [1/1], Batch [731], Loss: 0.079491
Epoch [1/1], Batch [741], Loss: 0.083456
Epoch [1/1], Batch [751], Loss: 0.077484
Epoch [1/1], Batch [761], Loss: 0.080560
Epoch [1/1], Batch [771], Loss: 0.079460
Epoch [1/1], Batch [781], Loss: 0.079084
Epoch [1/1], Batch [791], Loss: 0.077987
Epoch [1/1], Batch [801], Loss: 0.078183
Epoch [1/1], Batch [811], Loss: 0.075589
Epoch [1/1], Batch [821], Loss: 0.078817
Epoch [1/1], Batch [831], Loss: 0.078723
Epoch [1/1], Batch [841], Loss: 0.079832
Epoch [1/1], Batch [851], Loss: 0.078685
Epoch [1/1], Batch [861], Loss: 0.077607
Epoch [1/1], Batch [871], Loss: 0.080345
Epoch [1/1], Batch [881], Loss: 0.080396
Epoch [1/1], Batch [891], Loss: 0.073905
Epoch [1/1], Batch [901], Loss: 0.076671
Epoch [1/1], Batch [911], Loss: 0.081883
Epoch [1/1], Batch [921], Loss: 0.080664
Epoch [1/1], Batch [931], Loss: 0.076174
Epoch [1/1], Batch [941], Loss: 0.074066
Epoch [1/1], Batch [951], Loss: 0.077925
Epoch [1/1], Batch [961], Loss: 0.076597
Epoch [1/1], Batch [971], Loss: 0.077334
Epoch [1/1], Batch [981], Loss: 0.076429
Epoch [1/1], Batch [991], Loss: 0.076497
Epoch [1/1], Batch [1001], Loss: 0.076591
Epoch [1/1], Batch [1011], Loss: 0.078505
Epoch [1/1], Batch [1021], Loss: 0.079947
Epoch [1/1], Batch [1031], Loss: 0.075137
Epoch [1/1], Batch [1041], Loss: 0.076048
Epoch [1/1], Batch [1051], Loss: 0.076988
Epoch [1/1], Batch [1061], Loss: 0.076694
Epoch [1/1], Batch [1071], Loss: 0.073169
Epoch [1/1], Batch [1081], Loss: 0.075132
Epoch [1/1], Batch [1091], Loss: 0.079901
Epoch [1/1], Batch [1101], Loss: 0.078336
Epoch [1/1], Batch [1111], Loss: 0.074451
Epoch [1/1], Batch [1121], Loss: 0.075692
Epoch [1/1], Batch [1131], Loss: 0.076020
Epoch [1/1], Batch [1141], Loss: 0.074715
Epoch [1/1], Batch [1151], Loss: 0.076291
Epoch [1/1], Batch [1161], Loss: 0.072915
Epoch [1/1], Batch [1171], Loss: 0.078681
Epoch [1/1], Batch [1181], Loss: 0.073635
Epoch [1/1], Batch [1191], Loss: 0.075615
Epoch [1/1], Batch [1201], Loss: 0.076341
Epoch [1/1], Batch [1211], Loss: 0.076368
Epoch [1/1], Batch [1221], Loss: 0.073441
Epoch [1/1], Batch [1231], Loss: 0.073920
Epoch [1/1], Batch [1241], Loss: 0.076224
Epoch [1/1], Batch [1251], Loss: 0.073454
Epoch [1/1], Batch [1261], Loss: 0.072929
Epoch [1/1], Batch [1271], Loss: 0.078387
Epoch [1/1], Batch [1281], Loss: 0.072112
Epoch [1/1], Batch [1291], Loss: 0.073711
Epoch [1/1], Batch [1301], Loss: 0.074890
Epoch [1/1], Batch [1311], Loss: 0.074074
Epoch [1/1], Batch [1321], Loss: 0.078319
Epoch [1/1], Batch [1331], Loss: 0.075342
Epoch [1/1], Batch [1341], Loss: 0.074634
Epoch [1/1], Batch [1351], Loss: 0.076508
Epoch [1/1], Batch [1361], Loss: 0.073259
Epoch [1/1], Batch [1371], Loss: 0.074040
Epoch [1/1], Batch [1381], Loss: 0.075783
Epoch [1/1], Batch [1391], Loss: 0.076382
Epoch [1/1], Batch [1401], Loss: 0.074074
Epoch [1/1], Batch [1411], Loss: 0.074727
Epoch [1/1], Batch [1421], Loss: 0.072560
Epoch [1/1], Batch [1431], Loss: 0.075658
Epoch [1/1], Batch [1441], Loss: 0.075931
Epoch [1/1], Batch [1451], Loss: 0.070688
Epoch [1/1], Batch [1461], Loss: 0.075565
Epoch [1/1], Batch [1471], Loss: 0.068979
Epoch [1/1], Batch [1481], Loss: 0.075107
Epoch [1/1], Batch [1491], Loss: 0.073806
Epoch [1/1], Batch [1501], Loss: 0.072131
Epoch [1/1], Batch [1511], Loss: 0.078389
Epoch [1/1], Batch [1521], Loss: 0.072014
Epoch [1/1], Batch [1531], Loss: 0.074537
Epoch [1/1], Batch [1541], Loss: 0.071328
Epoch [1/1], Batch [1551], Loss: 0.074373
Epoch [1/1], Batch [1561], Loss: 0.074750
Epoch [1/1], Batch [1571], Loss: 0.073144
Epoch [1/1], Batch [1581], Loss: 0.075073
Epoch [1/1], Batch [1591], Loss: 0.074367
Epoch [1/1], Batch [1601], Loss: 0.072400
Epoch [1/1], Batch [1611], Loss: 0.071750
Epoch [1/1], Batch [1621], Loss: 0.076442
Epoch [1/1], Batch [1631], Loss: 0.074962
Epoch [1/1], Batch [1641], Loss: 0.074353
Epoch [1/1], Batch [1651], Loss: 0.072598
Epoch [1/1], Batch [1661], Loss: 0.071651
Epoch [1/1], Batch [1671], Loss: 0.074156
Epoch [1/1], Batch [1681], Loss: 0.071821
Epoch [1/1], Batch [1691], Loss: 0.071090
Epoch [1/1], Batch [1701], Loss: 0.073311
Epoch [1/1], Batch [1711], Loss: 0.073651
Epoch [1/1], Batch [1721], Loss: 0.068016
Epoch [1/1], Batch [1731], Loss: 0.074576
Epoch [1/1], Batch [1741], Loss: 0.073901
Epoch [1/1], Batch [1751], Loss: 0.077230
Epoch [1/1], Batch [1761], Loss: 0.068128
Epoch [1/1], Batch [1771], Loss: 0.071660
Epoch [1/1], Batch [1781], Loss: 0.072892
Epoch [1/1], Batch [1791], Loss: 0.073304
Epoch [1/1], Batch [1801], Loss: 0.072858
Epoch [1/1], Batch [1811], Loss: 0.067529
Epoch [1/1], Batch [1821], Loss: 0.072021
Epoch [1/1], Batch [1831], Loss: 0.073327
Epoch [1/1], Batch [1841], Loss: 0.074463
Epoch [1/1], Batch [1851], Loss: 0.071381
Epoch [1/1], Batch [1861], Loss: 0.073094
Epoch [1/1], Batch [1871], Loss: 0.072901
Epoch [1/1], Batch [1881], Loss: 0.072378
Epoch [1/1], Batch [1891], Loss: 0.069945
Epoch [1/1], Batch [1901], Loss: 0.071647
Epoch [1/1], Batch [1911], Loss: 0.072868
Epoch [1/1], Batch [1921], Loss: 0.073077
Epoch [1/1], Batch [1931], Loss: 0.073820
Epoch [1/1], Batch [1941], Loss: 0.073711
Epoch [1/1], Batch [1951], Loss: 0.074373
Epoch [1/1], Batch [1961], Loss: 0.072145
Epoch [1/1], Batch [1971], Loss: 0.072813
Epoch [1/1], Batch [1981], Loss: 0.070847
Epoch [1/1], Batch [1991], Loss: 0.073532
Epoch [1/1], Batch [2001], Loss: 0.073681
Epoch [1/1], Batch [2011], Loss: 0.074429
Epoch [1/1], Batch [2021], Loss: 0.077963
Epoch [1/1], Batch [2031], Loss: 0.070192
Epoch [1/1], Batch [2041], Loss: 0.069027
Epoch [1/1], Batch [2051], Loss: 0.072125
Epoch [1/1], Batch [2061], Loss: 0.072931
Epoch [1/1], Batch [2071], Loss: 0.070764
Epoch [1/1], Batch [2081], Loss: 0.073034
Epoch [1/1], Batch [2091], Loss: 0.072044
Epoch [1/1], Batch [2101], Loss: 0.072639
Epoch [1/1], Batch [2111], Loss: 0.071292
Epoch [1/1], Batch [2121], Loss: 0.071884
Epoch [1/1], Batch [2131], Loss: 0.072823
Epoch [1/1], Batch [2141], Loss: 0.070239
Epoch [1/1], Batch [2151], Loss: 0.076554
Epoch [1/1], Batch [2161], Loss: 0.069863
Epoch [1/1], Batch [2171], Loss: 0.070597
Epoch [1/1], Batch [2181], Loss: 0.074375
Epoch [1/1], Batch [2191], Loss: 0.074733
Epoch [1/1], Batch [2201], Loss: 0.072254
Epoch [1/1], Batch [2211], Loss: 0.071913
Epoch [1/1], Batch [2221], Loss: 0.076210
Epoch [1/1], Batch [2231], Loss: 0.071436
Epoch [1/1], Batch [2241], Loss: 0.068825
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 0.0809
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 0.0713
Elapsed time: 505.53 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 0.0722
Elapsed time: 526.38 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 0.088478
Epoch [1/1], Batch [11], Loss: 0.099617
Epoch [1/1], Batch [21], Loss: 0.093431
Epoch [1/1], Batch [31], Loss: 0.087771
Epoch [1/1], Batch [41], Loss: 0.082721
Epoch [1/1], Batch [51], Loss: 0.082796
Epoch [1/1], Batch [61], Loss: 0.080736
Epoch [1/1], Batch [71], Loss: 0.077279
Epoch [1/1], Batch [81], Loss: 0.079745
Epoch [1/1], Batch [91], Loss: 0.078092
Epoch [1/1], Batch [101], Loss: 0.078892
Epoch [1/1], Batch [111], Loss: 0.081426
Epoch [1/1], Batch [121], Loss: 0.074732
Epoch [1/1], Batch [131], Loss: 0.077390
Epoch [1/1], Batch [141], Loss: 0.076190
Epoch [1/1], Batch [151], Loss: 0.075908
Epoch [1/1], Batch [161], Loss: 0.080216
Epoch [1/1], Batch [171], Loss: 0.078240
Epoch [1/1], Batch [181], Loss: 0.077845
Epoch [1/1], Batch [191], Loss: 0.077131
Epoch [1/1], Batch [201], Loss: 0.075049
Epoch [1/1], Batch [211], Loss: 0.080551
Epoch [1/1], Batch [221], Loss: 0.074120
Epoch [1/1], Batch [231], Loss: 0.077643
Epoch [1/1], Batch [241], Loss: 0.081390
Epoch [1/1], Batch [251], Loss: 0.076892
Epoch [1/1], Batch [261], Loss: 0.074820
Epoch [1/1], Batch [271], Loss: 0.074817
Epoch [1/1], Batch [281], Loss: 0.073783
Epoch [1/1], Batch [291], Loss: 0.074756
Epoch [1/1], Batch [301], Loss: 0.076564
Epoch [1/1], Batch [311], Loss: 0.074351
Epoch [1/1], Batch [321], Loss: 0.075629
Epoch [1/1], Batch [331], Loss: 0.078956
Epoch [1/1], Batch [341], Loss: 0.077476
Epoch [1/1], Batch [351], Loss: 0.074109
Epoch [1/1], Batch [361], Loss: 0.073455
Epoch [1/1], Batch [371], Loss: 0.071593
Epoch [1/1], Batch [381], Loss: 0.072217
Epoch [1/1], Batch [391], Loss: 0.078851
Epoch [1/1], Batch [401], Loss: 0.077917
Epoch [1/1], Batch [411], Loss: 0.070304
Epoch [1/1], Batch [421], Loss: 0.071727
Epoch [1/1], Batch [431], Loss: 0.077040
Epoch [1/1], Batch [441], Loss: 0.074987
Epoch [1/1], Batch [451], Loss: 0.072163
Epoch [1/1], Batch [461], Loss: 0.074750
Epoch [1/1], Batch [471], Loss: 0.074685
Epoch [1/1], Batch [481], Loss: 0.073206
Epoch [1/1], Batch [491], Loss: 0.073715
Epoch [1/1], Batch [501], Loss: 0.074997
Epoch [1/1], Batch [511], Loss: 0.076550
Epoch [1/1], Batch [521], Loss: 0.077766
Epoch [1/1], Batch [531], Loss: 0.072212
Epoch [1/1], Batch [541], Loss: 0.076596
Epoch [1/1], Batch [551], Loss: 0.076037
Epoch [1/1], Batch [561], Loss: 0.073729
Epoch [1/1], Batch [571], Loss: 0.076454
Epoch [1/1], Batch [581], Loss: 0.072264
Epoch [1/1], Batch [591], Loss: 0.076959
Epoch [1/1], Batch [601], Loss: 0.073778
Epoch [1/1], Batch [611], Loss: 0.077707
Epoch [1/1], Batch [621], Loss: 0.075166
Epoch [1/1], Batch [631], Loss: 0.068538
Epoch [1/1], Batch [641], Loss: 0.077621
Epoch [1/1], Batch [651], Loss: 0.072851
Epoch [1/1], Batch [661], Loss: 0.068859
Epoch [1/1], Batch [671], Loss: 0.069863
Epoch [1/1], Batch [681], Loss: 0.075744
Epoch [1/1], Batch [691], Loss: 0.074732
Epoch [1/1], Batch [701], Loss: 0.077412
Epoch [1/1], Batch [711], Loss: 0.068029
Epoch [1/1], Batch [721], Loss: 0.072683
Epoch [1/1], Batch [731], Loss: 0.075324
Epoch [1/1], Batch [741], Loss: 0.071026
Epoch [1/1], Batch [751], Loss: 0.073494
Epoch [1/1], Batch [761], Loss: 0.075618
Epoch [1/1], Batch [771], Loss: 0.069392
Epoch [1/1], Batch [781], Loss: 0.072555
Epoch [1/1], Batch [791], Loss: 0.074970
Epoch [1/1], Batch [801], Loss: 0.070977
Epoch [1/1], Batch [811], Loss: 0.076438
Epoch [1/1], Batch [821], Loss: 0.071827
Epoch [1/1], Batch [831], Loss: 0.073025
Epoch [1/1], Batch [841], Loss: 0.070168
Epoch [1/1], Batch [851], Loss: 0.072123
Epoch [1/1], Batch [861], Loss: 0.072834
Epoch [1/1], Batch [871], Loss: 0.067831
Epoch [1/1], Batch [881], Loss: 0.071822
Epoch [1/1], Batch [891], Loss: 0.070400
Epoch [1/1], Batch [901], Loss: 0.069774
Epoch [1/1], Batch [911], Loss: 0.070115
Epoch [1/1], Batch [921], Loss: 0.070266
Epoch [1/1], Batch [931], Loss: 0.066701
Epoch [1/1], Batch [941], Loss: 0.072737
Epoch [1/1], Batch [951], Loss: 0.071927
Epoch [1/1], Batch [961], Loss: 0.070476
Epoch [1/1], Batch [971], Loss: 0.069905
Epoch [1/1], Batch [981], Loss: 0.071822
Epoch [1/1], Batch [991], Loss: 0.075889
Epoch [1/1], Batch [1001], Loss: 0.073534
Epoch [1/1], Batch [1011], Loss: 0.073036
Epoch [1/1], Batch [1021], Loss: 0.071023
Epoch [1/1], Batch [1031], Loss: 0.068945
Epoch [1/1], Batch [1041], Loss: 0.070552
Epoch [1/1], Batch [1051], Loss: 0.075886
Epoch [1/1], Batch [1061], Loss: 0.067610
Epoch [1/1], Batch [1071], Loss: 0.071486
Epoch [1/1], Batch [1081], Loss: 0.073876
Epoch [1/1], Batch [1091], Loss: 0.069088
Epoch [1/1], Batch [1101], Loss: 0.067748
Epoch [1/1], Batch [1111], Loss: 0.069959
Epoch [1/1], Batch [1121], Loss: 0.068574
Epoch [1/1], Batch [1131], Loss: 0.072081
Epoch [1/1], Batch [1141], Loss: 0.074318
Epoch [1/1], Batch [1151], Loss: 0.069157
Epoch [1/1], Batch [1161], Loss: 0.070432
Epoch [1/1], Batch [1171], Loss: 0.069727
Epoch [1/1], Batch [1181], Loss: 0.067949
Epoch [1/1], Batch [1191], Loss: 0.073611
Epoch [1/1], Batch [1201], Loss: 0.071243
Epoch [1/1], Batch [1211], Loss: 0.069799
Epoch [1/1], Batch [1221], Loss: 0.071161
Epoch [1/1], Batch [1231], Loss: 0.071131
Epoch [1/1], Batch [1241], Loss: 0.070927
Epoch [1/1], Batch [1251], Loss: 0.069681
Epoch [1/1], Batch [1261], Loss: 0.066972
Epoch [1/1], Batch [1271], Loss: 0.071429
Epoch [1/1], Batch [1281], Loss: 0.068685
Epoch [1/1], Batch [1291], Loss: 0.072572
Epoch [1/1], Batch [1301], Loss: 0.071512
Epoch [1/1], Batch [1311], Loss: 0.065550
Epoch [1/1], Batch [1321], Loss: 0.067788
Epoch [1/1], Batch [1331], Loss: 0.071226
Epoch [1/1], Batch [1341], Loss: 0.071214
Epoch [1/1], Batch [1351], Loss: 0.072052
Epoch [1/1], Batch [1361], Loss: 0.071945
Epoch [1/1], Batch [1371], Loss: 0.069668
Epoch [1/1], Batch [1381], Loss: 0.070455
Epoch [1/1], Batch [1391], Loss: 0.070030
Epoch [1/1], Batch [1401], Loss: 0.072021
Epoch [1/1], Batch [1411], Loss: 0.068844
Epoch [1/1], Batch [1421], Loss: 0.072845
Epoch [1/1], Batch [1431], Loss: 0.070964
Epoch [1/1], Batch [1441], Loss: 0.074433
Epoch [1/1], Batch [1451], Loss: 0.070338
Epoch [1/1], Batch [1461], Loss: 0.073146
Epoch [1/1], Batch [1471], Loss: 0.070908
Epoch [1/1], Batch [1481], Loss: 0.068341
Epoch [1/1], Batch [1491], Loss: 0.071185
Epoch [1/1], Batch [1501], Loss: 0.066156
Epoch [1/1], Batch [1511], Loss: 0.070392
Epoch [1/1], Batch [1521], Loss: 0.070331
Epoch [1/1], Batch [1531], Loss: 0.071503
Epoch [1/1], Batch [1541], Loss: 0.071209
Epoch [1/1], Batch [1551], Loss: 0.069741
Epoch [1/1], Batch [1561], Loss: 0.067007
Epoch [1/1], Batch [1571], Loss: 0.075120
Epoch [1/1], Batch [1581], Loss: 0.068704
Epoch [1/1], Batch [1591], Loss: 0.070135
Epoch [1/1], Batch [1601], Loss: 0.070248
Epoch [1/1], Batch [1611], Loss: 0.069159
Epoch [1/1], Batch [1621], Loss: 0.071089
Epoch [1/1], Batch [1631], Loss: 0.067824
Epoch [1/1], Batch [1641], Loss: 0.070643
Epoch [1/1], Batch [1651], Loss: 0.069552
Epoch [1/1], Batch [1661], Loss: 0.068966
Epoch [1/1], Batch [1671], Loss: 0.067716
Epoch [1/1], Batch [1681], Loss: 0.070560
Epoch [1/1], Batch [1691], Loss: 0.069738
Epoch [1/1], Batch [1701], Loss: 0.069549
Epoch [1/1], Batch [1711], Loss: 0.073166
Epoch [1/1], Batch [1721], Loss: 0.069187
Epoch [1/1], Batch [1731], Loss: 0.072572
Epoch [1/1], Batch [1741], Loss: 0.071374
Epoch [1/1], Batch [1751], Loss: 0.069732
Epoch [1/1], Batch [1761], Loss: 0.067456
Epoch [1/1], Batch [1771], Loss: 0.067225
Epoch [1/1], Batch [1781], Loss: 0.072826
Epoch [1/1], Batch [1791], Loss: 0.069535
Epoch [1/1], Batch [1801], Loss: 0.071147
Epoch [1/1], Batch [1811], Loss: 0.067833
Epoch [1/1], Batch [1821], Loss: 0.068344
Epoch [1/1], Batch [1831], Loss: 0.068181
Epoch [1/1], Batch [1841], Loss: 0.068969
Epoch [1/1], Batch [1851], Loss: 0.068004
Epoch [1/1], Batch [1861], Loss: 0.066726
Epoch [1/1], Batch [1871], Loss: 0.068572
Epoch [1/1], Batch [1881], Loss: 0.068109
Epoch [1/1], Batch [1891], Loss: 0.066822
Epoch [1/1], Batch [1901], Loss: 0.065119
Epoch [1/1], Batch [1911], Loss: 0.071034
Epoch [1/1], Batch [1921], Loss: 0.071654
Epoch [1/1], Batch [1931], Loss: 0.071233
Epoch [1/1], Batch [1941], Loss: 0.067521
Epoch [1/1], Batch [1951], Loss: 0.069834
Epoch [1/1], Batch [1961], Loss: 0.068196
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 0.0730
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 0.0675
Elapsed time: 1249.93 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 0.0691
Elapsed time: 1274.95 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 0.076592
Epoch [1/1], Batch [11], Loss: 0.079933
Epoch [1/1], Batch [21], Loss: 0.081518
Epoch [1/1], Batch [31], Loss: 0.073025
Epoch [1/1], Batch [41], Loss: 0.073737
Epoch [1/1], Batch [51], Loss: 0.073319
Epoch [1/1], Batch [61], Loss: 0.076062
Epoch [1/1], Batch [71], Loss: 0.073370
Epoch [1/1], Batch [81], Loss: 0.070983
Epoch [1/1], Batch [91], Loss: 0.069516
Epoch [1/1], Batch [101], Loss: 0.070781
Epoch [1/1], Batch [111], Loss: 0.073079
Epoch [1/1], Batch [121], Loss: 0.068812
Epoch [1/1], Batch [131], Loss: 0.073443
Epoch [1/1], Batch [141], Loss: 0.072073
Epoch [1/1], Batch [151], Loss: 0.071872
Epoch [1/1], Batch [161], Loss: 0.070114
Epoch [1/1], Batch [171], Loss: 0.072878
Epoch [1/1], Batch [181], Loss: 0.075044
Epoch [1/1], Batch [191], Loss: 0.073200
Epoch [1/1], Batch [201], Loss: 0.069140
Epoch [1/1], Batch [211], Loss: 0.073216
Epoch [1/1], Batch [221], Loss: 0.071599
Epoch [1/1], Batch [231], Loss: 0.072332
Epoch [1/1], Batch [241], Loss: 0.068898
Epoch [1/1], Batch [251], Loss: 0.070547
Epoch [1/1], Batch [261], Loss: 0.067062
Epoch [1/1], Batch [271], Loss: 0.069617
Epoch [1/1], Batch [281], Loss: 0.070243
Epoch [1/1], Batch [291], Loss: 0.069226
Epoch [1/1], Batch [301], Loss: 0.070655
Epoch [1/1], Batch [311], Loss: 0.070105
Epoch [1/1], Batch [321], Loss: 0.068498
Epoch [1/1], Batch [331], Loss: 0.069699
Epoch [1/1], Batch [341], Loss: 0.072520
Epoch [1/1], Batch [351], Loss: 0.069339
Epoch [1/1], Batch [361], Loss: 0.073592
Epoch [1/1], Batch [371], Loss: 0.071374
Epoch [1/1], Batch [381], Loss: 0.070501
Epoch [1/1], Batch [391], Loss: 0.068395
Epoch [1/1], Batch [401], Loss: 0.073182
Epoch [1/1], Batch [411], Loss: 0.070286
Epoch [1/1], Batch [421], Loss: 0.071665
Epoch [1/1], Batch [431], Loss: 0.072448
Epoch [1/1], Batch [441], Loss: 0.071738
Epoch [1/1], Batch [451], Loss: 0.073731
Epoch [1/1], Batch [461], Loss: 0.071987
Epoch [1/1], Batch [471], Loss: 0.068673
Epoch [1/1], Batch [481], Loss: 0.067471
Epoch [1/1], Batch [491], Loss: 0.072268
Epoch [1/1], Batch [501], Loss: 0.071233
Epoch [1/1], Batch [511], Loss: 0.068609
Epoch [1/1], Batch [521], Loss: 0.070785
Epoch [1/1], Batch [531], Loss: 0.070825
Epoch [1/1], Batch [541], Loss: 0.069592
Epoch [1/1], Batch [551], Loss: 0.068898
Epoch [1/1], Batch [561], Loss: 0.070783
Epoch [1/1], Batch [571], Loss: 0.070977
Epoch [1/1], Batch [581], Loss: 0.069395
Epoch [1/1], Batch [591], Loss: 0.070399
Epoch [1/1], Batch [601], Loss: 0.070865
Epoch [1/1], Batch [611], Loss: 0.071888
Epoch [1/1], Batch [621], Loss: 0.071128
Epoch [1/1], Batch [631], Loss: 0.073283
Epoch [1/1], Batch [641], Loss: 0.068569
Epoch [1/1], Batch [651], Loss: 0.072357
Epoch [1/1], Batch [661], Loss: 0.069888
Epoch [1/1], Batch [671], Loss: 0.073765
Epoch [1/1], Batch [681], Loss: 0.070095
Epoch [1/1], Batch [691], Loss: 0.069826
Epoch [1/1], Batch [701], Loss: 0.067441
Epoch [1/1], Batch [711], Loss: 0.073546
Epoch [1/1], Batch [721], Loss: 0.070780
Epoch [1/1], Batch [731], Loss: 0.071138
Epoch [1/1], Batch [741], Loss: 0.071740
Epoch [1/1], Batch [751], Loss: 0.071188
Epoch [1/1], Batch [761], Loss: 0.071738
Epoch [1/1], Batch [771], Loss: 0.067464
Epoch [1/1], Batch [781], Loss: 0.070446
Epoch [1/1], Batch [791], Loss: 0.068807
Epoch [1/1], Batch [801], Loss: 0.066695
Epoch [1/1], Batch [811], Loss: 0.068382
Epoch [1/1], Batch [821], Loss: 0.067471
Epoch [1/1], Batch [831], Loss: 0.068386
Epoch [1/1], Batch [841], Loss: 0.068345
Epoch [1/1], Batch [851], Loss: 0.069611
Epoch [1/1], Batch [861], Loss: 0.072474
Epoch [1/1], Batch [871], Loss: 0.067900
Epoch [1/1], Batch [881], Loss: 0.073065
Epoch [1/1], Batch [891], Loss: 0.071146
Epoch [1/1], Batch [901], Loss: 0.069350
Epoch [1/1], Batch [911], Loss: 0.068578
Epoch [1/1], Batch [921], Loss: 0.069086
Epoch [1/1], Batch [931], Loss: 0.069946
Epoch [1/1], Batch [941], Loss: 0.071438
Epoch [1/1], Batch [951], Loss: 0.069967
Epoch [1/1], Batch [961], Loss: 0.065441
Epoch [1/1], Batch [971], Loss: 0.071214
Epoch [1/1], Batch [981], Loss: 0.070935
Epoch [1/1], Batch [991], Loss: 0.071090
Epoch [1/1], Batch [1001], Loss: 0.072855
Epoch [1/1], Batch [1011], Loss: 0.067227
Epoch [1/1], Batch [1021], Loss: 0.069363
Epoch [1/1], Batch [1031], Loss: 0.066188
Epoch [1/1], Batch [1041], Loss: 0.071771
Epoch [1/1], Batch [1051], Loss: 0.068709
Epoch [1/1], Batch [1061], Loss: 0.068881
Epoch [1/1], Batch [1071], Loss: 0.071664
Epoch [1/1], Batch [1081], Loss: 0.070099
Epoch [1/1], Batch [1091], Loss: 0.070085
Epoch [1/1], Batch [1101], Loss: 0.069544
Epoch [1/1], Batch [1111], Loss: 0.067879
Epoch [1/1], Batch [1121], Loss: 0.067533
Epoch [1/1], Batch [1131], Loss: 0.068886
Epoch [1/1], Batch [1141], Loss: 0.068726
Epoch [1/1], Batch [1151], Loss: 0.070152
Epoch [1/1], Batch [1161], Loss: 0.072458
Epoch [1/1], Batch [1171], Loss: 0.065937
Epoch [1/1], Batch [1181], Loss: 0.072078
Epoch [1/1], Batch [1191], Loss: 0.070240
Epoch [1/1], Batch [1201], Loss: 0.069393
Epoch [1/1], Batch [1211], Loss: 0.070376
Epoch [1/1], Batch [1221], Loss: 0.068020
Epoch [1/1], Batch [1231], Loss: 0.066216
Epoch [1/1], Batch [1241], Loss: 0.070333
Epoch [1/1], Batch [1251], Loss: 0.069126
Epoch [1/1], Batch [1261], Loss: 0.068197
Epoch [1/1], Batch [1271], Loss: 0.069562
Epoch [1/1], Batch [1281], Loss: 0.068211
Epoch [1/1], Batch [1291], Loss: 0.068181
Epoch [1/1], Batch [1301], Loss: 0.074186
Epoch [1/1], Batch [1311], Loss: 0.069595
Epoch [1/1], Batch [1321], Loss: 0.067763
Epoch [1/1], Batch [1331], Loss: 0.068477
Epoch [1/1], Batch [1341], Loss: 0.066924
Epoch [1/1], Batch [1351], Loss: 0.067519
Epoch [1/1], Batch [1361], Loss: 0.070075
Epoch [1/1], Batch [1371], Loss: 0.066010
Epoch [1/1], Batch [1381], Loss: 0.073843
Epoch [1/1], Batch [1391], Loss: 0.069308
Epoch [1/1], Batch [1401], Loss: 0.069858
Epoch [1/1], Batch [1411], Loss: 0.067718
Epoch [1/1], Batch [1421], Loss: 0.073355
Epoch [1/1], Batch [1431], Loss: 0.073768
Epoch [1/1], Batch [1441], Loss: 0.070166
Epoch [1/1], Batch [1451], Loss: 0.067453
Epoch [1/1], Batch [1461], Loss: 0.065541
Epoch [1/1], Batch [1471], Loss: 0.068111
Epoch [1/1], Batch [1481], Loss: 0.066542
Epoch [1/1], Batch [1491], Loss: 0.070489
Epoch [1/1], Batch [1501], Loss: 0.068950
Epoch [1/1], Batch [1511], Loss: 0.067841
Epoch [1/1], Batch [1521], Loss: 0.068836
Epoch [1/1], Batch [1531], Loss: 0.068958
Epoch [1/1], Batch [1541], Loss: 0.071457
Epoch [1/1], Batch [1551], Loss: 0.067767
Epoch [1/1], Batch [1561], Loss: 0.071324
Epoch [1/1], Batch [1571], Loss: 0.063289
Epoch [1/1], Batch [1581], Loss: 0.070365
Epoch [1/1], Batch [1591], Loss: 0.066439
Epoch [1/1], Batch [1601], Loss: 0.065415
Epoch [1/1], Batch [1611], Loss: 0.064358
Epoch [1/1], Batch [1621], Loss: 0.065716
Epoch [1/1], Batch [1631], Loss: 0.068368
Epoch [1/1], Batch [1641], Loss: 0.066615
Epoch [1/1], Batch [1651], Loss: 0.070879
Epoch [1/1], Batch [1661], Loss: 0.067161
Epoch [1/1], Batch [1671], Loss: 0.068553
Epoch [1/1], Batch [1681], Loss: 0.067595
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 0.0704
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 0.0669
Elapsed time: 1974.44 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 0.0692
Elapsed time: 2001.71 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 0.072248
Epoch [1/1], Batch [11], Loss: 0.079025
Epoch [1/1], Batch [21], Loss: 0.071088
Epoch [1/1], Batch [31], Loss: 0.070693
Epoch [1/1], Batch [41], Loss: 0.072692
Epoch [1/1], Batch [51], Loss: 0.070114
Epoch [1/1], Batch [61], Loss: 0.072142
Epoch [1/1], Batch [71], Loss: 0.072881
Epoch [1/1], Batch [81], Loss: 0.073050
Epoch [1/1], Batch [91], Loss: 0.071707
Epoch [1/1], Batch [101], Loss: 0.074653
Epoch [1/1], Batch [111], Loss: 0.071970
Epoch [1/1], Batch [121], Loss: 0.067217
Epoch [1/1], Batch [131], Loss: 0.071688
Epoch [1/1], Batch [141], Loss: 0.066263
Epoch [1/1], Batch [151], Loss: 0.068121
Epoch [1/1], Batch [161], Loss: 0.073484
Epoch [1/1], Batch [171], Loss: 0.069480
Epoch [1/1], Batch [181], Loss: 0.069136
Epoch [1/1], Batch [191], Loss: 0.069380
Epoch [1/1], Batch [201], Loss: 0.070200
Epoch [1/1], Batch [211], Loss: 0.071336
Epoch [1/1], Batch [221], Loss: 0.070800
Epoch [1/1], Batch [231], Loss: 0.070338
Epoch [1/1], Batch [241], Loss: 0.072030
Epoch [1/1], Batch [251], Loss: 0.067035
Epoch [1/1], Batch [261], Loss: 0.073222
Epoch [1/1], Batch [271], Loss: 0.072519
Epoch [1/1], Batch [281], Loss: 0.069481
Epoch [1/1], Batch [291], Loss: 0.070176
Epoch [1/1], Batch [301], Loss: 0.073188
Epoch [1/1], Batch [311], Loss: 0.070163
Epoch [1/1], Batch [321], Loss: 0.071796
Epoch [1/1], Batch [331], Loss: 0.071395
Epoch [1/1], Batch [341], Loss: 0.073523
Epoch [1/1], Batch [351], Loss: 0.075128
Epoch [1/1], Batch [361], Loss: 0.068275
Epoch [1/1], Batch [371], Loss: 0.068978
Epoch [1/1], Batch [381], Loss: 0.071181
Epoch [1/1], Batch [391], Loss: 0.072880
Epoch [1/1], Batch [401], Loss: 0.070691
Epoch [1/1], Batch [411], Loss: 0.070260
Epoch [1/1], Batch [421], Loss: 0.071871
Epoch [1/1], Batch [431], Loss: 0.074353
Epoch [1/1], Batch [441], Loss: 0.072434
Epoch [1/1], Batch [451], Loss: 0.068422
Epoch [1/1], Batch [461], Loss: 0.072609
Epoch [1/1], Batch [471], Loss: 0.070614
Epoch [1/1], Batch [481], Loss: 0.070484
Epoch [1/1], Batch [491], Loss: 0.068973
Epoch [1/1], Batch [501], Loss: 0.068623
Epoch [1/1], Batch [511], Loss: 0.071185
Epoch [1/1], Batch [521], Loss: 0.069653
Epoch [1/1], Batch [531], Loss: 0.069915
Epoch [1/1], Batch [541], Loss: 0.068175
Epoch [1/1], Batch [551], Loss: 0.066980
Epoch [1/1], Batch [561], Loss: 0.071630
Epoch [1/1], Batch [571], Loss: 0.066062
Epoch [1/1], Batch [581], Loss: 0.069650
Epoch [1/1], Batch [591], Loss: 0.073267
Epoch [1/1], Batch [601], Loss: 0.066944
Epoch [1/1], Batch [611], Loss: 0.069145
Epoch [1/1], Batch [621], Loss: 0.071259
Epoch [1/1], Batch [631], Loss: 0.068492
Epoch [1/1], Batch [641], Loss: 0.072809
Epoch [1/1], Batch [651], Loss: 0.067940
Epoch [1/1], Batch [661], Loss: 0.072708
Epoch [1/1], Batch [671], Loss: 0.069964
Epoch [1/1], Batch [681], Loss: 0.068787
Epoch [1/1], Batch [691], Loss: 0.071371
Epoch [1/1], Batch [701], Loss: 0.069472
Epoch [1/1], Batch [711], Loss: 0.068399
Epoch [1/1], Batch [721], Loss: 0.068419
Epoch [1/1], Batch [731], Loss: 0.066120
Epoch [1/1], Batch [741], Loss: 0.074222
Epoch [1/1], Batch [751], Loss: 0.074553
Epoch [1/1], Batch [761], Loss: 0.071533
Epoch [1/1], Batch [771], Loss: 0.070164
Epoch [1/1], Batch [781], Loss: 0.072889
Epoch [1/1], Batch [791], Loss: 0.069694
Epoch [1/1], Batch [801], Loss: 0.070011
Epoch [1/1], Batch [811], Loss: 0.070104
Epoch [1/1], Batch [821], Loss: 0.069432
Epoch [1/1], Batch [831], Loss: 0.070620
Epoch [1/1], Batch [841], Loss: 0.071761
Epoch [1/1], Batch [851], Loss: 0.065394
Epoch [1/1], Batch [861], Loss: 0.071735
Epoch [1/1], Batch [871], Loss: 0.069365
Epoch [1/1], Batch [881], Loss: 0.069759
Epoch [1/1], Batch [891], Loss: 0.068812
Epoch [1/1], Batch [901], Loss: 0.070479
Epoch [1/1], Batch [911], Loss: 0.071381
Epoch [1/1], Batch [921], Loss: 0.070094
Epoch [1/1], Batch [931], Loss: 0.067909
Epoch [1/1], Batch [941], Loss: 0.068210
Epoch [1/1], Batch [951], Loss: 0.067015
Epoch [1/1], Batch [961], Loss: 0.068354
Epoch [1/1], Batch [971], Loss: 0.072756
Epoch [1/1], Batch [981], Loss: 0.067152
Epoch [1/1], Batch [991], Loss: 0.067761
Epoch [1/1], Batch [1001], Loss: 0.066185
Epoch [1/1], Batch [1011], Loss: 0.071090
Epoch [1/1], Batch [1021], Loss: 0.069190
Epoch [1/1], Batch [1031], Loss: 0.067563
Epoch [1/1], Batch [1041], Loss: 0.067519
Epoch [1/1], Batch [1051], Loss: 0.067343
Epoch [1/1], Batch [1061], Loss: 0.067839
Epoch [1/1], Batch [1071], Loss: 0.069492
Epoch [1/1], Batch [1081], Loss: 0.071594
Epoch [1/1], Batch [1091], Loss: 0.069617
Epoch [1/1], Batch [1101], Loss: 0.068023
Epoch [1/1], Batch [1111], Loss: 0.071474
Epoch [1/1], Batch [1121], Loss: 0.069600
Epoch [1/1], Batch [1131], Loss: 0.069654
Epoch [1/1], Batch [1141], Loss: 0.071772
Epoch [1/1], Batch [1151], Loss: 0.072759
Epoch [1/1], Batch [1161], Loss: 0.072373
Epoch [1/1], Batch [1171], Loss: 0.070441
Epoch [1/1], Batch [1181], Loss: 0.071791
Epoch [1/1], Batch [1191], Loss: 0.066728
Epoch [1/1], Batch [1201], Loss: 0.074730
Epoch [1/1], Batch [1211], Loss: 0.067580
Epoch [1/1], Batch [1221], Loss: 0.072152
Epoch [1/1], Batch [1231], Loss: 0.068734
Epoch [1/1], Batch [1241], Loss: 0.071143
Epoch [1/1], Batch [1251], Loss: 0.069288
Epoch [1/1], Batch [1261], Loss: 0.067705
Epoch [1/1], Batch [1271], Loss: 0.072801
Epoch [1/1], Batch [1281], Loss: 0.067997
Epoch [1/1], Batch [1291], Loss: 0.070805
Epoch [1/1], Batch [1301], Loss: 0.069621
Epoch [1/1], Batch [1311], Loss: 0.071680
Epoch [1/1], Batch [1321], Loss: 0.068237
Epoch [1/1], Batch [1331], Loss: 0.068982
Epoch [1/1], Batch [1341], Loss: 0.066524
Epoch [1/1], Batch [1351], Loss: 0.069666
Epoch [1/1], Batch [1361], Loss: 0.070093
Epoch [1/1], Batch [1371], Loss: 0.070882
Epoch [1/1], Batch [1381], Loss: 0.069496
Epoch [1/1], Batch [1391], Loss: 0.065827
Epoch [1/1], Batch [1401], Loss: 0.066173
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 0.0700
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 0.0666
Elapsed time: 2724.22 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 0.0690
Elapsed time: 2751.77 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 0.072561
Epoch [1/1], Batch [11], Loss: 0.079966
Epoch [1/1], Batch [21], Loss: 0.073672
Epoch [1/1], Batch [31], Loss: 0.073728
Epoch [1/1], Batch [41], Loss: 0.075602
Epoch [1/1], Batch [51], Loss: 0.071518
Epoch [1/1], Batch [61], Loss: 0.071493
Epoch [1/1], Batch [71], Loss: 0.066967
Epoch [1/1], Batch [81], Loss: 0.071293
Epoch [1/1], Batch [91], Loss: 0.071056
Epoch [1/1], Batch [101], Loss: 0.069901
Epoch [1/1], Batch [111], Loss: 0.067039
Epoch [1/1], Batch [121], Loss: 0.068664
Epoch [1/1], Batch [131], Loss: 0.073858
Epoch [1/1], Batch [141], Loss: 0.069882
Epoch [1/1], Batch [151], Loss: 0.068537
Epoch [1/1], Batch [161], Loss: 0.070553
Epoch [1/1], Batch [171], Loss: 0.068845
Epoch [1/1], Batch [181], Loss: 0.071760
Epoch [1/1], Batch [191], Loss: 0.068766
Epoch [1/1], Batch [201], Loss: 0.067478
Epoch [1/1], Batch [211], Loss: 0.072186
Epoch [1/1], Batch [221], Loss: 0.066838
Epoch [1/1], Batch [231], Loss: 0.069688
Epoch [1/1], Batch [241], Loss: 0.069275
Epoch [1/1], Batch [251], Loss: 0.066921
Epoch [1/1], Batch [261], Loss: 0.070268
Epoch [1/1], Batch [271], Loss: 0.068676
Epoch [1/1], Batch [281], Loss: 0.070455
Epoch [1/1], Batch [291], Loss: 0.072650
Epoch [1/1], Batch [301], Loss: 0.068888
Epoch [1/1], Batch [311], Loss: 0.072028
Epoch [1/1], Batch [321], Loss: 0.072595
Epoch [1/1], Batch [331], Loss: 0.070718
Epoch [1/1], Batch [341], Loss: 0.072804
Epoch [1/1], Batch [351], Loss: 0.067879
Epoch [1/1], Batch [361], Loss: 0.071485
Epoch [1/1], Batch [371], Loss: 0.069839
Epoch [1/1], Batch [381], Loss: 0.072901
Epoch [1/1], Batch [391], Loss: 0.072402
Epoch [1/1], Batch [401], Loss: 0.071044
Epoch [1/1], Batch [411], Loss: 0.072390
Epoch [1/1], Batch [421], Loss: 0.073183
Epoch [1/1], Batch [431], Loss: 0.072346
Epoch [1/1], Batch [441], Loss: 0.065462
Epoch [1/1], Batch [451], Loss: 0.070582
Epoch [1/1], Batch [461], Loss: 0.067023
Epoch [1/1], Batch [471], Loss: 0.070581
Epoch [1/1], Batch [481], Loss: 0.069157
Epoch [1/1], Batch [491], Loss: 0.070120
Epoch [1/1], Batch [501], Loss: 0.069244
Epoch [1/1], Batch [511], Loss: 0.071424
Epoch [1/1], Batch [521], Loss: 0.066901
Epoch [1/1], Batch [531], Loss: 0.072166
Epoch [1/1], Batch [541], Loss: 0.070742
Epoch [1/1], Batch [551], Loss: 0.069895
Epoch [1/1], Batch [561], Loss: 0.068572
Epoch [1/1], Batch [571], Loss: 0.071360
Epoch [1/1], Batch [581], Loss: 0.065989
Epoch [1/1], Batch [591], Loss: 0.072768
Epoch [1/1], Batch [601], Loss: 0.067362
Epoch [1/1], Batch [611], Loss: 0.069715
Epoch [1/1], Batch [621], Loss: 0.068617
Epoch [1/1], Batch [631], Loss: 0.071873
Epoch [1/1], Batch [641], Loss: 0.066025
Epoch [1/1], Batch [651], Loss: 0.072156
Epoch [1/1], Batch [661], Loss: 0.071584
Epoch [1/1], Batch [671], Loss: 0.073462
Epoch [1/1], Batch [681], Loss: 0.069355
Epoch [1/1], Batch [691], Loss: 0.070941
Epoch [1/1], Batch [701], Loss: 0.072500
Epoch [1/1], Batch [711], Loss: 0.068042
Epoch [1/1], Batch [721], Loss: 0.072242
Epoch [1/1], Batch [731], Loss: 0.069835
Epoch [1/1], Batch [741], Loss: 0.069044
Epoch [1/1], Batch [751], Loss: 0.067447
Epoch [1/1], Batch [761], Loss: 0.070948
Epoch [1/1], Batch [771], Loss: 0.070481
Epoch [1/1], Batch [781], Loss: 0.067723
Epoch [1/1], Batch [791], Loss: 0.067999
Epoch [1/1], Batch [801], Loss: 0.067473
Epoch [1/1], Batch [811], Loss: 0.070925
Epoch [1/1], Batch [821], Loss: 0.072718
Epoch [1/1], Batch [831], Loss: 0.069321
Epoch [1/1], Batch [841], Loss: 0.069749
Epoch [1/1], Batch [851], Loss: 0.070291
Epoch [1/1], Batch [861], Loss: 0.067871
Epoch [1/1], Batch [871], Loss: 0.068990
Epoch [1/1], Batch [881], Loss: 0.068119
Epoch [1/1], Batch [891], Loss: 0.068366
Epoch [1/1], Batch [901], Loss: 0.068072
Epoch [1/1], Batch [911], Loss: 0.066560
Epoch [1/1], Batch [921], Loss: 0.065189
Epoch [1/1], Batch [931], Loss: 0.071095
Epoch [1/1], Batch [941], Loss: 0.066283
Epoch [1/1], Batch [951], Loss: 0.068238
Epoch [1/1], Batch [961], Loss: 0.070666
Epoch [1/1], Batch [971], Loss: 0.072859
Epoch [1/1], Batch [981], Loss: 0.070780
Epoch [1/1], Batch [991], Loss: 0.067229
Epoch [1/1], Batch [1001], Loss: 0.070835
Epoch [1/1], Batch [1011], Loss: 0.070098
Epoch [1/1], Batch [1021], Loss: 0.068709
Epoch [1/1], Batch [1031], Loss: 0.070909
Epoch [1/1], Batch [1041], Loss: 0.068012
Epoch [1/1], Batch [1051], Loss: 0.071068
Epoch [1/1], Batch [1061], Loss: 0.070533
Epoch [1/1], Batch [1071], Loss: 0.069690
Epoch [1/1], Batch [1081], Loss: 0.071291
Epoch [1/1], Batch [1091], Loss: 0.071133
Epoch [1/1], Batch [1101], Loss: 0.073189
Epoch [1/1], Batch [1111], Loss: 0.067971
Epoch [1/1], Batch [1121], Loss: 0.069352
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 0.0700
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 0.0671
Elapsed time: 3434.93 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 0.0704
Elapsed time: 3460.78 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 0.072906
Epoch [1/1], Batch [11], Loss: 0.074201
Epoch [1/1], Batch [21], Loss: 0.073670
Epoch [1/1], Batch [31], Loss: 0.070253
Epoch [1/1], Batch [41], Loss: 0.072397
Epoch [1/1], Batch [51], Loss: 0.071136
Epoch [1/1], Batch [61], Loss: 0.068530
Epoch [1/1], Batch [71], Loss: 0.069762
Epoch [1/1], Batch [81], Loss: 0.070473
Epoch [1/1], Batch [91], Loss: 0.068376
Epoch [1/1], Batch [101], Loss: 0.066366
Epoch [1/1], Batch [111], Loss: 0.069542
Epoch [1/1], Batch [121], Loss: 0.069742
Epoch [1/1], Batch [131], Loss: 0.067224
Epoch [1/1], Batch [141], Loss: 0.066772
Epoch [1/1], Batch [151], Loss: 0.068266
Epoch [1/1], Batch [161], Loss: 0.067643
Epoch [1/1], Batch [171], Loss: 0.066600
Epoch [1/1], Batch [181], Loss: 0.073485
Epoch [1/1], Batch [191], Loss: 0.071947
Epoch [1/1], Batch [201], Loss: 0.068798
Epoch [1/1], Batch [211], Loss: 0.072110
Epoch [1/1], Batch [221], Loss: 0.070220
Epoch [1/1], Batch [231], Loss: 0.072252
Epoch [1/1], Batch [241], Loss: 0.070177
Epoch [1/1], Batch [251], Loss: 0.069469
Epoch [1/1], Batch [261], Loss: 0.069202
Epoch [1/1], Batch [271], Loss: 0.070985
Epoch [1/1], Batch [281], Loss: 0.072159
Epoch [1/1], Batch [291], Loss: 0.072096
Epoch [1/1], Batch [301], Loss: 0.071055
Epoch [1/1], Batch [311], Loss: 0.065955
Epoch [1/1], Batch [321], Loss: 0.073246
Epoch [1/1], Batch [331], Loss: 0.072270
Epoch [1/1], Batch [341], Loss: 0.069086
Epoch [1/1], Batch [351], Loss: 0.071447
Epoch [1/1], Batch [361], Loss: 0.067696
Epoch [1/1], Batch [371], Loss: 0.071939
Epoch [1/1], Batch [381], Loss: 0.070351
Epoch [1/1], Batch [391], Loss: 0.069563
Epoch [1/1], Batch [401], Loss: 0.068231
Epoch [1/1], Batch [411], Loss: 0.069673
Epoch [1/1], Batch [421], Loss: 0.069534
Epoch [1/1], Batch [431], Loss: 0.072744
Epoch [1/1], Batch [441], Loss: 0.068696
Epoch [1/1], Batch [451], Loss: 0.066485
Epoch [1/1], Batch [461], Loss: 0.069383
Epoch [1/1], Batch [471], Loss: 0.070741
Epoch [1/1], Batch [481], Loss: 0.067827
Epoch [1/1], Batch [491], Loss: 0.072063
Epoch [1/1], Batch [501], Loss: 0.066684
Epoch [1/1], Batch [511], Loss: 0.069986
Epoch [1/1], Batch [521], Loss: 0.070428
Epoch [1/1], Batch [531], Loss: 0.068699
Epoch [1/1], Batch [541], Loss: 0.076307
Epoch [1/1], Batch [551], Loss: 0.071800
Epoch [1/1], Batch [561], Loss: 0.070532
Epoch [1/1], Batch [571], Loss: 0.073093
Epoch [1/1], Batch [581], Loss: 0.071728
Epoch [1/1], Batch [591], Loss: 0.071912
Epoch [1/1], Batch [601], Loss: 0.068172
Epoch [1/1], Batch [611], Loss: 0.071149
Epoch [1/1], Batch [621], Loss: 0.069625
Epoch [1/1], Batch [631], Loss: 0.070110
Epoch [1/1], Batch [641], Loss: 0.072858
Epoch [1/1], Batch [651], Loss: 0.071739
Epoch [1/1], Batch [661], Loss: 0.071077
Epoch [1/1], Batch [671], Loss: 0.068683
Epoch [1/1], Batch [681], Loss: 0.068085
Epoch [1/1], Batch [691], Loss: 0.069324
Epoch [1/1], Batch [701], Loss: 0.069108
Epoch [1/1], Batch [711], Loss: 0.068824
Epoch [1/1], Batch [721], Loss: 0.070466
Epoch [1/1], Batch [731], Loss: 0.070557
Epoch [1/1], Batch [741], Loss: 0.071565
Epoch [1/1], Batch [751], Loss: 0.072551
Epoch [1/1], Batch [761], Loss: 0.068650
Epoch [1/1], Batch [771], Loss: 0.070285
Epoch [1/1], Batch [781], Loss: 0.065765
Epoch [1/1], Batch [791], Loss: 0.070683
Epoch [1/1], Batch [801], Loss: 0.068363
Epoch [1/1], Batch [811], Loss: 0.069744
Epoch [1/1], Batch [821], Loss: 0.068693
Epoch [1/1], Batch [831], Loss: 0.069672
Epoch [1/1], Batch [841], Loss: 0.069278
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 0.0703
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 0.0670
Elapsed time: 4057.12 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 0.0716
Elapsed time: 4079.45 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 0.069774
Epoch [1/1], Batch [11], Loss: 0.084797
Epoch [1/1], Batch [21], Loss: 0.076197
Epoch [1/1], Batch [31], Loss: 0.072736
Epoch [1/1], Batch [41], Loss: 0.070640
Epoch [1/1], Batch [51], Loss: 0.071381
Epoch [1/1], Batch [61], Loss: 0.069864
Epoch [1/1], Batch [71], Loss: 0.071580
Epoch [1/1], Batch [81], Loss: 0.066203
Epoch [1/1], Batch [91], Loss: 0.070113
Epoch [1/1], Batch [101], Loss: 0.069907
Epoch [1/1], Batch [111], Loss: 0.069262
Epoch [1/1], Batch [121], Loss: 0.070713
Epoch [1/1], Batch [131], Loss: 0.068685
Epoch [1/1], Batch [141], Loss: 0.071197
Epoch [1/1], Batch [151], Loss: 0.070314
Epoch [1/1], Batch [161], Loss: 0.068405
Epoch [1/1], Batch [171], Loss: 0.071010
Epoch [1/1], Batch [181], Loss: 0.067617
Epoch [1/1], Batch [191], Loss: 0.071534
Epoch [1/1], Batch [201], Loss: 0.070974
Epoch [1/1], Batch [211], Loss: 0.070956
Epoch [1/1], Batch [221], Loss: 0.067310
Epoch [1/1], Batch [231], Loss: 0.068511
Epoch [1/1], Batch [241], Loss: 0.072073
Epoch [1/1], Batch [251], Loss: 0.070400
Epoch [1/1], Batch [261], Loss: 0.071754
Epoch [1/1], Batch [271], Loss: 0.069763
Epoch [1/1], Batch [281], Loss: 0.068866
Epoch [1/1], Batch [291], Loss: 0.071712
Epoch [1/1], Batch [301], Loss: 0.072808
Epoch [1/1], Batch [311], Loss: 0.070127
Epoch [1/1], Batch [321], Loss: 0.069218
Epoch [1/1], Batch [331], Loss: 0.071739
Epoch [1/1], Batch [341], Loss: 0.071717
Epoch [1/1], Batch [351], Loss: 0.071744
Epoch [1/1], Batch [361], Loss: 0.069838
Epoch [1/1], Batch [371], Loss: 0.068487
Epoch [1/1], Batch [381], Loss: 0.074961
Epoch [1/1], Batch [391], Loss: 0.068648
Epoch [1/1], Batch [401], Loss: 0.069566
Epoch [1/1], Batch [411], Loss: 0.071967
Epoch [1/1], Batch [421], Loss: 0.072069
Epoch [1/1], Batch [431], Loss: 0.074773
Epoch [1/1], Batch [441], Loss: 0.067769
Epoch [1/1], Batch [451], Loss: 0.070027
Epoch [1/1], Batch [461], Loss: 0.071734
Epoch [1/1], Batch [471], Loss: 0.066318
Epoch [1/1], Batch [481], Loss: 0.070704
Epoch [1/1], Batch [491], Loss: 0.069375
Epoch [1/1], Batch [501], Loss: 0.070210
Epoch [1/1], Batch [511], Loss: 0.069674
Epoch [1/1], Batch [521], Loss: 0.070757
Epoch [1/1], Batch [531], Loss: 0.067841
Epoch [1/1], Batch [541], Loss: 0.069903
Epoch [1/1], Batch [551], Loss: 0.072618
Epoch [1/1], Batch [561], Loss: 0.071348
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 0.0711
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 0.0691
Elapsed time: 4565.41 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 0.0729
Elapsed time: 4582.25 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 0.073250
Epoch [1/1], Batch [11], Loss: 0.086429
Epoch [1/1], Batch [21], Loss: 0.077019
Epoch [1/1], Batch [31], Loss: 0.076989
Epoch [1/1], Batch [41], Loss: 0.070195
Epoch [1/1], Batch [51], Loss: 0.070174
Epoch [1/1], Batch [61], Loss: 0.069864
Epoch [1/1], Batch [71], Loss: 0.069959
Epoch [1/1], Batch [81], Loss: 0.072432
Epoch [1/1], Batch [91], Loss: 0.070382
Epoch [1/1], Batch [101], Loss: 0.070331
Epoch [1/1], Batch [111], Loss: 0.071823
Epoch [1/1], Batch [121], Loss: 0.070016
Epoch [1/1], Batch [131], Loss: 0.071208
Epoch [1/1], Batch [141], Loss: 0.071509
Epoch [1/1], Batch [151], Loss: 0.068367
Epoch [1/1], Batch [161], Loss: 0.070118
Epoch [1/1], Batch [171], Loss: 0.067244
Epoch [1/1], Batch [181], Loss: 0.071913
Epoch [1/1], Batch [191], Loss: 0.071762
Epoch [1/1], Batch [201], Loss: 0.069905
Epoch [1/1], Batch [211], Loss: 0.071852
Epoch [1/1], Batch [221], Loss: 0.070597
Epoch [1/1], Batch [231], Loss: 0.073029
Epoch [1/1], Batch [241], Loss: 0.069176
Epoch [1/1], Batch [251], Loss: 0.069096
Epoch [1/1], Batch [261], Loss: 0.068908
Epoch [1/1], Batch [271], Loss: 0.070721
Epoch [1/1], Batch [281], Loss: 0.066238
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 0.0720
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 0.0685
Elapsed time: 4881.86 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 0.0712
Elapsed time: 4891.30 seconds

Training complete!
Totoal elapsed time: 4891.30 seconds
CUDA is available!
