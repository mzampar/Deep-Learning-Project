Starting job 1008756
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 3, 3],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = False,
    transpose = True,
    use_lstm_output = True,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 337336.781250
Epoch [1/1], Batch [11], Loss: 127978.484375
Epoch [1/1], Batch [21], Loss: 104446.351562
Epoch [1/1], Batch [31], Loss: 94144.218750
Epoch [1/1], Batch [41], Loss: 92008.460938
Epoch [1/1], Batch [51], Loss: 91944.757812
Epoch [1/1], Batch [61], Loss: 89301.726562
Epoch [1/1], Batch [71], Loss: 91333.328125
Epoch [1/1], Batch [81], Loss: 89984.101562
Epoch [1/1], Batch [91], Loss: 89280.093750
Epoch [1/1], Batch [101], Loss: 84245.960938
Epoch [1/1], Batch [111], Loss: 88500.273438
Epoch [1/1], Batch [121], Loss: 86907.781250
Epoch [1/1], Batch [131], Loss: 84839.640625
Epoch [1/1], Batch [141], Loss: 82749.265625
Epoch [1/1], Batch [151], Loss: 78727.984375
Epoch [1/1], Batch [161], Loss: 71449.906250
Epoch [1/1], Batch [171], Loss: 64482.730469
Epoch [1/1], Batch [181], Loss: 61103.871094
Epoch [1/1], Batch [191], Loss: 55528.835938
Epoch [1/1], Batch [201], Loss: 55188.648438
Epoch [1/1], Batch [211], Loss: 53044.554688
Epoch [1/1], Batch [221], Loss: 50995.765625
Epoch [1/1], Batch [231], Loss: 51558.585938
Epoch [1/1], Batch [241], Loss: 51079.398438
Epoch [1/1], Batch [251], Loss: 50830.859375
Epoch [1/1], Batch [261], Loss: 49510.777344
Epoch [1/1], Batch [271], Loss: 48866.765625
Epoch [1/1], Batch [281], Loss: 48360.062500
Epoch [1/1], Batch [291], Loss: 50828.515625
Epoch [1/1], Batch [301], Loss: 50384.433594
Epoch [1/1], Batch [311], Loss: 48249.644531
Epoch [1/1], Batch [321], Loss: 46475.949219
Epoch [1/1], Batch [331], Loss: 47535.945312
Epoch [1/1], Batch [341], Loss: 47788.597656
Epoch [1/1], Batch [351], Loss: 46717.218750
Epoch [1/1], Batch [361], Loss: 47857.578125
Epoch [1/1], Batch [371], Loss: 49841.375000
Epoch [1/1], Batch [381], Loss: 49377.515625
Epoch [1/1], Batch [391], Loss: 48569.960938
Epoch [1/1], Batch [401], Loss: 46962.750000
Epoch [1/1], Batch [411], Loss: 47025.429688
Epoch [1/1], Batch [421], Loss: 45854.457031
Epoch [1/1], Batch [431], Loss: 47900.242188
Epoch [1/1], Batch [441], Loss: 47134.414062
Epoch [1/1], Batch [451], Loss: 48708.632812
Epoch [1/1], Batch [461], Loss: 46881.726562
Epoch [1/1], Batch [471], Loss: 46843.968750
Epoch [1/1], Batch [481], Loss: 45773.343750
Epoch [1/1], Batch [491], Loss: 46895.593750
Epoch [1/1], Batch [501], Loss: 46828.339844
Epoch [1/1], Batch [511], Loss: 46374.496094
Epoch [1/1], Batch [521], Loss: 45110.984375
Epoch [1/1], Batch [531], Loss: 45635.750000
Epoch [1/1], Batch [541], Loss: 46889.796875
Epoch [1/1], Batch [551], Loss: 44930.929688
Epoch [1/1], Batch [561], Loss: 46059.449219
Epoch [1/1], Batch [571], Loss: 44656.132812
Epoch [1/1], Batch [581], Loss: 45668.820312
Epoch [1/1], Batch [591], Loss: 44828.171875
Epoch [1/1], Batch [601], Loss: 44806.976562
Epoch [1/1], Batch [611], Loss: 45259.558594
Epoch [1/1], Batch [621], Loss: 44412.710938
Epoch [1/1], Batch [631], Loss: 46238.289062
Epoch [1/1], Batch [641], Loss: 44443.339844
Epoch [1/1], Batch [651], Loss: 43733.378906
Epoch [1/1], Batch [661], Loss: 45194.421875
Epoch [1/1], Batch [671], Loss: 43187.671875
Epoch [1/1], Batch [681], Loss: 43853.851562
Epoch [1/1], Batch [691], Loss: 45494.085938
Epoch [1/1], Batch [701], Loss: 47617.167969
Epoch [1/1], Batch [711], Loss: 47028.367188
Epoch [1/1], Batch [721], Loss: 45361.804688
Epoch [1/1], Batch [731], Loss: 45158.351562
Epoch [1/1], Batch [741], Loss: 44616.101562
Epoch [1/1], Batch [751], Loss: 43046.679688
Epoch [1/1], Batch [761], Loss: 44994.226562
Epoch [1/1], Batch [771], Loss: 46793.503906
Epoch [1/1], Batch [781], Loss: 43955.726562
Epoch [1/1], Batch [791], Loss: 44733.625000
Epoch [1/1], Batch [801], Loss: 44254.562500
Epoch [1/1], Batch [811], Loss: 43360.398438
Epoch [1/1], Batch [821], Loss: 44650.265625
Epoch [1/1], Batch [831], Loss: 45624.125000
Epoch [1/1], Batch [841], Loss: 45777.179688
Epoch [1/1], Batch [851], Loss: 45580.625000
Epoch [1/1], Batch [861], Loss: 44453.367188
Epoch [1/1], Batch [871], Loss: 41703.042969
Epoch [1/1], Batch [881], Loss: 43667.437500
Epoch [1/1], Batch [891], Loss: 43340.085938
Epoch [1/1], Batch [901], Loss: 44287.546875
Epoch [1/1], Batch [911], Loss: 43554.929688
Epoch [1/1], Batch [921], Loss: 43896.718750
Epoch [1/1], Batch [931], Loss: 42943.898438
Epoch [1/1], Batch [941], Loss: 42704.070312
Epoch [1/1], Batch [951], Loss: 44237.753906
Epoch [1/1], Batch [961], Loss: 43634.609375
Epoch [1/1], Batch [971], Loss: 42729.238281
Epoch [1/1], Batch [981], Loss: 42261.085938
Epoch [1/1], Batch [991], Loss: 43255.816406
Epoch [1/1], Batch [1001], Loss: 44947.074219
Epoch [1/1], Batch [1011], Loss: 43035.687500
Epoch [1/1], Batch [1021], Loss: 41409.671875
Epoch [1/1], Batch [1031], Loss: 42177.851562
Epoch [1/1], Batch [1041], Loss: 42991.277344
Epoch [1/1], Batch [1051], Loss: 41835.375000
Epoch [1/1], Batch [1061], Loss: 42389.738281
Epoch [1/1], Batch [1071], Loss: 44070.132812
Epoch [1/1], Batch [1081], Loss: 41760.562500
Epoch [1/1], Batch [1091], Loss: 44140.351562
Epoch [1/1], Batch [1101], Loss: 42621.000000
Epoch [1/1], Batch [1111], Loss: 43741.187500
Epoch [1/1], Batch [1121], Loss: 43128.035156
Epoch [1/1], Batch [1131], Loss: 43539.835938
Epoch [1/1], Batch [1141], Loss: 43372.183594
Epoch [1/1], Batch [1151], Loss: 40703.578125
Epoch [1/1], Batch [1161], Loss: 42550.445312
Epoch [1/1], Batch [1171], Loss: 38736.117188
Epoch [1/1], Batch [1181], Loss: 41921.304688
Epoch [1/1], Batch [1191], Loss: 41804.332031
Epoch [1/1], Batch [1201], Loss: 44383.796875
Epoch [1/1], Batch [1211], Loss: 40879.046875
Epoch [1/1], Batch [1221], Loss: 40921.953125
Epoch [1/1], Batch [1231], Loss: 42855.320312
Epoch [1/1], Batch [1241], Loss: 43524.539062
Epoch [1/1], Batch [1251], Loss: 41988.804688
Epoch [1/1], Batch [1261], Loss: 43067.000000
Epoch [1/1], Batch [1271], Loss: 43639.750000
Epoch [1/1], Batch [1281], Loss: 40759.179688
Epoch [1/1], Batch [1291], Loss: 42416.187500
Epoch [1/1], Batch [1301], Loss: 43183.523438
Epoch [1/1], Batch [1311], Loss: 43314.500000
Epoch [1/1], Batch [1321], Loss: 43285.769531
Epoch [1/1], Batch [1331], Loss: 40851.656250
Epoch [1/1], Batch [1341], Loss: 44181.515625
Epoch [1/1], Batch [1351], Loss: 40963.257812
Epoch [1/1], Batch [1361], Loss: 40969.238281
Epoch [1/1], Batch [1371], Loss: 43102.625000
Epoch [1/1], Batch [1381], Loss: 41808.851562
Epoch [1/1], Batch [1391], Loss: 39044.363281
Epoch [1/1], Batch [1401], Loss: 42822.437500
Epoch [1/1], Batch [1411], Loss: 41338.914062
Epoch [1/1], Batch [1421], Loss: 42796.597656
Epoch [1/1], Batch [1431], Loss: 43155.671875
Epoch [1/1], Batch [1441], Loss: 43310.570312
Epoch [1/1], Batch [1451], Loss: 41508.039062
Epoch [1/1], Batch [1461], Loss: 39152.539062
Epoch [1/1], Batch [1471], Loss: 40813.851562
Epoch [1/1], Batch [1481], Loss: 42282.796875
Epoch [1/1], Batch [1491], Loss: 43798.527344
Epoch [1/1], Batch [1501], Loss: 41138.164062
Epoch [1/1], Batch [1511], Loss: 40110.273438
Epoch [1/1], Batch [1521], Loss: 41973.484375
Epoch [1/1], Batch [1531], Loss: 41266.835938
Epoch [1/1], Batch [1541], Loss: 40548.210938
Epoch [1/1], Batch [1551], Loss: 42031.117188
Epoch [1/1], Batch [1561], Loss: 41583.738281
Epoch [1/1], Batch [1571], Loss: 42154.296875
Epoch [1/1], Batch [1581], Loss: 41592.175781
Epoch [1/1], Batch [1591], Loss: 41316.574219
Epoch [1/1], Batch [1601], Loss: 42092.718750
Epoch [1/1], Batch [1611], Loss: 42621.335938
Epoch [1/1], Batch [1621], Loss: 41282.691406
Epoch [1/1], Batch [1631], Loss: 40380.007812
Epoch [1/1], Batch [1641], Loss: 39581.457031
Epoch [1/1], Batch [1651], Loss: 40149.195312
Epoch [1/1], Batch [1661], Loss: 39673.390625
Epoch [1/1], Batch [1671], Loss: 41221.199219
Epoch [1/1], Batch [1681], Loss: 41801.109375
Epoch [1/1], Batch [1691], Loss: 37889.562500
Epoch [1/1], Batch [1701], Loss: 40983.921875
Epoch [1/1], Batch [1711], Loss: 40884.437500
Epoch [1/1], Batch [1721], Loss: 41128.640625
Epoch [1/1], Batch [1731], Loss: 42491.882812
Epoch [1/1], Batch [1741], Loss: 40072.812500
Epoch [1/1], Batch [1751], Loss: 38336.316406
Epoch [1/1], Batch [1761], Loss: 41310.593750
Epoch [1/1], Batch [1771], Loss: 42646.578125
Epoch [1/1], Batch [1781], Loss: 39251.824219
Epoch [1/1], Batch [1791], Loss: 41543.398438
Epoch [1/1], Batch [1801], Loss: 42914.054688
Epoch [1/1], Batch [1811], Loss: 39379.984375
Epoch [1/1], Batch [1821], Loss: 40799.152344
Epoch [1/1], Batch [1831], Loss: 42832.394531
Epoch [1/1], Batch [1841], Loss: 41447.960938
Epoch [1/1], Batch [1851], Loss: 41794.937500
Epoch [1/1], Batch [1861], Loss: 39364.210938
Epoch [1/1], Batch [1871], Loss: 42185.335938
Epoch [1/1], Batch [1881], Loss: 40050.992188
Epoch [1/1], Batch [1891], Loss: 40812.730469
Epoch [1/1], Batch [1901], Loss: 40357.972656
Epoch [1/1], Batch [1911], Loss: 40745.203125
Epoch [1/1], Batch [1921], Loss: 42319.703125
Epoch [1/1], Batch [1931], Loss: 41259.718750
Epoch [1/1], Batch [1941], Loss: 40520.949219
Epoch [1/1], Batch [1951], Loss: 40537.585938
Epoch [1/1], Batch [1961], Loss: 39105.050781
Epoch [1/1], Batch [1971], Loss: 40708.519531
Epoch [1/1], Batch [1981], Loss: 41088.519531
Epoch [1/1], Batch [1991], Loss: 43463.101562
Epoch [1/1], Batch [2001], Loss: 40687.292969
Epoch [1/1], Batch [2011], Loss: 39375.898438
Epoch [1/1], Batch [2021], Loss: 42672.324219
Epoch [1/1], Batch [2031], Loss: 41314.843750
Epoch [1/1], Batch [2041], Loss: 42000.273438
Epoch [1/1], Batch [2051], Loss: 42412.070312
Epoch [1/1], Batch [2061], Loss: 40739.746094
Epoch [1/1], Batch [2071], Loss: 42011.605469
Epoch [1/1], Batch [2081], Loss: 37978.140625
Epoch [1/1], Batch [2091], Loss: 40545.750000
Epoch [1/1], Batch [2101], Loss: 41184.976562
Epoch [1/1], Batch [2111], Loss: 39410.953125
Epoch [1/1], Batch [2121], Loss: 39058.671875
Epoch [1/1], Batch [2131], Loss: 41929.578125
Epoch [1/1], Batch [2141], Loss: 39488.230469
Epoch [1/1], Batch [2151], Loss: 39501.718750
Epoch [1/1], Batch [2161], Loss: 40421.773438
Epoch [1/1], Batch [2171], Loss: 37574.531250
Epoch [1/1], Batch [2181], Loss: 41215.414062
Epoch [1/1], Batch [2191], Loss: 40778.429688
Epoch [1/1], Batch [2201], Loss: 39071.488281
Epoch [1/1], Batch [2211], Loss: 39220.312500
Epoch [1/1], Batch [2221], Loss: 42678.882812
Epoch [1/1], Batch [2231], Loss: 39926.281250
Epoch [1/1], Batch [2241], Loss: 39850.304688
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 47174.8594
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 39741.1006
Elapsed time: 477.68 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 40194.2263
Elapsed time: 496.93 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 80449.648438
Epoch [1/1], Batch [11], Loss: 84397.773438
Epoch [1/1], Batch [21], Loss: 81321.203125
Epoch [1/1], Batch [31], Loss: 73702.546875
Epoch [1/1], Batch [41], Loss: 72749.023438
Epoch [1/1], Batch [51], Loss: 74363.710938
Epoch [1/1], Batch [61], Loss: 68552.601562
Epoch [1/1], Batch [71], Loss: 72341.664062
Epoch [1/1], Batch [81], Loss: 71078.296875
Epoch [1/1], Batch [91], Loss: 68210.546875
Epoch [1/1], Batch [101], Loss: 70504.843750
Epoch [1/1], Batch [111], Loss: 68057.898438
Epoch [1/1], Batch [121], Loss: 69164.742188
Epoch [1/1], Batch [131], Loss: 66749.640625
Epoch [1/1], Batch [141], Loss: 66788.328125
Epoch [1/1], Batch [151], Loss: 66787.093750
Epoch [1/1], Batch [161], Loss: 65795.687500
Epoch [1/1], Batch [171], Loss: 70049.585938
Epoch [1/1], Batch [181], Loss: 65729.429688
Epoch [1/1], Batch [191], Loss: 67306.296875
Epoch [1/1], Batch [201], Loss: 64506.734375
Epoch [1/1], Batch [211], Loss: 66835.460938
Epoch [1/1], Batch [221], Loss: 66014.078125
Epoch [1/1], Batch [231], Loss: 66296.484375
Epoch [1/1], Batch [241], Loss: 64477.089844
Epoch [1/1], Batch [251], Loss: 66113.328125
Epoch [1/1], Batch [261], Loss: 69108.960938
Epoch [1/1], Batch [271], Loss: 64502.523438
Epoch [1/1], Batch [281], Loss: 60626.890625
Epoch [1/1], Batch [291], Loss: 63319.195312
Epoch [1/1], Batch [301], Loss: 65375.476562
Epoch [1/1], Batch [311], Loss: 67399.906250
Epoch [1/1], Batch [321], Loss: 65563.015625
Epoch [1/1], Batch [331], Loss: 67287.890625
Epoch [1/1], Batch [341], Loss: 67232.210938
Epoch [1/1], Batch [351], Loss: 63108.437500
Epoch [1/1], Batch [361], Loss: 66738.343750
Epoch [1/1], Batch [371], Loss: 64344.152344
Epoch [1/1], Batch [381], Loss: 67574.445312
Epoch [1/1], Batch [391], Loss: 63382.609375
Epoch [1/1], Batch [401], Loss: 67148.257812
Epoch [1/1], Batch [411], Loss: 66752.929688
Epoch [1/1], Batch [421], Loss: 64428.742188
Epoch [1/1], Batch [431], Loss: 64534.855469
Epoch [1/1], Batch [441], Loss: 65758.406250
Epoch [1/1], Batch [451], Loss: 65096.359375
Epoch [1/1], Batch [461], Loss: 65193.363281
Epoch [1/1], Batch [471], Loss: 63246.515625
Epoch [1/1], Batch [481], Loss: 66823.031250
Epoch [1/1], Batch [491], Loss: 63366.101562
Epoch [1/1], Batch [501], Loss: 66090.375000
Epoch [1/1], Batch [511], Loss: 66114.968750
Epoch [1/1], Batch [521], Loss: 61909.656250
Epoch [1/1], Batch [531], Loss: 64018.550781
Epoch [1/1], Batch [541], Loss: 62879.468750
Epoch [1/1], Batch [551], Loss: 64771.117188
Epoch [1/1], Batch [561], Loss: 65650.843750
Epoch [1/1], Batch [571], Loss: 62363.656250
Epoch [1/1], Batch [581], Loss: 63351.929688
Epoch [1/1], Batch [591], Loss: 64295.890625
Epoch [1/1], Batch [601], Loss: 60722.992188
Epoch [1/1], Batch [611], Loss: 58907.238281
Epoch [1/1], Batch [621], Loss: 60645.835938
Epoch [1/1], Batch [631], Loss: 58910.261719
Epoch [1/1], Batch [641], Loss: 65705.546875
Epoch [1/1], Batch [651], Loss: 61876.492188
Epoch [1/1], Batch [661], Loss: 63187.429688
Epoch [1/1], Batch [671], Loss: 62067.816406
Epoch [1/1], Batch [681], Loss: 61550.953125
Epoch [1/1], Batch [691], Loss: 64699.550781
Epoch [1/1], Batch [701], Loss: 64686.125000
Epoch [1/1], Batch [711], Loss: 62475.515625
Epoch [1/1], Batch [721], Loss: 64526.699219
Epoch [1/1], Batch [731], Loss: 62751.921875
Epoch [1/1], Batch [741], Loss: 63644.734375
Epoch [1/1], Batch [751], Loss: 66335.992188
Epoch [1/1], Batch [761], Loss: 61452.437500
Epoch [1/1], Batch [771], Loss: 61183.781250
Epoch [1/1], Batch [781], Loss: 62242.554688
Epoch [1/1], Batch [791], Loss: 60547.914062
Epoch [1/1], Batch [801], Loss: 63376.292969
Epoch [1/1], Batch [811], Loss: 63920.734375
Epoch [1/1], Batch [821], Loss: 64072.250000
Epoch [1/1], Batch [831], Loss: 62737.414062
Epoch [1/1], Batch [841], Loss: 61792.296875
Epoch [1/1], Batch [851], Loss: 61158.250000
Epoch [1/1], Batch [861], Loss: 64470.371094
Epoch [1/1], Batch [871], Loss: 60302.382812
Epoch [1/1], Batch [881], Loss: 63049.875000
Epoch [1/1], Batch [891], Loss: 65084.113281
Epoch [1/1], Batch [901], Loss: 63143.148438
Epoch [1/1], Batch [911], Loss: 60483.417969
Epoch [1/1], Batch [921], Loss: 65803.171875
Epoch [1/1], Batch [931], Loss: 62878.250000
Epoch [1/1], Batch [941], Loss: 62715.710938
Epoch [1/1], Batch [951], Loss: 57510.554688
Epoch [1/1], Batch [961], Loss: 63534.937500
Epoch [1/1], Batch [971], Loss: 59921.644531
Epoch [1/1], Batch [981], Loss: 61261.007812
Epoch [1/1], Batch [991], Loss: 64867.539062
Epoch [1/1], Batch [1001], Loss: 63623.042969
Epoch [1/1], Batch [1011], Loss: 59898.988281
Epoch [1/1], Batch [1021], Loss: 64821.929688
Epoch [1/1], Batch [1031], Loss: 60443.441406
Epoch [1/1], Batch [1041], Loss: 59715.250000
Epoch [1/1], Batch [1051], Loss: 60027.046875
Epoch [1/1], Batch [1061], Loss: 62318.734375
Epoch [1/1], Batch [1071], Loss: 61411.246094
Epoch [1/1], Batch [1081], Loss: 62252.726562
Epoch [1/1], Batch [1091], Loss: 62075.187500
Epoch [1/1], Batch [1101], Loss: 61597.210938
Epoch [1/1], Batch [1111], Loss: 62775.601562
Epoch [1/1], Batch [1121], Loss: 62804.476562
Epoch [1/1], Batch [1131], Loss: 61856.292969
Epoch [1/1], Batch [1141], Loss: 60991.789062
Epoch [1/1], Batch [1151], Loss: 60854.519531
Epoch [1/1], Batch [1161], Loss: 61016.941406
Epoch [1/1], Batch [1171], Loss: 61990.238281
Epoch [1/1], Batch [1181], Loss: 63427.460938
Epoch [1/1], Batch [1191], Loss: 61419.101562
Epoch [1/1], Batch [1201], Loss: 60227.867188
Epoch [1/1], Batch [1211], Loss: 63835.523438
Epoch [1/1], Batch [1221], Loss: 59074.859375
Epoch [1/1], Batch [1231], Loss: 62477.640625
Epoch [1/1], Batch [1241], Loss: 59116.617188
Epoch [1/1], Batch [1251], Loss: 61320.843750
Epoch [1/1], Batch [1261], Loss: 57444.503906
Epoch [1/1], Batch [1271], Loss: 59993.160156
Epoch [1/1], Batch [1281], Loss: 59906.703125
Epoch [1/1], Batch [1291], Loss: 61958.078125
Epoch [1/1], Batch [1301], Loss: 61968.382812
Epoch [1/1], Batch [1311], Loss: 63509.441406
Epoch [1/1], Batch [1321], Loss: 60539.710938
Epoch [1/1], Batch [1331], Loss: 63394.812500
Epoch [1/1], Batch [1341], Loss: 63864.347656
Epoch [1/1], Batch [1351], Loss: 63800.972656
Epoch [1/1], Batch [1361], Loss: 61399.960938
Epoch [1/1], Batch [1371], Loss: 61824.695312
Epoch [1/1], Batch [1381], Loss: 59413.601562
Epoch [1/1], Batch [1391], Loss: 58809.125000
Epoch [1/1], Batch [1401], Loss: 60037.964844
Epoch [1/1], Batch [1411], Loss: 59719.851562
Epoch [1/1], Batch [1421], Loss: 60657.714844
Epoch [1/1], Batch [1431], Loss: 60583.976562
Epoch [1/1], Batch [1441], Loss: 58924.226562
Epoch [1/1], Batch [1451], Loss: 60789.562500
Epoch [1/1], Batch [1461], Loss: 63328.597656
Epoch [1/1], Batch [1471], Loss: 63566.000000
Epoch [1/1], Batch [1481], Loss: 60722.671875
Epoch [1/1], Batch [1491], Loss: 60865.406250
Epoch [1/1], Batch [1501], Loss: 60289.128906
Epoch [1/1], Batch [1511], Loss: 58069.644531
Epoch [1/1], Batch [1521], Loss: 58201.843750
Epoch [1/1], Batch [1531], Loss: 58013.179688
Epoch [1/1], Batch [1541], Loss: 61240.187500
Epoch [1/1], Batch [1551], Loss: 62071.925781
Epoch [1/1], Batch [1561], Loss: 59989.003906
Epoch [1/1], Batch [1571], Loss: 59184.683594
Epoch [1/1], Batch [1581], Loss: 59142.000000
Epoch [1/1], Batch [1591], Loss: 58155.484375
Epoch [1/1], Batch [1601], Loss: 61228.601562
Epoch [1/1], Batch [1611], Loss: 59231.218750
Epoch [1/1], Batch [1621], Loss: 58221.109375
Epoch [1/1], Batch [1631], Loss: 60168.992188
Epoch [1/1], Batch [1641], Loss: 60353.351562
Epoch [1/1], Batch [1651], Loss: 58792.750000
Epoch [1/1], Batch [1661], Loss: 62642.960938
Epoch [1/1], Batch [1671], Loss: 59151.015625
Epoch [1/1], Batch [1681], Loss: 61277.960938
Epoch [1/1], Batch [1691], Loss: 59911.398438
Epoch [1/1], Batch [1701], Loss: 59246.675781
Epoch [1/1], Batch [1711], Loss: 60305.523438
Epoch [1/1], Batch [1721], Loss: 59560.972656
Epoch [1/1], Batch [1731], Loss: 58542.914062
Epoch [1/1], Batch [1741], Loss: 58915.023438
Epoch [1/1], Batch [1751], Loss: 61084.597656
Epoch [1/1], Batch [1761], Loss: 58194.343750
Epoch [1/1], Batch [1771], Loss: 58577.542969
Epoch [1/1], Batch [1781], Loss: 57186.175781
Epoch [1/1], Batch [1791], Loss: 58811.445312
Epoch [1/1], Batch [1801], Loss: 59630.164062
Epoch [1/1], Batch [1811], Loss: 59595.664062
Epoch [1/1], Batch [1821], Loss: 56450.527344
Epoch [1/1], Batch [1831], Loss: 59996.226562
Epoch [1/1], Batch [1841], Loss: 60055.253906
Epoch [1/1], Batch [1851], Loss: 57394.406250
Epoch [1/1], Batch [1861], Loss: 57692.238281
Epoch [1/1], Batch [1871], Loss: 57887.515625
Epoch [1/1], Batch [1881], Loss: 59470.492188
Epoch [1/1], Batch [1891], Loss: 62900.449219
Epoch [1/1], Batch [1901], Loss: 59898.121094
Epoch [1/1], Batch [1911], Loss: 61947.304688
Epoch [1/1], Batch [1921], Loss: 58193.312500
Epoch [1/1], Batch [1931], Loss: 58328.226562
Epoch [1/1], Batch [1941], Loss: 59220.132812
Epoch [1/1], Batch [1951], Loss: 59974.789062
Epoch [1/1], Batch [1961], Loss: 58982.375000
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 62808.3878
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 58084.7586
Elapsed time: 1099.37 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 59277.2098
Elapsed time: 1122.40 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 87008.734375
Epoch [1/1], Batch [11], Loss: 88345.812500
Epoch [1/1], Batch [21], Loss: 85053.437500
Epoch [1/1], Batch [31], Loss: 91556.281250
Epoch [1/1], Batch [41], Loss: 83027.484375
Epoch [1/1], Batch [51], Loss: 85958.281250
Epoch [1/1], Batch [61], Loss: 88901.195312
Epoch [1/1], Batch [71], Loss: 85970.476562
Epoch [1/1], Batch [81], Loss: 85644.937500
Epoch [1/1], Batch [91], Loss: 84341.367188
Epoch [1/1], Batch [101], Loss: 85779.937500
Epoch [1/1], Batch [111], Loss: 84121.531250
Epoch [1/1], Batch [121], Loss: 83642.359375
Epoch [1/1], Batch [131], Loss: 86272.179688
Epoch [1/1], Batch [141], Loss: 85783.476562
Epoch [1/1], Batch [151], Loss: 86297.015625
Epoch [1/1], Batch [161], Loss: 85163.257812
Epoch [1/1], Batch [171], Loss: 87088.734375
Epoch [1/1], Batch [181], Loss: 79590.578125
Epoch [1/1], Batch [191], Loss: 84465.390625
Epoch [1/1], Batch [201], Loss: 81272.093750
Epoch [1/1], Batch [211], Loss: 89288.242188
Epoch [1/1], Batch [221], Loss: 80925.375000
Epoch [1/1], Batch [231], Loss: 84639.984375
Epoch [1/1], Batch [241], Loss: 82794.375000
Epoch [1/1], Batch [251], Loss: 83698.921875
Epoch [1/1], Batch [261], Loss: 83967.734375
Epoch [1/1], Batch [271], Loss: 81370.289062
Epoch [1/1], Batch [281], Loss: 83096.398438
Epoch [1/1], Batch [291], Loss: 85411.906250
Epoch [1/1], Batch [301], Loss: 80715.656250
Epoch [1/1], Batch [311], Loss: 84446.101562
Epoch [1/1], Batch [321], Loss: 82924.210938
Epoch [1/1], Batch [331], Loss: 82194.453125
Epoch [1/1], Batch [341], Loss: 81634.437500
Epoch [1/1], Batch [351], Loss: 83511.531250
Epoch [1/1], Batch [361], Loss: 81334.593750
Epoch [1/1], Batch [371], Loss: 84122.789062
Epoch [1/1], Batch [381], Loss: 82597.101562
Epoch [1/1], Batch [391], Loss: 84387.898438
Epoch [1/1], Batch [401], Loss: 84693.468750
Epoch [1/1], Batch [411], Loss: 82045.656250
Epoch [1/1], Batch [421], Loss: 81770.562500
Epoch [1/1], Batch [431], Loss: 82038.781250
Epoch [1/1], Batch [441], Loss: 80780.546875
Epoch [1/1], Batch [451], Loss: 81522.296875
Epoch [1/1], Batch [461], Loss: 82405.234375
Epoch [1/1], Batch [471], Loss: 81461.609375
Epoch [1/1], Batch [481], Loss: 82133.414062
Epoch [1/1], Batch [491], Loss: 81603.125000
Epoch [1/1], Batch [501], Loss: 82282.984375
Epoch [1/1], Batch [511], Loss: 80861.226562
Epoch [1/1], Batch [521], Loss: 79676.914062
Epoch [1/1], Batch [531], Loss: 82617.500000
Epoch [1/1], Batch [541], Loss: 83730.500000
Epoch [1/1], Batch [551], Loss: 81407.937500
Epoch [1/1], Batch [561], Loss: 82334.562500
Epoch [1/1], Batch [571], Loss: 81818.046875
Epoch [1/1], Batch [581], Loss: 81767.406250
Epoch [1/1], Batch [591], Loss: 79089.625000
Epoch [1/1], Batch [601], Loss: 80788.750000
Epoch [1/1], Batch [611], Loss: 82334.500000
Epoch [1/1], Batch [621], Loss: 82916.218750
Epoch [1/1], Batch [631], Loss: 81469.593750
Epoch [1/1], Batch [641], Loss: 81446.234375
Epoch [1/1], Batch [651], Loss: 80706.210938
Epoch [1/1], Batch [661], Loss: 79622.609375
Epoch [1/1], Batch [671], Loss: 80852.640625
Epoch [1/1], Batch [681], Loss: 78353.875000
Epoch [1/1], Batch [691], Loss: 81427.460938
Epoch [1/1], Batch [701], Loss: 82846.453125
Epoch [1/1], Batch [711], Loss: 83786.984375
Epoch [1/1], Batch [721], Loss: 81059.109375
Epoch [1/1], Batch [731], Loss: 79431.343750
Epoch [1/1], Batch [741], Loss: 81201.281250
Epoch [1/1], Batch [751], Loss: 79262.515625
Epoch [1/1], Batch [761], Loss: 83069.570312
Epoch [1/1], Batch [771], Loss: 84858.585938
Epoch [1/1], Batch [781], Loss: 85220.500000
Epoch [1/1], Batch [791], Loss: 78700.156250
Epoch [1/1], Batch [801], Loss: 81516.593750
Epoch [1/1], Batch [811], Loss: 78911.125000
Epoch [1/1], Batch [821], Loss: 82506.203125
Epoch [1/1], Batch [831], Loss: 82838.921875
Epoch [1/1], Batch [841], Loss: 82876.296875
Epoch [1/1], Batch [851], Loss: 79856.125000
Epoch [1/1], Batch [861], Loss: 84881.718750
Epoch [1/1], Batch [871], Loss: 85254.445312
Epoch [1/1], Batch [881], Loss: 77413.234375
Epoch [1/1], Batch [891], Loss: 80533.281250
Epoch [1/1], Batch [901], Loss: 81749.304688
Epoch [1/1], Batch [911], Loss: 81026.953125
Epoch [1/1], Batch [921], Loss: 76747.937500
Epoch [1/1], Batch [931], Loss: 83753.023438
Epoch [1/1], Batch [941], Loss: 78054.625000
Epoch [1/1], Batch [951], Loss: 82026.195312
Epoch [1/1], Batch [961], Loss: 84229.062500
Epoch [1/1], Batch [971], Loss: 81827.945312
Epoch [1/1], Batch [981], Loss: 80130.296875
Epoch [1/1], Batch [991], Loss: 80411.093750
Epoch [1/1], Batch [1001], Loss: 78512.679688
Epoch [1/1], Batch [1011], Loss: 81790.531250
Epoch [1/1], Batch [1021], Loss: 77532.929688
Epoch [1/1], Batch [1031], Loss: 80686.203125
Epoch [1/1], Batch [1041], Loss: 78183.500000
Epoch [1/1], Batch [1051], Loss: 77821.375000
Epoch [1/1], Batch [1061], Loss: 80494.390625
Epoch [1/1], Batch [1071], Loss: 74626.226562
Epoch [1/1], Batch [1081], Loss: 80401.687500
Epoch [1/1], Batch [1091], Loss: 77904.359375
Epoch [1/1], Batch [1101], Loss: 81233.757812
Epoch [1/1], Batch [1111], Loss: 81799.890625
Epoch [1/1], Batch [1121], Loss: 79098.562500
Epoch [1/1], Batch [1131], Loss: 79393.984375
Epoch [1/1], Batch [1141], Loss: 78502.851562
Epoch [1/1], Batch [1151], Loss: 78357.085938
Epoch [1/1], Batch [1161], Loss: 84172.000000
Epoch [1/1], Batch [1171], Loss: 78576.359375
Epoch [1/1], Batch [1181], Loss: 81434.953125
Epoch [1/1], Batch [1191], Loss: 81691.453125
Epoch [1/1], Batch [1201], Loss: 77037.601562
Epoch [1/1], Batch [1211], Loss: 78168.078125
Epoch [1/1], Batch [1221], Loss: 77811.937500
Epoch [1/1], Batch [1231], Loss: 74672.625000
Epoch [1/1], Batch [1241], Loss: 78679.039062
Epoch [1/1], Batch [1251], Loss: 79489.312500
Epoch [1/1], Batch [1261], Loss: 79386.093750
Epoch [1/1], Batch [1271], Loss: 81411.437500
Epoch [1/1], Batch [1281], Loss: 81359.375000
Epoch [1/1], Batch [1291], Loss: 80736.460938
Epoch [1/1], Batch [1301], Loss: 83487.375000
Epoch [1/1], Batch [1311], Loss: 76997.273438
Epoch [1/1], Batch [1321], Loss: 80408.679688
Epoch [1/1], Batch [1331], Loss: 82085.085938
Epoch [1/1], Batch [1341], Loss: 80052.609375
Epoch [1/1], Batch [1351], Loss: 79607.687500
Epoch [1/1], Batch [1361], Loss: 80827.976562
Epoch [1/1], Batch [1371], Loss: 78924.875000
Epoch [1/1], Batch [1381], Loss: 78363.750000
Epoch [1/1], Batch [1391], Loss: 81714.734375
Epoch [1/1], Batch [1401], Loss: 76717.398438
Epoch [1/1], Batch [1411], Loss: 78070.476562
Epoch [1/1], Batch [1421], Loss: 78622.500000
Epoch [1/1], Batch [1431], Loss: 78549.562500
Epoch [1/1], Batch [1441], Loss: 81640.031250
Epoch [1/1], Batch [1451], Loss: 81341.289062
Epoch [1/1], Batch [1461], Loss: 77948.109375
Epoch [1/1], Batch [1471], Loss: 76081.390625
Epoch [1/1], Batch [1481], Loss: 80351.843750
Epoch [1/1], Batch [1491], Loss: 76825.843750
Epoch [1/1], Batch [1501], Loss: 82312.085938
Epoch [1/1], Batch [1511], Loss: 81828.312500
Epoch [1/1], Batch [1521], Loss: 79975.968750
Epoch [1/1], Batch [1531], Loss: 76797.585938
Epoch [1/1], Batch [1541], Loss: 79485.625000
Epoch [1/1], Batch [1551], Loss: 80632.781250
Epoch [1/1], Batch [1561], Loss: 78088.820312
Epoch [1/1], Batch [1571], Loss: 75438.062500
Epoch [1/1], Batch [1581], Loss: 83295.078125
Epoch [1/1], Batch [1591], Loss: 81466.976562
Epoch [1/1], Batch [1601], Loss: 78916.750000
Epoch [1/1], Batch [1611], Loss: 76866.593750
Epoch [1/1], Batch [1621], Loss: 77490.906250
Epoch [1/1], Batch [1631], Loss: 80085.773438
Epoch [1/1], Batch [1641], Loss: 78705.109375
Epoch [1/1], Batch [1651], Loss: 81670.132812
Epoch [1/1], Batch [1661], Loss: 77144.625000
Epoch [1/1], Batch [1671], Loss: 76066.179688
Epoch [1/1], Batch [1681], Loss: 83632.507812
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 81248.5187
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 78323.5848
Elapsed time: 1791.94 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 80222.5393
Elapsed time: 1816.93 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 112922.015625
Epoch [1/1], Batch [11], Loss: 104266.210938
Epoch [1/1], Batch [21], Loss: 104905.468750
Epoch [1/1], Batch [31], Loss: 103330.812500
Epoch [1/1], Batch [41], Loss: 96863.156250
Epoch [1/1], Batch [51], Loss: 103299.781250
Epoch [1/1], Batch [61], Loss: 105464.289062
Epoch [1/1], Batch [71], Loss: 103861.187500
Epoch [1/1], Batch [81], Loss: 109844.750000
Epoch [1/1], Batch [91], Loss: 106749.164062
Epoch [1/1], Batch [101], Loss: 98440.867188
Epoch [1/1], Batch [111], Loss: 103484.265625
Epoch [1/1], Batch [121], Loss: 102231.703125
Epoch [1/1], Batch [131], Loss: 103043.296875
Epoch [1/1], Batch [141], Loss: 106377.218750
Epoch [1/1], Batch [151], Loss: 102370.750000
Epoch [1/1], Batch [161], Loss: 104576.140625
Epoch [1/1], Batch [171], Loss: 103912.906250
Epoch [1/1], Batch [181], Loss: 107911.015625
Epoch [1/1], Batch [191], Loss: 102785.859375
Epoch [1/1], Batch [201], Loss: 104437.234375
Epoch [1/1], Batch [211], Loss: 104256.296875
Epoch [1/1], Batch [221], Loss: 102712.023438
Epoch [1/1], Batch [231], Loss: 106443.750000
Epoch [1/1], Batch [241], Loss: 97384.578125
Epoch [1/1], Batch [251], Loss: 98608.203125
Epoch [1/1], Batch [261], Loss: 104801.812500
Epoch [1/1], Batch [271], Loss: 103281.734375
Epoch [1/1], Batch [281], Loss: 102495.031250
Epoch [1/1], Batch [291], Loss: 107715.609375
Epoch [1/1], Batch [301], Loss: 102155.265625
Epoch [1/1], Batch [311], Loss: 107951.359375
Epoch [1/1], Batch [321], Loss: 101347.203125
Epoch [1/1], Batch [331], Loss: 105831.296875
Epoch [1/1], Batch [341], Loss: 96185.390625
Epoch [1/1], Batch [351], Loss: 105217.468750
Epoch [1/1], Batch [361], Loss: 100672.531250
Epoch [1/1], Batch [371], Loss: 103037.859375
Epoch [1/1], Batch [381], Loss: 99898.320312
Epoch [1/1], Batch [391], Loss: 106853.250000
Epoch [1/1], Batch [401], Loss: 97075.898438
Epoch [1/1], Batch [411], Loss: 103784.046875
Epoch [1/1], Batch [421], Loss: 102198.062500
Epoch [1/1], Batch [431], Loss: 102425.398438
Epoch [1/1], Batch [441], Loss: 100368.242188
Epoch [1/1], Batch [451], Loss: 98577.171875
Epoch [1/1], Batch [461], Loss: 103148.242188
Epoch [1/1], Batch [471], Loss: 105485.039062
Epoch [1/1], Batch [481], Loss: 102778.460938
Epoch [1/1], Batch [491], Loss: 98989.921875
Epoch [1/1], Batch [501], Loss: 99840.656250
Epoch [1/1], Batch [511], Loss: 105153.046875
Epoch [1/1], Batch [521], Loss: 96656.140625
Epoch [1/1], Batch [531], Loss: 103462.914062
Epoch [1/1], Batch [541], Loss: 97410.546875
Epoch [1/1], Batch [551], Loss: 99431.734375
Epoch [1/1], Batch [561], Loss: 104755.062500
Epoch [1/1], Batch [571], Loss: 101438.781250
Epoch [1/1], Batch [581], Loss: 99342.125000
Epoch [1/1], Batch [591], Loss: 102173.765625
Epoch [1/1], Batch [601], Loss: 100910.859375
Epoch [1/1], Batch [611], Loss: 103114.484375
Epoch [1/1], Batch [621], Loss: 98225.835938
Epoch [1/1], Batch [631], Loss: 101834.171875
Epoch [1/1], Batch [641], Loss: 102573.710938
Epoch [1/1], Batch [651], Loss: 100719.250000
Epoch [1/1], Batch [661], Loss: 96389.507812
Epoch [1/1], Batch [671], Loss: 100099.609375
Epoch [1/1], Batch [681], Loss: 106198.171875
Epoch [1/1], Batch [691], Loss: 98931.171875
Epoch [1/1], Batch [701], Loss: 104250.296875
Epoch [1/1], Batch [711], Loss: 101616.320312
Epoch [1/1], Batch [721], Loss: 105103.656250
Epoch [1/1], Batch [731], Loss: 102202.898438
Epoch [1/1], Batch [741], Loss: 102704.203125
Epoch [1/1], Batch [751], Loss: 102368.625000
Epoch [1/1], Batch [761], Loss: 100078.687500
Epoch [1/1], Batch [771], Loss: 100096.046875
Epoch [1/1], Batch [781], Loss: 100678.281250
Epoch [1/1], Batch [791], Loss: 102309.062500
Epoch [1/1], Batch [801], Loss: 97520.953125
Epoch [1/1], Batch [811], Loss: 99783.187500
Epoch [1/1], Batch [821], Loss: 103506.828125
Epoch [1/1], Batch [831], Loss: 101398.515625
Epoch [1/1], Batch [841], Loss: 102993.125000
Epoch [1/1], Batch [851], Loss: 100083.312500
Epoch [1/1], Batch [861], Loss: 100866.843750
Epoch [1/1], Batch [871], Loss: 100091.726562
Epoch [1/1], Batch [881], Loss: 103401.250000
Epoch [1/1], Batch [891], Loss: 102535.882812
Epoch [1/1], Batch [901], Loss: 98250.234375
Epoch [1/1], Batch [911], Loss: 99516.937500
Epoch [1/1], Batch [921], Loss: 98775.218750
Epoch [1/1], Batch [931], Loss: 96708.195312
Epoch [1/1], Batch [941], Loss: 100180.781250
Epoch [1/1], Batch [951], Loss: 95969.921875
Epoch [1/1], Batch [961], Loss: 99467.679688
Epoch [1/1], Batch [971], Loss: 100384.343750
Epoch [1/1], Batch [981], Loss: 98404.515625
Epoch [1/1], Batch [991], Loss: 97180.875000
Epoch [1/1], Batch [1001], Loss: 97413.601562
Epoch [1/1], Batch [1011], Loss: 97778.664062
Epoch [1/1], Batch [1021], Loss: 98966.304688
Epoch [1/1], Batch [1031], Loss: 100171.054688
Epoch [1/1], Batch [1041], Loss: 97995.570312
Epoch [1/1], Batch [1051], Loss: 101098.406250
Epoch [1/1], Batch [1061], Loss: 99052.109375
Epoch [1/1], Batch [1071], Loss: 98211.265625
Epoch [1/1], Batch [1081], Loss: 100752.476562
Epoch [1/1], Batch [1091], Loss: 99026.343750
Epoch [1/1], Batch [1101], Loss: 100606.875000
Epoch [1/1], Batch [1111], Loss: 94978.000000
Epoch [1/1], Batch [1121], Loss: 97473.507812
Epoch [1/1], Batch [1131], Loss: 99388.609375
Epoch [1/1], Batch [1141], Loss: 105812.820312
Epoch [1/1], Batch [1151], Loss: 104435.851562
Epoch [1/1], Batch [1161], Loss: 99945.687500
Epoch [1/1], Batch [1171], Loss: 100814.101562
Epoch [1/1], Batch [1181], Loss: 103286.281250
Epoch [1/1], Batch [1191], Loss: 101292.867188
Epoch [1/1], Batch [1201], Loss: 100993.921875
Epoch [1/1], Batch [1211], Loss: 102527.578125
Epoch [1/1], Batch [1221], Loss: 97282.093750
Epoch [1/1], Batch [1231], Loss: 98848.562500
Epoch [1/1], Batch [1241], Loss: 102809.703125
Epoch [1/1], Batch [1251], Loss: 99886.351562
Epoch [1/1], Batch [1261], Loss: 99306.296875
Epoch [1/1], Batch [1271], Loss: 97516.312500
Epoch [1/1], Batch [1281], Loss: 101328.562500
Epoch [1/1], Batch [1291], Loss: 101252.296875
Epoch [1/1], Batch [1301], Loss: 96536.429688
Epoch [1/1], Batch [1311], Loss: 94069.335938
Epoch [1/1], Batch [1321], Loss: 99416.625000
Epoch [1/1], Batch [1331], Loss: 99424.023438
Epoch [1/1], Batch [1341], Loss: 100222.000000
Epoch [1/1], Batch [1351], Loss: 101665.000000
Epoch [1/1], Batch [1361], Loss: 98531.054688
Epoch [1/1], Batch [1371], Loss: 99390.742188
Epoch [1/1], Batch [1381], Loss: 101961.968750
Epoch [1/1], Batch [1391], Loss: 96703.179688
Epoch [1/1], Batch [1401], Loss: 100518.921875
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 101162.7955
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 97415.2687
Elapsed time: 2505.40 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 101086.4494
Elapsed time: 2531.73 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 123925.210938
Epoch [1/1], Batch [11], Loss: 127748.671875
Epoch [1/1], Batch [21], Loss: 127170.023438
Epoch [1/1], Batch [31], Loss: 127962.640625
Epoch [1/1], Batch [41], Loss: 124230.898438
Epoch [1/1], Batch [51], Loss: 122161.968750
Epoch [1/1], Batch [61], Loss: 126938.976562
Epoch [1/1], Batch [71], Loss: 120585.546875
Epoch [1/1], Batch [81], Loss: 122181.273438
Epoch [1/1], Batch [91], Loss: 124422.296875
Epoch [1/1], Batch [101], Loss: 121566.093750
Epoch [1/1], Batch [111], Loss: 121388.242188
Epoch [1/1], Batch [121], Loss: 127905.210938
Epoch [1/1], Batch [131], Loss: 124277.531250
Epoch [1/1], Batch [141], Loss: 122598.250000
Epoch [1/1], Batch [151], Loss: 120090.640625
Epoch [1/1], Batch [161], Loss: 128994.328125
Epoch [1/1], Batch [171], Loss: 122448.632812
Epoch [1/1], Batch [181], Loss: 120016.781250
Epoch [1/1], Batch [191], Loss: 123367.265625
Epoch [1/1], Batch [201], Loss: 119177.015625
Epoch [1/1], Batch [211], Loss: 129275.062500
Epoch [1/1], Batch [221], Loss: 126597.671875
Epoch [1/1], Batch [231], Loss: 123454.914062
Epoch [1/1], Batch [241], Loss: 120536.906250
Epoch [1/1], Batch [251], Loss: 122202.609375
Epoch [1/1], Batch [261], Loss: 119109.210938
Epoch [1/1], Batch [271], Loss: 127077.507812
Epoch [1/1], Batch [281], Loss: 131927.921875
Epoch [1/1], Batch [291], Loss: 124896.835938
Epoch [1/1], Batch [301], Loss: 123082.781250
Epoch [1/1], Batch [311], Loss: 125763.921875
Epoch [1/1], Batch [321], Loss: 117885.453125
Epoch [1/1], Batch [331], Loss: 121325.750000
Epoch [1/1], Batch [341], Loss: 125914.718750
Epoch [1/1], Batch [351], Loss: 120857.562500
Epoch [1/1], Batch [361], Loss: 123869.773438
Epoch [1/1], Batch [371], Loss: 124812.578125
Epoch [1/1], Batch [381], Loss: 121724.492188
Epoch [1/1], Batch [391], Loss: 118810.296875
Epoch [1/1], Batch [401], Loss: 123767.406250
Epoch [1/1], Batch [411], Loss: 115020.109375
Epoch [1/1], Batch [421], Loss: 123605.390625
Epoch [1/1], Batch [431], Loss: 125056.953125
Epoch [1/1], Batch [441], Loss: 122802.242188
Epoch [1/1], Batch [451], Loss: 126541.648438
Epoch [1/1], Batch [461], Loss: 123646.304688
Epoch [1/1], Batch [471], Loss: 122148.726562
Epoch [1/1], Batch [481], Loss: 122971.531250
Epoch [1/1], Batch [491], Loss: 124937.421875
Epoch [1/1], Batch [501], Loss: 126563.679688
Epoch [1/1], Batch [511], Loss: 117894.945312
Epoch [1/1], Batch [521], Loss: 119699.820312
Epoch [1/1], Batch [531], Loss: 117966.156250
Epoch [1/1], Batch [541], Loss: 123990.484375
Epoch [1/1], Batch [551], Loss: 121026.250000
Epoch [1/1], Batch [561], Loss: 124299.476562
Epoch [1/1], Batch [571], Loss: 127522.828125
Epoch [1/1], Batch [581], Loss: 121195.531250
Epoch [1/1], Batch [591], Loss: 121665.250000
Epoch [1/1], Batch [601], Loss: 126369.859375
Epoch [1/1], Batch [611], Loss: 119277.742188
Epoch [1/1], Batch [621], Loss: 122224.812500
Epoch [1/1], Batch [631], Loss: 125495.039062
Epoch [1/1], Batch [641], Loss: 118209.046875
Epoch [1/1], Batch [651], Loss: 123638.875000
Epoch [1/1], Batch [661], Loss: 117801.671875
Epoch [1/1], Batch [671], Loss: 123423.937500
Epoch [1/1], Batch [681], Loss: 125740.429688
Epoch [1/1], Batch [691], Loss: 118047.109375
Epoch [1/1], Batch [701], Loss: 123487.125000
Epoch [1/1], Batch [711], Loss: 121886.179688
Epoch [1/1], Batch [721], Loss: 120368.554688
Epoch [1/1], Batch [731], Loss: 118197.601562
Epoch [1/1], Batch [741], Loss: 121026.250000
Epoch [1/1], Batch [751], Loss: 118319.382812
Epoch [1/1], Batch [761], Loss: 119471.992188
Epoch [1/1], Batch [771], Loss: 120318.000000
Epoch [1/1], Batch [781], Loss: 117745.718750
Epoch [1/1], Batch [791], Loss: 120363.828125
Epoch [1/1], Batch [801], Loss: 121415.796875
Epoch [1/1], Batch [811], Loss: 122774.468750
Epoch [1/1], Batch [821], Loss: 128393.890625
Epoch [1/1], Batch [831], Loss: 127665.609375
Epoch [1/1], Batch [841], Loss: 123786.218750
Epoch [1/1], Batch [851], Loss: 119881.281250
Epoch [1/1], Batch [861], Loss: 119021.187500
Epoch [1/1], Batch [871], Loss: 121271.062500
Epoch [1/1], Batch [881], Loss: 120145.718750
Epoch [1/1], Batch [891], Loss: 119256.953125
Epoch [1/1], Batch [901], Loss: 122344.125000
Epoch [1/1], Batch [911], Loss: 122174.000000
Epoch [1/1], Batch [921], Loss: 116215.812500
Epoch [1/1], Batch [931], Loss: 124051.437500
Epoch [1/1], Batch [941], Loss: 119929.843750
Epoch [1/1], Batch [951], Loss: 121350.687500
Epoch [1/1], Batch [961], Loss: 119832.015625
Epoch [1/1], Batch [971], Loss: 118993.179688
Epoch [1/1], Batch [981], Loss: 120654.875000
Epoch [1/1], Batch [991], Loss: 121779.656250
Epoch [1/1], Batch [1001], Loss: 120624.507812
Epoch [1/1], Batch [1011], Loss: 117640.898438
Epoch [1/1], Batch [1021], Loss: 120102.664062
Epoch [1/1], Batch [1031], Loss: 121444.921875
Epoch [1/1], Batch [1041], Loss: 119856.835938
Epoch [1/1], Batch [1051], Loss: 118705.234375
Epoch [1/1], Batch [1061], Loss: 120775.054688
Epoch [1/1], Batch [1071], Loss: 121671.070312
Epoch [1/1], Batch [1081], Loss: 117018.156250
Epoch [1/1], Batch [1091], Loss: 128715.078125
Epoch [1/1], Batch [1101], Loss: 120280.835938
Epoch [1/1], Batch [1111], Loss: 120949.187500
Epoch [1/1], Batch [1121], Loss: 121661.656250
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 122356.5344
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 116589.8710
Elapsed time: 3185.44 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 121814.8254
Elapsed time: 3209.07 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 150412.734375
Epoch [1/1], Batch [11], Loss: 153135.109375
Epoch [1/1], Batch [21], Loss: 153253.187500
Epoch [1/1], Batch [31], Loss: 146726.000000
Epoch [1/1], Batch [41], Loss: 144976.765625
Epoch [1/1], Batch [51], Loss: 146738.796875
Epoch [1/1], Batch [61], Loss: 145282.718750
Epoch [1/1], Batch [71], Loss: 150578.000000
Epoch [1/1], Batch [81], Loss: 136262.968750
Epoch [1/1], Batch [91], Loss: 138044.125000
Epoch [1/1], Batch [101], Loss: 142961.046875
Epoch [1/1], Batch [111], Loss: 150301.750000
Epoch [1/1], Batch [121], Loss: 143671.218750
Epoch [1/1], Batch [131], Loss: 148139.656250
Epoch [1/1], Batch [141], Loss: 146700.656250
Epoch [1/1], Batch [151], Loss: 144910.250000
Epoch [1/1], Batch [161], Loss: 148773.390625
Epoch [1/1], Batch [171], Loss: 143424.359375
Epoch [1/1], Batch [181], Loss: 148457.890625
Epoch [1/1], Batch [191], Loss: 146668.250000
Epoch [1/1], Batch [201], Loss: 145382.781250
Epoch [1/1], Batch [211], Loss: 149976.718750
Epoch [1/1], Batch [221], Loss: 145441.656250
Epoch [1/1], Batch [231], Loss: 147213.625000
Epoch [1/1], Batch [241], Loss: 145181.109375
Epoch [1/1], Batch [251], Loss: 155740.156250
Epoch [1/1], Batch [261], Loss: 139780.375000
Epoch [1/1], Batch [271], Loss: 148026.406250
Epoch [1/1], Batch [281], Loss: 145803.593750
Epoch [1/1], Batch [291], Loss: 144259.250000
Epoch [1/1], Batch [301], Loss: 140340.203125
Epoch [1/1], Batch [311], Loss: 142622.250000
Epoch [1/1], Batch [321], Loss: 146436.812500
Epoch [1/1], Batch [331], Loss: 139956.203125
Epoch [1/1], Batch [341], Loss: 137936.593750
Epoch [1/1], Batch [351], Loss: 150754.031250
Epoch [1/1], Batch [361], Loss: 143377.421875
Epoch [1/1], Batch [371], Loss: 145324.218750
Epoch [1/1], Batch [381], Loss: 144512.609375
Epoch [1/1], Batch [391], Loss: 146522.312500
Epoch [1/1], Batch [401], Loss: 138660.531250
Epoch [1/1], Batch [411], Loss: 148179.421875
Epoch [1/1], Batch [421], Loss: 139381.250000
Epoch [1/1], Batch [431], Loss: 145512.250000
Epoch [1/1], Batch [441], Loss: 144483.656250
Epoch [1/1], Batch [451], Loss: 139278.015625
Epoch [1/1], Batch [461], Loss: 143385.281250
Epoch [1/1], Batch [471], Loss: 144220.687500
Epoch [1/1], Batch [481], Loss: 141718.140625
Epoch [1/1], Batch [491], Loss: 144938.062500
Epoch [1/1], Batch [501], Loss: 143593.640625
Epoch [1/1], Batch [511], Loss: 144209.250000
Epoch [1/1], Batch [521], Loss: 145992.250000
Epoch [1/1], Batch [531], Loss: 148331.156250
Epoch [1/1], Batch [541], Loss: 147163.312500
Epoch [1/1], Batch [551], Loss: 142832.375000
Epoch [1/1], Batch [561], Loss: 140818.062500
Epoch [1/1], Batch [571], Loss: 148061.093750
Epoch [1/1], Batch [581], Loss: 146663.140625
Epoch [1/1], Batch [591], Loss: 148136.500000
Epoch [1/1], Batch [601], Loss: 139003.515625
Epoch [1/1], Batch [611], Loss: 136097.187500
Epoch [1/1], Batch [621], Loss: 141313.406250
Epoch [1/1], Batch [631], Loss: 143311.906250
Epoch [1/1], Batch [641], Loss: 144150.468750
Epoch [1/1], Batch [651], Loss: 141490.218750
Epoch [1/1], Batch [661], Loss: 138397.328125
Epoch [1/1], Batch [671], Loss: 146934.453125
Epoch [1/1], Batch [681], Loss: 136580.984375
Epoch [1/1], Batch [691], Loss: 138787.437500
Epoch [1/1], Batch [701], Loss: 139091.765625
Epoch [1/1], Batch [711], Loss: 146011.812500
Epoch [1/1], Batch [721], Loss: 140473.156250
Epoch [1/1], Batch [731], Loss: 142148.078125
Epoch [1/1], Batch [741], Loss: 143243.687500
Epoch [1/1], Batch [751], Loss: 144902.750000
Epoch [1/1], Batch [761], Loss: 142945.140625
Epoch [1/1], Batch [771], Loss: 141548.000000
Epoch [1/1], Batch [781], Loss: 145689.593750
Epoch [1/1], Batch [791], Loss: 147176.312500
Epoch [1/1], Batch [801], Loss: 142565.750000
Epoch [1/1], Batch [811], Loss: 140742.125000
Epoch [1/1], Batch [821], Loss: 139931.781250
Epoch [1/1], Batch [831], Loss: 145296.828125
Epoch [1/1], Batch [841], Loss: 144162.593750
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 143934.1230
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 137406.9475
Elapsed time: 3793.17 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 146726.2711
Elapsed time: 3813.51 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 169150.343750
Epoch [1/1], Batch [11], Loss: 163249.437500
Epoch [1/1], Batch [21], Loss: 170085.906250
Epoch [1/1], Batch [31], Loss: 169826.906250
Epoch [1/1], Batch [41], Loss: 173960.406250
Epoch [1/1], Batch [51], Loss: 170813.187500
Epoch [1/1], Batch [61], Loss: 165561.484375
Epoch [1/1], Batch [71], Loss: 163424.250000
Epoch [1/1], Batch [81], Loss: 172563.406250
Epoch [1/1], Batch [91], Loss: 165581.687500
Epoch [1/1], Batch [101], Loss: 168866.593750
Epoch [1/1], Batch [111], Loss: 173627.234375
Epoch [1/1], Batch [121], Loss: 167778.015625
Epoch [1/1], Batch [131], Loss: 162770.125000
Epoch [1/1], Batch [141], Loss: 170878.203125
Epoch [1/1], Batch [151], Loss: 169907.718750
Epoch [1/1], Batch [161], Loss: 175794.515625
Epoch [1/1], Batch [171], Loss: 170204.078125
Epoch [1/1], Batch [181], Loss: 162716.375000
Epoch [1/1], Batch [191], Loss: 165446.859375
Epoch [1/1], Batch [201], Loss: 168841.765625
Epoch [1/1], Batch [211], Loss: 159499.781250
Epoch [1/1], Batch [221], Loss: 167889.281250
Epoch [1/1], Batch [231], Loss: 163591.953125
Epoch [1/1], Batch [241], Loss: 169925.765625
Epoch [1/1], Batch [251], Loss: 167663.593750
Epoch [1/1], Batch [261], Loss: 169402.687500
Epoch [1/1], Batch [271], Loss: 166196.937500
Epoch [1/1], Batch [281], Loss: 170853.578125
Epoch [1/1], Batch [291], Loss: 161279.718750
Epoch [1/1], Batch [301], Loss: 164864.671875
Epoch [1/1], Batch [311], Loss: 170533.375000
Epoch [1/1], Batch [321], Loss: 162397.125000
Epoch [1/1], Batch [331], Loss: 165200.171875
Epoch [1/1], Batch [341], Loss: 166096.312500
Epoch [1/1], Batch [351], Loss: 167284.843750
Epoch [1/1], Batch [361], Loss: 161345.656250
Epoch [1/1], Batch [371], Loss: 169841.656250
Epoch [1/1], Batch [381], Loss: 162604.406250
Epoch [1/1], Batch [391], Loss: 165348.000000
Epoch [1/1], Batch [401], Loss: 165308.859375
Epoch [1/1], Batch [411], Loss: 170344.562500
Epoch [1/1], Batch [421], Loss: 169292.656250
Epoch [1/1], Batch [431], Loss: 164970.750000
Epoch [1/1], Batch [441], Loss: 161500.218750
Epoch [1/1], Batch [451], Loss: 162339.343750
Epoch [1/1], Batch [461], Loss: 160427.171875
Epoch [1/1], Batch [471], Loss: 167393.546875
Epoch [1/1], Batch [481], Loss: 166996.281250
Epoch [1/1], Batch [491], Loss: 162661.640625
Epoch [1/1], Batch [501], Loss: 168359.531250
Epoch [1/1], Batch [511], Loss: 172070.578125
Epoch [1/1], Batch [521], Loss: 164114.312500
Epoch [1/1], Batch [531], Loss: 163536.531250
Epoch [1/1], Batch [541], Loss: 168498.437500
Epoch [1/1], Batch [551], Loss: 171770.187500
Epoch [1/1], Batch [561], Loss: 162875.937500
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 167079.0036
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 158743.3832
Elapsed time: 4293.46 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 166718.3576
Elapsed time: 4308.79 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 198623.781250
Epoch [1/1], Batch [11], Loss: 190317.312500
Epoch [1/1], Batch [21], Loss: 192177.406250
Epoch [1/1], Batch [31], Loss: 195807.625000
Epoch [1/1], Batch [41], Loss: 187711.093750
Epoch [1/1], Batch [51], Loss: 193282.656250
Epoch [1/1], Batch [61], Loss: 187102.343750
Epoch [1/1], Batch [71], Loss: 200585.640625
Epoch [1/1], Batch [81], Loss: 192092.312500
Epoch [1/1], Batch [91], Loss: 196335.781250
Epoch [1/1], Batch [101], Loss: 195931.218750
Epoch [1/1], Batch [111], Loss: 201259.953125
Epoch [1/1], Batch [121], Loss: 196354.375000
Epoch [1/1], Batch [131], Loss: 193785.671875
Epoch [1/1], Batch [141], Loss: 190389.187500
Epoch [1/1], Batch [151], Loss: 189434.406250
Epoch [1/1], Batch [161], Loss: 189397.093750
Epoch [1/1], Batch [171], Loss: 192010.968750
Epoch [1/1], Batch [181], Loss: 193858.687500
Epoch [1/1], Batch [191], Loss: 188196.531250
Epoch [1/1], Batch [201], Loss: 195745.296875
Epoch [1/1], Batch [211], Loss: 191156.718750
Epoch [1/1], Batch [221], Loss: 193514.140625
Epoch [1/1], Batch [231], Loss: 187006.187500
Epoch [1/1], Batch [241], Loss: 191366.875000
Epoch [1/1], Batch [251], Loss: 198522.812500
Epoch [1/1], Batch [261], Loss: 193969.203125
Epoch [1/1], Batch [271], Loss: 194011.125000
Epoch [1/1], Batch [281], Loss: 194082.500000
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 191728.8097
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 185947.4517
Elapsed time: 4603.25 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 197476.9421
Elapsed time: 4611.77 seconds

Training complete!
Totoal elapsed time: 4611.77 seconds
CUDA is available!
