Starting job 1008761
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [3, 3, 5, 5],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = False,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 356245.312500
Epoch [1/1], Batch [11], Loss: 149974.531250
Epoch [1/1], Batch [21], Loss: 93778.953125
Epoch [1/1], Batch [31], Loss: 73290.734375
Epoch [1/1], Batch [41], Loss: 61604.523438
Epoch [1/1], Batch [51], Loss: 55284.046875
Epoch [1/1], Batch [61], Loss: 52957.375000
Epoch [1/1], Batch [71], Loss: 53953.093750
Epoch [1/1], Batch [81], Loss: 50565.570312
Epoch [1/1], Batch [91], Loss: 49189.601562
Epoch [1/1], Batch [101], Loss: 48950.500000
Epoch [1/1], Batch [111], Loss: 51171.742188
Epoch [1/1], Batch [121], Loss: 47350.261719
Epoch [1/1], Batch [131], Loss: 49938.394531
Epoch [1/1], Batch [141], Loss: 48620.882812
Epoch [1/1], Batch [151], Loss: 47627.570312
Epoch [1/1], Batch [161], Loss: 47194.664062
Epoch [1/1], Batch [171], Loss: 48582.652344
Epoch [1/1], Batch [181], Loss: 45324.519531
Epoch [1/1], Batch [191], Loss: 45651.593750
Epoch [1/1], Batch [201], Loss: 43504.843750
Epoch [1/1], Batch [211], Loss: 44267.148438
Epoch [1/1], Batch [221], Loss: 42999.640625
Epoch [1/1], Batch [231], Loss: 46973.621094
Epoch [1/1], Batch [241], Loss: 45793.328125
Epoch [1/1], Batch [251], Loss: 45418.574219
Epoch [1/1], Batch [261], Loss: 44698.816406
Epoch [1/1], Batch [271], Loss: 42681.339844
Epoch [1/1], Batch [281], Loss: 46001.296875
Epoch [1/1], Batch [291], Loss: 42851.960938
Epoch [1/1], Batch [301], Loss: 42021.921875
Epoch [1/1], Batch [311], Loss: 43230.601562
Epoch [1/1], Batch [321], Loss: 42556.765625
Epoch [1/1], Batch [331], Loss: 43740.808594
Epoch [1/1], Batch [341], Loss: 42181.183594
Epoch [1/1], Batch [351], Loss: 41589.304688
Epoch [1/1], Batch [361], Loss: 41260.750000
Epoch [1/1], Batch [371], Loss: 42469.175781
Epoch [1/1], Batch [381], Loss: 42068.300781
Epoch [1/1], Batch [391], Loss: 43682.156250
Epoch [1/1], Batch [401], Loss: 39674.253906
Epoch [1/1], Batch [411], Loss: 41282.609375
Epoch [1/1], Batch [421], Loss: 40131.781250
Epoch [1/1], Batch [431], Loss: 41841.835938
Epoch [1/1], Batch [441], Loss: 41129.593750
Epoch [1/1], Batch [451], Loss: 41813.742188
Epoch [1/1], Batch [461], Loss: 41051.085938
Epoch [1/1], Batch [471], Loss: 39966.750000
Epoch [1/1], Batch [481], Loss: 40529.304688
Epoch [1/1], Batch [491], Loss: 40881.917969
Epoch [1/1], Batch [501], Loss: 40730.250000
Epoch [1/1], Batch [511], Loss: 40488.964844
Epoch [1/1], Batch [521], Loss: 40794.226562
Epoch [1/1], Batch [531], Loss: 41360.226562
Epoch [1/1], Batch [541], Loss: 39744.308594
Epoch [1/1], Batch [551], Loss: 38514.242188
Epoch [1/1], Batch [561], Loss: 38827.464844
Epoch [1/1], Batch [571], Loss: 39676.292969
Epoch [1/1], Batch [581], Loss: 40383.406250
Epoch [1/1], Batch [591], Loss: 39354.156250
Epoch [1/1], Batch [601], Loss: 37878.070312
Epoch [1/1], Batch [611], Loss: 39360.843750
Epoch [1/1], Batch [621], Loss: 39666.500000
Epoch [1/1], Batch [631], Loss: 38151.843750
Epoch [1/1], Batch [641], Loss: 39379.777344
Epoch [1/1], Batch [651], Loss: 40532.050781
Epoch [1/1], Batch [661], Loss: 38754.054688
Epoch [1/1], Batch [671], Loss: 38696.195312
Epoch [1/1], Batch [681], Loss: 38702.996094
Epoch [1/1], Batch [691], Loss: 40350.273438
Epoch [1/1], Batch [701], Loss: 39766.453125
Epoch [1/1], Batch [711], Loss: 39277.937500
Epoch [1/1], Batch [721], Loss: 36782.023438
Epoch [1/1], Batch [731], Loss: 39500.738281
Epoch [1/1], Batch [741], Loss: 37519.828125
Epoch [1/1], Batch [751], Loss: 39199.316406
Epoch [1/1], Batch [761], Loss: 37820.464844
Epoch [1/1], Batch [771], Loss: 37306.468750
Epoch [1/1], Batch [781], Loss: 39556.093750
Epoch [1/1], Batch [791], Loss: 38754.000000
Epoch [1/1], Batch [801], Loss: 36195.613281
Epoch [1/1], Batch [811], Loss: 37205.195312
Epoch [1/1], Batch [821], Loss: 40261.621094
Epoch [1/1], Batch [831], Loss: 37329.015625
Epoch [1/1], Batch [841], Loss: 38898.781250
Epoch [1/1], Batch [851], Loss: 38221.195312
Epoch [1/1], Batch [861], Loss: 39456.777344
Epoch [1/1], Batch [871], Loss: 38691.765625
Epoch [1/1], Batch [881], Loss: 38809.992188
Epoch [1/1], Batch [891], Loss: 37848.472656
Epoch [1/1], Batch [901], Loss: 38618.835938
Epoch [1/1], Batch [911], Loss: 36281.171875
Epoch [1/1], Batch [921], Loss: 36901.617188
Epoch [1/1], Batch [931], Loss: 38035.828125
Epoch [1/1], Batch [941], Loss: 38918.074219
Epoch [1/1], Batch [951], Loss: 38290.582031
Epoch [1/1], Batch [961], Loss: 37440.000000
Epoch [1/1], Batch [971], Loss: 35048.996094
Epoch [1/1], Batch [981], Loss: 37275.300781
Epoch [1/1], Batch [991], Loss: 36140.093750
Epoch [1/1], Batch [1001], Loss: 40127.941406
Epoch [1/1], Batch [1011], Loss: 36066.093750
Epoch [1/1], Batch [1021], Loss: 38234.324219
Epoch [1/1], Batch [1031], Loss: 37180.703125
Epoch [1/1], Batch [1041], Loss: 36380.558594
Epoch [1/1], Batch [1051], Loss: 37517.109375
Epoch [1/1], Batch [1061], Loss: 37040.117188
Epoch [1/1], Batch [1071], Loss: 36749.296875
Epoch [1/1], Batch [1081], Loss: 35791.707031
Epoch [1/1], Batch [1091], Loss: 36294.421875
Epoch [1/1], Batch [1101], Loss: 40275.914062
Epoch [1/1], Batch [1111], Loss: 37188.031250
Epoch [1/1], Batch [1121], Loss: 37568.652344
Epoch [1/1], Batch [1131], Loss: 37439.464844
Epoch [1/1], Batch [1141], Loss: 38394.906250
Epoch [1/1], Batch [1151], Loss: 37818.453125
Epoch [1/1], Batch [1161], Loss: 36709.695312
Epoch [1/1], Batch [1171], Loss: 37638.613281
Epoch [1/1], Batch [1181], Loss: 37110.109375
Epoch [1/1], Batch [1191], Loss: 38697.085938
Epoch [1/1], Batch [1201], Loss: 36319.531250
Epoch [1/1], Batch [1211], Loss: 35186.156250
Epoch [1/1], Batch [1221], Loss: 37437.226562
Epoch [1/1], Batch [1231], Loss: 36053.191406
Epoch [1/1], Batch [1241], Loss: 39396.234375
Epoch [1/1], Batch [1251], Loss: 35845.929688
Epoch [1/1], Batch [1261], Loss: 35835.441406
Epoch [1/1], Batch [1271], Loss: 38900.714844
Epoch [1/1], Batch [1281], Loss: 39473.492188
Epoch [1/1], Batch [1291], Loss: 36745.031250
Epoch [1/1], Batch [1301], Loss: 35161.035156
Epoch [1/1], Batch [1311], Loss: 36964.132812
Epoch [1/1], Batch [1321], Loss: 37214.148438
Epoch [1/1], Batch [1331], Loss: 37575.476562
Epoch [1/1], Batch [1341], Loss: 36528.679688
Epoch [1/1], Batch [1351], Loss: 35435.078125
Epoch [1/1], Batch [1361], Loss: 35838.554688
Epoch [1/1], Batch [1371], Loss: 36390.734375
Epoch [1/1], Batch [1381], Loss: 38199.851562
Epoch [1/1], Batch [1391], Loss: 34405.312500
Epoch [1/1], Batch [1401], Loss: 35142.265625
Epoch [1/1], Batch [1411], Loss: 34790.078125
Epoch [1/1], Batch [1421], Loss: 37356.972656
Epoch [1/1], Batch [1431], Loss: 35448.007812
Epoch [1/1], Batch [1441], Loss: 35515.562500
Epoch [1/1], Batch [1451], Loss: 38253.382812
Epoch [1/1], Batch [1461], Loss: 36685.777344
Epoch [1/1], Batch [1471], Loss: 37714.136719
Epoch [1/1], Batch [1481], Loss: 36052.082031
Epoch [1/1], Batch [1491], Loss: 37809.140625
Epoch [1/1], Batch [1501], Loss: 36641.875000
Epoch [1/1], Batch [1511], Loss: 35854.117188
Epoch [1/1], Batch [1521], Loss: 36270.476562
Epoch [1/1], Batch [1531], Loss: 36906.164062
Epoch [1/1], Batch [1541], Loss: 35882.796875
Epoch [1/1], Batch [1551], Loss: 39407.136719
Epoch [1/1], Batch [1561], Loss: 37753.539062
Epoch [1/1], Batch [1571], Loss: 36559.964844
Epoch [1/1], Batch [1581], Loss: 35251.378906
Epoch [1/1], Batch [1591], Loss: 36142.941406
Epoch [1/1], Batch [1601], Loss: 38151.664062
Epoch [1/1], Batch [1611], Loss: 36141.449219
Epoch [1/1], Batch [1621], Loss: 35995.687500
Epoch [1/1], Batch [1631], Loss: 35109.429688
Epoch [1/1], Batch [1641], Loss: 35282.003906
Epoch [1/1], Batch [1651], Loss: 37514.832031
Epoch [1/1], Batch [1661], Loss: 37578.183594
Epoch [1/1], Batch [1671], Loss: 34964.109375
Epoch [1/1], Batch [1681], Loss: 37018.945312
Epoch [1/1], Batch [1691], Loss: 37103.914062
Epoch [1/1], Batch [1701], Loss: 35993.054688
Epoch [1/1], Batch [1711], Loss: 34652.003906
Epoch [1/1], Batch [1721], Loss: 36423.429688
Epoch [1/1], Batch [1731], Loss: 34455.421875
Epoch [1/1], Batch [1741], Loss: 35111.171875
Epoch [1/1], Batch [1751], Loss: 33819.558594
Epoch [1/1], Batch [1761], Loss: 36601.105469
Epoch [1/1], Batch [1771], Loss: 36202.589844
Epoch [1/1], Batch [1781], Loss: 36356.000000
Epoch [1/1], Batch [1791], Loss: 34490.414062
Epoch [1/1], Batch [1801], Loss: 36092.765625
Epoch [1/1], Batch [1811], Loss: 38499.937500
Epoch [1/1], Batch [1821], Loss: 36082.960938
Epoch [1/1], Batch [1831], Loss: 34123.761719
Epoch [1/1], Batch [1841], Loss: 36384.789062
Epoch [1/1], Batch [1851], Loss: 34063.726562
Epoch [1/1], Batch [1861], Loss: 34709.492188
Epoch [1/1], Batch [1871], Loss: 36788.242188
Epoch [1/1], Batch [1881], Loss: 35063.746094
Epoch [1/1], Batch [1891], Loss: 36608.867188
Epoch [1/1], Batch [1901], Loss: 35511.972656
Epoch [1/1], Batch [1911], Loss: 35729.109375
Epoch [1/1], Batch [1921], Loss: 37910.117188
Epoch [1/1], Batch [1931], Loss: 36226.218750
Epoch [1/1], Batch [1941], Loss: 38486.023438
Epoch [1/1], Batch [1951], Loss: 36150.210938
Epoch [1/1], Batch [1961], Loss: 37429.015625
Epoch [1/1], Batch [1971], Loss: 35376.843750
Epoch [1/1], Batch [1981], Loss: 35831.335938
Epoch [1/1], Batch [1991], Loss: 35367.007812
Epoch [1/1], Batch [2001], Loss: 36925.718750
Epoch [1/1], Batch [2011], Loss: 37142.828125
Epoch [1/1], Batch [2021], Loss: 34231.992188
Epoch [1/1], Batch [2031], Loss: 35528.078125
Epoch [1/1], Batch [2041], Loss: 35272.648438
Epoch [1/1], Batch [2051], Loss: 35686.621094
Epoch [1/1], Batch [2061], Loss: 35282.382812
Epoch [1/1], Batch [2071], Loss: 35194.546875
Epoch [1/1], Batch [2081], Loss: 34629.636719
Epoch [1/1], Batch [2091], Loss: 33559.726562
Epoch [1/1], Batch [2101], Loss: 35670.718750
Epoch [1/1], Batch [2111], Loss: 35598.472656
Epoch [1/1], Batch [2121], Loss: 35739.585938
Epoch [1/1], Batch [2131], Loss: 34398.191406
Epoch [1/1], Batch [2141], Loss: 33940.652344
Epoch [1/1], Batch [2151], Loss: 36428.390625
Epoch [1/1], Batch [2161], Loss: 36809.039062
Epoch [1/1], Batch [2171], Loss: 33236.281250
Epoch [1/1], Batch [2181], Loss: 35970.617188
Epoch [1/1], Batch [2191], Loss: 34734.039062
Epoch [1/1], Batch [2201], Loss: 36865.824219
Epoch [1/1], Batch [2211], Loss: 34107.878906
Epoch [1/1], Batch [2221], Loss: 33443.000000
Epoch [1/1], Batch [2231], Loss: 34830.167969
Epoch [1/1], Batch [2241], Loss: 36759.871094
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 40262.6272
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 35028.1379
Elapsed time: 511.31 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 35626.6565
Elapsed time: 532.25 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 75443.625000
Epoch [1/1], Batch [11], Loss: 77646.796875
Epoch [1/1], Batch [21], Loss: 66187.187500
Epoch [1/1], Batch [31], Loss: 67488.820312
Epoch [1/1], Batch [41], Loss: 69620.062500
Epoch [1/1], Batch [51], Loss: 62537.437500
Epoch [1/1], Batch [61], Loss: 60606.437500
Epoch [1/1], Batch [71], Loss: 61119.019531
Epoch [1/1], Batch [81], Loss: 60934.617188
Epoch [1/1], Batch [91], Loss: 60495.531250
Epoch [1/1], Batch [101], Loss: 59437.507812
Epoch [1/1], Batch [111], Loss: 57065.406250
Epoch [1/1], Batch [121], Loss: 58985.593750
Epoch [1/1], Batch [131], Loss: 60593.523438
Epoch [1/1], Batch [141], Loss: 61538.421875
Epoch [1/1], Batch [151], Loss: 57108.089844
Epoch [1/1], Batch [161], Loss: 57574.882812
Epoch [1/1], Batch [171], Loss: 59261.601562
Epoch [1/1], Batch [181], Loss: 59766.593750
Epoch [1/1], Batch [191], Loss: 56641.070312
Epoch [1/1], Batch [201], Loss: 56526.304688
Epoch [1/1], Batch [211], Loss: 55412.742188
Epoch [1/1], Batch [221], Loss: 57103.015625
Epoch [1/1], Batch [231], Loss: 56537.414062
Epoch [1/1], Batch [241], Loss: 55688.593750
Epoch [1/1], Batch [251], Loss: 54115.664062
Epoch [1/1], Batch [261], Loss: 53414.152344
Epoch [1/1], Batch [271], Loss: 60032.265625
Epoch [1/1], Batch [281], Loss: 53994.089844
Epoch [1/1], Batch [291], Loss: 58194.914062
Epoch [1/1], Batch [301], Loss: 56833.429688
Epoch [1/1], Batch [311], Loss: 58924.398438
Epoch [1/1], Batch [321], Loss: 56489.703125
Epoch [1/1], Batch [331], Loss: 59473.925781
Epoch [1/1], Batch [341], Loss: 54355.761719
Epoch [1/1], Batch [351], Loss: 52668.156250
Epoch [1/1], Batch [361], Loss: 52850.835938
Epoch [1/1], Batch [371], Loss: 55755.371094
Epoch [1/1], Batch [381], Loss: 56294.593750
Epoch [1/1], Batch [391], Loss: 54845.035156
Epoch [1/1], Batch [401], Loss: 57033.320312
Epoch [1/1], Batch [411], Loss: 56190.332031
Epoch [1/1], Batch [421], Loss: 54240.054688
Epoch [1/1], Batch [431], Loss: 55123.621094
Epoch [1/1], Batch [441], Loss: 56437.921875
Epoch [1/1], Batch [451], Loss: 54482.742188
Epoch [1/1], Batch [461], Loss: 55909.351562
Epoch [1/1], Batch [471], Loss: 55185.593750
Epoch [1/1], Batch [481], Loss: 56543.601562
Epoch [1/1], Batch [491], Loss: 54966.910156
Epoch [1/1], Batch [501], Loss: 55075.847656
Epoch [1/1], Batch [511], Loss: 54134.300781
Epoch [1/1], Batch [521], Loss: 55569.488281
Epoch [1/1], Batch [531], Loss: 54077.933594
Epoch [1/1], Batch [541], Loss: 54959.289062
Epoch [1/1], Batch [551], Loss: 56129.765625
Epoch [1/1], Batch [561], Loss: 53452.078125
Epoch [1/1], Batch [571], Loss: 52363.699219
Epoch [1/1], Batch [581], Loss: 55539.812500
Epoch [1/1], Batch [591], Loss: 54250.632812
Epoch [1/1], Batch [601], Loss: 55017.390625
Epoch [1/1], Batch [611], Loss: 53177.054688
Epoch [1/1], Batch [621], Loss: 55885.472656
Epoch [1/1], Batch [631], Loss: 52207.250000
Epoch [1/1], Batch [641], Loss: 55059.882812
Epoch [1/1], Batch [651], Loss: 52538.734375
Epoch [1/1], Batch [661], Loss: 52131.820312
Epoch [1/1], Batch [671], Loss: 51451.445312
Epoch [1/1], Batch [681], Loss: 52916.902344
Epoch [1/1], Batch [691], Loss: 54419.046875
Epoch [1/1], Batch [701], Loss: 53167.953125
Epoch [1/1], Batch [711], Loss: 57550.917969
Epoch [1/1], Batch [721], Loss: 52940.304688
Epoch [1/1], Batch [731], Loss: 53256.402344
Epoch [1/1], Batch [741], Loss: 52970.347656
Epoch [1/1], Batch [751], Loss: 51493.828125
Epoch [1/1], Batch [761], Loss: 54106.789062
Epoch [1/1], Batch [771], Loss: 52730.132812
Epoch [1/1], Batch [781], Loss: 54343.656250
Epoch [1/1], Batch [791], Loss: 52162.398438
Epoch [1/1], Batch [801], Loss: 51595.820312
Epoch [1/1], Batch [811], Loss: 52587.300781
Epoch [1/1], Batch [821], Loss: 52462.218750
Epoch [1/1], Batch [831], Loss: 54394.277344
Epoch [1/1], Batch [841], Loss: 55356.550781
Epoch [1/1], Batch [851], Loss: 53129.375000
Epoch [1/1], Batch [861], Loss: 51596.492188
Epoch [1/1], Batch [871], Loss: 53238.656250
Epoch [1/1], Batch [881], Loss: 51906.867188
Epoch [1/1], Batch [891], Loss: 53615.414062
Epoch [1/1], Batch [901], Loss: 53645.695312
Epoch [1/1], Batch [911], Loss: 50995.179688
Epoch [1/1], Batch [921], Loss: 50902.796875
Epoch [1/1], Batch [931], Loss: 53522.074219
Epoch [1/1], Batch [941], Loss: 52264.367188
Epoch [1/1], Batch [951], Loss: 49700.750000
Epoch [1/1], Batch [961], Loss: 55789.128906
Epoch [1/1], Batch [971], Loss: 53812.625000
Epoch [1/1], Batch [981], Loss: 51703.863281
Epoch [1/1], Batch [991], Loss: 52082.164062
Epoch [1/1], Batch [1001], Loss: 50859.371094
Epoch [1/1], Batch [1011], Loss: 54133.812500
Epoch [1/1], Batch [1021], Loss: 52429.960938
Epoch [1/1], Batch [1031], Loss: 52097.199219
Epoch [1/1], Batch [1041], Loss: 54544.628906
Epoch [1/1], Batch [1051], Loss: 52476.804688
Epoch [1/1], Batch [1061], Loss: 52284.863281
Epoch [1/1], Batch [1071], Loss: 51934.523438
Epoch [1/1], Batch [1081], Loss: 55430.898438
Epoch [1/1], Batch [1091], Loss: 53204.453125
Epoch [1/1], Batch [1101], Loss: 52180.496094
Epoch [1/1], Batch [1111], Loss: 50516.277344
Epoch [1/1], Batch [1121], Loss: 51255.175781
Epoch [1/1], Batch [1131], Loss: 51805.867188
Epoch [1/1], Batch [1141], Loss: 53620.863281
Epoch [1/1], Batch [1151], Loss: 54012.710938
Epoch [1/1], Batch [1161], Loss: 52671.101562
Epoch [1/1], Batch [1171], Loss: 54898.851562
Epoch [1/1], Batch [1181], Loss: 53589.148438
Epoch [1/1], Batch [1191], Loss: 52063.281250
Epoch [1/1], Batch [1201], Loss: 50850.046875
Epoch [1/1], Batch [1211], Loss: 51634.273438
Epoch [1/1], Batch [1221], Loss: 53634.632812
Epoch [1/1], Batch [1231], Loss: 51625.339844
Epoch [1/1], Batch [1241], Loss: 53607.976562
Epoch [1/1], Batch [1251], Loss: 54557.648438
Epoch [1/1], Batch [1261], Loss: 51924.867188
Epoch [1/1], Batch [1271], Loss: 51858.312500
Epoch [1/1], Batch [1281], Loss: 50366.984375
Epoch [1/1], Batch [1291], Loss: 50787.265625
Epoch [1/1], Batch [1301], Loss: 52402.320312
Epoch [1/1], Batch [1311], Loss: 53031.546875
Epoch [1/1], Batch [1321], Loss: 54851.781250
Epoch [1/1], Batch [1331], Loss: 51877.921875
Epoch [1/1], Batch [1341], Loss: 52778.488281
Epoch [1/1], Batch [1351], Loss: 53947.835938
Epoch [1/1], Batch [1361], Loss: 51932.000000
Epoch [1/1], Batch [1371], Loss: 51395.164062
Epoch [1/1], Batch [1381], Loss: 53878.953125
Epoch [1/1], Batch [1391], Loss: 48890.203125
Epoch [1/1], Batch [1401], Loss: 54188.179688
Epoch [1/1], Batch [1411], Loss: 49408.425781
Epoch [1/1], Batch [1421], Loss: 54690.382812
Epoch [1/1], Batch [1431], Loss: 49046.582031
Epoch [1/1], Batch [1441], Loss: 54820.007812
Epoch [1/1], Batch [1451], Loss: 51880.859375
Epoch [1/1], Batch [1461], Loss: 52789.152344
Epoch [1/1], Batch [1471], Loss: 52279.835938
Epoch [1/1], Batch [1481], Loss: 52847.523438
Epoch [1/1], Batch [1491], Loss: 53146.359375
Epoch [1/1], Batch [1501], Loss: 49731.347656
Epoch [1/1], Batch [1511], Loss: 50719.984375
Epoch [1/1], Batch [1521], Loss: 52395.320312
Epoch [1/1], Batch [1531], Loss: 49633.117188
Epoch [1/1], Batch [1541], Loss: 49753.945312
Epoch [1/1], Batch [1551], Loss: 52410.492188
Epoch [1/1], Batch [1561], Loss: 51611.003906
Epoch [1/1], Batch [1571], Loss: 54393.316406
Epoch [1/1], Batch [1581], Loss: 51522.242188
Epoch [1/1], Batch [1591], Loss: 51010.628906
Epoch [1/1], Batch [1601], Loss: 50617.765625
Epoch [1/1], Batch [1611], Loss: 52115.773438
Epoch [1/1], Batch [1621], Loss: 53973.886719
Epoch [1/1], Batch [1631], Loss: 51732.968750
Epoch [1/1], Batch [1641], Loss: 51117.718750
Epoch [1/1], Batch [1651], Loss: 50377.296875
Epoch [1/1], Batch [1661], Loss: 52167.464844
Epoch [1/1], Batch [1671], Loss: 51095.183594
Epoch [1/1], Batch [1681], Loss: 50922.113281
Epoch [1/1], Batch [1691], Loss: 49125.933594
Epoch [1/1], Batch [1701], Loss: 50394.367188
Epoch [1/1], Batch [1711], Loss: 48420.585938
Epoch [1/1], Batch [1721], Loss: 52494.652344
Epoch [1/1], Batch [1731], Loss: 52463.273438
Epoch [1/1], Batch [1741], Loss: 51993.589844
Epoch [1/1], Batch [1751], Loss: 50773.886719
Epoch [1/1], Batch [1761], Loss: 50809.484375
Epoch [1/1], Batch [1771], Loss: 52508.835938
Epoch [1/1], Batch [1781], Loss: 52105.429688
Epoch [1/1], Batch [1791], Loss: 50696.148438
Epoch [1/1], Batch [1801], Loss: 48949.878906
Epoch [1/1], Batch [1811], Loss: 51812.824219
Epoch [1/1], Batch [1821], Loss: 52644.718750
Epoch [1/1], Batch [1831], Loss: 52685.187500
Epoch [1/1], Batch [1841], Loss: 51197.023438
Epoch [1/1], Batch [1851], Loss: 50490.441406
Epoch [1/1], Batch [1861], Loss: 50900.796875
Epoch [1/1], Batch [1871], Loss: 52406.218750
Epoch [1/1], Batch [1881], Loss: 50384.679688
Epoch [1/1], Batch [1891], Loss: 51439.648438
Epoch [1/1], Batch [1901], Loss: 50086.914062
Epoch [1/1], Batch [1911], Loss: 48967.250000
Epoch [1/1], Batch [1921], Loss: 55095.855469
Epoch [1/1], Batch [1931], Loss: 52256.128906
Epoch [1/1], Batch [1941], Loss: 47865.367188
Epoch [1/1], Batch [1951], Loss: 51749.500000
Epoch [1/1], Batch [1961], Loss: 48389.468750
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 53980.6867
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 49936.5941
Elapsed time: 1184.56 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 51436.9564
Elapsed time: 1209.82 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 73498.429688
Epoch [1/1], Batch [11], Loss: 71300.296875
Epoch [1/1], Batch [21], Loss: 74154.742188
Epoch [1/1], Batch [31], Loss: 74877.250000
Epoch [1/1], Batch [41], Loss: 77220.703125
Epoch [1/1], Batch [51], Loss: 74287.937500
Epoch [1/1], Batch [61], Loss: 72370.062500
Epoch [1/1], Batch [71], Loss: 74696.343750
Epoch [1/1], Batch [81], Loss: 70434.851562
Epoch [1/1], Batch [91], Loss: 71868.187500
Epoch [1/1], Batch [101], Loss: 70228.781250
Epoch [1/1], Batch [111], Loss: 69468.468750
Epoch [1/1], Batch [121], Loss: 70887.890625
Epoch [1/1], Batch [131], Loss: 74709.523438
Epoch [1/1], Batch [141], Loss: 71902.187500
Epoch [1/1], Batch [151], Loss: 70615.296875
Epoch [1/1], Batch [161], Loss: 73944.593750
Epoch [1/1], Batch [171], Loss: 69619.445312
Epoch [1/1], Batch [181], Loss: 66856.484375
Epoch [1/1], Batch [191], Loss: 77041.554688
Epoch [1/1], Batch [201], Loss: 71993.906250
Epoch [1/1], Batch [211], Loss: 69740.734375
Epoch [1/1], Batch [221], Loss: 71450.031250
Epoch [1/1], Batch [231], Loss: 70323.046875
Epoch [1/1], Batch [241], Loss: 72021.484375
Epoch [1/1], Batch [251], Loss: 75586.828125
Epoch [1/1], Batch [261], Loss: 71063.828125
Epoch [1/1], Batch [271], Loss: 71057.601562
Epoch [1/1], Batch [281], Loss: 71879.679688
Epoch [1/1], Batch [291], Loss: 68206.976562
Epoch [1/1], Batch [301], Loss: 74142.593750
Epoch [1/1], Batch [311], Loss: 68943.843750
Epoch [1/1], Batch [321], Loss: 72707.265625
Epoch [1/1], Batch [331], Loss: 71829.164062
Epoch [1/1], Batch [341], Loss: 71872.656250
Epoch [1/1], Batch [351], Loss: 72138.250000
Epoch [1/1], Batch [361], Loss: 72207.804688
Epoch [1/1], Batch [371], Loss: 72225.812500
Epoch [1/1], Batch [381], Loss: 74328.164062
Epoch [1/1], Batch [391], Loss: 69887.671875
Epoch [1/1], Batch [401], Loss: 72552.046875
Epoch [1/1], Batch [411], Loss: 68851.921875
Epoch [1/1], Batch [421], Loss: 71331.953125
Epoch [1/1], Batch [431], Loss: 71890.359375
Epoch [1/1], Batch [441], Loss: 69232.265625
Epoch [1/1], Batch [451], Loss: 68220.359375
Epoch [1/1], Batch [461], Loss: 74075.468750
Epoch [1/1], Batch [471], Loss: 72041.562500
Epoch [1/1], Batch [481], Loss: 68548.546875
Epoch [1/1], Batch [491], Loss: 71639.078125
Epoch [1/1], Batch [501], Loss: 70697.203125
Epoch [1/1], Batch [511], Loss: 67517.828125
Epoch [1/1], Batch [521], Loss: 69083.828125
Epoch [1/1], Batch [531], Loss: 69836.234375
Epoch [1/1], Batch [541], Loss: 66377.023438
Epoch [1/1], Batch [551], Loss: 66400.132812
Epoch [1/1], Batch [561], Loss: 69405.687500
Epoch [1/1], Batch [571], Loss: 68032.468750
Epoch [1/1], Batch [581], Loss: 69481.281250
Epoch [1/1], Batch [591], Loss: 65497.750000
Epoch [1/1], Batch [601], Loss: 70385.937500
Epoch [1/1], Batch [611], Loss: 72481.515625
Epoch [1/1], Batch [621], Loss: 67468.070312
Epoch [1/1], Batch [631], Loss: 71092.289062
Epoch [1/1], Batch [641], Loss: 67116.718750
Epoch [1/1], Batch [651], Loss: 69633.742188
Epoch [1/1], Batch [661], Loss: 68250.453125
Epoch [1/1], Batch [671], Loss: 70024.515625
Epoch [1/1], Batch [681], Loss: 62959.070312
Epoch [1/1], Batch [691], Loss: 73044.992188
Epoch [1/1], Batch [701], Loss: 68397.992188
Epoch [1/1], Batch [711], Loss: 69730.695312
Epoch [1/1], Batch [721], Loss: 67692.804688
Epoch [1/1], Batch [731], Loss: 70734.984375
Epoch [1/1], Batch [741], Loss: 67350.804688
Epoch [1/1], Batch [751], Loss: 67062.937500
Epoch [1/1], Batch [761], Loss: 69624.304688
Epoch [1/1], Batch [771], Loss: 69103.125000
Epoch [1/1], Batch [781], Loss: 68328.031250
Epoch [1/1], Batch [791], Loss: 67500.031250
Epoch [1/1], Batch [801], Loss: 72539.953125
Epoch [1/1], Batch [811], Loss: 70927.390625
Epoch [1/1], Batch [821], Loss: 71066.671875
Epoch [1/1], Batch [831], Loss: 69068.007812
Epoch [1/1], Batch [841], Loss: 68277.593750
Epoch [1/1], Batch [851], Loss: 69481.921875
Epoch [1/1], Batch [861], Loss: 68258.984375
Epoch [1/1], Batch [871], Loss: 70579.296875
Epoch [1/1], Batch [881], Loss: 68295.601562
Epoch [1/1], Batch [891], Loss: 69128.578125
Epoch [1/1], Batch [901], Loss: 68502.125000
Epoch [1/1], Batch [911], Loss: 71774.312500
Epoch [1/1], Batch [921], Loss: 70092.421875
Epoch [1/1], Batch [931], Loss: 67776.265625
Epoch [1/1], Batch [941], Loss: 66858.515625
Epoch [1/1], Batch [951], Loss: 70526.906250
Epoch [1/1], Batch [961], Loss: 67206.968750
Epoch [1/1], Batch [971], Loss: 67447.093750
Epoch [1/1], Batch [981], Loss: 67663.500000
Epoch [1/1], Batch [991], Loss: 68492.796875
Epoch [1/1], Batch [1001], Loss: 70947.093750
Epoch [1/1], Batch [1011], Loss: 71268.109375
Epoch [1/1], Batch [1021], Loss: 66320.500000
Epoch [1/1], Batch [1031], Loss: 67786.046875
Epoch [1/1], Batch [1041], Loss: 68664.179688
Epoch [1/1], Batch [1051], Loss: 69318.015625
Epoch [1/1], Batch [1061], Loss: 68297.250000
Epoch [1/1], Batch [1071], Loss: 67730.812500
Epoch [1/1], Batch [1081], Loss: 68494.078125
Epoch [1/1], Batch [1091], Loss: 67206.812500
Epoch [1/1], Batch [1101], Loss: 70840.992188
Epoch [1/1], Batch [1111], Loss: 65573.937500
Epoch [1/1], Batch [1121], Loss: 67808.968750
Epoch [1/1], Batch [1131], Loss: 68877.984375
Epoch [1/1], Batch [1141], Loss: 68478.898438
Epoch [1/1], Batch [1151], Loss: 63640.640625
Epoch [1/1], Batch [1161], Loss: 68290.312500
Epoch [1/1], Batch [1171], Loss: 68250.921875
Epoch [1/1], Batch [1181], Loss: 67277.484375
Epoch [1/1], Batch [1191], Loss: 67097.023438
Epoch [1/1], Batch [1201], Loss: 64631.546875
Epoch [1/1], Batch [1211], Loss: 69127.585938
Epoch [1/1], Batch [1221], Loss: 69045.343750
Epoch [1/1], Batch [1231], Loss: 67616.898438
Epoch [1/1], Batch [1241], Loss: 66301.687500
Epoch [1/1], Batch [1251], Loss: 69403.953125
Epoch [1/1], Batch [1261], Loss: 71589.593750
Epoch [1/1], Batch [1271], Loss: 69170.796875
Epoch [1/1], Batch [1281], Loss: 67172.093750
Epoch [1/1], Batch [1291], Loss: 67606.046875
Epoch [1/1], Batch [1301], Loss: 68519.468750
Epoch [1/1], Batch [1311], Loss: 66281.625000
Epoch [1/1], Batch [1321], Loss: 66313.453125
Epoch [1/1], Batch [1331], Loss: 67777.218750
Epoch [1/1], Batch [1341], Loss: 68598.171875
Epoch [1/1], Batch [1351], Loss: 72709.671875
Epoch [1/1], Batch [1361], Loss: 64370.187500
Epoch [1/1], Batch [1371], Loss: 69160.468750
Epoch [1/1], Batch [1381], Loss: 70720.023438
Epoch [1/1], Batch [1391], Loss: 67328.218750
Epoch [1/1], Batch [1401], Loss: 65778.875000
Epoch [1/1], Batch [1411], Loss: 66545.859375
Epoch [1/1], Batch [1421], Loss: 68220.679688
Epoch [1/1], Batch [1431], Loss: 68916.515625
Epoch [1/1], Batch [1441], Loss: 67092.046875
Epoch [1/1], Batch [1451], Loss: 67713.929688
Epoch [1/1], Batch [1461], Loss: 63149.875000
Epoch [1/1], Batch [1471], Loss: 65891.195312
Epoch [1/1], Batch [1481], Loss: 67521.367188
Epoch [1/1], Batch [1491], Loss: 66305.929688
Epoch [1/1], Batch [1501], Loss: 68472.835938
Epoch [1/1], Batch [1511], Loss: 68646.156250
Epoch [1/1], Batch [1521], Loss: 65542.593750
Epoch [1/1], Batch [1531], Loss: 64222.917969
Epoch [1/1], Batch [1541], Loss: 66748.453125
Epoch [1/1], Batch [1551], Loss: 64757.496094
Epoch [1/1], Batch [1561], Loss: 62759.289062
Epoch [1/1], Batch [1571], Loss: 67398.437500
Epoch [1/1], Batch [1581], Loss: 69936.359375
Epoch [1/1], Batch [1591], Loss: 69196.390625
Epoch [1/1], Batch [1601], Loss: 65510.046875
Epoch [1/1], Batch [1611], Loss: 66138.281250
Epoch [1/1], Batch [1621], Loss: 70978.632812
Epoch [1/1], Batch [1631], Loss: 66050.703125
Epoch [1/1], Batch [1641], Loss: 66341.992188
Epoch [1/1], Batch [1651], Loss: 63654.648438
Epoch [1/1], Batch [1661], Loss: 67144.703125
Epoch [1/1], Batch [1671], Loss: 65435.507812
Epoch [1/1], Batch [1681], Loss: 68384.125000
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 69209.7241
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 65197.4455
Elapsed time: 1941.18 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 68133.5003
Elapsed time: 1968.72 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 91057.101562
Epoch [1/1], Batch [11], Loss: 87706.109375
Epoch [1/1], Batch [21], Loss: 88830.234375
Epoch [1/1], Batch [31], Loss: 87854.765625
Epoch [1/1], Batch [41], Loss: 91581.648438
Epoch [1/1], Batch [51], Loss: 89843.796875
Epoch [1/1], Batch [61], Loss: 84475.398438
Epoch [1/1], Batch [71], Loss: 87061.085938
Epoch [1/1], Batch [81], Loss: 89459.265625
Epoch [1/1], Batch [91], Loss: 86208.593750
Epoch [1/1], Batch [101], Loss: 87039.500000
Epoch [1/1], Batch [111], Loss: 88461.140625
Epoch [1/1], Batch [121], Loss: 87824.562500
Epoch [1/1], Batch [131], Loss: 88418.593750
Epoch [1/1], Batch [141], Loss: 90223.351562
Epoch [1/1], Batch [151], Loss: 87898.265625
Epoch [1/1], Batch [161], Loss: 91391.515625
Epoch [1/1], Batch [171], Loss: 83909.460938
Epoch [1/1], Batch [181], Loss: 85707.195312
Epoch [1/1], Batch [191], Loss: 88033.914062
Epoch [1/1], Batch [201], Loss: 90918.335938
Epoch [1/1], Batch [211], Loss: 88416.937500
Epoch [1/1], Batch [221], Loss: 85877.687500
Epoch [1/1], Batch [231], Loss: 83863.109375
Epoch [1/1], Batch [241], Loss: 88489.953125
Epoch [1/1], Batch [251], Loss: 90786.734375
Epoch [1/1], Batch [261], Loss: 90868.359375
Epoch [1/1], Batch [271], Loss: 89820.664062
Epoch [1/1], Batch [281], Loss: 90540.390625
Epoch [1/1], Batch [291], Loss: 84411.843750
Epoch [1/1], Batch [301], Loss: 85881.796875
Epoch [1/1], Batch [311], Loss: 82501.484375
Epoch [1/1], Batch [321], Loss: 84097.093750
Epoch [1/1], Batch [331], Loss: 85779.093750
Epoch [1/1], Batch [341], Loss: 82093.632812
Epoch [1/1], Batch [351], Loss: 85826.406250
Epoch [1/1], Batch [361], Loss: 88904.515625
Epoch [1/1], Batch [371], Loss: 83573.796875
Epoch [1/1], Batch [381], Loss: 87647.781250
Epoch [1/1], Batch [391], Loss: 89001.265625
Epoch [1/1], Batch [401], Loss: 89447.898438
Epoch [1/1], Batch [411], Loss: 83296.101562
Epoch [1/1], Batch [421], Loss: 88416.796875
Epoch [1/1], Batch [431], Loss: 86876.132812
Epoch [1/1], Batch [441], Loss: 83805.132812
Epoch [1/1], Batch [451], Loss: 83816.109375
Epoch [1/1], Batch [461], Loss: 85253.992188
Epoch [1/1], Batch [471], Loss: 80501.781250
Epoch [1/1], Batch [481], Loss: 85986.921875
Epoch [1/1], Batch [491], Loss: 87280.812500
Epoch [1/1], Batch [501], Loss: 87332.585938
Epoch [1/1], Batch [511], Loss: 85310.156250
Epoch [1/1], Batch [521], Loss: 88723.140625
Epoch [1/1], Batch [531], Loss: 83501.789062
Epoch [1/1], Batch [541], Loss: 87275.867188
Epoch [1/1], Batch [551], Loss: 84760.546875
Epoch [1/1], Batch [561], Loss: 86638.250000
Epoch [1/1], Batch [571], Loss: 91411.375000
Epoch [1/1], Batch [581], Loss: 82748.031250
Epoch [1/1], Batch [591], Loss: 84637.000000
Epoch [1/1], Batch [601], Loss: 87507.460938
Epoch [1/1], Batch [611], Loss: 85387.578125
Epoch [1/1], Batch [621], Loss: 82732.218750
Epoch [1/1], Batch [631], Loss: 84647.523438
Epoch [1/1], Batch [641], Loss: 85347.429688
Epoch [1/1], Batch [651], Loss: 87801.234375
Epoch [1/1], Batch [661], Loss: 87949.187500
Epoch [1/1], Batch [671], Loss: 88739.867188
Epoch [1/1], Batch [681], Loss: 80101.992188
Epoch [1/1], Batch [691], Loss: 86701.562500
Epoch [1/1], Batch [701], Loss: 86256.179688
Epoch [1/1], Batch [711], Loss: 89839.851562
Epoch [1/1], Batch [721], Loss: 84357.453125
Epoch [1/1], Batch [731], Loss: 83974.187500
Epoch [1/1], Batch [741], Loss: 85360.203125
Epoch [1/1], Batch [751], Loss: 84390.960938
Epoch [1/1], Batch [761], Loss: 80360.078125
Epoch [1/1], Batch [771], Loss: 81811.953125
Epoch [1/1], Batch [781], Loss: 83175.546875
Epoch [1/1], Batch [791], Loss: 89359.718750
Epoch [1/1], Batch [801], Loss: 85956.812500
Epoch [1/1], Batch [811], Loss: 87056.867188
Epoch [1/1], Batch [821], Loss: 82970.531250
Epoch [1/1], Batch [831], Loss: 85328.406250
Epoch [1/1], Batch [841], Loss: 80920.593750
Epoch [1/1], Batch [851], Loss: 84289.164062
Epoch [1/1], Batch [861], Loss: 84543.140625
Epoch [1/1], Batch [871], Loss: 89233.953125
Epoch [1/1], Batch [881], Loss: 85040.007812
Epoch [1/1], Batch [891], Loss: 84533.664062
Epoch [1/1], Batch [901], Loss: 88774.390625
Epoch [1/1], Batch [911], Loss: 81181.617188
Epoch [1/1], Batch [921], Loss: 90567.117188
Epoch [1/1], Batch [931], Loss: 85201.109375
Epoch [1/1], Batch [941], Loss: 80409.460938
Epoch [1/1], Batch [951], Loss: 84621.992188
Epoch [1/1], Batch [961], Loss: 84460.156250
Epoch [1/1], Batch [971], Loss: 85886.460938
Epoch [1/1], Batch [981], Loss: 90441.203125
Epoch [1/1], Batch [991], Loss: 83200.335938
Epoch [1/1], Batch [1001], Loss: 86388.304688
Epoch [1/1], Batch [1011], Loss: 83642.453125
Epoch [1/1], Batch [1021], Loss: 83811.375000
Epoch [1/1], Batch [1031], Loss: 91669.375000
Epoch [1/1], Batch [1041], Loss: 79389.203125
Epoch [1/1], Batch [1051], Loss: 82108.546875
Epoch [1/1], Batch [1061], Loss: 80980.484375
Epoch [1/1], Batch [1071], Loss: 77780.171875
Epoch [1/1], Batch [1081], Loss: 82795.671875
Epoch [1/1], Batch [1091], Loss: 80661.359375
Epoch [1/1], Batch [1101], Loss: 81819.718750
Epoch [1/1], Batch [1111], Loss: 84640.468750
Epoch [1/1], Batch [1121], Loss: 82845.687500
Epoch [1/1], Batch [1131], Loss: 83603.359375
Epoch [1/1], Batch [1141], Loss: 83588.773438
Epoch [1/1], Batch [1151], Loss: 79262.734375
Epoch [1/1], Batch [1161], Loss: 84252.343750
Epoch [1/1], Batch [1171], Loss: 82128.625000
Epoch [1/1], Batch [1181], Loss: 83437.820312
Epoch [1/1], Batch [1191], Loss: 84483.437500
Epoch [1/1], Batch [1201], Loss: 81827.171875
Epoch [1/1], Batch [1211], Loss: 84721.687500
Epoch [1/1], Batch [1221], Loss: 80734.085938
Epoch [1/1], Batch [1231], Loss: 85361.437500
Epoch [1/1], Batch [1241], Loss: 77774.257812
Epoch [1/1], Batch [1251], Loss: 86487.679688
Epoch [1/1], Batch [1261], Loss: 84518.187500
Epoch [1/1], Batch [1271], Loss: 83755.015625
Epoch [1/1], Batch [1281], Loss: 83429.062500
Epoch [1/1], Batch [1291], Loss: 81286.953125
Epoch [1/1], Batch [1301], Loss: 87190.500000
Epoch [1/1], Batch [1311], Loss: 85026.281250
Epoch [1/1], Batch [1321], Loss: 86611.445312
Epoch [1/1], Batch [1331], Loss: 84431.828125
Epoch [1/1], Batch [1341], Loss: 83174.843750
Epoch [1/1], Batch [1351], Loss: 84144.234375
Epoch [1/1], Batch [1361], Loss: 84278.031250
Epoch [1/1], Batch [1371], Loss: 84501.656250
Epoch [1/1], Batch [1381], Loss: 83152.437500
Epoch [1/1], Batch [1391], Loss: 84008.148438
Epoch [1/1], Batch [1401], Loss: 80040.421875
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 85277.1145
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 81046.2257
Elapsed time: 2723.37 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 86567.9806
Elapsed time: 2751.28 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 107021.851562
Epoch [1/1], Batch [11], Loss: 100066.812500
Epoch [1/1], Batch [21], Loss: 107027.953125
Epoch [1/1], Batch [31], Loss: 105415.750000
Epoch [1/1], Batch [41], Loss: 99383.015625
Epoch [1/1], Batch [51], Loss: 102960.187500
Epoch [1/1], Batch [61], Loss: 108714.765625
Epoch [1/1], Batch [71], Loss: 101608.062500
Epoch [1/1], Batch [81], Loss: 107783.898438
Epoch [1/1], Batch [91], Loss: 102953.406250
Epoch [1/1], Batch [101], Loss: 97468.765625
Epoch [1/1], Batch [111], Loss: 101806.078125
Epoch [1/1], Batch [121], Loss: 101319.296875
Epoch [1/1], Batch [131], Loss: 99873.476562
Epoch [1/1], Batch [141], Loss: 100028.234375
Epoch [1/1], Batch [151], Loss: 104608.609375
Epoch [1/1], Batch [161], Loss: 102837.632812
Epoch [1/1], Batch [171], Loss: 105827.906250
Epoch [1/1], Batch [181], Loss: 102136.195312
Epoch [1/1], Batch [191], Loss: 105651.437500
Epoch [1/1], Batch [201], Loss: 105876.015625
Epoch [1/1], Batch [211], Loss: 100928.289062
Epoch [1/1], Batch [221], Loss: 107660.109375
Epoch [1/1], Batch [231], Loss: 97726.890625
Epoch [1/1], Batch [241], Loss: 99632.593750
Epoch [1/1], Batch [251], Loss: 102846.625000
Epoch [1/1], Batch [261], Loss: 104282.484375
Epoch [1/1], Batch [271], Loss: 99449.171875
Epoch [1/1], Batch [281], Loss: 104271.421875
Epoch [1/1], Batch [291], Loss: 110408.828125
Epoch [1/1], Batch [301], Loss: 102170.250000
Epoch [1/1], Batch [311], Loss: 103275.078125
Epoch [1/1], Batch [321], Loss: 102937.515625
Epoch [1/1], Batch [331], Loss: 101895.031250
Epoch [1/1], Batch [341], Loss: 102394.921875
Epoch [1/1], Batch [351], Loss: 105656.093750
Epoch [1/1], Batch [361], Loss: 102916.375000
Epoch [1/1], Batch [371], Loss: 97460.132812
Epoch [1/1], Batch [381], Loss: 102865.265625
Epoch [1/1], Batch [391], Loss: 103284.398438
Epoch [1/1], Batch [401], Loss: 103097.906250
Epoch [1/1], Batch [411], Loss: 100185.664062
Epoch [1/1], Batch [421], Loss: 98583.804688
Epoch [1/1], Batch [431], Loss: 96007.851562
Epoch [1/1], Batch [441], Loss: 99346.875000
Epoch [1/1], Batch [451], Loss: 101919.312500
Epoch [1/1], Batch [461], Loss: 101120.859375
Epoch [1/1], Batch [471], Loss: 100406.031250
Epoch [1/1], Batch [481], Loss: 103748.046875
Epoch [1/1], Batch [491], Loss: 101855.281250
Epoch [1/1], Batch [501], Loss: 99501.312500
Epoch [1/1], Batch [511], Loss: 100232.843750
Epoch [1/1], Batch [521], Loss: 102579.937500
Epoch [1/1], Batch [531], Loss: 107941.648438
Epoch [1/1], Batch [541], Loss: 104064.531250
Epoch [1/1], Batch [551], Loss: 102369.859375
Epoch [1/1], Batch [561], Loss: 97254.335938
Epoch [1/1], Batch [571], Loss: 97459.750000
Epoch [1/1], Batch [581], Loss: 97084.890625
Epoch [1/1], Batch [591], Loss: 101665.437500
Epoch [1/1], Batch [601], Loss: 99248.968750
Epoch [1/1], Batch [611], Loss: 102459.960938
Epoch [1/1], Batch [621], Loss: 100958.679688
Epoch [1/1], Batch [631], Loss: 103367.531250
Epoch [1/1], Batch [641], Loss: 101822.656250
Epoch [1/1], Batch [651], Loss: 96131.250000
Epoch [1/1], Batch [661], Loss: 98668.851562
Epoch [1/1], Batch [671], Loss: 99191.445312
Epoch [1/1], Batch [681], Loss: 96884.234375
Epoch [1/1], Batch [691], Loss: 105779.750000
Epoch [1/1], Batch [701], Loss: 101021.203125
Epoch [1/1], Batch [711], Loss: 100234.726562
Epoch [1/1], Batch [721], Loss: 101156.984375
Epoch [1/1], Batch [731], Loss: 99908.945312
Epoch [1/1], Batch [741], Loss: 106011.078125
Epoch [1/1], Batch [751], Loss: 97619.031250
Epoch [1/1], Batch [761], Loss: 102235.671875
Epoch [1/1], Batch [771], Loss: 100973.843750
Epoch [1/1], Batch [781], Loss: 97674.171875
Epoch [1/1], Batch [791], Loss: 100324.406250
Epoch [1/1], Batch [801], Loss: 103105.640625
Epoch [1/1], Batch [811], Loss: 101835.921875
Epoch [1/1], Batch [821], Loss: 103909.390625
Epoch [1/1], Batch [831], Loss: 98859.484375
Epoch [1/1], Batch [841], Loss: 103720.648438
Epoch [1/1], Batch [851], Loss: 99921.656250
Epoch [1/1], Batch [861], Loss: 98452.031250
Epoch [1/1], Batch [871], Loss: 103066.765625
Epoch [1/1], Batch [881], Loss: 98798.546875
Epoch [1/1], Batch [891], Loss: 104918.023438
Epoch [1/1], Batch [901], Loss: 95075.734375
Epoch [1/1], Batch [911], Loss: 97645.367188
Epoch [1/1], Batch [921], Loss: 99415.976562
Epoch [1/1], Batch [931], Loss: 101154.289062
Epoch [1/1], Batch [941], Loss: 96079.476562
Epoch [1/1], Batch [951], Loss: 97474.296875
Epoch [1/1], Batch [961], Loss: 101493.484375
Epoch [1/1], Batch [971], Loss: 98705.562500
Epoch [1/1], Batch [981], Loss: 97950.664062
Epoch [1/1], Batch [991], Loss: 101885.273438
Epoch [1/1], Batch [1001], Loss: 103459.296875
Epoch [1/1], Batch [1011], Loss: 102674.031250
Epoch [1/1], Batch [1021], Loss: 103053.031250
Epoch [1/1], Batch [1031], Loss: 99990.453125
Epoch [1/1], Batch [1041], Loss: 102503.046875
Epoch [1/1], Batch [1051], Loss: 98905.742188
Epoch [1/1], Batch [1061], Loss: 107908.585938
Epoch [1/1], Batch [1071], Loss: 98592.093750
Epoch [1/1], Batch [1081], Loss: 96182.296875
Epoch [1/1], Batch [1091], Loss: 98997.687500
Epoch [1/1], Batch [1101], Loss: 101730.171875
Epoch [1/1], Batch [1111], Loss: 105682.609375
Epoch [1/1], Batch [1121], Loss: 101868.750000
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 101339.5312
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 96144.4566
Elapsed time: 3518.35 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 106825.2051
Elapsed time: 3544.56 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 121283.750000
Epoch [1/1], Batch [11], Loss: 122840.851562
Epoch [1/1], Batch [21], Loss: 119393.265625
Epoch [1/1], Batch [31], Loss: 121535.867188
Epoch [1/1], Batch [41], Loss: 115465.187500
Epoch [1/1], Batch [51], Loss: 120836.156250
Epoch [1/1], Batch [61], Loss: 121050.015625
Epoch [1/1], Batch [71], Loss: 119498.468750
Epoch [1/1], Batch [81], Loss: 117585.343750
Epoch [1/1], Batch [91], Loss: 118393.757812
Epoch [1/1], Batch [101], Loss: 123098.914062
Epoch [1/1], Batch [111], Loss: 113217.031250
Epoch [1/1], Batch [121], Loss: 118676.796875
Epoch [1/1], Batch [131], Loss: 122295.648438
Epoch [1/1], Batch [141], Loss: 119293.656250
Epoch [1/1], Batch [151], Loss: 123599.570312
Epoch [1/1], Batch [161], Loss: 117375.218750
Epoch [1/1], Batch [171], Loss: 115914.242188
Epoch [1/1], Batch [181], Loss: 123798.414062
Epoch [1/1], Batch [191], Loss: 120220.718750
Epoch [1/1], Batch [201], Loss: 120044.078125
Epoch [1/1], Batch [211], Loss: 124637.062500
Epoch [1/1], Batch [221], Loss: 114221.218750
Epoch [1/1], Batch [231], Loss: 117990.085938
Epoch [1/1], Batch [241], Loss: 123216.171875
Epoch [1/1], Batch [251], Loss: 112353.984375
Epoch [1/1], Batch [261], Loss: 125623.914062
Epoch [1/1], Batch [271], Loss: 117255.218750
Epoch [1/1], Batch [281], Loss: 120611.539062
Epoch [1/1], Batch [291], Loss: 124249.765625
Epoch [1/1], Batch [301], Loss: 119070.687500
Epoch [1/1], Batch [311], Loss: 122872.171875
Epoch [1/1], Batch [321], Loss: 122528.609375
Epoch [1/1], Batch [331], Loss: 113508.664062
Epoch [1/1], Batch [341], Loss: 118858.671875
Epoch [1/1], Batch [351], Loss: 114386.156250
Epoch [1/1], Batch [361], Loss: 121898.406250
Epoch [1/1], Batch [371], Loss: 114942.187500
Epoch [1/1], Batch [381], Loss: 116446.117188
Epoch [1/1], Batch [391], Loss: 118917.218750
Epoch [1/1], Batch [401], Loss: 114621.210938
Epoch [1/1], Batch [411], Loss: 117029.625000
Epoch [1/1], Batch [421], Loss: 115022.695312
Epoch [1/1], Batch [431], Loss: 125702.187500
Epoch [1/1], Batch [441], Loss: 115168.468750
Epoch [1/1], Batch [451], Loss: 114069.906250
Epoch [1/1], Batch [461], Loss: 121292.921875
Epoch [1/1], Batch [471], Loss: 119385.179688
Epoch [1/1], Batch [481], Loss: 121126.585938
Epoch [1/1], Batch [491], Loss: 120322.148438
Epoch [1/1], Batch [501], Loss: 114922.234375
Epoch [1/1], Batch [511], Loss: 114294.921875
Epoch [1/1], Batch [521], Loss: 116537.187500
Epoch [1/1], Batch [531], Loss: 116174.625000
Epoch [1/1], Batch [541], Loss: 116818.585938
Epoch [1/1], Batch [551], Loss: 122372.617188
Epoch [1/1], Batch [561], Loss: 115013.609375
Epoch [1/1], Batch [571], Loss: 121295.640625
Epoch [1/1], Batch [581], Loss: 117677.789062
Epoch [1/1], Batch [591], Loss: 110834.281250
Epoch [1/1], Batch [601], Loss: 115164.492188
Epoch [1/1], Batch [611], Loss: 118303.054688
Epoch [1/1], Batch [621], Loss: 124115.023438
Epoch [1/1], Batch [631], Loss: 119380.796875
Epoch [1/1], Batch [641], Loss: 125290.078125
Epoch [1/1], Batch [651], Loss: 110252.289062
Epoch [1/1], Batch [661], Loss: 109492.968750
Epoch [1/1], Batch [671], Loss: 114075.992188
Epoch [1/1], Batch [681], Loss: 114379.343750
Epoch [1/1], Batch [691], Loss: 117896.453125
Epoch [1/1], Batch [701], Loss: 122897.625000
Epoch [1/1], Batch [711], Loss: 118210.148438
Epoch [1/1], Batch [721], Loss: 117838.734375
Epoch [1/1], Batch [731], Loss: 120158.085938
Epoch [1/1], Batch [741], Loss: 116619.632812
Epoch [1/1], Batch [751], Loss: 120085.156250
Epoch [1/1], Batch [761], Loss: 112590.054688
Epoch [1/1], Batch [771], Loss: 118535.203125
Epoch [1/1], Batch [781], Loss: 116924.601562
Epoch [1/1], Batch [791], Loss: 118076.218750
Epoch [1/1], Batch [801], Loss: 117417.023438
Epoch [1/1], Batch [811], Loss: 115220.890625
Epoch [1/1], Batch [821], Loss: 112806.156250
Epoch [1/1], Batch [831], Loss: 111628.945312
Epoch [1/1], Batch [841], Loss: 112614.718750
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 117929.4818
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 113105.6443
Elapsed time: 4251.69 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 129230.8121
Elapsed time: 4274.30 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 136223.375000
Epoch [1/1], Batch [11], Loss: 131221.218750
Epoch [1/1], Batch [21], Loss: 132680.609375
Epoch [1/1], Batch [31], Loss: 136596.656250
Epoch [1/1], Batch [41], Loss: 134156.890625
Epoch [1/1], Batch [51], Loss: 138263.906250
Epoch [1/1], Batch [61], Loss: 128065.218750
Epoch [1/1], Batch [71], Loss: 140200.000000
Epoch [1/1], Batch [81], Loss: 134863.593750
Epoch [1/1], Batch [91], Loss: 140063.796875
Epoch [1/1], Batch [101], Loss: 143874.093750
Epoch [1/1], Batch [111], Loss: 133100.906250
Epoch [1/1], Batch [121], Loss: 139019.312500
Epoch [1/1], Batch [131], Loss: 136068.093750
Epoch [1/1], Batch [141], Loss: 124316.281250
Epoch [1/1], Batch [151], Loss: 135750.109375
Epoch [1/1], Batch [161], Loss: 137311.937500
Epoch [1/1], Batch [171], Loss: 134284.562500
Epoch [1/1], Batch [181], Loss: 130651.148438
Epoch [1/1], Batch [191], Loss: 135223.734375
Epoch [1/1], Batch [201], Loss: 132422.031250
Epoch [1/1], Batch [211], Loss: 137778.312500
Epoch [1/1], Batch [221], Loss: 134577.625000
Epoch [1/1], Batch [231], Loss: 141202.812500
Epoch [1/1], Batch [241], Loss: 137642.812500
Epoch [1/1], Batch [251], Loss: 137333.890625
Epoch [1/1], Batch [261], Loss: 132687.125000
Epoch [1/1], Batch [271], Loss: 136879.484375
Epoch [1/1], Batch [281], Loss: 136299.234375
Epoch [1/1], Batch [291], Loss: 132583.296875
Epoch [1/1], Batch [301], Loss: 141783.843750
Epoch [1/1], Batch [311], Loss: 132831.031250
Epoch [1/1], Batch [321], Loss: 137453.484375
Epoch [1/1], Batch [331], Loss: 135086.968750
Epoch [1/1], Batch [341], Loss: 134995.562500
Epoch [1/1], Batch [351], Loss: 130805.812500
Epoch [1/1], Batch [361], Loss: 135292.156250
Epoch [1/1], Batch [371], Loss: 131633.156250
Epoch [1/1], Batch [381], Loss: 132925.265625
Epoch [1/1], Batch [391], Loss: 140638.968750
Epoch [1/1], Batch [401], Loss: 135745.343750
Epoch [1/1], Batch [411], Loss: 144171.812500
Epoch [1/1], Batch [421], Loss: 136547.921875
Epoch [1/1], Batch [431], Loss: 129768.156250
Epoch [1/1], Batch [441], Loss: 142377.593750
Epoch [1/1], Batch [451], Loss: 132343.250000
Epoch [1/1], Batch [461], Loss: 136197.968750
Epoch [1/1], Batch [471], Loss: 137926.312500
Epoch [1/1], Batch [481], Loss: 137795.765625
Epoch [1/1], Batch [491], Loss: 132069.375000
Epoch [1/1], Batch [501], Loss: 136165.906250
Epoch [1/1], Batch [511], Loss: 131682.781250
Epoch [1/1], Batch [521], Loss: 128429.937500
Epoch [1/1], Batch [531], Loss: 140432.031250
Epoch [1/1], Batch [541], Loss: 132425.343750
Epoch [1/1], Batch [551], Loss: 132232.796875
Epoch [1/1], Batch [561], Loss: 134421.250000
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 135594.6691
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 131626.4345
Elapsed time: 4813.32 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 151678.7882
Elapsed time: 4830.42 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 162033.562500
Epoch [1/1], Batch [11], Loss: 154362.187500
Epoch [1/1], Batch [21], Loss: 153793.625000
Epoch [1/1], Batch [31], Loss: 165776.375000
Epoch [1/1], Batch [41], Loss: 166153.968750
Epoch [1/1], Batch [51], Loss: 149431.265625
Epoch [1/1], Batch [61], Loss: 154159.468750
Epoch [1/1], Batch [71], Loss: 157003.656250
Epoch [1/1], Batch [81], Loss: 149194.375000
Epoch [1/1], Batch [91], Loss: 160182.406250
Epoch [1/1], Batch [101], Loss: 156516.406250
Epoch [1/1], Batch [111], Loss: 150627.218750
Epoch [1/1], Batch [121], Loss: 166970.281250
Epoch [1/1], Batch [131], Loss: 162860.656250
Epoch [1/1], Batch [141], Loss: 157556.046875
Epoch [1/1], Batch [151], Loss: 157521.218750
Epoch [1/1], Batch [161], Loss: 159426.484375
Epoch [1/1], Batch [171], Loss: 156713.031250
Epoch [1/1], Batch [181], Loss: 156750.937500
Epoch [1/1], Batch [191], Loss: 161003.812500
Epoch [1/1], Batch [201], Loss: 153803.265625
Epoch [1/1], Batch [211], Loss: 158707.187500
Epoch [1/1], Batch [221], Loss: 150061.859375
Epoch [1/1], Batch [231], Loss: 157422.562500
Epoch [1/1], Batch [241], Loss: 155070.843750
Epoch [1/1], Batch [251], Loss: 154502.656250
Epoch [1/1], Batch [261], Loss: 154100.375000
Epoch [1/1], Batch [271], Loss: 159944.000000
Epoch [1/1], Batch [281], Loss: 157766.406250
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 156778.4356
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 147744.7476
Elapsed time: 5149.14 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 154736.0646
Elapsed time: 5158.68 seconds

Training complete!
Totoal elapsed time: 5158.68 seconds
CUDA is available!
