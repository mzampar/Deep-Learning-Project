Starting job 1013555
Training with:
    architecture = [64, 32, 32, 16],
    stride = 2,
    filter_size = [5, 5, 5, 5],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 64,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = True,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.95.

CUDA is available!
Data shape: (20, 10000, 64, 64)

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 0.630350
Epoch [1/1], Batch [11], Loss: 0.221867
Epoch [1/1], Batch [21], Loss: 0.167307
Epoch [1/1], Batch [31], Loss: 0.151667
Epoch [1/1], Batch [41], Loss: 0.125163
Epoch [1/1], Batch [51], Loss: 0.119270
Epoch [1/1], Batch [61], Loss: 0.106562
Epoch [1/1], Batch [71], Loss: 0.109221
Epoch [1/1], Batch [81], Loss: 0.103257
Epoch [1/1], Batch [91], Loss: 0.095977
Epoch [1/1], Batch [101], Loss: 0.103288
Epoch [1/1], Batch [111], Loss: 0.100263
Epoch [1/1], Batch [121], Loss: 0.099818
Epoch [1/1], Batch [131], Loss: 0.099817
Epoch [1/1], Batch [141], Loss: 0.095166
Epoch [1/1], Batch [151], Loss: 0.094973
Epoch [1/1], Batch [161], Loss: 0.092391
Epoch [1/1], Batch [171], Loss: 0.092551
Epoch [1/1], Batch [181], Loss: 0.095100
Epoch [1/1], Batch [191], Loss: 0.095374
Epoch [1/1], Batch [201], Loss: 0.094823
Epoch [1/1], Batch [211], Loss: 0.092503
Epoch [1/1], Batch [221], Loss: 0.093491
Epoch [1/1], Batch [231], Loss: 0.090453
Epoch [1/1], Batch [241], Loss: 0.090605
Epoch [1/1], Batch [251], Loss: 0.086744
Epoch [1/1], Batch [261], Loss: 0.088534
Epoch [1/1], Batch [271], Loss: 0.091246
Epoch [1/1], Batch [281], Loss: 0.089721
Epoch [1/1], Batch [291], Loss: 0.087940
Epoch [1/1], Batch [301], Loss: 0.088191
Epoch [1/1], Batch [311], Loss: 0.088382
Epoch [1/1], Batch [321], Loss: 0.086343
Epoch [1/1], Batch [331], Loss: 0.084669
Epoch [1/1], Batch [341], Loss: 0.086914
Epoch [1/1], Batch [351], Loss: 0.087967
Epoch [1/1], Batch [361], Loss: 0.085723
Epoch [1/1], Batch [371], Loss: 0.083032
Epoch [1/1], Batch [381], Loss: 0.087463
Epoch [1/1], Batch [391], Loss: 0.091385
Epoch [1/1], Batch [401], Loss: 0.081797
Epoch [1/1], Batch [411], Loss: 0.080122
Epoch [1/1], Batch [421], Loss: 0.081536
Epoch [1/1], Batch [431], Loss: 0.083833
Epoch [1/1], Batch [441], Loss: 0.083282
Epoch [1/1], Batch [451], Loss: 0.081587
Epoch [1/1], Batch [461], Loss: 0.082109
Epoch [1/1], Batch [471], Loss: 0.083529
Epoch [1/1], Batch [481], Loss: 0.082710
Epoch [1/1], Batch [491], Loss: 0.086715
Epoch [1/1], Batch [501], Loss: 0.080603
Epoch [1/1], Batch [511], Loss: 0.078096
Epoch [1/1], Batch [521], Loss: 0.079219
Epoch [1/1], Batch [531], Loss: 0.082290
Epoch [1/1], Batch [541], Loss: 0.082492
Epoch [1/1], Batch [551], Loss: 0.085635
Epoch [1/1], Batch [561], Loss: 0.078842
Epoch [1/1], Batch [571], Loss: 0.080228
Epoch [1/1], Batch [581], Loss: 0.079691
Epoch [1/1], Batch [591], Loss: 0.084200
Epoch [1/1], Batch [601], Loss: 0.082364
Epoch [1/1], Batch [611], Loss: 0.082278
Epoch [1/1], Batch [621], Loss: 0.076741
Epoch [1/1], Batch [631], Loss: 0.079052
Epoch [1/1], Batch [641], Loss: 0.075813
Epoch [1/1], Batch [651], Loss: 0.076779
Epoch [1/1], Batch [661], Loss: 0.079395
Epoch [1/1], Batch [671], Loss: 0.080463
Epoch [1/1], Batch [681], Loss: 0.080042
Epoch [1/1], Batch [691], Loss: 0.079574
Epoch [1/1], Batch [701], Loss: 0.075460
Epoch [1/1], Batch [711], Loss: 0.079266
Epoch [1/1], Batch [721], Loss: 0.081600
Epoch [1/1], Batch [731], Loss: 0.078630
Epoch [1/1], Batch [741], Loss: 0.074756
Epoch [1/1], Batch [751], Loss: 0.075578
Epoch [1/1], Batch [761], Loss: 0.077564
Epoch [1/1], Batch [771], Loss: 0.076817
Epoch [1/1], Batch [781], Loss: 0.076934
Epoch [1/1], Batch [791], Loss: 0.078959
Epoch [1/1], Batch [801], Loss: 0.077002
Epoch [1/1], Batch [811], Loss: 0.078322
Epoch [1/1], Batch [821], Loss: 0.079879
Epoch [1/1], Batch [831], Loss: 0.076366
Epoch [1/1], Batch [841], Loss: 0.075503
Epoch [1/1], Batch [851], Loss: 0.077524
Epoch [1/1], Batch [861], Loss: 0.075563
Epoch [1/1], Batch [871], Loss: 0.078279
Epoch [1/1], Batch [881], Loss: 0.076319
Epoch [1/1], Batch [891], Loss: 0.076748
Epoch [1/1], Batch [901], Loss: 0.077567
Epoch [1/1], Batch [911], Loss: 0.075423
Epoch [1/1], Batch [921], Loss: 0.078818
Epoch [1/1], Batch [931], Loss: 0.078289
Epoch [1/1], Batch [941], Loss: 0.080910
Epoch [1/1], Batch [951], Loss: 0.075972
Epoch [1/1], Batch [961], Loss: 0.076647
Epoch [1/1], Batch [971], Loss: 0.076360
Epoch [1/1], Batch [981], Loss: 0.073897
Epoch [1/1], Batch [991], Loss: 0.075017
Epoch [1/1], Batch [1001], Loss: 0.078708
Epoch [1/1], Batch [1011], Loss: 0.075846
Epoch [1/1], Batch [1021], Loss: 0.073817
Epoch [1/1], Batch [1031], Loss: 0.075381
Epoch [1/1], Batch [1041], Loss: 0.076005
Epoch [1/1], Batch [1051], Loss: 0.074569
Epoch [1/1], Batch [1061], Loss: 0.076025
Epoch [1/1], Batch [1071], Loss: 0.069131
Epoch [1/1], Batch [1081], Loss: 0.076992
Epoch [1/1], Batch [1091], Loss: 0.073737
Epoch [1/1], Batch [1101], Loss: 0.071661
Epoch [1/1], Batch [1111], Loss: 0.073672
Epoch [1/1], Batch [1121], Loss: 0.070261
Epoch [1/1], Batch [1131], Loss: 0.072388
Epoch [1/1], Batch [1141], Loss: 0.070320
Epoch [1/1], Batch [1151], Loss: 0.074972
Epoch [1/1], Batch [1161], Loss: 0.073652
Epoch [1/1], Batch [1171], Loss: 0.077239
Epoch [1/1], Batch [1181], Loss: 0.073364
Epoch [1/1], Batch [1191], Loss: 0.073329
Epoch [1/1], Batch [1201], Loss: 0.075410
Epoch [1/1], Batch [1211], Loss: 0.075056
Epoch [1/1], Batch [1221], Loss: 0.073081
Epoch [1/1], Batch [1231], Loss: 0.073673
Epoch [1/1], Batch [1241], Loss: 0.072787
Epoch [1/1], Batch [1251], Loss: 0.071361
Epoch [1/1], Batch [1261], Loss: 0.072802
Epoch [1/1], Batch [1271], Loss: 0.075446
Epoch [1/1], Batch [1281], Loss: 0.071442
Epoch [1/1], Batch [1291], Loss: 0.071676
Epoch [1/1], Batch [1301], Loss: 0.072968
Epoch [1/1], Batch [1311], Loss: 0.072576
Epoch [1/1], Batch [1321], Loss: 0.073817
Epoch [1/1], Batch [1331], Loss: 0.074822
Epoch [1/1], Batch [1341], Loss: 0.070023
Epoch [1/1], Batch [1351], Loss: 0.074809
Epoch [1/1], Batch [1361], Loss: 0.073998
Epoch [1/1], Batch [1371], Loss: 0.070124
Epoch [1/1], Batch [1381], Loss: 0.069945
Epoch [1/1], Batch [1391], Loss: 0.070961
Epoch [1/1], Batch [1401], Loss: 0.073427
Epoch [1/1], Batch [1411], Loss: 0.069163
Epoch [1/1], Batch [1421], Loss: 0.072059
Epoch [1/1], Batch [1431], Loss: 0.072129
Epoch [1/1], Batch [1441], Loss: 0.074397
Epoch [1/1], Batch [1451], Loss: 0.073823
Epoch [1/1], Batch [1461], Loss: 0.069091
Epoch [1/1], Batch [1471], Loss: 0.072232
Epoch [1/1], Batch [1481], Loss: 0.072046
Epoch [1/1], Batch [1491], Loss: 0.075383
Epoch [1/1], Batch [1501], Loss: 0.068718
Epoch [1/1], Batch [1511], Loss: 0.076167
Epoch [1/1], Batch [1521], Loss: 0.068670
Epoch [1/1], Batch [1531], Loss: 0.075399
Epoch [1/1], Batch [1541], Loss: 0.068799
Epoch [1/1], Batch [1551], Loss: 0.070260
Epoch [1/1], Batch [1561], Loss: 0.068263
Epoch [1/1], Batch [1571], Loss: 0.072678
Epoch [1/1], Batch [1581], Loss: 0.070249
Epoch [1/1], Batch [1591], Loss: 0.071536
Epoch [1/1], Batch [1601], Loss: 0.069464
Epoch [1/1], Batch [1611], Loss: 0.068498
Epoch [1/1], Batch [1621], Loss: 0.068333
Epoch [1/1], Batch [1631], Loss: 0.072858
Epoch [1/1], Batch [1641], Loss: 0.070809
Epoch [1/1], Batch [1651], Loss: 0.074650
Epoch [1/1], Batch [1661], Loss: 0.068212
Epoch [1/1], Batch [1671], Loss: 0.072616
Epoch [1/1], Batch [1681], Loss: 0.073215
Epoch [1/1], Batch [1691], Loss: 0.071044
Epoch [1/1], Batch [1701], Loss: 0.068957
Epoch [1/1], Batch [1711], Loss: 0.068148
Epoch [1/1], Batch [1721], Loss: 0.071544
Epoch [1/1], Batch [1731], Loss: 0.068485
Epoch [1/1], Batch [1741], Loss: 0.073003
Epoch [1/1], Batch [1751], Loss: 0.071525
Epoch [1/1], Batch [1761], Loss: 0.072082
Epoch [1/1], Batch [1771], Loss: 0.071362
Epoch [1/1], Batch [1781], Loss: 0.071254
Epoch [1/1], Batch [1791], Loss: 0.071084
Epoch [1/1], Batch [1801], Loss: 0.070349
Epoch [1/1], Batch [1811], Loss: 0.069214
Epoch [1/1], Batch [1821], Loss: 0.071237
Epoch [1/1], Batch [1831], Loss: 0.070298
Epoch [1/1], Batch [1841], Loss: 0.073664
Epoch [1/1], Batch [1851], Loss: 0.070891
Epoch [1/1], Batch [1861], Loss: 0.069597
Epoch [1/1], Batch [1871], Loss: 0.072213
Epoch [1/1], Batch [1881], Loss: 0.071680
Epoch [1/1], Batch [1891], Loss: 0.070359
Epoch [1/1], Batch [1901], Loss: 0.074212
Epoch [1/1], Batch [1911], Loss: 0.066756
Epoch [1/1], Batch [1921], Loss: 0.070413
Epoch [1/1], Batch [1931], Loss: 0.073312
Epoch [1/1], Batch [1941], Loss: 0.068989
Epoch [1/1], Batch [1951], Loss: 0.070275
Epoch [1/1], Batch [1961], Loss: 0.070203
Epoch [1/1], Batch [1971], Loss: 0.070969
Epoch [1/1], Batch [1981], Loss: 0.074310
Epoch [1/1], Batch [1991], Loss: 0.068490
Epoch [1/1], Batch [2001], Loss: 0.066815
Epoch [1/1], Batch [2011], Loss: 0.065323
Epoch [1/1], Batch [2021], Loss: 0.070983
Epoch [1/1], Batch [2031], Loss: 0.068672
Epoch [1/1], Batch [2041], Loss: 0.069046
Epoch [1/1], Batch [2051], Loss: 0.069160
Epoch [1/1], Batch [2061], Loss: 0.068952
Epoch [1/1], Batch [2071], Loss: 0.067279
Epoch [1/1], Batch [2081], Loss: 0.068992
Epoch [1/1], Batch [2091], Loss: 0.071336
Epoch [1/1], Batch [2101], Loss: 0.067461
Epoch [1/1], Batch [2111], Loss: 0.071873
Epoch [1/1], Batch [2121], Loss: 0.070150
Epoch [1/1], Batch [2131], Loss: 0.068938
Epoch [1/1], Batch [2141], Loss: 0.069931
Epoch [1/1], Batch [2151], Loss: 0.069310
Epoch [1/1], Batch [2161], Loss: 0.067246
Epoch [1/1], Batch [2171], Loss: 0.071388
Epoch [1/1], Batch [2181], Loss: 0.071724
Epoch [1/1], Batch [2191], Loss: 0.070327
Epoch [1/1], Batch [2201], Loss: 0.066291
Epoch [1/1], Batch [2211], Loss: 0.069339
Epoch [1/1], Batch [2221], Loss: 0.068555
Epoch [1/1], Batch [2231], Loss: 0.070953
Epoch [1/1], Batch [2241], Loss: 0.068845
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 0.0795
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 0.0685
Elapsed time: 557.70 seconds
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 0.0693
Elapsed time: 583.21 seconds

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 0.099840
Epoch [1/1], Batch [11], Loss: 0.117324
Epoch [1/1], Batch [21], Loss: 0.109731
Epoch [1/1], Batch [31], Loss: 0.099279
Epoch [1/1], Batch [41], Loss: 0.098430
Epoch [1/1], Batch [51], Loss: 0.092947
Epoch [1/1], Batch [61], Loss: 0.089811
Epoch [1/1], Batch [71], Loss: 0.088485
Epoch [1/1], Batch [81], Loss: 0.091992
Epoch [1/1], Batch [91], Loss: 0.087234
Epoch [1/1], Batch [101], Loss: 0.084168
Epoch [1/1], Batch [111], Loss: 0.082780
Epoch [1/1], Batch [121], Loss: 0.085222
Epoch [1/1], Batch [131], Loss: 0.082347
Epoch [1/1], Batch [141], Loss: 0.081001
Epoch [1/1], Batch [151], Loss: 0.083688
Epoch [1/1], Batch [161], Loss: 0.082241
Epoch [1/1], Batch [171], Loss: 0.077685
Epoch [1/1], Batch [181], Loss: 0.077814
Epoch [1/1], Batch [191], Loss: 0.080024
Epoch [1/1], Batch [201], Loss: 0.076228
Epoch [1/1], Batch [211], Loss: 0.077614
Epoch [1/1], Batch [221], Loss: 0.075570
Epoch [1/1], Batch [231], Loss: 0.075811
Epoch [1/1], Batch [241], Loss: 0.077437
Epoch [1/1], Batch [251], Loss: 0.079274
Epoch [1/1], Batch [261], Loss: 0.078313
Epoch [1/1], Batch [271], Loss: 0.078112
Epoch [1/1], Batch [281], Loss: 0.079615
Epoch [1/1], Batch [291], Loss: 0.076797
Epoch [1/1], Batch [301], Loss: 0.073432
Epoch [1/1], Batch [311], Loss: 0.076472
Epoch [1/1], Batch [321], Loss: 0.078707
Epoch [1/1], Batch [331], Loss: 0.076517
Epoch [1/1], Batch [341], Loss: 0.077499
Epoch [1/1], Batch [351], Loss: 0.077640
Epoch [1/1], Batch [361], Loss: 0.075708
Epoch [1/1], Batch [371], Loss: 0.075599
Epoch [1/1], Batch [381], Loss: 0.072075
Epoch [1/1], Batch [391], Loss: 0.072830
Epoch [1/1], Batch [401], Loss: 0.072775
Epoch [1/1], Batch [411], Loss: 0.073157
Epoch [1/1], Batch [421], Loss: 0.075626
Epoch [1/1], Batch [431], Loss: 0.069733
Epoch [1/1], Batch [441], Loss: 0.076043
Epoch [1/1], Batch [451], Loss: 0.075283
Epoch [1/1], Batch [461], Loss: 0.073343
Epoch [1/1], Batch [471], Loss: 0.071424
Epoch [1/1], Batch [481], Loss: 0.073483
Epoch [1/1], Batch [491], Loss: 0.070038
Epoch [1/1], Batch [501], Loss: 0.069748
Epoch [1/1], Batch [511], Loss: 0.071851
Epoch [1/1], Batch [521], Loss: 0.069748
Epoch [1/1], Batch [531], Loss: 0.074637
Epoch [1/1], Batch [541], Loss: 0.075430
Epoch [1/1], Batch [551], Loss: 0.074249
Epoch [1/1], Batch [561], Loss: 0.073303
Epoch [1/1], Batch [571], Loss: 0.071039
Epoch [1/1], Batch [581], Loss: 0.074750
Epoch [1/1], Batch [591], Loss: 0.072388
Epoch [1/1], Batch [601], Loss: 0.072393
Epoch [1/1], Batch [611], Loss: 0.074428
Epoch [1/1], Batch [621], Loss: 0.073545
Epoch [1/1], Batch [631], Loss: 0.071698
Epoch [1/1], Batch [641], Loss: 0.072737
Epoch [1/1], Batch [651], Loss: 0.071797
Epoch [1/1], Batch [661], Loss: 0.069981
Epoch [1/1], Batch [671], Loss: 0.071055
Epoch [1/1], Batch [681], Loss: 0.075646
Epoch [1/1], Batch [691], Loss: 0.071372
Epoch [1/1], Batch [701], Loss: 0.072809
Epoch [1/1], Batch [711], Loss: 0.071260
Epoch [1/1], Batch [721], Loss: 0.072294
Epoch [1/1], Batch [731], Loss: 0.075090
Epoch [1/1], Batch [741], Loss: 0.068232
Epoch [1/1], Batch [751], Loss: 0.070922
Epoch [1/1], Batch [761], Loss: 0.071983
Epoch [1/1], Batch [771], Loss: 0.074550
Epoch [1/1], Batch [781], Loss: 0.072915
Epoch [1/1], Batch [791], Loss: 0.068808
Epoch [1/1], Batch [801], Loss: 0.070035
Epoch [1/1], Batch [811], Loss: 0.070367
Epoch [1/1], Batch [821], Loss: 0.070216
Epoch [1/1], Batch [831], Loss: 0.069763
Epoch [1/1], Batch [841], Loss: 0.068136
Epoch [1/1], Batch [851], Loss: 0.069695
Epoch [1/1], Batch [861], Loss: 0.067911
Epoch [1/1], Batch [871], Loss: 0.071955
Epoch [1/1], Batch [881], Loss: 0.071273
Epoch [1/1], Batch [891], Loss: 0.067377
Epoch [1/1], Batch [901], Loss: 0.069157
Epoch [1/1], Batch [911], Loss: 0.071510
Epoch [1/1], Batch [921], Loss: 0.072371
Epoch [1/1], Batch [931], Loss: 0.069837
Epoch [1/1], Batch [941], Loss: 0.070176
Epoch [1/1], Batch [951], Loss: 0.069220
Epoch [1/1], Batch [961], Loss: 0.071042
Epoch [1/1], Batch [971], Loss: 0.069712
Epoch [1/1], Batch [981], Loss: 0.073344
Epoch [1/1], Batch [991], Loss: 0.069546
Epoch [1/1], Batch [1001], Loss: 0.069764
Epoch [1/1], Batch [1011], Loss: 0.066834
Epoch [1/1], Batch [1021], Loss: 0.069868
Epoch [1/1], Batch [1031], Loss: 0.071376
Epoch [1/1], Batch [1041], Loss: 0.071679
Epoch [1/1], Batch [1051], Loss: 0.071982
Epoch [1/1], Batch [1061], Loss: 0.069733
Epoch [1/1], Batch [1071], Loss: 0.066397
Epoch [1/1], Batch [1081], Loss: 0.069593
Epoch [1/1], Batch [1091], Loss: 0.068027
Epoch [1/1], Batch [1101], Loss: 0.066993
Epoch [1/1], Batch [1111], Loss: 0.072049
Epoch [1/1], Batch [1121], Loss: 0.070799
Epoch [1/1], Batch [1131], Loss: 0.067432
Epoch [1/1], Batch [1141], Loss: 0.067413
Epoch [1/1], Batch [1151], Loss: 0.068006
Epoch [1/1], Batch [1161], Loss: 0.070029
Epoch [1/1], Batch [1171], Loss: 0.069644
Epoch [1/1], Batch [1181], Loss: 0.070898
Epoch [1/1], Batch [1191], Loss: 0.069241
Epoch [1/1], Batch [1201], Loss: 0.066629
Epoch [1/1], Batch [1211], Loss: 0.065764
Epoch [1/1], Batch [1221], Loss: 0.067862
Epoch [1/1], Batch [1231], Loss: 0.066805
Epoch [1/1], Batch [1241], Loss: 0.070687
Epoch [1/1], Batch [1251], Loss: 0.071832
Epoch [1/1], Batch [1261], Loss: 0.070440
Epoch [1/1], Batch [1271], Loss: 0.068835
Epoch [1/1], Batch [1281], Loss: 0.067929
Epoch [1/1], Batch [1291], Loss: 0.065800
Epoch [1/1], Batch [1301], Loss: 0.066493
Epoch [1/1], Batch [1311], Loss: 0.066461
Epoch [1/1], Batch [1321], Loss: 0.068985
Epoch [1/1], Batch [1331], Loss: 0.068301
Epoch [1/1], Batch [1341], Loss: 0.068348
Epoch [1/1], Batch [1351], Loss: 0.069671
Epoch [1/1], Batch [1361], Loss: 0.067666
Epoch [1/1], Batch [1371], Loss: 0.068821
Epoch [1/1], Batch [1381], Loss: 0.065899
Epoch [1/1], Batch [1391], Loss: 0.067221
Epoch [1/1], Batch [1401], Loss: 0.068241
Epoch [1/1], Batch [1411], Loss: 0.068595
Epoch [1/1], Batch [1421], Loss: 0.069111
Epoch [1/1], Batch [1431], Loss: 0.066591
Epoch [1/1], Batch [1441], Loss: 0.067417
Epoch [1/1], Batch [1451], Loss: 0.067810
Epoch [1/1], Batch [1461], Loss: 0.069324
Epoch [1/1], Batch [1471], Loss: 0.069136
Epoch [1/1], Batch [1481], Loss: 0.071297
Epoch [1/1], Batch [1491], Loss: 0.067963
Epoch [1/1], Batch [1501], Loss: 0.064992
Epoch [1/1], Batch [1511], Loss: 0.066415
Epoch [1/1], Batch [1521], Loss: 0.066377
Epoch [1/1], Batch [1531], Loss: 0.069764
Epoch [1/1], Batch [1541], Loss: 0.066361
Epoch [1/1], Batch [1551], Loss: 0.068509
Epoch [1/1], Batch [1561], Loss: 0.070005
Epoch [1/1], Batch [1571], Loss: 0.068055
Epoch [1/1], Batch [1581], Loss: 0.069416
Epoch [1/1], Batch [1591], Loss: 0.068348
Epoch [1/1], Batch [1601], Loss: 0.071680
Epoch [1/1], Batch [1611], Loss: 0.069036
Epoch [1/1], Batch [1621], Loss: 0.069864
Epoch [1/1], Batch [1631], Loss: 0.069246
Epoch [1/1], Batch [1641], Loss: 0.069519
Epoch [1/1], Batch [1651], Loss: 0.066164
Epoch [1/1], Batch [1661], Loss: 0.068007
Epoch [1/1], Batch [1671], Loss: 0.062993
Epoch [1/1], Batch [1681], Loss: 0.067493
Epoch [1/1], Batch [1691], Loss: 0.067939
Epoch [1/1], Batch [1701], Loss: 0.069707
Epoch [1/1], Batch [1711], Loss: 0.066733
Epoch [1/1], Batch [1721], Loss: 0.063355
Epoch [1/1], Batch [1731], Loss: 0.067587
Epoch [1/1], Batch [1741], Loss: 0.066921
Epoch [1/1], Batch [1751], Loss: 0.069279
Epoch [1/1], Batch [1761], Loss: 0.064263
Epoch [1/1], Batch [1771], Loss: 0.068078
Epoch [1/1], Batch [1781], Loss: 0.066027
Epoch [1/1], Batch [1791], Loss: 0.066133
Epoch [1/1], Batch [1801], Loss: 0.062923
Epoch [1/1], Batch [1811], Loss: 0.066128
Epoch [1/1], Batch [1821], Loss: 0.065722
Epoch [1/1], Batch [1831], Loss: 0.067190
Epoch [1/1], Batch [1841], Loss: 0.069819
Epoch [1/1], Batch [1851], Loss: 0.062213
Epoch [1/1], Batch [1861], Loss: 0.062736
Epoch [1/1], Batch [1871], Loss: 0.062626
Epoch [1/1], Batch [1881], Loss: 0.069160
Epoch [1/1], Batch [1891], Loss: 0.063175
Epoch [1/1], Batch [1901], Loss: 0.065943
Epoch [1/1], Batch [1911], Loss: 0.067620
Epoch [1/1], Batch [1921], Loss: 0.069422
Epoch [1/1], Batch [1931], Loss: 0.067648
Epoch [1/1], Batch [1941], Loss: 0.070020
Epoch [1/1], Batch [1951], Loss: 0.064754
Epoch [1/1], Batch [1961], Loss: 0.065865
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 0.0723
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 0.0660
Elapsed time: 1298.81 seconds
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 0.0676
Elapsed time: 1329.83 seconds

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 0.075935
Epoch [1/1], Batch [11], Loss: 0.074037
Epoch [1/1], Batch [21], Loss: 0.071210
Epoch [1/1], Batch [31], Loss: 0.071355
Epoch [1/1], Batch [41], Loss: 0.070473
Epoch [1/1], Batch [51], Loss: 0.070893
Epoch [1/1], Batch [61], Loss: 0.070179
Epoch [1/1], Batch [71], Loss: 0.067636
Epoch [1/1], Batch [81], Loss: 0.070159
Epoch [1/1], Batch [91], Loss: 0.069914
Epoch [1/1], Batch [101], Loss: 0.071354
Epoch [1/1], Batch [111], Loss: 0.069937
Epoch [1/1], Batch [121], Loss: 0.070090
Epoch [1/1], Batch [131], Loss: 0.070035
Epoch [1/1], Batch [141], Loss: 0.069462
Epoch [1/1], Batch [151], Loss: 0.068972
Epoch [1/1], Batch [161], Loss: 0.070559
Epoch [1/1], Batch [171], Loss: 0.070361
Epoch [1/1], Batch [181], Loss: 0.068193
Epoch [1/1], Batch [191], Loss: 0.069179
Epoch [1/1], Batch [201], Loss: 0.068709
Epoch [1/1], Batch [211], Loss: 0.070072
Epoch [1/1], Batch [221], Loss: 0.064810
Epoch [1/1], Batch [231], Loss: 0.071162
Epoch [1/1], Batch [241], Loss: 0.068532
Epoch [1/1], Batch [251], Loss: 0.067471
Epoch [1/1], Batch [261], Loss: 0.071180
Epoch [1/1], Batch [271], Loss: 0.069402
Epoch [1/1], Batch [281], Loss: 0.070318
Epoch [1/1], Batch [291], Loss: 0.068116
Epoch [1/1], Batch [301], Loss: 0.069800
Epoch [1/1], Batch [311], Loss: 0.070552
Epoch [1/1], Batch [321], Loss: 0.069200
Epoch [1/1], Batch [331], Loss: 0.064616
Epoch [1/1], Batch [341], Loss: 0.065411
Epoch [1/1], Batch [351], Loss: 0.067861
Epoch [1/1], Batch [361], Loss: 0.068652
Epoch [1/1], Batch [371], Loss: 0.066779
Epoch [1/1], Batch [381], Loss: 0.073523
Epoch [1/1], Batch [391], Loss: 0.069225
Epoch [1/1], Batch [401], Loss: 0.067334
Epoch [1/1], Batch [411], Loss: 0.072057
Epoch [1/1], Batch [421], Loss: 0.070082
Epoch [1/1], Batch [431], Loss: 0.070711
Epoch [1/1], Batch [441], Loss: 0.064969
Epoch [1/1], Batch [451], Loss: 0.066841
Epoch [1/1], Batch [461], Loss: 0.066879
Epoch [1/1], Batch [471], Loss: 0.067887
Epoch [1/1], Batch [481], Loss: 0.067573
Epoch [1/1], Batch [491], Loss: 0.068563
Epoch [1/1], Batch [501], Loss: 0.065761
Epoch [1/1], Batch [511], Loss: 0.065632
Epoch [1/1], Batch [521], Loss: 0.069779
Epoch [1/1], Batch [531], Loss: 0.067741
Epoch [1/1], Batch [541], Loss: 0.066263
Epoch [1/1], Batch [551], Loss: 0.067835
Epoch [1/1], Batch [561], Loss: 0.064917
Epoch [1/1], Batch [571], Loss: 0.067454
Epoch [1/1], Batch [581], Loss: 0.068675
Epoch [1/1], Batch [591], Loss: 0.067772
Epoch [1/1], Batch [601], Loss: 0.068144
Epoch [1/1], Batch [611], Loss: 0.068183
Epoch [1/1], Batch [621], Loss: 0.069474
Epoch [1/1], Batch [631], Loss: 0.067515
Epoch [1/1], Batch [641], Loss: 0.067402
Epoch [1/1], Batch [651], Loss: 0.073899
Epoch [1/1], Batch [661], Loss: 0.067152
Epoch [1/1], Batch [671], Loss: 0.067116
Epoch [1/1], Batch [681], Loss: 0.065186
Epoch [1/1], Batch [691], Loss: 0.069576
Epoch [1/1], Batch [701], Loss: 0.069001
Epoch [1/1], Batch [711], Loss: 0.063032
Epoch [1/1], Batch [721], Loss: 0.068613
Epoch [1/1], Batch [731], Loss: 0.066725
Epoch [1/1], Batch [741], Loss: 0.066345
Epoch [1/1], Batch [751], Loss: 0.065928
Epoch [1/1], Batch [761], Loss: 0.067556
Epoch [1/1], Batch [771], Loss: 0.068311
Epoch [1/1], Batch [781], Loss: 0.068865
Epoch [1/1], Batch [791], Loss: 0.065584
Epoch [1/1], Batch [801], Loss: 0.065592
Epoch [1/1], Batch [811], Loss: 0.070377
Epoch [1/1], Batch [821], Loss: 0.065774
Epoch [1/1], Batch [831], Loss: 0.069252
Epoch [1/1], Batch [841], Loss: 0.066889
Epoch [1/1], Batch [851], Loss: 0.068237
Epoch [1/1], Batch [861], Loss: 0.070397
Epoch [1/1], Batch [871], Loss: 0.068541
Epoch [1/1], Batch [881], Loss: 0.064232
Epoch [1/1], Batch [891], Loss: 0.069589
Epoch [1/1], Batch [901], Loss: 0.065435
Epoch [1/1], Batch [911], Loss: 0.065177
Epoch [1/1], Batch [921], Loss: 0.066200
Epoch [1/1], Batch [931], Loss: 0.066907
Epoch [1/1], Batch [941], Loss: 0.065688
Epoch [1/1], Batch [951], Loss: 0.069812
Epoch [1/1], Batch [961], Loss: 0.063511
Epoch [1/1], Batch [971], Loss: 0.066115
Epoch [1/1], Batch [981], Loss: 0.067014
Epoch [1/1], Batch [991], Loss: 0.068707
Epoch [1/1], Batch [1001], Loss: 0.061179
Epoch [1/1], Batch [1011], Loss: 0.064790
Epoch [1/1], Batch [1021], Loss: 0.065311
Epoch [1/1], Batch [1031], Loss: 0.068672
Epoch [1/1], Batch [1041], Loss: 0.070359
Epoch [1/1], Batch [1051], Loss: 0.066205
Epoch [1/1], Batch [1061], Loss: 0.066427
Epoch [1/1], Batch [1071], Loss: 0.063751
Epoch [1/1], Batch [1081], Loss: 0.068831
Epoch [1/1], Batch [1091], Loss: 0.065925
Epoch [1/1], Batch [1101], Loss: 0.064647
Epoch [1/1], Batch [1111], Loss: 0.061409
Epoch [1/1], Batch [1121], Loss: 0.067484
Epoch [1/1], Batch [1131], Loss: 0.067212
Epoch [1/1], Batch [1141], Loss: 0.064333
Epoch [1/1], Batch [1151], Loss: 0.067784
Epoch [1/1], Batch [1161], Loss: 0.067773
Epoch [1/1], Batch [1171], Loss: 0.064606
Epoch [1/1], Batch [1181], Loss: 0.067716
Epoch [1/1], Batch [1191], Loss: 0.064043
Epoch [1/1], Batch [1201], Loss: 0.066110
Epoch [1/1], Batch [1211], Loss: 0.065573
Epoch [1/1], Batch [1221], Loss: 0.065028
Epoch [1/1], Batch [1231], Loss: 0.064428
Epoch [1/1], Batch [1241], Loss: 0.065164
Epoch [1/1], Batch [1251], Loss: 0.067387
Epoch [1/1], Batch [1261], Loss: 0.065129
Epoch [1/1], Batch [1271], Loss: 0.064516
Epoch [1/1], Batch [1281], Loss: 0.063904
Epoch [1/1], Batch [1291], Loss: 0.063723
Epoch [1/1], Batch [1301], Loss: 0.066844
Epoch [1/1], Batch [1311], Loss: 0.062769
Epoch [1/1], Batch [1321], Loss: 0.066584
Epoch [1/1], Batch [1331], Loss: 0.068058
Epoch [1/1], Batch [1341], Loss: 0.066364
Epoch [1/1], Batch [1351], Loss: 0.063910
Epoch [1/1], Batch [1361], Loss: 0.064046
Epoch [1/1], Batch [1371], Loss: 0.063659
Epoch [1/1], Batch [1381], Loss: 0.067572
Epoch [1/1], Batch [1391], Loss: 0.065698
Epoch [1/1], Batch [1401], Loss: 0.065233
Epoch [1/1], Batch [1411], Loss: 0.065942
Epoch [1/1], Batch [1421], Loss: 0.066974
Epoch [1/1], Batch [1431], Loss: 0.066444
Epoch [1/1], Batch [1441], Loss: 0.064351
Epoch [1/1], Batch [1451], Loss: 0.063955
Epoch [1/1], Batch [1461], Loss: 0.064385
Epoch [1/1], Batch [1471], Loss: 0.068358
Epoch [1/1], Batch [1481], Loss: 0.062437
Epoch [1/1], Batch [1491], Loss: 0.066858
Epoch [1/1], Batch [1501], Loss: 0.065903
Epoch [1/1], Batch [1511], Loss: 0.063609
Epoch [1/1], Batch [1521], Loss: 0.063632
Epoch [1/1], Batch [1531], Loss: 0.067743
Epoch [1/1], Batch [1541], Loss: 0.062958
Epoch [1/1], Batch [1551], Loss: 0.063107
Epoch [1/1], Batch [1561], Loss: 0.062805
Epoch [1/1], Batch [1571], Loss: 0.068685
Epoch [1/1], Batch [1581], Loss: 0.063717
Epoch [1/1], Batch [1591], Loss: 0.065014
Epoch [1/1], Batch [1601], Loss: 0.064468
Epoch [1/1], Batch [1611], Loss: 0.064583
Epoch [1/1], Batch [1621], Loss: 0.066465
Epoch [1/1], Batch [1631], Loss: 0.064336
Epoch [1/1], Batch [1641], Loss: 0.065555
Epoch [1/1], Batch [1651], Loss: 0.065120
Epoch [1/1], Batch [1661], Loss: 0.065333
Epoch [1/1], Batch [1671], Loss: 0.066890
Epoch [1/1], Batch [1681], Loss: 0.058583
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 0.0671
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 0.0629
Elapsed time: 2135.36 seconds
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 0.0648
Elapsed time: 2169.57 seconds

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 0.066233
Epoch [1/1], Batch [11], Loss: 0.068379
Epoch [1/1], Batch [21], Loss: 0.071904
Epoch [1/1], Batch [31], Loss: 0.072184
Epoch [1/1], Batch [41], Loss: 0.065122
Epoch [1/1], Batch [51], Loss: 0.068907
Epoch [1/1], Batch [61], Loss: 0.067950
Epoch [1/1], Batch [71], Loss: 0.067206
Epoch [1/1], Batch [81], Loss: 0.068333
Epoch [1/1], Batch [91], Loss: 0.065364
Epoch [1/1], Batch [101], Loss: 0.065372
Epoch [1/1], Batch [111], Loss: 0.064820
Epoch [1/1], Batch [121], Loss: 0.063949
Epoch [1/1], Batch [131], Loss: 0.066963
Epoch [1/1], Batch [141], Loss: 0.064203
Epoch [1/1], Batch [151], Loss: 0.071083
Epoch [1/1], Batch [161], Loss: 0.067796
Epoch [1/1], Batch [171], Loss: 0.066297
Epoch [1/1], Batch [181], Loss: 0.064446
Epoch [1/1], Batch [191], Loss: 0.069076
Epoch [1/1], Batch [201], Loss: 0.066750
Epoch [1/1], Batch [211], Loss: 0.066758
Epoch [1/1], Batch [221], Loss: 0.064745
Epoch [1/1], Batch [231], Loss: 0.066307
Epoch [1/1], Batch [241], Loss: 0.071245
Epoch [1/1], Batch [251], Loss: 0.064840
Epoch [1/1], Batch [261], Loss: 0.064466
Epoch [1/1], Batch [271], Loss: 0.065864
Epoch [1/1], Batch [281], Loss: 0.065424
Epoch [1/1], Batch [291], Loss: 0.067673
Epoch [1/1], Batch [301], Loss: 0.068878
Epoch [1/1], Batch [311], Loss: 0.064713
Epoch [1/1], Batch [321], Loss: 0.063395
Epoch [1/1], Batch [331], Loss: 0.062546
Epoch [1/1], Batch [341], Loss: 0.061755
Epoch [1/1], Batch [351], Loss: 0.065486
Epoch [1/1], Batch [361], Loss: 0.062507
Epoch [1/1], Batch [371], Loss: 0.067991
Epoch [1/1], Batch [381], Loss: 0.068539
Epoch [1/1], Batch [391], Loss: 0.065966
Epoch [1/1], Batch [401], Loss: 0.067345
Epoch [1/1], Batch [411], Loss: 0.068251
Epoch [1/1], Batch [421], Loss: 0.064965
Epoch [1/1], Batch [431], Loss: 0.062988
Epoch [1/1], Batch [441], Loss: 0.065678
Epoch [1/1], Batch [451], Loss: 0.063515
Epoch [1/1], Batch [461], Loss: 0.065174
Epoch [1/1], Batch [471], Loss: 0.065682
Epoch [1/1], Batch [481], Loss: 0.061098
Epoch [1/1], Batch [491], Loss: 0.065993
Epoch [1/1], Batch [501], Loss: 0.066750
Epoch [1/1], Batch [511], Loss: 0.065806
Epoch [1/1], Batch [521], Loss: 0.063467
Epoch [1/1], Batch [531], Loss: 0.065704
Epoch [1/1], Batch [541], Loss: 0.064840
Epoch [1/1], Batch [551], Loss: 0.064435
Epoch [1/1], Batch [561], Loss: 0.063895
Epoch [1/1], Batch [571], Loss: 0.064515
Epoch [1/1], Batch [581], Loss: 0.063857
Epoch [1/1], Batch [591], Loss: 0.065482
Epoch [1/1], Batch [601], Loss: 0.063897
Epoch [1/1], Batch [611], Loss: 0.065518
Epoch [1/1], Batch [621], Loss: 0.064487
Epoch [1/1], Batch [631], Loss: 0.065184
Epoch [1/1], Batch [641], Loss: 0.068774
Epoch [1/1], Batch [651], Loss: 0.064554
Epoch [1/1], Batch [661], Loss: 0.064455
Epoch [1/1], Batch [671], Loss: 0.068997
Epoch [1/1], Batch [681], Loss: 0.065107
Epoch [1/1], Batch [691], Loss: 0.067891
Epoch [1/1], Batch [701], Loss: 0.061687
Epoch [1/1], Batch [711], Loss: 0.066106
Epoch [1/1], Batch [721], Loss: 0.066769
Epoch [1/1], Batch [731], Loss: 0.068104
Epoch [1/1], Batch [741], Loss: 0.064188
Epoch [1/1], Batch [751], Loss: 0.063634
Epoch [1/1], Batch [761], Loss: 0.067763
Epoch [1/1], Batch [771], Loss: 0.064102
Epoch [1/1], Batch [781], Loss: 0.066618
Epoch [1/1], Batch [791], Loss: 0.065338
Epoch [1/1], Batch [801], Loss: 0.066409
Epoch [1/1], Batch [811], Loss: 0.063826
Epoch [1/1], Batch [821], Loss: 0.066350
Epoch [1/1], Batch [831], Loss: 0.067201
Epoch [1/1], Batch [841], Loss: 0.064573
Epoch [1/1], Batch [851], Loss: 0.062582
Epoch [1/1], Batch [861], Loss: 0.064370
Epoch [1/1], Batch [871], Loss: 0.062852
Epoch [1/1], Batch [881], Loss: 0.064370
Epoch [1/1], Batch [891], Loss: 0.070087
Epoch [1/1], Batch [901], Loss: 0.066554
Epoch [1/1], Batch [911], Loss: 0.063708
Epoch [1/1], Batch [921], Loss: 0.066001
Epoch [1/1], Batch [931], Loss: 0.066922
Epoch [1/1], Batch [941], Loss: 0.065010
Epoch [1/1], Batch [951], Loss: 0.063188
Epoch [1/1], Batch [961], Loss: 0.064140
Epoch [1/1], Batch [971], Loss: 0.065728
Epoch [1/1], Batch [981], Loss: 0.063259
Epoch [1/1], Batch [991], Loss: 0.065756
Epoch [1/1], Batch [1001], Loss: 0.067589
Epoch [1/1], Batch [1011], Loss: 0.065069
Epoch [1/1], Batch [1021], Loss: 0.063247
Epoch [1/1], Batch [1031], Loss: 0.065807
Epoch [1/1], Batch [1041], Loss: 0.064480
Epoch [1/1], Batch [1051], Loss: 0.065211
Epoch [1/1], Batch [1061], Loss: 0.065140
Epoch [1/1], Batch [1071], Loss: 0.061435
Epoch [1/1], Batch [1081], Loss: 0.066619
Epoch [1/1], Batch [1091], Loss: 0.063616
Epoch [1/1], Batch [1101], Loss: 0.067019
Epoch [1/1], Batch [1111], Loss: 0.063010
Epoch [1/1], Batch [1121], Loss: 0.066780
Epoch [1/1], Batch [1131], Loss: 0.065567
Epoch [1/1], Batch [1141], Loss: 0.063997
Epoch [1/1], Batch [1151], Loss: 0.063459
Epoch [1/1], Batch [1161], Loss: 0.064568
Epoch [1/1], Batch [1171], Loss: 0.065279
Epoch [1/1], Batch [1181], Loss: 0.064076
Epoch [1/1], Batch [1191], Loss: 0.063160
Epoch [1/1], Batch [1201], Loss: 0.064658
Epoch [1/1], Batch [1211], Loss: 0.065474
Epoch [1/1], Batch [1221], Loss: 0.063015
Epoch [1/1], Batch [1231], Loss: 0.063204
Epoch [1/1], Batch [1241], Loss: 0.065095
Epoch [1/1], Batch [1251], Loss: 0.066897
Epoch [1/1], Batch [1261], Loss: 0.063327
Epoch [1/1], Batch [1271], Loss: 0.063577
Epoch [1/1], Batch [1281], Loss: 0.062015
Epoch [1/1], Batch [1291], Loss: 0.064748
Epoch [1/1], Batch [1301], Loss: 0.065502
Epoch [1/1], Batch [1311], Loss: 0.062590
Epoch [1/1], Batch [1321], Loss: 0.063748
Epoch [1/1], Batch [1331], Loss: 0.063819
Epoch [1/1], Batch [1341], Loss: 0.064065
Epoch [1/1], Batch [1351], Loss: 0.063314
Epoch [1/1], Batch [1361], Loss: 0.062501
Epoch [1/1], Batch [1371], Loss: 0.062047
Epoch [1/1], Batch [1381], Loss: 0.062613
Epoch [1/1], Batch [1391], Loss: 0.063105
Epoch [1/1], Batch [1401], Loss: 0.064891
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 0.0652
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 0.0617
Elapsed time: 3008.04 seconds
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 0.0654
Elapsed time: 3043.08 seconds

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 0.068874
Epoch [1/1], Batch [11], Loss: 0.065318
Epoch [1/1], Batch [21], Loss: 0.068026
Epoch [1/1], Batch [31], Loss: 0.065088
Epoch [1/1], Batch [41], Loss: 0.064435
Epoch [1/1], Batch [51], Loss: 0.067182
Epoch [1/1], Batch [61], Loss: 0.068261
Epoch [1/1], Batch [71], Loss: 0.063817
Epoch [1/1], Batch [81], Loss: 0.065086
Epoch [1/1], Batch [91], Loss: 0.066149
Epoch [1/1], Batch [101], Loss: 0.067187
Epoch [1/1], Batch [111], Loss: 0.067194
Epoch [1/1], Batch [121], Loss: 0.063276
Epoch [1/1], Batch [131], Loss: 0.065057
Epoch [1/1], Batch [141], Loss: 0.064677
Epoch [1/1], Batch [151], Loss: 0.065130
Epoch [1/1], Batch [161], Loss: 0.068157
Epoch [1/1], Batch [171], Loss: 0.063645
Epoch [1/1], Batch [181], Loss: 0.062474
Epoch [1/1], Batch [191], Loss: 0.064469
Epoch [1/1], Batch [201], Loss: 0.063811
Epoch [1/1], Batch [211], Loss: 0.063544
Epoch [1/1], Batch [221], Loss: 0.062828
Epoch [1/1], Batch [231], Loss: 0.067123
Epoch [1/1], Batch [241], Loss: 0.065192
Epoch [1/1], Batch [251], Loss: 0.064289
Epoch [1/1], Batch [261], Loss: 0.063719
Epoch [1/1], Batch [271], Loss: 0.062919
Epoch [1/1], Batch [281], Loss: 0.066236
Epoch [1/1], Batch [291], Loss: 0.062579
Epoch [1/1], Batch [301], Loss: 0.068181
Epoch [1/1], Batch [311], Loss: 0.063024
Epoch [1/1], Batch [321], Loss: 0.068926
Epoch [1/1], Batch [331], Loss: 0.067825
Epoch [1/1], Batch [341], Loss: 0.064976
Epoch [1/1], Batch [351], Loss: 0.067252
Epoch [1/1], Batch [361], Loss: 0.067082
Epoch [1/1], Batch [371], Loss: 0.063436
Epoch [1/1], Batch [381], Loss: 0.063560
Epoch [1/1], Batch [391], Loss: 0.066183
Epoch [1/1], Batch [401], Loss: 0.065176
Epoch [1/1], Batch [411], Loss: 0.065272
Epoch [1/1], Batch [421], Loss: 0.064763
Epoch [1/1], Batch [431], Loss: 0.063657
Epoch [1/1], Batch [441], Loss: 0.063335
Epoch [1/1], Batch [451], Loss: 0.066380
Epoch [1/1], Batch [461], Loss: 0.061450
Epoch [1/1], Batch [471], Loss: 0.061689
Epoch [1/1], Batch [481], Loss: 0.066259
Epoch [1/1], Batch [491], Loss: 0.064224
Epoch [1/1], Batch [501], Loss: 0.064883
Epoch [1/1], Batch [511], Loss: 0.066870
Epoch [1/1], Batch [521], Loss: 0.064350
Epoch [1/1], Batch [531], Loss: 0.062564
Epoch [1/1], Batch [541], Loss: 0.067228
Epoch [1/1], Batch [551], Loss: 0.062260
Epoch [1/1], Batch [561], Loss: 0.062492
Epoch [1/1], Batch [571], Loss: 0.063987
Epoch [1/1], Batch [581], Loss: 0.066355
Epoch [1/1], Batch [591], Loss: 0.064332
Epoch [1/1], Batch [601], Loss: 0.063389
Epoch [1/1], Batch [611], Loss: 0.063512
Epoch [1/1], Batch [621], Loss: 0.064300
Epoch [1/1], Batch [631], Loss: 0.062981
Epoch [1/1], Batch [641], Loss: 0.066920
Epoch [1/1], Batch [651], Loss: 0.063363
Epoch [1/1], Batch [661], Loss: 0.066068
Epoch [1/1], Batch [671], Loss: 0.062126
Epoch [1/1], Batch [681], Loss: 0.065527
Epoch [1/1], Batch [691], Loss: 0.062312
Epoch [1/1], Batch [701], Loss: 0.060901
Epoch [1/1], Batch [711], Loss: 0.062603
Epoch [1/1], Batch [721], Loss: 0.062268
Epoch [1/1], Batch [731], Loss: 0.062837
Epoch [1/1], Batch [741], Loss: 0.062796
Epoch [1/1], Batch [751], Loss: 0.063639
Epoch [1/1], Batch [761], Loss: 0.063302
Epoch [1/1], Batch [771], Loss: 0.067146
Epoch [1/1], Batch [781], Loss: 0.060905
Epoch [1/1], Batch [791], Loss: 0.063517
Epoch [1/1], Batch [801], Loss: 0.064426
Epoch [1/1], Batch [811], Loss: 0.061160
Epoch [1/1], Batch [821], Loss: 0.065455
Epoch [1/1], Batch [831], Loss: 0.067113
Epoch [1/1], Batch [841], Loss: 0.063843
Epoch [1/1], Batch [851], Loss: 0.064039
Epoch [1/1], Batch [861], Loss: 0.061704
Epoch [1/1], Batch [871], Loss: 0.066897
Epoch [1/1], Batch [881], Loss: 0.061798
Epoch [1/1], Batch [891], Loss: 0.064367
Epoch [1/1], Batch [901], Loss: 0.063732
Epoch [1/1], Batch [911], Loss: 0.061435
Epoch [1/1], Batch [921], Loss: 0.066078
Epoch [1/1], Batch [931], Loss: 0.062055
Epoch [1/1], Batch [941], Loss: 0.061639
Epoch [1/1], Batch [951], Loss: 0.063189
Epoch [1/1], Batch [961], Loss: 0.062635
Epoch [1/1], Batch [971], Loss: 0.063342
Epoch [1/1], Batch [981], Loss: 0.066016
Epoch [1/1], Batch [991], Loss: 0.065090
Epoch [1/1], Batch [1001], Loss: 0.066150
Epoch [1/1], Batch [1011], Loss: 0.060814
Epoch [1/1], Batch [1021], Loss: 0.064129
Epoch [1/1], Batch [1031], Loss: 0.061660
Epoch [1/1], Batch [1041], Loss: 0.061846
Epoch [1/1], Batch [1051], Loss: 0.061077
Epoch [1/1], Batch [1061], Loss: 0.064152
Epoch [1/1], Batch [1071], Loss: 0.062949
Epoch [1/1], Batch [1081], Loss: 0.064316
Epoch [1/1], Batch [1091], Loss: 0.066128
Epoch [1/1], Batch [1101], Loss: 0.062679
Epoch [1/1], Batch [1111], Loss: 0.064249
Epoch [1/1], Batch [1121], Loss: 0.063009
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 0.0640
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 0.0614
Elapsed time: 3841.73 seconds
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 0.0674
Elapsed time: 3874.68 seconds

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 0.067848
Epoch [1/1], Batch [11], Loss: 0.067182
Epoch [1/1], Batch [21], Loss: 0.065148
Epoch [1/1], Batch [31], Loss: 0.065395
Epoch [1/1], Batch [41], Loss: 0.064219
Epoch [1/1], Batch [51], Loss: 0.064509
Epoch [1/1], Batch [61], Loss: 0.063827
Epoch [1/1], Batch [71], Loss: 0.066492
Epoch [1/1], Batch [81], Loss: 0.062374
Epoch [1/1], Batch [91], Loss: 0.062349
Epoch [1/1], Batch [101], Loss: 0.062457
Epoch [1/1], Batch [111], Loss: 0.067045
Epoch [1/1], Batch [121], Loss: 0.064432
Epoch [1/1], Batch [131], Loss: 0.067248
Epoch [1/1], Batch [141], Loss: 0.061079
Epoch [1/1], Batch [151], Loss: 0.066331
Epoch [1/1], Batch [161], Loss: 0.062492
Epoch [1/1], Batch [171], Loss: 0.063267
Epoch [1/1], Batch [181], Loss: 0.061809
Epoch [1/1], Batch [191], Loss: 0.065332
Epoch [1/1], Batch [201], Loss: 0.062445
Epoch [1/1], Batch [211], Loss: 0.062713
Epoch [1/1], Batch [221], Loss: 0.064185
Epoch [1/1], Batch [231], Loss: 0.068880
Epoch [1/1], Batch [241], Loss: 0.061580
Epoch [1/1], Batch [251], Loss: 0.062951
Epoch [1/1], Batch [261], Loss: 0.064127
Epoch [1/1], Batch [271], Loss: 0.063626
Epoch [1/1], Batch [281], Loss: 0.062424
Epoch [1/1], Batch [291], Loss: 0.065629
Epoch [1/1], Batch [301], Loss: 0.063347
Epoch [1/1], Batch [311], Loss: 0.065709
Epoch [1/1], Batch [321], Loss: 0.062558
Epoch [1/1], Batch [331], Loss: 0.064466
Epoch [1/1], Batch [341], Loss: 0.065531
Epoch [1/1], Batch [351], Loss: 0.063436
Epoch [1/1], Batch [361], Loss: 0.063054
Epoch [1/1], Batch [371], Loss: 0.063365
Epoch [1/1], Batch [381], Loss: 0.062894
Epoch [1/1], Batch [391], Loss: 0.066566
Epoch [1/1], Batch [401], Loss: 0.063599
Epoch [1/1], Batch [411], Loss: 0.062046
Epoch [1/1], Batch [421], Loss: 0.068504
Epoch [1/1], Batch [431], Loss: 0.060891
Epoch [1/1], Batch [441], Loss: 0.063783
Epoch [1/1], Batch [451], Loss: 0.062172
Epoch [1/1], Batch [461], Loss: 0.063962
Epoch [1/1], Batch [471], Loss: 0.062254
Epoch [1/1], Batch [481], Loss: 0.059653
Epoch [1/1], Batch [491], Loss: 0.063633
Epoch [1/1], Batch [501], Loss: 0.063853
Epoch [1/1], Batch [511], Loss: 0.062885
Epoch [1/1], Batch [521], Loss: 0.066167
Epoch [1/1], Batch [531], Loss: 0.063373
Epoch [1/1], Batch [541], Loss: 0.068060
Epoch [1/1], Batch [551], Loss: 0.065281
Epoch [1/1], Batch [561], Loss: 0.060056
Epoch [1/1], Batch [571], Loss: 0.062066
Epoch [1/1], Batch [581], Loss: 0.065364
Epoch [1/1], Batch [591], Loss: 0.063273
Epoch [1/1], Batch [601], Loss: 0.063112
Epoch [1/1], Batch [611], Loss: 0.063209
Epoch [1/1], Batch [621], Loss: 0.060880
Epoch [1/1], Batch [631], Loss: 0.062727
Epoch [1/1], Batch [641], Loss: 0.061450
Epoch [1/1], Batch [651], Loss: 0.066440
Epoch [1/1], Batch [661], Loss: 0.062679
Epoch [1/1], Batch [671], Loss: 0.064750
Epoch [1/1], Batch [681], Loss: 0.059989
Epoch [1/1], Batch [691], Loss: 0.063201
Epoch [1/1], Batch [701], Loss: 0.061533
Epoch [1/1], Batch [711], Loss: 0.066176
Epoch [1/1], Batch [721], Loss: 0.064080
Epoch [1/1], Batch [731], Loss: 0.063097
Epoch [1/1], Batch [741], Loss: 0.062057
Epoch [1/1], Batch [751], Loss: 0.060331
Epoch [1/1], Batch [761], Loss: 0.060928
Epoch [1/1], Batch [771], Loss: 0.058729
Epoch [1/1], Batch [781], Loss: 0.060552
Epoch [1/1], Batch [791], Loss: 0.059413
Epoch [1/1], Batch [801], Loss: 0.062539
Epoch [1/1], Batch [811], Loss: 0.058990
Epoch [1/1], Batch [821], Loss: 0.062301
Epoch [1/1], Batch [831], Loss: 0.060241
Epoch [1/1], Batch [841], Loss: 0.062954
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 0.0635
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 0.0608
Elapsed time: 4572.80 seconds
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 0.0656
Elapsed time: 4601.27 seconds

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 0.061604
Epoch [1/1], Batch [11], Loss: 0.065321
Epoch [1/1], Batch [21], Loss: 0.062216
Epoch [1/1], Batch [31], Loss: 0.065223
Epoch [1/1], Batch [41], Loss: 0.063870
Epoch [1/1], Batch [51], Loss: 0.062015
Epoch [1/1], Batch [61], Loss: 0.062283
Epoch [1/1], Batch [71], Loss: 0.062302
Epoch [1/1], Batch [81], Loss: 0.066162
Epoch [1/1], Batch [91], Loss: 0.062798
Epoch [1/1], Batch [101], Loss: 0.064282
Epoch [1/1], Batch [111], Loss: 0.064298
Epoch [1/1], Batch [121], Loss: 0.063951
Epoch [1/1], Batch [131], Loss: 0.062626
Epoch [1/1], Batch [141], Loss: 0.063650
Epoch [1/1], Batch [151], Loss: 0.063215
Epoch [1/1], Batch [161], Loss: 0.062497
Epoch [1/1], Batch [171], Loss: 0.063502
Epoch [1/1], Batch [181], Loss: 0.063802
Epoch [1/1], Batch [191], Loss: 0.065738
Epoch [1/1], Batch [201], Loss: 0.060283
Epoch [1/1], Batch [211], Loss: 0.064821
Epoch [1/1], Batch [221], Loss: 0.062378
Epoch [1/1], Batch [231], Loss: 0.059681
Epoch [1/1], Batch [241], Loss: 0.060633
Epoch [1/1], Batch [251], Loss: 0.063177
Epoch [1/1], Batch [261], Loss: 0.061519
Epoch [1/1], Batch [271], Loss: 0.062090
Epoch [1/1], Batch [281], Loss: 0.062764
Epoch [1/1], Batch [291], Loss: 0.064593
Epoch [1/1], Batch [301], Loss: 0.062760
Epoch [1/1], Batch [311], Loss: 0.061980
Epoch [1/1], Batch [321], Loss: 0.066488
Epoch [1/1], Batch [331], Loss: 0.062976
Epoch [1/1], Batch [341], Loss: 0.066710
Epoch [1/1], Batch [351], Loss: 0.063447
Epoch [1/1], Batch [361], Loss: 0.061042
Epoch [1/1], Batch [371], Loss: 0.062335
Epoch [1/1], Batch [381], Loss: 0.064560
Epoch [1/1], Batch [391], Loss: 0.061164
Epoch [1/1], Batch [401], Loss: 0.061230
Epoch [1/1], Batch [411], Loss: 0.064191
Epoch [1/1], Batch [421], Loss: 0.063306
Epoch [1/1], Batch [431], Loss: 0.061074
Epoch [1/1], Batch [441], Loss: 0.061991
Epoch [1/1], Batch [451], Loss: 0.063959
Epoch [1/1], Batch [461], Loss: 0.061944
Epoch [1/1], Batch [471], Loss: 0.062367
Epoch [1/1], Batch [481], Loss: 0.063799
Epoch [1/1], Batch [491], Loss: 0.060404
Epoch [1/1], Batch [501], Loss: 0.061196
Epoch [1/1], Batch [511], Loss: 0.060219
Epoch [1/1], Batch [521], Loss: 0.062048
Epoch [1/1], Batch [531], Loss: 0.060276
Epoch [1/1], Batch [541], Loss: 0.061786
Epoch [1/1], Batch [551], Loss: 0.059194
Epoch [1/1], Batch [561], Loss: 0.062142
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 0.0629
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 0.0607
Elapsed time: 5139.47 seconds
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 0.0676
Elapsed time: 5161.04 seconds

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 0.063081
Epoch [1/1], Batch [11], Loss: 0.062875
Epoch [1/1], Batch [21], Loss: 0.060369
Epoch [1/1], Batch [31], Loss: 0.061547
Epoch [1/1], Batch [41], Loss: 0.064397
Epoch [1/1], Batch [51], Loss: 0.065277
Epoch [1/1], Batch [61], Loss: 0.068216
Epoch [1/1], Batch [71], Loss: 0.063771
Epoch [1/1], Batch [81], Loss: 0.060171
Epoch [1/1], Batch [91], Loss: 0.060045
Epoch [1/1], Batch [101], Loss: 0.063513
Epoch [1/1], Batch [111], Loss: 0.063076
Epoch [1/1], Batch [121], Loss: 0.062819
Epoch [1/1], Batch [131], Loss: 0.064631
Epoch [1/1], Batch [141], Loss: 0.063271
Epoch [1/1], Batch [151], Loss: 0.063750
Epoch [1/1], Batch [161], Loss: 0.062069
Epoch [1/1], Batch [171], Loss: 0.061498
Epoch [1/1], Batch [181], Loss: 0.066355
Epoch [1/1], Batch [191], Loss: 0.064011
Epoch [1/1], Batch [201], Loss: 0.067356
Epoch [1/1], Batch [211], Loss: 0.066971
Epoch [1/1], Batch [221], Loss: 0.059939
Epoch [1/1], Batch [231], Loss: 0.062640
Epoch [1/1], Batch [241], Loss: 0.061034
Epoch [1/1], Batch [251], Loss: 0.062087
Epoch [1/1], Batch [261], Loss: 0.065203
Epoch [1/1], Batch [271], Loss: 0.060234
Epoch [1/1], Batch [281], Loss: 0.063717
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 0.0636
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 0.0608
Elapsed time: 5495.55 seconds
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 0.0644
Elapsed time: 5507.58 seconds

Training complete!
Totoal elapsed time: 5507.58 seconds
CUDA is available!
