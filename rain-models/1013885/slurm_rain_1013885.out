Starting job 1013885
Training with:
    architecture = [60, 40, 40, 20],
    stride = 2,
    filter_size = [5, 5, 5, 5],
    leaky_slope = 0.2,
    max_pool = True,
    layer norm = True,
    loss = BCELoss(),
    batch size = 16,
    num_epochs = 1,
    scheduled_sampling = False,
    bias = True,
    transpose = True,
    use_lstm_output = False,
    scheduler = False,
    initial_lr = 0.01,
    gamma = 0.5.

Average train sequence lenght: 115.2406015037594.
Average test sequence lenght:, 124.0.
Average test rain:, 2.1325389489429316.
Average train rain:, 2.2383985386372576.

CUDA is available!

Training with sequence length 2.
Epoch [1/1], Batch [1], Loss: 0.699477
Epoch [1/1], Batch [11], Loss: 0.693127
Epoch [1/1], Batch [21], Loss: 0.693164
Epoch [1/1], Batch [31], Loss: 0.693178
Epoch [1/1], Batch [41], Loss: 0.693092
Epoch [1/1], Batch [51], Loss: 0.693025
Epoch [1/1], Batch [61], Loss: 0.693100
Epoch [1/1], Batch [71], Loss: 0.693111
Epoch [1/1], Batch [81], Loss: 0.692973
Epoch [1/1], Batch [91], Loss: 0.693104
Epoch [1/1], Batch [101], Loss: 0.692741
Epoch [1/1], Batch [111], Loss: 0.692809
Epoch [1/1], Batch [121], Loss: 0.691918
Epoch [1/1], Batch [131], Loss: 0.692015
Epoch [1/1], Batch [141], Loss: 0.691524
Epoch [1/1], Batch [151], Loss: 0.691662
Epoch [1/1], Batch [161], Loss: 0.691495
Epoch [1/1], Batch [171], Loss: 0.690807
Epoch [1/1], Batch [181], Loss: 0.691263
Epoch [1/1], Batch [191], Loss: 0.691335
Epoch [1/1], Batch [201], Loss: 0.691215
Epoch [1/1], Batch [211], Loss: 0.691252
Epoch [1/1], Batch [221], Loss: 0.691177
Epoch [1/1], Batch [231], Loss: 0.691150
Epoch [1/1], Batch [241], Loss: 0.690385
Epoch [1/1], Batch [251], Loss: 0.690664
Epoch [1/1], Batch [261], Loss: 0.691572
Seq_Len: 2, Epoch [1/1] - Average Train Loss: 0.6922
Seq_Len: 2, Epoch [1/1] - Average Validation Loss: 0.6944
Elapsed time: 1.99 minutes.
Seq_Len: 2, Epoch [1/1] - Average Test Loss: 0.6909
Elapsed time: 2.09 minutes.

Training with sequence length 3.
Epoch [1/1], Batch [1], Loss: 0.691572
Epoch [1/1], Batch [11], Loss: 0.691364
Epoch [1/1], Batch [21], Loss: 0.690801
Epoch [1/1], Batch [31], Loss: 0.691307
Epoch [1/1], Batch [41], Loss: 0.690750
Epoch [1/1], Batch [51], Loss: 0.690365
Epoch [1/1], Batch [61], Loss: 0.691269
Epoch [1/1], Batch [71], Loss: 0.691320
Epoch [1/1], Batch [81], Loss: 0.691069
Epoch [1/1], Batch [91], Loss: 0.690895
Epoch [1/1], Batch [101], Loss: 0.690823
Epoch [1/1], Batch [111], Loss: 0.691046
Epoch [1/1], Batch [121], Loss: 0.691249
Epoch [1/1], Batch [131], Loss: 0.691146
Epoch [1/1], Batch [141], Loss: 0.690994
Epoch [1/1], Batch [151], Loss: 0.690317
Epoch [1/1], Batch [161], Loss: 0.690915
Epoch [1/1], Batch [171], Loss: 0.690867
Epoch [1/1], Batch [181], Loss: 0.691062
Epoch [1/1], Batch [191], Loss: 0.690884
Epoch [1/1], Batch [201], Loss: 0.690858
Epoch [1/1], Batch [211], Loss: 0.691101
Epoch [1/1], Batch [221], Loss: 0.690697
Epoch [1/1], Batch [231], Loss: 0.690778
Epoch [1/1], Batch [241], Loss: 0.690456
Epoch [1/1], Batch [251], Loss: 0.690552
Epoch [1/1], Batch [261], Loss: 0.690837
Epoch [1/1], Batch [271], Loss: 0.691072
Seq_Len: 3, Epoch [1/1] - Average Train Loss: 0.6909
Seq_Len: 3, Epoch [1/1] - Average Validation Loss: 0.6937
Elapsed time: 4.93 minutes.
Seq_Len: 3, Epoch [1/1] - Average Test Loss: 0.6908
Elapsed time: 5.07 minutes.

Training with sequence length 4.
Epoch [1/1], Batch [1], Loss: 0.691211
Epoch [1/1], Batch [11], Loss: 0.691514
Epoch [1/1], Batch [21], Loss: 0.690675
Epoch [1/1], Batch [31], Loss: 0.690957
Epoch [1/1], Batch [41], Loss: 0.690826
Epoch [1/1], Batch [51], Loss: 0.690532
Epoch [1/1], Batch [61], Loss: 0.690905
Epoch [1/1], Batch [71], Loss: 0.691162
Epoch [1/1], Batch [81], Loss: 0.690673
Epoch [1/1], Batch [91], Loss: 0.691189
Epoch [1/1], Batch [101], Loss: 0.690618
Epoch [1/1], Batch [111], Loss: 0.690777
Epoch [1/1], Batch [121], Loss: 0.691236
Epoch [1/1], Batch [131], Loss: 0.691199
Epoch [1/1], Batch [141], Loss: 0.690768
Epoch [1/1], Batch [151], Loss: 0.691086
Epoch [1/1], Batch [161], Loss: 0.690525
Epoch [1/1], Batch [171], Loss: 0.691047
Epoch [1/1], Batch [181], Loss: 0.690837
Epoch [1/1], Batch [191], Loss: 0.690787
Epoch [1/1], Batch [201], Loss: 0.690773
Epoch [1/1], Batch [211], Loss: 0.690937
Epoch [1/1], Batch [221], Loss: 0.690517
Epoch [1/1], Batch [231], Loss: 0.691718
Epoch [1/1], Batch [241], Loss: 0.690453
Epoch [1/1], Batch [251], Loss: 0.690570
Epoch [1/1], Batch [261], Loss: 0.691026
Epoch [1/1], Batch [271], Loss: 0.691016
Seq_Len: 4, Epoch [1/1] - Average Train Loss: 0.6910
Seq_Len: 4, Epoch [1/1] - Average Validation Loss: 0.6934
Elapsed time: 8.75 minutes.
Seq_Len: 4, Epoch [1/1] - Average Test Loss: 0.6911
Elapsed time: 8.93 minutes.

Training with sequence length 5.
Epoch [1/1], Batch [1], Loss: 0.691317
Epoch [1/1], Batch [11], Loss: 0.691457
Epoch [1/1], Batch [21], Loss: 0.691019
Epoch [1/1], Batch [31], Loss: 0.691316
Epoch [1/1], Batch [41], Loss: 0.691375
Epoch [1/1], Batch [51], Loss: 0.691107
Epoch [1/1], Batch [61], Loss: 0.691153
Epoch [1/1], Batch [71], Loss: 0.691215
Epoch [1/1], Batch [81], Loss: 0.691384
Epoch [1/1], Batch [91], Loss: 0.691243
Epoch [1/1], Batch [101], Loss: 0.691348
Epoch [1/1], Batch [111], Loss: 0.691491
Epoch [1/1], Batch [121], Loss: 0.691094
Epoch [1/1], Batch [131], Loss: 0.691024
Epoch [1/1], Batch [141], Loss: 0.690895
Epoch [1/1], Batch [151], Loss: 0.691326
Epoch [1/1], Batch [161], Loss: 0.691083
Epoch [1/1], Batch [171], Loss: 0.690519
Epoch [1/1], Batch [181], Loss: 0.690773
Epoch [1/1], Batch [191], Loss: 0.691202
Epoch [1/1], Batch [201], Loss: 0.690753
Epoch [1/1], Batch [211], Loss: 0.690859
Epoch [1/1], Batch [221], Loss: 0.690394
Epoch [1/1], Batch [231], Loss: 0.691037
Epoch [1/1], Batch [241], Loss: 0.690843
Epoch [1/1], Batch [251], Loss: 0.690656
Epoch [1/1], Batch [261], Loss: 0.690742
Epoch [1/1], Batch [271], Loss: 0.690683
Epoch [1/1], Batch [281], Loss: 0.691185
Seq_Len: 5, Epoch [1/1] - Average Train Loss: 0.6910
Seq_Len: 5, Epoch [1/1] - Average Validation Loss: 0.6934
Elapsed time: 13.53 minutes.
Seq_Len: 5, Epoch [1/1] - Average Test Loss: 0.6911
Elapsed time: 13.76 minutes.

Training with sequence length 6.
Epoch [1/1], Batch [1], Loss: 0.691160
Epoch [1/1], Batch [11], Loss: 0.691195
Epoch [1/1], Batch [21], Loss: 0.691070
Epoch [1/1], Batch [31], Loss: 0.690653
Epoch [1/1], Batch [41], Loss: 0.690825
Epoch [1/1], Batch [51], Loss: 0.691079
Epoch [1/1], Batch [61], Loss: 0.690699
Epoch [1/1], Batch [71], Loss: 0.690948
Epoch [1/1], Batch [81], Loss: 0.691325
Epoch [1/1], Batch [91], Loss: 0.690945
Epoch [1/1], Batch [101], Loss: 0.691222
Epoch [1/1], Batch [111], Loss: 0.691034
Epoch [1/1], Batch [121], Loss: 0.691036
Epoch [1/1], Batch [131], Loss: 0.691142
Epoch [1/1], Batch [141], Loss: 0.691495
Epoch [1/1], Batch [151], Loss: 0.690463
Epoch [1/1], Batch [161], Loss: 0.691432
Epoch [1/1], Batch [171], Loss: 0.690609
Epoch [1/1], Batch [181], Loss: 0.691257
Epoch [1/1], Batch [191], Loss: 0.690882
Epoch [1/1], Batch [201], Loss: 0.691158
Epoch [1/1], Batch [211], Loss: 0.691090
Epoch [1/1], Batch [221], Loss: 0.691192
Epoch [1/1], Batch [231], Loss: 0.691359
Epoch [1/1], Batch [241], Loss: 0.690951
Epoch [1/1], Batch [251], Loss: 0.691134
Epoch [1/1], Batch [261], Loss: 0.690744
Epoch [1/1], Batch [271], Loss: 0.691265
Epoch [1/1], Batch [281], Loss: 0.691041
Seq_Len: 6, Epoch [1/1] - Average Train Loss: 0.6911
Seq_Len: 6, Epoch [1/1] - Average Validation Loss: 0.6944
Elapsed time: 19.19 minutes.
Seq_Len: 6, Epoch [1/1] - Average Test Loss: 0.6913
Elapsed time: 19.46 minutes.

Training with sequence length 7.
Epoch [1/1], Batch [1], Loss: 0.691349
Epoch [1/1], Batch [11], Loss: 0.690830
Epoch [1/1], Batch [21], Loss: 0.691084
Epoch [1/1], Batch [31], Loss: 0.691176
Epoch [1/1], Batch [41], Loss: 0.691394
Epoch [1/1], Batch [51], Loss: 0.691228
Epoch [1/1], Batch [61], Loss: 0.691191
Epoch [1/1], Batch [71], Loss: 0.690834
Epoch [1/1], Batch [81], Loss: 0.691104
Epoch [1/1], Batch [91], Loss: 0.691239
Epoch [1/1], Batch [101], Loss: 0.691140
Epoch [1/1], Batch [111], Loss: 0.691208
Epoch [1/1], Batch [121], Loss: 0.691530
Epoch [1/1], Batch [131], Loss: 0.691337
Epoch [1/1], Batch [141], Loss: 0.691184
Epoch [1/1], Batch [151], Loss: 0.690959
Epoch [1/1], Batch [161], Loss: 0.691354
Epoch [1/1], Batch [171], Loss: 0.691132
Epoch [1/1], Batch [181], Loss: 0.691062
Epoch [1/1], Batch [191], Loss: 0.690800
Epoch [1/1], Batch [201], Loss: 0.691285
Epoch [1/1], Batch [211], Loss: 0.690544
Epoch [1/1], Batch [221], Loss: 0.690456
Epoch [1/1], Batch [231], Loss: 0.691374
Epoch [1/1], Batch [241], Loss: 0.691345
Epoch [1/1], Batch [251], Loss: 0.691207
Epoch [1/1], Batch [261], Loss: 0.691297
Epoch [1/1], Batch [271], Loss: 0.691293
Epoch [1/1], Batch [281], Loss: 0.690956
Seq_Len: 7, Epoch [1/1] - Average Train Loss: 0.6911
Seq_Len: 7, Epoch [1/1] - Average Validation Loss: 0.6938
Elapsed time: 25.77 minutes.
Seq_Len: 7, Epoch [1/1] - Average Test Loss: 0.6914
Elapsed time: 26.08 minutes.

Training with sequence length 8.
Epoch [1/1], Batch [1], Loss: 0.691350
Epoch [1/1], Batch [11], Loss: 0.690880
Epoch [1/1], Batch [21], Loss: 0.691504
Epoch [1/1], Batch [31], Loss: 0.691137
Epoch [1/1], Batch [41], Loss: 0.690556
Epoch [1/1], Batch [51], Loss: 0.691614
Epoch [1/1], Batch [61], Loss: 0.691108
Epoch [1/1], Batch [71], Loss: 0.691450
Epoch [1/1], Batch [81], Loss: 0.691131
Epoch [1/1], Batch [91], Loss: 0.690652
Epoch [1/1], Batch [101], Loss: 0.691058
Epoch [1/1], Batch [111], Loss: 0.691033
Epoch [1/1], Batch [121], Loss: 0.690324
Epoch [1/1], Batch [131], Loss: 0.691208
Epoch [1/1], Batch [141], Loss: 0.691263
Epoch [1/1], Batch [151], Loss: 0.691154
Epoch [1/1], Batch [161], Loss: 0.691605
Epoch [1/1], Batch [171], Loss: 0.691295
Epoch [1/1], Batch [181], Loss: 0.691203
Epoch [1/1], Batch [191], Loss: 0.691244
Epoch [1/1], Batch [201], Loss: 0.690811
Epoch [1/1], Batch [211], Loss: 0.691251
Epoch [1/1], Batch [221], Loss: 0.691381
Epoch [1/1], Batch [231], Loss: 0.691081
Epoch [1/1], Batch [241], Loss: 0.691256
Epoch [1/1], Batch [251], Loss: 0.690826
Epoch [1/1], Batch [261], Loss: 0.690600
Epoch [1/1], Batch [271], Loss: 0.691365
Epoch [1/1], Batch [281], Loss: 0.691052
Epoch [1/1], Batch [291], Loss: 0.690882
Seq_Len: 8, Epoch [1/1] - Average Train Loss: 0.6911
Seq_Len: 8, Epoch [1/1] - Average Validation Loss: 0.6947
Elapsed time: 33.28 minutes.
Seq_Len: 8, Epoch [1/1] - Average Test Loss: 0.6916
Elapsed time: 33.64 minutes.

Training with sequence length 9.
Epoch [1/1], Batch [1], Loss: 0.691025
Epoch [1/1], Batch [11], Loss: 0.690573
Epoch [1/1], Batch [21], Loss: 0.691246
Epoch [1/1], Batch [31], Loss: 0.690964
Epoch [1/1], Batch [41], Loss: 0.691423
Epoch [1/1], Batch [51], Loss: 0.691004
Epoch [1/1], Batch [61], Loss: 0.691316
Epoch [1/1], Batch [71], Loss: 0.690520
Epoch [1/1], Batch [81], Loss: 0.691198
Epoch [1/1], Batch [91], Loss: 0.690729
Epoch [1/1], Batch [101], Loss: 0.691491
Epoch [1/1], Batch [111], Loss: 0.691413
Epoch [1/1], Batch [121], Loss: 0.691401
Epoch [1/1], Batch [131], Loss: 0.691290
Epoch [1/1], Batch [141], Loss: 0.690544
Epoch [1/1], Batch [151], Loss: 0.691253
Epoch [1/1], Batch [161], Loss: 0.690863
Epoch [1/1], Batch [171], Loss: 0.691348
Epoch [1/1], Batch [181], Loss: 0.690902
Epoch [1/1], Batch [191], Loss: 0.690826
Epoch [1/1], Batch [201], Loss: 0.691301
Epoch [1/1], Batch [211], Loss: 0.690970
Epoch [1/1], Batch [221], Loss: 0.690978
Epoch [1/1], Batch [231], Loss: 0.690695
Epoch [1/1], Batch [241], Loss: 0.690523
Epoch [1/1], Batch [251], Loss: 0.691058
Epoch [1/1], Batch [261], Loss: 0.690888
Epoch [1/1], Batch [271], Loss: 0.690934
Epoch [1/1], Batch [281], Loss: 0.691213
Epoch [1/1], Batch [291], Loss: 0.690785
Seq_Len: 9, Epoch [1/1] - Average Train Loss: 0.6910
Seq_Len: 9, Epoch [1/1] - Average Validation Loss: 0.6936
Elapsed time: 41.69 minutes.
Seq_Len: 9, Epoch [1/1] - Average Test Loss: 0.6916
Elapsed time: 42.10 minutes.

Training complete!
Total elapsed time: 42.10 minutes.
Sequence Length 2: Median Loss = 0.691918
Sequence Length 3: Median Loss = 0.690905
Sequence Length 4: Median Loss = 0.690871
Sequence Length 5: Median Loss = 0.691094
Sequence Length 6: Median Loss = 0.691079
Sequence Length 7: Median Loss = 0.691191
Sequence Length 8: Median Loss = 0.691145
Sequence Length 9: Median Loss = 0.690991
CUDA is available!
